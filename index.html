<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-08T00:00:00Z">2024-08-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing <span class="highlight-title">Prompt</span> Learning through an External Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19674v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19674v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangming Cui, Xun Yang, Chao Wu, Liang Xiao, Xinmei Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning represents a promising method for adapting pre-trained
vision-language models (VLMs) to various downstream tasks by learning a set of
text embeddings. One challenge inherent to these methods is the poor
generalization performance due to the invalidity of the learned text embeddings
for unseen tasks. A straightforward approach to bridge this gap is to freeze
the text embeddings in prompts, which results in a lack of capacity to adapt
VLMs for downstream tasks. To address this dilemma, we propose a paradigm
called EnPrompt with a novel External Layer (EnLa). Specifically, we propose a
textual external layer and learnable visual embeddings for adapting VLMs to
downstream tasks. The learnable external layer is built upon valid embeddings
of pre-trained CLIP. This design considers the balance of learning capabilities
between the two branches. To align the textual and visual features, we propose
a novel two-pronged approach: i) we introduce the optimal transport as the
discrepancy metric to align the vision and text modalities, and ii) we
introduce a novel strengthening feature to enhance the interaction between
these two modalities. Four representative experiments (i.e., base-to-novel
generalization, few-shot learning, cross-dataset generalization, domain shifts
generalization) across 15 datasets demonstrate that our method outperforms the
existing prompt learning method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESP-MedSAM: Efficient Self-<span class="highlight-title">Prompt</span>ing SAM for Universal
  Domain-Generalized Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14153v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14153v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Xu, Jiaxuan Li, Xiangjian He, Ziyu Liu, Zhen Chen, Wenting Duan, Chenxin Li, Maggie M. He, Fiseha B. Tesema, Wooi P. Cheah, Yi Wang, Rong Qu, Jonathan M. Garibaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The universality of deep neural networks across different modalities and
their generalization capabilities to unseen domains play an essential role in
medical image segmentation. The recent Segment Anything Model (SAM) has
demonstrated its potential in both settings. However, the huge computational
costs, demand for manual annotations as prompts and conflict-prone decoding
process of SAM degrade its generalizability and applicability in clinical
scenarios. To address these issues, we propose an efficient self-prompting SAM
for universal domain-generalized medical image segmentation, named ESP-MedSAM.
Specifically, we first devise the Multi-Modal Decoupled Knowledge Distillation
(MMDKD) strategy to construct a lightweight semi-parameter sharing image
encoder that produces discriminative visual features for diverse modalities.
Further, we introduce the Self-Patch Prompt Generator (SPPG) to automatically
generate high-quality dense prompt embeddings for guiding segmentation
decoding. Finally, we design the Query-Decoupled Modality Decoder (QDMD) that
leverages a one-to-one strategy to provide an independent decoding channel for
every modality. Extensive experiments indicate that ESP-MedSAM outperforms
state-of-the-arts in diverse medical imaging segmentation tasks, displaying
superior modality universality and generalization capabilities. Especially,
ESP-MedSAM uses only 4.5\% parameters compared to SAM-H. The source code is
available at https://github.com/xq141839/ESP-MedSAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compression-Realized Deep Structural Network for Video Quality
  Enhancement <span class="chip">ACM MM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06342v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06342v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanchi Sun, Xiaohong Liu, Xinyang Jiang, Yifei Shen, Dongsheng Li, Xiongkuo Min, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the task of quality enhancement for compressed videos.
Although deep network-based video restorers achieve impressive progress, most
of the existing methods lack a structured design to optimally leverage the
priors within compression codecs. Since the quality degradation of the video is
primarily induced by the compression algorithm, a new paradigm is urgently
needed for a more ``conscious'' process of quality enhancement. As a result, we
propose the Compression-Realized Deep Structural Network (CRDS), introducing
three inductive biases aligned with the three primary processes in the classic
compression codec, merging the strengths of classical encoder architecture with
deep network capabilities. Inspired by the residual extraction and domain
transformation process in the codec, a pre-trained Latent Degradation Residual
Auto-Encoder is proposed to transform video frames into a latent feature space,
and the mutual neighborhood attention mechanism is integrated for precise
motion estimation and residual extraction. Furthermore, drawing inspiration
from the quantization noise distribution of the codec, CRDS proposes a novel
Progressive Denoising framework with intermediate supervision that decomposes
the quality enhancement into a series of simpler denoising sub-tasks.
Experimental results on datasets like LDV 2.0 and MFQE 2.0 indicate our
approach surpasses state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynopGround: A Large-Scale <span class="highlight-title">Dataset</span> for Multi-Paragraph Video Grounding
  from TV Dramas and Synopses <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01669v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01669v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaolei Tan, Zihang Lin, Junfu Pu, Zhongang Qi, Wei-Yi Pei, Zhi Qu, Yexin Wang, Ying Shan, Wei-Shi Zheng, Jian-Fang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video grounding is a fundamental problem in multimodal content understanding,
aiming to localize specific natural language queries in an untrimmed video.
However, current video grounding datasets merely focus on simple events and are
either limited to shorter videos or brief sentences, which hinders the model
from evolving toward stronger multimodal understanding capabilities. To address
these limitations, we present a large-scale video grounding dataset named
SynopGround, in which more than 2800 hours of videos are sourced from popular
TV dramas and are paired with accurately localized human-written synopses. Each
paragraph in the synopsis serves as a language query and is manually annotated
with precise temporal boundaries in the long video. These paragraph queries are
tightly correlated to each other and contain a wealth of abstract expressions
summarizing video storylines and specific descriptions portraying event
details, which enables the model to learn multimodal perception on more
intricate concepts over longer context dependencies. Based on the dataset, we
further introduce a more complex setting of video grounding dubbed
Multi-Paragraph Video Grounding (MPVG), which takes as input multiple
paragraphs and a long video for grounding each paragraph query to its temporal
interval. In addition, we propose a novel Local-Global Multimodal Reasoner
(LGMR) to explicitly model the local-global structures of long-term multimodal
inputs for MPVG. Our method provides an effective baseline solution to the
multi-paragraph video grounding problem. Extensive experiments verify the
proposed model's effectiveness as well as its superiority in long-term
multi-paragraph video grounding over prior state-of-the-arts. Dataset and code
are publicly available. Project page: https://synopground.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2024. Project page: https://synopground.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11652v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11652v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny Gupta, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a privacy-preserving approach to train models
on decentralized data. Its potential in healthcare is significant, but
challenges arise due to cross-client variations in medical image data,
exacerbated by limited annotations. This paper introduces Cross-Client
Variations Adaptive Federated Learning (CCVA-FL) to address these issues.
CCVA-FL aims to minimize cross-client variations by transforming images into a
common feature space. It involves expert annotation of a subset of images from
each client, followed by the selection of a client with the least data
complexity as the target. Synthetic medical images are then generated using
Scalable Diffusion Models with Transformers (DiT) based on the target client's
annotated images. These synthetic images, capturing diversity and representing
the original data, are shared with other clients. Each client then translates
its local images into the target image space using image-to-image translation.
The translated images are subsequently used in a federated learning setting to
develop a server model. Our results demonstrate that CCVA-FL outperforms
Vanilla Federated Averaging by effectively addressing data distribution
differences across clients without compromising privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I found critical errors in the manuscript affecting its validity. I
  need to correct these before resubmitting. Major changes to methodology and
  results are underway, significantly altering the content. I will resubmit the
  revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards
  General Medical AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, Benyou Wang, Shaoting Zhang, Bin Fu, Jianfei Cai, Bohan Zhuang, Eric J Seibel, Junjun He, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are capable of handling diverse data
types such as imaging, text, and physiological signals, and can be applied in
various fields. In the medical field, LVLMs have a high potential to offer
substantial assistance for diagnosis and treatment. Before that, it is crucial
to develop benchmarks to evaluate LVLMs' effectiveness in various medical
applications. Current benchmarks are often built upon specific academic
literature, mainly focusing on a single domain, and lacking varying perceptual
granularities. Thus, they face specific challenges, including limited clinical
relevance, incomplete evaluations, and insufficient guidance for interactive
LVLMs. To address these limitations, we developed the GMAI-MMBench, the most
comprehensive general medical AI benchmark with well-categorized data structure
and multi-perceptual granularity to date. It is constructed from 285 datasets
across 39 medical image modalities, 18 clinical-related tasks, 18 departments,
and 4 perceptual granularities in a Visual Question Answering (VQA) format.
Additionally, we implemented a lexical tree structure that allows users to
customize evaluation tasks, accommodating various assessment needs and
substantially supporting medical AI research and applications. We evaluated 50
LVLMs, and the results show that even the advanced GPT-4o only achieves an
accuracy of 52%, indicating significant room for improvement. Moreover, we
identified five key insufficiencies in current cutting-edge LVLMs that need to
be addressed to advance the development of better medical applications. We
believe that GMAI-MMBench will stimulate the community to build the next
generation of LVLMs toward GMAI.
  Project Page: https://uni-medical.github.io/GMAI-MMBench.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with
  Audio2Video Diffusion Model under Weak Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17485v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17485v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the challenge of enhancing the realism and
expressiveness in talking head video generation by focusing on the dynamic and
nuanced relationship between audio cues and facial movements. We identify the
limitations of traditional techniques that often fail to capture the full
spectrum of human expressions and the uniqueness of individual facial styles.
To address these issues, we propose EMO, a novel framework that utilizes a
direct audio-to-video synthesis approach, bypassing the need for intermediate
3D models or facial landmarks. Our method ensures seamless frame transitions
and consistent identity preservation throughout the video, resulting in highly
expressive and lifelike animations. Experimental results demonsrate that EMO is
able to produce not only convincing speaking videos but also singing videos in
various styles, significantly outperforming existing state-of-the-art
methodologies in terms of expressiveness and realism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nighttime Pedestrian Detection Based on Fore-Background Contrast
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Yao, Yongjun Zhang, Huachun Jian, Li Zhang, Ruzhong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significance of background information is frequently overlooked in
contemporary research concerning channel attention mechanisms. This study
addresses the issue of suboptimal single-spectral nighttime pedestrian
detection performance under low-light conditions by incorporating background
information into the channel attention mechanism. Despite numerous studies
focusing on the development of efficient channel attention mechanisms, the
relevance of background information has been largely disregarded. By adopting a
contrast learning approach, we reexamine channel attention with regard to
pedestrian objects and background information for nighttime pedestrian
detection, resulting in the proposed Fore-Background Contrast Attention (FBCA).
FBCA possesses two primary attributes: (1) channel descriptors form remote
dependencies with global spatial feature information; (2) the integration of
background information enhances the distinction between channels concentrating
on low-light pedestrian features and those focusing on background information.
Consequently, the acquired channel descriptors exhibit a higher semantic level
and spatial accuracy. Experimental outcomes demonstrate that FBCA significantly
outperforms existing methods in single-spectral nighttime pedestrian detection,
achieving state-of-the-art results on the NightOwls and TJU-DHD-pedestrian
datasets. Furthermore, this methodology also yields performance improvements
for the multispectral LLVIP dataset. These findings indicate that integrating
background information into the channel attention mechanism effectively
mitigates detector performance degradation caused by illumination factors in
nighttime scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RL-ADN: A High-Performance Deep Reinforcement Learning Environment for
  Optimal Energy Storage Systems Dispatch in Active Distribution Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengren Hou, Shuyi Gao, Weijie Xia, Edgar Mauricio Salazar Duque, Peter Palensky, Pedro P. Vergara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) presents a promising avenue for optimizing
Energy Storage Systems (ESSs) dispatch in distribution networks. This paper
introduces RL-ADN, an innovative open-source library specifically designed for
solving the optimal ESSs dispatch in active distribution networks. RL-ADN
offers unparalleled flexibility in modeling distribution networks, and ESSs,
accommodating a wide range of research goals. A standout feature of RL-ADN is
its data augmentation module, based on Gaussian Mixture Model and Copula (GMC)
functions, which elevates the performance ceiling of DRL agents. Additionally,
RL-ADN incorporates the Laurent power flow solver, significantly reducing the
computational burden of power flow calculations during training without
sacrificing accuracy. The effectiveness of RL-ADN is demonstrated using in
different sizes of distribution networks, showing marked performance
improvements in the adaptability of DRL algorithms for ESS dispatch tasks. This
enhancement is particularly beneficial from the increased diversity of training
scenarios. Furthermore, RL-ADN achieves a tenfold increase in computational
efficiency during training, making it highly suitable for large-scale network
applications. The library sets a new benchmark in DRL-based ESSs dispatch in
distribution networks and it is poised to advance DRL applications in
distribution network operations significantly. RL-ADN is available at:
https://github.com/ShengrenHou/RL-ADN and
https://github.com/distributionnetworksTUDelft/RL-ADN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11652v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11652v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny Gupta, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a privacy-preserving approach to train models
on decentralized data. Its potential in healthcare is significant, but
challenges arise due to cross-client variations in medical image data,
exacerbated by limited annotations. This paper introduces Cross-Client
Variations Adaptive Federated Learning (CCVA-FL) to address these issues.
CCVA-FL aims to minimize cross-client variations by transforming images into a
common feature space. It involves expert annotation of a subset of images from
each client, followed by the selection of a client with the least data
complexity as the target. Synthetic medical images are then generated using
Scalable Diffusion Models with Transformers (DiT) based on the target client's
annotated images. These synthetic images, capturing diversity and representing
the original data, are shared with other clients. Each client then translates
its local images into the target image space using image-to-image translation.
The translated images are subsequently used in a federated learning setting to
develop a server model. Our results demonstrate that CCVA-FL outperforms
Vanilla Federated Averaging by effectively addressing data distribution
differences across clients without compromising privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I found critical errors in the manuscript affecting its validity. I
  need to correct these before resubmitting. Major changes to methodology and
  results are underway, significantly altering the content. I will resubmit the
  revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous, Self-driving Multi-Step Growth of Semiconductor
  Heterostructures Guided by Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Shen, Wenkang Zhan, Hongyu Sun, Kaiyao Xin, Bo Xu, Zhanguo Wang, Chao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The semiconductor industry has prioritized automating repetitive tasks by
closed-loop, autonomous experimentation which enables accelerated optimization
of complex multi-step processes. The emergence of machine learning (ML) has
ushered in automated process with minimal human intervention. In this work, we
develop SemiEpi, a self-driving automation platform capable of executing
molecular beam epitaxy (MBE) growth with multi-steps, continuous in-situ
monitoring, and on-the-fly feedback control. By integrating standard hardware,
homemade software, curve fitting, and multiple ML models, SemiEpi operates
autonomously, eliminating the need for extensive expertise in MBE processes to
achieve optimal outcomes. The platform actively learns from previous
experimental results, identifying favorable conditions and proposing new
experiments to achieve the desired results. We standardize and optimize growth
for InAs/GaAs quantum dots (QDs) heterostructures to showcase the power of
ML-guided multi-step growth. A temperature calibration was implemented to get
the initial growth condition, and fine control of the process was executed
using ML. Leveraging RHEED movies acquired during the growth, SemiEpi
successfully identified and optimized a novel route for multi-step
heterostructure growth. This work demonstrates the capabilities of closed-loop,
ML-guided systems in addressing challenges in multi-step growth for any device.
Our method is critical to achieve repeatable materials growth using
commercially scalable tools. Our strategy facilitates the development of a
hardware-independent process and enhancing process repeatability and stability,
even without exhaustive knowledge of growth parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-07T00:00:00Z">2024-08-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">108</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Well Can Vision Language Models See Image Details? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhui Gou, Abdulwahab Felemban, Faizan Farooq Khan, Deyao Zhu, Jianfei Cai, Hamid Rezatofighi, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model-based Vision-Language Models (LLM-based VLMs) have
demonstrated impressive results in various vision-language understanding tasks.
However, how well these VLMs can see image detail beyond the semantic level
remains unclear. In our study, we introduce a pixel value prediction task (PVP)
to explore "How Well Can Vision Language Models See Image Details?" and to
assist VLMs in perceiving more details. Typically, these models comprise a
frozen CLIP visual encoder, a large language model, and a connecting module.
After fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to
predict precise pixel values by only fine-tuning the connection module and LLM;
and 2) prediction precision is significantly improved when the vision encoder
is also adapted. Additionally, our research reveals that incorporating pixel
value prediction as one of the VLM pre-training tasks and vision encoder
adaptation markedly boosts VLM performance on downstream image-language
understanding tasks requiring detailed image perception, such as referring
image segmentation (with an average +10.19 cIoU improvement) and video game
decision making (with average score improvements of +80.34 and +70.54 on two
games, respectively).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Sprite Decomposition from Animated Graphics <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoyuki Suzuki, Kotaro Kikuchi, Kota Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an approach to decomposing animated graphics into
sprites, a set of basic elements or layers. Our approach builds on the
optimization of sprite parameters to fit the raster video. For efficiency, we
assume static textures for sprites to reduce the search space while preventing
artifacts using a texture prior model. To further speed up the optimization, we
introduce the initialization of the sprite parameters utilizing a pre-trained
video object segmentation model and user input of single frame annotations. For
our study, we construct the Crello Animation dataset from an online design
service and define quantitative metrics to measure the quality of the extracted
sprites. Experiments show that our method significantly outperforms baselines
for similar decomposition tasks in terms of the quality/efficiency tradeoff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published ECCV 2024, project page:
  https://cyberagentailab.github.io/sprite-decompose/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FMiFood: Multi-modal Contrastive Learning for Food Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Pan, Jiangpeng He, Fengqing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food image classification is the fundamental step in image-based dietary
assessment, which aims to estimate participants' nutrient intake from eating
occasion images. A common challenge of food images is the intra-class diversity
and inter-class similarity, which can significantly hinder classification
performance. To address this issue, we introduce a novel multi-modal
contrastive learning framework called FMiFood, which learns more discriminative
features by integrating additional contextual information, such as food
category text descriptions, to enhance classification accuracy. Specifically,
we propose a flexible matching technique that improves the similarity matching
between text and image embeddings to focus on multiple key information.
Furthermore, we incorporate the classification objectives into the framework
and explore the use of GPT-4 to enrich the text descriptions and provide more
detailed context. Our method demonstrates improved performance on both the
UPMC-101 and VFN datasets compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdapMTL: Adaptive Pruning Framework for Multitask Learning Model <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingcan Xiang, Steven Jiaxun Tang, Qizheng Yang, Hui Guan, Tongping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of multimedia and multimodal processing, the efficient handling
of diverse data streams such as images, video, and sensor data is paramount.
Model compression and multitask learning (MTL) are crucial in this field,
offering the potential to address the resource-intensive demands of processing
and interpreting multiple forms of media simultaneously. However, effectively
compressing a multitask model presents significant challenges due to the
complexities of balancing sparsity allocation and accuracy performance across
multiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive
pruning framework for MTL models. AdapMTL leverages multiple learnable soft
thresholds independently assigned to the shared backbone and the task-specific
heads to capture the nuances in different components' sensitivity to pruning.
During training, it co-optimizes the soft thresholds and MTL model weights to
automatically determine the suitable sparsity level at each component to
achieve both high task accuracy and high overall sparsity. It further
incorporates an adaptive weighting mechanism that dynamically adjusts the
importance of task-specific losses based on each task's robustness to pruning.
We demonstrate the effectiveness of AdapMTL through comprehensive experiments
on popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different
architectures, showcasing superior performance compared to state-of-the-art
pruning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, Published at ACM Multimedia (ACM MM) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Video Denoising Using a Classic Bayesian Backbone <span class="chip">ICME 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Bled, François Pitié
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, state-of-the-art image and video denoising networks have
become increasingly large, requiring millions of trainable parameters to
achieve best-in-class performance. Improved denoising quality has come at the
cost of denoising speed, where modern transformer networks are far slower to
run than smaller denoising networks such as FastDVDnet and classic Bayesian
denoisers such as the Wiener filter.
  In this paper, we implement a hybrid Wiener filter which leverages small
ancillary networks to increase the original denoiser performance, while
retaining fast denoising speeds. These networks are used to refine the Wiener
coring estimate, optimise windowing functions and estimate the unknown noise
profile. Using these methods, we outperform several popular denoisers and
remain within 0.2 dB, on average, of the popular VRT transformer. Our method
was found to be over x10 faster than the transformer method, with a far lower
parameter cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted to ICME 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Modeling Decouple Distillation for Unsupervised Anomaly Detection <span class="chip">ACM MM '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Liu, Jianyuan Wang, Biao Leng, Shuo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation based on student-teacher network is one of the
mainstream solution paradigms for the challenging unsupervised Anomaly
Detection task, utilizing the difference in representation capabilities of the
teacher and student networks to implement anomaly localization. However,
over-generalization of the student network to the teacher network may lead to
negligible differences in representation capabilities of anomaly, thus
affecting the detection effectiveness. Existing methods address the possible
over-generalization by using differentiated students and teachers from the
structural perspective or explicitly expanding distilled information from the
content perspective, which inevitably result in an increased likelihood of
underfitting of the student network and poor anomaly detection capabilities in
anomaly center or edge. In this paper, we propose Dual-Modeling Decouple
Distillation (DMDD) for the unsupervised anomaly detection. In DMDD, a Decouple
Student-Teacher Network is proposed to decouple the initial student features
into normality and abnormality features. We further introduce Dual-Modeling
Distillation based on normal-anomaly image pairs, fitting normality features of
anomalous image and the teacher features of the corresponding normal image,
widening the distance between abnormality features and the teacher features in
anomalous regions. Synthesizing these two distillation ideas, we achieve
anomaly detection which focuses on both edge and center of anomaly. Finally, a
Multi-perception Segmentation Network is proposed to achieve focused anomaly
map fusion based on multiple attention. Experimental results on MVTec AD show
that DMDD surpasses SOTA localization performance of previous knowledge
distillation-based methods, reaching 98.85% on pixel-level AUC and 96.13% on
PRO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, Accepted to ACM MM '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global-Local Progressive Integration Network for Blind Image Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqi Wang, Yun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) excel in computer vision for modeling long-term
dependencies, yet face two key challenges for image quality assessment (IQA):
discarding fine details during patch embedding, and requiring extensive
training data due to lack of inductive biases. In this study, we propose a
Global-Local progressive INTegration network for IQA, called GlintIQA, to
address these issues through three key components: 1) Hybrid feature extraction
combines ViT-based global feature extractor (VGFE) and convolutional neural
networks (CNNs)-based local feature extractor (CLFE) to capture global
coarse-grained features and local fine-grained features, respectively. The
incorporation of CNNs mitigates the patch-level information loss and inductive
bias constraints inherent to ViT architectures. 2) Progressive feature
integration leverages diverse kernel sizes in embedding to spatially align
coarse- and fine-grained features, and progressively aggregate these features
by interactively stacking channel-wise attention and spatial enhancement
modules to build effective quality-aware representations. 3) Content
similarity-based labeling approach is proposed that automatically assigns
quality labels to images with diverse content based on subjective quality
scores. This addresses the scarcity of labeled training data in synthetic
datasets and bolsters model generalization. The experimental results
demonstrate the efficacy of our approach, yielding 5.04% average SROCC gains on
cross-authentic dataset evaluations. Moreover, our model and its counterpart
pre-trained on the proposed dataset respectively exhibited 5.40% and 13.23%
improvements on across-synthetic datasets evaluation. The codes and proposed
dataset will be released at https://github.com/XiaoqiWang/GlintIQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surgformer: Surgical <span class="highlight-title">Transformer</span> with Hierarchical Temporal Attention
  for Surgical Phase Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Yang, Luyang Luo, Qiong Wang, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing state-of-the-art methods for surgical phase recognition either rely
on the extraction of spatial-temporal features at a short-range temporal
resolution or adopt the sequential extraction of the spatial and temporal
features across the entire temporal resolution. However, these methods have
limitations in modeling spatial-temporal dependency and addressing
spatial-temporal redundancy: 1) These methods fail to effectively model
spatial-temporal dependency, due to the lack of long-range information or joint
spatial-temporal modeling. 2) These methods utilize dense spatial features
across the entire temporal resolution, resulting in significant
spatial-temporal redundancy. In this paper, we propose the Surgical Transformer
(Surgformer) to address the issues of spatial-temporal modeling and redundancy
in an end-to-end manner, which employs divided spatial-temporal attention and
takes a limited set of sparse frames as input. Moreover, we propose a novel
Hierarchical Temporal Attention (HTA) to capture both global and local
information within varied temporal resolutions from a target frame-centric
perspective. Distinct from conventional temporal attention that primarily
emphasizes dense long-range similarity, HTA not only captures long-term
information but also considers local latent consistency among informative
frames. HTA then employs pyramid feature aggregation to effectively utilize
temporal information across diverse temporal resolutions, thereby enhancing the
overall temporal representation. Extensive experiments on two challenging
benchmark datasets verify that our proposed Surgformer performs favorably
against the state-of-the-art methods. The code is released at
https://github.com/isyangshu/Surgformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bi-Level Spatial and Channel-aware <span class="highlight-title">Transformer</span> for Learned Image
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Soltani, Erfan Ghasemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in learned image compression (LIC) methods have
demonstrated superior performance over traditional hand-crafted codecs. These
learning-based methods often employ convolutional neural networks (CNNs) or
Transformer-based architectures. However, these nonlinear approaches frequently
overlook the frequency characteristics of images, which limits their
compression efficiency. To address this issue, we propose a novel
Transformer-based image compression method that enhances the transformation
stage by considering frequency components within the feature map. Our method
integrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),
where a spatial-based branch independently handles high and low frequencies at
the attention layer, and a Channel-aware Self-Attention (CaSA) module captures
information across channels, significantly improving compression performance.
Additionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)
within the Transformer block to enhance the extraction of diverse and rich
information, which is crucial for effective compression. These innovations
collectively improve the transformation's ability to project data into a more
decorrelated latent space, thereby boosting overall compression efficiency.
Experimental results demonstrate that our framework surpasses state-of-the-art
LIC methods in rate-distortion performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using a Distance Sensor to Detect Deviations in a Planar Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carter Sifferman, William Sun, Mohit Gupta, Michael Gleicher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate methods for determining if a planar surface contains geometric
deviations (e.g., protrusions, objects, divots, or cliffs) using only an
instantaneous measurement from a miniature optical time-of-flight sensor. The
key to our method is to utilize the entirety of information encoded in raw
time-of-flight data captured by off-the-shelf distance sensors. We provide an
analysis of the problem in which we identify the key ambiguity between geometry
and surface photometrics. To overcome this challenging ambiguity, we fit a
Gaussian mixture model to a small dataset of planar surface measurements. This
model implicitly captures the expected geometry and distribution of
photometrics of the planar surface and is used to identify measurements that
are likely to contain deviations. We characterize our method on a variety of
surfaces and planar deviations across a range of scenarios. We find that our
method utilizing raw time-of-flight data outperforms baselines which use only
derived distance estimates. We build an example application in which our method
enables mobile robot obstacle and cliff avoidance over a wide field-of-view.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target <span class="highlight-title">Prompt</span>ing for Information Extraction with Vision Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dipankar Medhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent trend in the Large Vision and Language model has brought a new
change in how information extraction systems are built. VLMs have set a new
benchmark with their State-of-the-art techniques in understanding documents and
building question-answering systems across various industries. They are
significantly better at generating text from document images and providing
accurate answers to questions. However, there are still some challenges in
effectively utilizing these models to build a precise conversational system.
General prompting techniques used with large language models are often not
suitable for these specially designed vision language models. The output
generated by such generic input prompts is ordinary and may contain information
gaps when compared with the actual content of the document. To obtain more
accurate and specific answers, a well-targeted prompt is required by the vision
language model, along with the document image. In this paper, a technique is
discussed called Target prompting, which focuses on explicitly targeting parts
of document images and generating related answers from those specific regions
only. The paper also covers the evaluation of response for each prompting
technique using different user queries and input prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real-Time Gaussian Splatting: Accelerating 3DGS through
  Photometric SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Song Hu, Dayou Mao, Yuhao Chen, John Zelek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous
Localization and Mapping (VSLAM) demonstrate the generation of high-quality
volumetric reconstructions from monocular video streams. However, despite these
promising advancements, current 3DGS integrations have reduced tracking
performance and lower operating speeds compared to traditional VSLAM. To
address these issues, we propose integrating 3DGS with Direct Sparse Odometry,
a monocular photometric SLAM system. We have done preliminary experiments
showing that using Direct Sparse Odometry point cloud outputs, as opposed to
standard structure-from-motion methods, significantly shortens the training
time needed to achieve high-quality renders. Reducing 3DGS training time
enables the development of 3DGS-integrated SLAM systems that operate in
real-time on mobile hardware. These promising initial findings suggest further
exploration is warranted in combining traditional VSLAM systems with 3DGS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This extended abstract has been submitted to be presented at an IEEE
  conference. It will be made available online by IEEE but will not be
  published in IEEE Xplore. Copyright may be transferred without notice, after
  which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, Eunbyung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting (3DGS) has recently emerged as an alternative
representation that leverages a 3D Gaussian-based representation and introduces
an approximated volumetric rendering, achieving very fast rendering speed and
promising image quality. Furthermore, subsequent studies have successfully
extended 3DGS to dynamic 3D scenes, demonstrating its wide range of
applications. However, a significant drawback arises as 3DGS and its following
methods entail a substantial number of Gaussians to maintain the high fidelity
of the rendered images, which requires a large amount of memory and storage. To
address this critical issue, we place a specific emphasis on two key
objectives: reducing the number of Gaussian points without sacrificing
performance and compressing the Gaussian attributes, such as view-dependent
color and covariance. To this end, we propose a learnable mask strategy that
significantly reduces the number of Gaussians while preserving high
performance. In addition, we propose a compact but effective representation of
view-dependent color by employing a grid-based neural field rather than relying
on spherical harmonics. Finally, we learn codebooks to compactly represent the
geometric and temporal attributes by residual vector quantization. With model
compression techniques such as quantization and entropy coding, we consistently
show over 25x reduced storage and enhanced rendering speed compared to 3DGS for
static scenes, while maintaining the quality of the scene representation. For
dynamic scenes, our approach achieves more than 12x storage efficiency and
retains a high-quality reconstruction compared to the existing state-of-the-art
methods. Our work provides a comprehensive framework for 3D scene
representation, achieving high performance, fast training, compactness, and
real-time rendering. Our project page is available at
https://maincold2.github.io/c3dgs/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://maincold2.github.io/c3dgs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Guidance for LiDAR-based Unsupervised 3D Object
  Detection <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Fruhwirth-Reisinger, Wei Lin, Dušan Malić, Horst Bischof, Horst Possegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D object detection in LiDAR point clouds is crucial for autonomous
driving systems. To achieve state-of-the-art performance, the supervised
training of detectors requires large amounts of human-annotated data, which is
expensive to obtain and restricted to predefined object categories. To mitigate
manual labeling efforts, recent unsupervised object detection approaches
generate class-agnostic pseudo-labels for moving objects, subsequently serving
as supervision signal to bootstrap a detector. Despite promising results, these
approaches do not provide class labels or generalize well to static objects.
Furthermore, they are mostly restricted to data containing multiple drives from
the same scene or images from a precisely calibrated and synchronized camera
setup. To overcome these limitations, we propose a vision-language-guided
unsupervised 3D detection approach that operates exclusively on LiDAR point
clouds. We transfer CLIP knowledge to classify point clusters of static and
moving objects, which we discover by exploiting the inherent spatio-temporal
information of LiDAR point clouds for clustering, tracking, as well as box and
label refinement. Our approach outperforms state-of-the-art unsupervised 3D
object detectors on the Waymo Open Dataset ($+23~\text{AP}_{3D}$) and Argoverse
2 ($+7.9~\text{AP}_{3D}$) and provides class labels not solely based on object
size assumptions, marking a significant advancement in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactuals and Uncertainty-Based Explainable Paradigm for the
  Automated Detection and Segmentation of Renal Cysts in Computed Tomography
  Images: A Multi-Center Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zohaib Salahuddin, Abdalla Ibrahim, Sheng Kuang, Yousif Widaatalla, Razvan L. Miclea, Oliver Morin, Spencer Behr, Marnix P. M. Kop, Tom Marcelissen, Patricia Zondervan, Auke Jager, Philippe Lambin, Henry C Woodruff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Routine computed tomography (CT) scans often detect a wide range of renal
cysts, some of which may be malignant. Early and precise localization of these
cysts can significantly aid quantitative image analysis. Current segmentation
methods, however, do not offer sufficient interpretability at the feature and
pixel levels, emphasizing the necessity for an explainable framework that can
detect and rectify model inaccuracies. We developed an interpretable
segmentation framework and validated it on a multi-centric dataset. A
Variational Autoencoder Generative Adversarial Network (VAE-GAN) was employed
to learn the latent representation of 3D input patches and reconstruct input
images. Modifications in the latent representation using the gradient of the
segmentation model generated counterfactual explanations for varying dice
similarity coefficients (DSC). Radiomics features extracted from these
counterfactual images, using a ground truth cyst mask, were analyzed to
determine their correlation with segmentation performance. The DSCs for the
original and VAE-GAN reconstructed images for counterfactual image generation
showed no significant differences. Counterfactual explanations highlighted how
variations in cyst image features influence segmentation outcomes and showed
model discrepancies. Radiomics features correlating positively and negatively
with dice scores were identified. The uncertainty of the predicted segmentation
masks was estimated using posterior sampling of the weight space. The
combination of counterfactual explanations and uncertainty maps provided a
deeper understanding of the image features within the segmented renal cysts
that lead to high uncertainty. The proposed segmentation framework not only
achieved high segmentation accuracy but also increased interpretability
regarding how image features impact segmentation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methodological Explainability Evaluation of an Interpretable Deep
  Learning Model for Post-Hepatectomy Liver Failure Prediction Incorporating
  Counterfactual Explanations and Layerwise Relevance Propagation: A
  Prospective In Silico Trial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Zhong, Zohaib Salahuddin, Yi Chen, Henry C Woodruff, Haiyi Long, Jianyun Peng, Nuwan Udawatte, Roberto Casale, Ayoub Mokhtari, Xiaoer Zhang, Jiayao Huang, Qingyu Wu, Li Tan, Lili Chen, Dongming Li, Xiaoyan Xie, Manxia Lin, Philippe Lambin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI)-based decision support systems have demonstrated
value in predicting post-hepatectomy liver failure (PHLF) in hepatocellular
carcinoma (HCC). However, they often lack transparency, and the impact of model
explanations on clinicians' decisions has not been thoroughly evaluated.
Building on prior research, we developed a variational autoencoder-multilayer
perceptron (VAE-MLP) model for preoperative PHLF prediction. This model
integrated counterfactuals and layerwise relevance propagation (LRP) to provide
insights into its decision-making mechanism. Additionally, we proposed a
methodological framework for evaluating the explainability of AI systems. This
framework includes qualitative and quantitative assessments of explanations
against recognized biomarkers, usability evaluations, and an in silico clinical
trial. Our evaluations demonstrated that the model's explanation correlated
with established biomarkers and exhibited high usability at both the case and
system levels. Furthermore, results from the three-track in silico clinical
trial showed that clinicians' prediction accuracy and confidence increased when
AI explanations were provided.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqing Guo, Qianhui Men, J. Alison Noble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first automated multimodal summary generation system,
MMSummary, for medical imaging video, particularly with a focus on fetal
ultrasound analysis. Imitating the examination process performed by a human
sonographer, MMSummary is designed as a three-stage pipeline, progressing from
keyframe detection to keyframe captioning and finally anatomy segmentation and
measurement. In the keyframe detection stage, an innovative automated workflow
is proposed to progressively select a concise set of keyframes, preserving
sufficient video information without redundancy. Subsequently, we adapt a large
language model to generate meaningful captions for fetal ultrasound keyframes
in the keyframe captioning stage. If a keyframe is captioned as fetal biometry,
the segmentation and measurement stage estimates biometric parameters by
segmenting the region of interest according to the textual prior. The MMSummary
system provides comprehensive summaries for fetal ultrasound examinations and
based on reported experiments is estimated to reduce scanning time by
approximately 31.5%, thereby suggesting the potential to enhance clinical
workflow efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Jun Tang, Tat-Jen Cham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of 3D Gaussians as representation of radiance fields has enabled high
quality novel view synthesis at real-time rendering speed. However, the choice
of optimising the outgoing radiance of each Gaussian independently as spherical
harmonics results in unsatisfactory view dependent effects. In response to
these limitations, our work, Factorised Tensorial Illumination for 3D Gaussian
Splatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering
quality. Instead of optimising a single outgoing radiance parameter, 3iGS
enhances 3DGS view-dependent effects by expressing the outgoing radiance as a
function of a local illumination field and Bidirectional Reflectance
Distribution Function (BRDF) features. We optimise a continuous incident
illumination field through a Tensorial Factorisation representation, while
separately fine-tuning the BRDF features of each 3D Gaussian relative to this
illumination field. Our methodology significantly enhances the rendering
quality of specular view-dependent effects of 3DGS, while maintaining rapid
training and rendering speeds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 18th European Conference on Computer Vision ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Generation Scheme for Thermal Modality with Edge-Guided Adversarial
  Conditional Diffusion Model <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Zhu, Honghu Pan, Qiang Wang, Chao Tian, Chao Yang, Zhenyu He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In challenging low light and adverse weather conditions,thermal vision
algorithms,especially object detection,have exhibited remarkable
potential,contrasting with the frequent struggles encountered by visible vision
algorithms. Nevertheless,the efficacy of thermal vision algorithms driven by
deep learning models remains constrained by the paucity of available training
data samples. To this end,this paper introduces a novel approach termed the
edge guided conditional diffusion model. This framework aims to produce
meticulously aligned pseudo thermal images at the pixel level,leveraging edge
information extracted from visible images. By utilizing edges as contextual
cues from the visible domain,the diffusion model achieves meticulous control
over the delineation of objects within the generated images. To alleviate the
impacts of those visible-specific edge information that should not appear in
the thermal domain,a two-stage modality adversarial training strategy is
proposed to filter them out from the generated images by differentiating the
visible and thermal modality. Extensive experiments on LLVIP demonstrate ECDM s
superiority over existing state-of-the-art approaches in terms of image
generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ACM MM 2024/ACM MM24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intuitionistic Fuzzy Cognitive Maps for Interpretable Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgia Sovatzidi, Michael D. Vasilakakis, Dimitris K. Iakovidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interpretability of machine learning models is critical, as users may be
reluctant to rely on their inferences. Intuitionistic FCMs (iFCMs) have been
proposed as an extension of FCMs offering a natural mechanism to assess the
quality of their output through the estimation of hesitancy, a concept
resembling to human hesitation in decision making. To address the challenge of
interpretable image classification, this paper introduces a novel framework,
named Interpretable Intuitionistic FCM (I2FCM) which is domain-independent,
simple to implement, and can be applied on Convolutional Neural Network (CNN)
models, rendering them interpretable. To the best of our knowledge this is the
first time iFCMs are applied for image classification. Further novel
contributions include: a feature extraction process focusing on the most
informative image regions; a learning algorithm for data-driven determination
of the intuitionistic fuzzy interconnections of the iFCM; an inherently
interpretable classification approach based on image contents. In the context
of image classification, hesitancy is considered as a degree of inconfidence
with which an image is categorized to a class. The constructed iFCM model
distinguishes the most representative image semantics and analyses them
utilizing cause-and-effect relations. The effectiveness of the introduced
framework is evaluated on publicly available datasets, and the experimental
results confirm that it can provide enhanced classification performance, while
providing interpretable inferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted for possible journal publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Multimodal Large Language Models with Quantization-Aware Scale
  Learning for Efficient Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Xie, Yuxin Zhang, Mingbao Lin, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first study to explore the potential of parameter
quantization for multimodal large language models to alleviate the significant
resource constraint encountered during vision-language instruction tuning. We
introduce a Quantization-aware Scale LeArning method based on multimodal
Warmup, termed QSLAW. This method is grounded in two key innovations: (1) The
learning of group-wise scale factors for quantized LLM weights to mitigate the
quantization error arising from activation outliers and achieve more effective
vision-language instruction tuning; (2) The implementation of a multimodal
warmup that progressively integrates linguistic and multimodal training
samples, thereby preventing overfitting of the quantized model to multimodal
data while ensuring stable adaptation of multimodal large language models to
downstream vision-language tasks. Extensive experiments demonstrate that models
quantized by QSLAW perform on par with, or even surpass, their full-precision
counterparts, while facilitating up to 1.4 times reduction in VL tuning time
and GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft-Hard Attention U-Net Model and Benchmark <span class="highlight-title">Dataset</span> for Multiscale
  Image Shadow Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eirini Cholopoulou, Dimitrios E. Diamantis, Dimitra-Christina C. Koutsiou, Dimitris K. Iakovidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective shadow removal is pivotal in enhancing the visual quality of images
in various applications, ranging from computer vision to digital photography.
During the last decades physics and machine learning -based methodologies have
been proposed; however, most of them have limited capacity in capturing complex
shadow patterns due to restrictive model assumptions, neglecting the fact that
shadows usually appear at different scales. Also, current datasets used for
benchmarking shadow removal are composed of a limited number of images with
simple scenes containing mainly uniform shadows cast by single objects, whereas
only a few of them include both manual shadow annotations and paired
shadow-free images. Aiming to address all these limitations in the context of
natural scene imaging, including urban environments with complex scenes, the
contribution of this study is twofold: a) it proposes a novel deep learning
architecture, named Soft-Hard Attention U-net (SHAU), focusing on multiscale
shadow removal; b) it provides a novel synthetic dataset, named Multiscale
Shadow Removal Dataset (MSRD), containing complex shadow patterns of multiple
scales, aiming to serve as a privacy-preserving dataset for a more
comprehensive benchmarking of future shadow removal methodologies. Key
architectural components of SHAU are the soft and hard attention modules, which
along with multiscale feature extraction blocks enable effective shadow removal
of different scales and intensities. The results demonstrate the effectiveness
of SHAU over the relevant state-of-the-art shadow removal methods across
various benchmark datasets, improving the Peak Signal-to-Noise Ratio and Root
Mean Square Error for the shadow area by 25.1% and 61.3%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pick of the Bunch: Detecting Infrared Small Targets Beyond Hit-Miss
  Trade-Offs via Selective Rank-Aware Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimian Dai, Peiwen Pan, Yulei Qian, Yuxuan Li, Xiang Li, Jian Yang, Huan Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target detection faces the inherent challenge of precisely
localizing dim targets amidst complex background clutter. Traditional
approaches struggle to balance detection precision and false alarm rates. To
break this dilemma, we propose SeRankDet, a deep network that achieves high
accuracy beyond the conventional hit-miss trade-off, by following the ``Pick of
the Bunch'' principle. At its core lies our Selective Rank-Aware Attention
(SeRank) module, employing a non-linear Top-K selection process that preserves
the most salient responses, preventing target signal dilution while maintaining
constant complexity. Furthermore, we replace the static concatenation typical
in U-Net structures with our Large Selective Feature Fusion (LSFF) module, a
dynamic fusion strategy that empowers SeRankDet with adaptive feature
integration, enhancing its ability to discriminate true targets from false
alarms. The network's discernment is further refined by our Dilated Difference
Convolution (DDC) module, which merges differential convolution aimed at
amplifying subtle target characteristics with dilated convolution to expand the
receptive field, thereby substantially improving target-background separation.
Despite its lightweight architecture, the proposed SeRankDet sets new
benchmarks in state-of-the-art performance across multiple public datasets. The
code is available at https://github.com/GrokCV/SeRankDet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAS-ViT: Convolutional Additive Self-attention Vision <span class="highlight-title">Transformer</span>s for
  Efficient Mobile Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianfang Zhang, Lei Li, Yang Zhou, Wentao Liu, Chen Qian, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) mark a revolutionary advance in neural networks
with their token mixer's powerful global context capability. However, the
pairwise token affinity and complex matrix operations limit its deployment on
resource-constrained scenarios and real-time applications, such as mobile
devices, although considerable efforts have been made in previous works. In
this paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision
Transformers, to achieve a balance between efficiency and performance in mobile
applications. Firstly, we argue that the capability of token mixers to obtain
global contextual information hinges on multiple information interactions, such
as spatial and channel domains. Subsequently, we construct a novel additive
similarity function following this paradigm and present an efficient
implementation named Convolutional Additive Token Mixer (CATM). This
simplification leads to a significant reduction in computational overhead. We
evaluate CAS-ViT across a variety of vision tasks, including image
classification, object detection, instance segmentation, and semantic
segmentation. Our experiments, conducted on GPUs, ONNX, and iPhones,
demonstrate that CAS-ViT achieves a competitive performance when compared to
other state-of-the-art backbones, establishing it as a viable option for
efficient mobile vision applications. Our code and model are available at:
\url{https://github.com/Tianfang-Zhang/CAS-ViT}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Openstory++: A Large-scale <span class="highlight-title">Dataset</span> and Benchmark for Instance-aware
  Open-domain Visual Storytelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilyu Ye, Jinxiu Liu, Ruotian Peng, Jinjin Cao, Zhiyang Chen, Yiyang Zhang, Ziwei Xuan, Mingyuan Zhou, Xiaoqian Shen, Mohamed Elhoseiny, Qi Liu, Guo-Jun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent image generation models excel at creating high-quality images from
brief captions. However, they fail to maintain consistency of multiple
instances across images when encountering lengthy contexts. This inconsistency
is largely due to in existing training datasets the absence of granular
instance feature labeling in existing training datasets. To tackle these
issues, we introduce Openstory++, a large-scale dataset combining additional
instance-level annotations with both images and text. Furthermore, we develop a
training methodology that emphasizes entity-centric image-text generation,
ensuring that the models learn to effectively interweave visual and textual
information. Specifically, Openstory++ streamlines the process of keyframe
extraction from open-domain videos, employing vision-language models to
generate captions that are then polished by a large language model for
narrative continuity. It surpasses previous datasets by offering a more
expansive open-domain resource, which incorporates automated captioning,
high-resolution imagery tailored for instance count, and extensive frame
sequences for temporal consistency. Additionally, we present Cohere-Bench, a
pioneering benchmark framework for evaluating the image generation tasks when
long multimodal context is provided, including the ability to keep the
background, style, instances in the given context coherent. Compared to
existing benchmarks, our work fills critical gaps in multi-modal generation,
propelling the development of models that can adeptly generate and interpret
complex narratives in open-domain environments. Experiments conducted within
Cohere-Bench confirm the superiority of Openstory++ in nurturing high-quality
visual storytelling models, enhancing their ability to address open-domain
generation tasks. More details can be found at https://openstorypp.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Huang, Ziyu Xu, Hai Wu, Jinlong Wang, Qiming Xia, Yan Xia, Jonathan Li, Kyle Gao, Chenglu Wen, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based vision systems are integral for 3D object detection, which is
crucial for autonomous navigation. However, they suffer from performance
degradation in adverse weather conditions due to the quality deterioration of
LiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is
expected to solve this problem. However, the fusion of LiDAR and 4D radar is
challenging because they differ significantly in terms of data quality and the
degree of degradation in adverse weather. To address these issues, we introduce
L4DR, a weather-robust 3D object detection method that effectively achieves
LiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and
Foreground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is
the first exploration of the complementarity of early fusion between LiDAR and
4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )
parallel feature extraction backbone coupled with a Multi-Scale Gated Fusion
(MSGF) module to counteract the varying degrees of sensor degradation under
adverse weather conditions. Experimental evaluation on a VoD dataset with
simulated fog proves that L4DR is more adaptable to changing weather
conditions. It delivers a significant performance increase under different fog
levels, improving the 3D mAP by up to 18.17% over the traditional LiDAR-only
approach. Moreover, the results on the K-Radar dataset validate the consistent
performance improvement of L4DR in real-world adverse weather conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing Extremely Memory-Efficient CNNs for On-device Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewook Lee, Yoel Park, Seulki Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a memory-efficient CNN (convolutional neural
network), which enables resource-constrained low-end embedded and IoT devices
to perform on-device vision tasks, such as image classification and object
detection, using extremely low memory, i.e., only 63 KB on ImageNet
classification. Based on the bottleneck block of MobileNet, we propose three
design principles that significantly curtail the peak memory usage of a CNN so
that it can fit the limited KB memory of the low-end device. First, 'input
segmentation' divides an input image into a set of patches, including the
central patch overlapped with the others, reducing the size (and memory
requirement) of a large input image. Second, 'patch tunneling' builds
independent tunnel-like paths consisting of multiple bottleneck blocks per
patch, penetrating through the entire model from an input patch to the last
layer of the network, maintaining lightweight memory usage throughout the whole
network. Lastly, 'bottleneck reordering' rearranges the execution order of
convolution operations inside the bottleneck block such that the memory usage
remains constant regardless of the size of the convolution output channels. The
experiment result shows that the proposed network classifies ImageNet with
extremely low memory (i.e., 63 KB) while achieving competitive top-1 accuracy
(i.e., 61.58\%). To the best of our knowledge, the memory usage of the proposed
network is far smaller than state-of-the-art memory-efficient networks, i.e.,
up to 89x and 3.1x smaller than MobileNet (i.e., 5.6 MB) and MCUNet (i.e., 196
KB), respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PHOCUS: Physics-Based Deconvolution for Ultrasound Resolution
  Enhancement <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Duelmer, Walter Simson, Mohammad Farid Azampour, Magdalena Wysocki, Angelos Karlas, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound is widely used in medical diagnostics allowing for accessible and
powerful imaging but suffers from resolution limitations due to diffraction and
the finite aperture of the imaging system, which restricts diagnostic use. The
impulse function of an ultrasound imaging system is called the point spread
function (PSF), which is convolved with the spatial distribution of reflectors
in the image formation process. Recovering high-resolution reflector
distributions by removing image distortions induced by the convolution process
improves image clarity and detail. Conventionally, deconvolution techniques
attempt to rectify the imaging system's dependent PSF, working directly on the
radio-frequency (RF) data. However, RF data is often not readily accessible.
Therefore, we introduce a physics-based deconvolution process using a modeled
PSF, working directly on the more commonly available B-mode images. By
leveraging Implicit Neural Representations (INRs), we learn a continuous
mapping from spatial locations to their respective echogenicity values,
effectively compensating for the discretized image space. Our contribution
consists of a novel methodology for retrieving a continuous echogenicity map
directly from a B-mode image through a differentiable physics-based rendering
pipeline for ultrasound resolution enhancement. We qualitatively and
quantitatively evaluate our approach on synthetic data, demonstrating
improvements over traditional methods in metrics such as PSNR and SSIM.
Furthermore, we show qualitative enhancements on an ultrasound phantom and an
in-vivo acquisition of a carotid artery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Workshop of Advances in Simplifying Medical
  Ultrasound at MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Detection of Fetal Brain Anomalies using Denoising
  Diffusion Models <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Ditlev Sjøgren Olsen, Jakob Ambsdorf, Manxi Lin, Caroline Taksøe-Vester, Morten Bo Søndergaard Svendsen, Anders Nymark Christensen, Mads Nielsen, Martin Grønnebæk Tolsgaard, Aasa Feragen, Paraskevas Pegios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Congenital malformations of the brain are among the most common fetal
abnormalities that impact fetal development. Previous anomaly detection methods
on ultrasound images are based on supervised learning, rely on manual
annotations, and risk missing underrepresented categories. In this work, we
frame fetal brain anomaly detection as an unsupervised task using diffusion
models. To this end, we employ an inpainting-based Noise Agnostic Anomaly
Detection approach that identifies the abnormality using
diffusion-reconstructed fetal brain images from multiple noise levels. Our
approach only requires normal fetal brain ultrasound images for training,
addressing the limited availability of abnormal data. Our experiments on a
real-world clinical dataset show the potential of using unsupervised methods
for fetal brain anomaly detection. Additionally, we comprehensively evaluate
how different noise types affect diffusion models in the fetal anomaly
detection domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ASMUS@MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAM2-PATH: A better segment anything model for semantic segmentation in
  digital pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingya Zhang, Liang Wang, Limei Gu, Zhao Li, Yaohui Wang, Tingshen Ling, Xianping Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The semantic segmentation task in pathology plays an indispensable role in
assisting physicians in determining the condition of tissue lesions. Foundation
models, such as the SAM (Segment Anything Model) and SAM2, exhibit exceptional
performance in instance segmentation within everyday natural scenes. SAM-PATH
has also achieved impressive results in semantic segmentation within the field
of pathology. However, in computational pathology, the models mentioned above
still have the following limitations. The pre-trained encoder models suffer
from a scarcity of pathology image data; SAM and SAM2 are not suitable for
semantic segmentation. In this paper, we have designed a trainable
Kolmogorov-Arnold Networks(KAN) classification module within the SAM2 workflow,
and we have introduced the largest pretrained vision encoder for histopathology
(UNI) to date. Our proposed framework, SAM2-PATH, augments SAM2's capability to
perform semantic segmentation in digital pathology autonomously, eliminating
the need for human provided input prompts. The experimental results demonstrate
that, after fine-tuning the KAN classification module and decoder, Our dataset
has achieved competitive results on publicly available pathology data. The code
has been open-sourced and can be found at the following address:
https://github.com/simzhangbest/SAM2PATH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages , 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TALE: Training-free Cross-domain Image Composition via Adaptive Latent
  Manipulation and Energy-guided Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kien T. Pham, Jingye Chen, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TALE, a novel training-free framework harnessing the generative
capabilities of text-to-image diffusion models to address the cross-domain
image composition task that focuses on flawlessly incorporating user-specified
objects into a designated visual contexts regardless of domain disparity.
Previous methods often involve either training auxiliary networks or finetuning
diffusion models on customized datasets, which are expensive and may undermine
the robust textual and visual priors of pre-trained diffusion models. Some
recent works attempt to break the barrier by proposing training-free
workarounds that rely on manipulating attention maps to tame the denoising
process implicitly. However, composing via attention maps does not necessarily
yield desired compositional outcomes. These approaches could only retain some
semantic information and usually fall short in preserving identity
characteristics of input objects or exhibit limited background-object style
adaptation in generated images. In contrast, TALE is a novel method that
operates directly on latent space to provide explicit and effective guidance
for the composition process to resolve these problems. Specifically, we equip
TALE with two mechanisms dubbed Adaptive Latent Manipulation and Energy-guided
Latent Optimization. The former formulates noisy latents conducive to
initiating and steering the composition process by directly leveraging
background and foreground latents at corresponding timesteps, and the latter
exploits designated energy functions to further optimize intermediate latents
conforming to specific conditions that complement the former to generate
desired final results. Our experiments demonstrate that TALE surpasses prior
baselines and attains state-of-the-art performance in image-guided composition
across various photorealistic and artistic domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 32nd ACM Multimedia Conference (MM '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concept Conductor: Orchestrating Multiple Personalized Concepts in
  Text-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zebin Yao, Fangxiang Feng, Ruifan Li, Xiaojie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The customization of text-to-image models has seen significant advancements,
yet generating multiple personalized concepts remains a challenging task.
Current methods struggle with attribute leakage and layout confusion when
handling multiple concepts, leading to reduced concept fidelity and semantic
consistency. In this work, we introduce a novel training-free framework,
Concept Conductor, designed to ensure visual fidelity and correct layout in
multi-concept customization. Concept Conductor isolates the sampling processes
of multiple custom models to prevent attribute leakage between different
concepts and corrects erroneous layouts through self-attention-based spatial
guidance. Additionally, we present a concept injection technique that employs
shape-aware masks to specify the generation area for each concept. This
technique injects the structure and appearance of personalized concepts through
feature fusion in the attention layers, ensuring harmony in the final image.
Extensive qualitative and quantitative experiments demonstrate that Concept
Conductor can consistently generate composite images with accurate layouts
while preserving the visual details of each concept. Compared to existing
baselines, Concept Conductor shows significant performance improvements. Our
method supports the combination of any number of concepts and maintains high
fidelity even when dealing with visually similar concepts. The code and models
are available at https://github.com/Nihukat/Concept-Conductor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Page: https://github.com/Nihukat/Concept-Conductor</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Contrastive Learning via Batch Instance Discrimination and
  Feature Clustering for Small Sample SAR ATR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikui Zhai, Wenlve Zhou, Bing Sun, Jingwen Li, Qirui Ke, Zilu Ying, Junying Gan, Chaoyun Mai, Ruggero Donida Labati, Vincenzo Piuri, Fabio Scotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, impressive performance of deep learning technology has been
recognized in Synthetic Aperture Radar (SAR) Automatic Target Recognition
(ATR). Since a large amount of annotated data is required in this technique, it
poses a trenchant challenge to the issue of obtaining a high recognition rate
through less labeled data. To overcome this problem, inspired by the
contrastive learning, we proposed a novel framework named Batch Instance
Discrimination and Feature Clustering (BIDFC). In this framework, different
from that of the objective of general contrastive learning methods, embedding
distance between samples should be moderate because of the high similarity
between samples in the SAR images. Consequently, our flexible framework is
equipped with adjustable distance between embedding, which we term as weakly
contrastive learning. Technically, instance labels are assigned to the
unlabeled data in per batch and random augmentation and training are performed
few times on these augmented data. Meanwhile, a novel Dynamic-Weighted Variance
loss (DWV loss) function is also posed to cluster the embedding of enhanced
versions for each sample. Experimental results on the moving and stationary
target acquisition and recognition (MSTAR) database indicate a 91.25%
classification accuracy of our method fine-tuned on only 3.13% training data.
Even though a linear evaluation is performed on the same training data, the
accuracy can still reach 90.13%. We also verified the effectiveness of BIDFC in
OpenSarShip database, indicating that our method can be generalized to other
datasets. Our code is avaliable at:
https://github.com/Wenlve-Zhou/BIDFC-master.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentsCoMerge: Large Language Model Empowered Collaborative Decision
  Making for Ramp Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senkang Hu, Zhengru Fang, Zihan Fang, Yiqin Deng, Xianhao Chen, Yuguang Fang, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ramp merging is one of the bottlenecks in traffic systems, which commonly
cause traffic congestion, accidents, and severe carbon emissions. In order to
address this essential issue and enhance the safety and efficiency of connected
and autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel
collaborative decision-making framework, named AgentsCoMerge, to leverage large
language models (LLMs). Specifically, we first design a scene observation and
understanding module to allow an agent to capture the traffic environment. Then
we propose a hierarchical planning module to enable the agent to make decisions
and plan trajectories based on the observation and the agent's own state. In
addition, in order to facilitate collaboration among multiple agents, we
introduce a communication module to enable the surrounding agents to exchange
necessary information and coordinate their actions. Finally, we develop a
reinforcement reflection guided training paradigm to further enhance the
decision-making capability of the framework. Extensive experiments are
conducted to evaluate the performance of our proposed method, demonstrating its
superior efficiency and effectiveness for multi-agent collaborative
decision-making under various ramp merging scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distillation Learning Guided by Image Reconstruction for One-Shot
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Zhou, Yanjie Zhou, Longjie Wang, Yun Peng, David E. Carlson, Liyun Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional one-shot medical image segmentation (MIS) methods use
registration networks to propagate labels from a reference atlas or rely on
comprehensive sampling strategies to generate synthetic labeled data for
training. However, these methods often struggle with registration errors and
low-quality synthetic images, leading to poor performance and generalization.
To overcome this, we introduce a novel one-shot MIS framework based on
knowledge distillation, which allows the network to directly 'see' real images
through a distillation process guided by image reconstruction. It focuses on
anatomical structures in a single labeled image and a few unlabeled ones. A
registration-based data augmentation network creates realistic, labeled
samples, while a feature distillation module helps the student network learn
segmentation from these samples, guided by the teacher network. During
inference, the streamlined student network accurately segments new images.
Evaluations on three public datasets (OASIS for T1 brain MRI, BCV for abdomen
CT, and VerSe for vertebrae CT) show superior segmentation performance and
generalization across different medical image datasets and modalities compared
to leading methods. Our code is available at
https://github.com/NoviceFodder/OS-MedSeg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JARViS: Detecting Actions in Video Using Unified Actor-Scene Context
  Relation Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seok Hwan Lee, Taein Son, Soo Won Seo, Jisong Kim, Jun Won Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video action detection (VAD) is a formidable vision task that involves the
localization and classification of actions within the spatial and temporal
dimensions of a video clip. Among the myriad VAD architectures, two-stage VAD
methods utilize a pre-trained person detector to extract the region of interest
features, subsequently employing these features for action detection. However,
the performance of two-stage VAD methods has been limited as they depend solely
on localized actor features to infer action semantics. In this study, we
propose a new two-stage VAD framework called Joint Actor-scene context Relation
modeling based on Visual Semantics (JARViS), which effectively consolidates
cross-modal action semantics distributed globally across spatial and temporal
dimensions using Transformer attention. JARViS employs a person detector to
produce densely sampled actor features from a keyframe. Concurrently, it uses a
video backbone to create spatio-temporal scene features from a video clip.
Finally, the fine-grained interactions between actors and scenes are modeled
through a Unified Action-Scene Context Transformer to directly output the final
set of actions in parallel. Our experimental results demonstrate that JARViS
outperforms existing methods by significant margins and achieves
state-of-the-art performance on three popular VAD datasets, including AVA,
UCF101-24, and JHMDB51-21.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InPer: Whole-Process Domain Generalization via Causal Intervention and
  Perturbation <span class="chip">BMVC2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Xinghao Ding, Yue Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable advancements achieved by deep neural networks, their
performance tends to degenerate when the test environment diverges from the
training ones. Domain generalization (DG) solves this issue by learning
representations independent of domain-related information, thus facilitating
extrapolation to unseen environments. Existing approaches typically focus on
formulating tailored training objectives to extract shared features from the
source data. However, the disjointed training and testing procedures may
compromise robustness, particularly in the face of unforeseen variations during
deployment. In this paper, we propose a novel and holistic framework based on
causality, named InPer, designed to enhance model generalization by
incorporating causal intervention during training and causal perturbation
during testing. Specifically, during the training phase, we employ
entropy-based causal intervention (EnIn) to refine the selection of causal
variables. To identify samples with anti-interference causal variables from the
target domain, we propose a novel metric, homeostatic score, through causal
perturbation (HoPer) to construct a prototype classifier in test time.
Experimental results across multiple cross-domain tasks confirm the efficacy of
InPer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BMVC2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRISM: PRogressive dependency maxImization for Scale-invariant image
  Matching <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Cai, Yongcai Wang, Lun Luo, Minhang Wang, Deying Li, Jintao Xu, Weihao Gu, Rui Ai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image matching aims at identifying corresponding points between a pair of
images. Currently, detector-free methods have shown impressive performance in
challenging scenarios, thanks to their capability of generating dense matches
and global receptive field. However, performing feature interaction and
proposing matches across the entire image is unnecessary, because not all image
regions contribute to the matching process. Interacting and matching in
unmatchable areas can introduce errors, reducing matching accuracy and
efficiency. Meanwhile, the scale discrepancy issue still troubles existing
methods. To address above issues, we propose PRogressive dependency
maxImization for Scale-invariant image Matching (PRISM), which jointly prunes
irrelevant patch features and tackles the scale discrepancy. To do this, we
firstly present a Multi-scale Pruning Module (MPM) to adaptively prune
irrelevant features by maximizing the dependency between the two feature sets.
Moreover, we design the Scale-Aware Dynamic Pruning Attention (SADPA) to
aggregate information from different scales via a hierarchical design. Our
method's superior matching performance and generalization capability are
confirmed by leading accuracy across various evaluation benchmarks and
downstream tasks. The code is publicly available at
https://github.com/Master-cai/PRISM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, ACM MM 2024. Supplementary materials are
  included</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Quantum Control Gates for Functional MRI Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Bac Nguyen, Hoang-Quan Nguyen, Hugh Churchill, Samee U. Khan, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing has emerged as a powerful tool for solving complex problems
intractable for classical computers, particularly in popular fields such as
cryptography, optimization, and neurocomputing. In this paper, we present a new
quantum-based approach named the Hierarchical Quantum Control Gates (HQCG)
method for efficient understanding of Functional Magnetic Resonance Imaging
(fMRI) data. This approach includes two novel modules: the Local Quantum
Control Gate (LQCG) and the Global Quantum Control Gate (GQCG), which are
designed to extract local and global features of fMRI signals, respectively.
Our method operates end-to-end on a quantum machine, leveraging quantum
mechanics to learn patterns within extremely high-dimensional fMRI signals,
such as 30,000 samples which is a challenge for classical computers. Empirical
results demonstrate that our approach significantly outperforms classical
methods. Additionally, we found that the proposed quantum model is more stable
and less prone to overfitting than the classical methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HistoSPACE: Histology-Inspired Spatial Transcriptome Prediction And
  Characterization Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Kumar, Samrat Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial transcriptomics (ST) enables the visualization of gene expression
within the context of tissue morphology. This emerging discipline has the
potential to serve as a foundation for developing tools to design precision
medicines. However, due to the higher costs and expertise required for such
experiments, its translation into a regular clinical practice might be
challenging. Despite the implementation of modern deep learning to enhance
information obtained from histological images using AI, efforts have been
constrained by limitations in the diversity of information. In this paper, we
developed a model, HistoSPACE that explore the diversity of histological images
available with ST data to extract molecular insights from tissue image. Our
proposed study built an image encoder derived from universal image autoencoder.
This image encoder was connected to convolution blocks to built the final
model. It was further fine tuned with the help of ST-Data. This model is
notably lightweight in compared to traditional histological models. Our
developed model demonstrates significant efficiency compared to contemporary
algorithms, revealing a correlation of 0.56 in leave-one-out cross-validation.
Finally, its robustness was validated through an independent dataset, showing a
well matched preditction with predefined disease pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focal Depth Estimation: A Calibration-Free, Subject- and Daytime
  Invariant Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt W. Hosp, Björn Severitt, Rajat Agarwala, Evgenia Rusak, Yannick Sauer, Siegfried Wahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where personalized technology is increasingly intertwined with
daily life, traditional eye-tracking systems and autofocal glasses face a
significant challenge: the need for frequent, user-specific calibration, which
impedes their practicality. This study introduces a groundbreaking
calibration-free method for estimating focal depth, leveraging machine learning
techniques to analyze eye movement features within short sequences. Our
approach, distinguished by its innovative use of LSTM networks and
domain-specific feature engineering, achieves a mean absolute error (MAE) of
less than 10 cm, setting a new focal depth estimation accuracy standard. This
advancement promises to enhance the usability of autofocal glasses and pave the
way for their seamless integration into extended reality environments, marking
a significant leap forward in personalized visual technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teach CLIP to Develop a Number Sense for Ordinal Regression <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Du, Qiang Zhai, Weihang Dai, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ordinal regression is a fundamental problem within the field of computer
vision, with customised well-trained models on specific tasks. While
pre-trained vision-language models (VLMs) have exhibited impressive performance
on various vision tasks, their potential for ordinal regression has received
less exploration. In this study, we first investigate CLIP's potential for
ordinal regression, from which we expect the model could generalise to
different ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP
fails on this task, since current VLMs have a well-documented limitation of
encapsulating compositional concepts such as number sense. We propose a simple
yet effective method called NumCLIP to improve the quantitative understanding
of VLMs. We disassemble the exact image to number-specific text matching
problem into coarse classification and fine prediction stages. We discretize
and phrase each numerical bin with common language concept to better leverage
the available pre-trained alignment in CLIP. To consider the inherent
continuous property of ordinal regression, we propose a novel fine-grained
cross-modal ranking-based regularisation loss specifically designed to keep
both semantic and ordinal alignment in CLIP's feature space. Experimental
results on three general ordinal regression tasks demonstrate the effectiveness
of NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating
and image aesthetics assessment task, respectively. Code is publicly available
at https://github.com/xmed-lab/NumCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative study of generative adversarial networks for image
  recognition algorithms based on deep learning and traditional methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Zhong, Yijing Wei, Yingbin Liang, Xiqing Liu, Rongwei Ji, Yiru Cang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, an image recognition algorithm based on the combination of
deep learning and generative adversarial network (GAN) is studied, and compared
with traditional image recognition methods. The purpose of this study is to
evaluate the advantages and application prospects of deep learning technology,
especially GAN, in the field of image recognition. Firstly, this paper reviews
the basic principles and techniques of traditional image recognition methods,
including the classical algorithms based on feature extraction such as SIFT,
HOG and their combination with support vector machine (SVM), random forest, and
other classifiers. Then, the working principle, network structure, and unique
advantages of GAN in image generation and recognition are introduced. In order
to verify the effectiveness of GAN in image recognition, a series of
experiments are designed and carried out using multiple public image data sets
for training and testing. The experimental results show that compared with
traditional methods, GAN has excellent performance in processing complex
images, recognition accuracy, and anti-noise ability. Specifically, Gans are
better able to capture high-dimensional features and details of images,
significantly improving recognition performance. In addition, Gans shows unique
advantages in dealing with image noise, partial missing information, and
generating high-quality images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Exocentric Video-Language Data for Egocentric Video
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi-Yi Dou, Xitong Yang, Tushar Nagarajan, Huiyu Wang, Jing Huang, Nanyun Peng, Kris Kitani, Fu-Jen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present EMBED (Egocentric Models Built with Exocentric Data), a method
designed to transform exocentric video-language data for egocentric video
representation learning. Large-scale exocentric data covers diverse activities
with significant potential for egocentric learning, but inherent disparities
between egocentric and exocentric data pose challenges in utilizing one view
for the other seamlessly. Egocentric videos predominantly feature close-up
hand-object interactions, whereas exocentric videos offer a broader perspective
on human activities. Additionally, narratives in egocentric datasets are
typically more action-centric and closely linked with the visual content, in
contrast to the narrative styles found in exocentric datasets. To address these
challenges, we employ a data transformation framework to adapt exocentric data
for egocentric training, focusing on identifying specific video clips that
emphasize hand-object interactions and transforming narration styles to align
with egocentric perspectives. By applying both vision and language style
transfer, our framework creates a new egocentric dataset derived from
exocentric video-language data. Through extensive evaluations, we demonstrate
the effectiveness of EMBED, achieving state-of-the-art results across various
egocentric downstream tasks, including an absolute improvement of 4.7% on the
Epic-Kitchens-100 multi-instance retrieval and 6.2% on the EGTEA classification
benchmarks in zero-shot settings. Furthermore, EMBED enables egocentric
video-language models to perform competitively in exocentric tasks. Finally, we
showcase EMBED's application across various exocentric datasets, exhibiting
strong generalization capabilities when applied to different exocentric
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Underwater litter monitoring using consumer-grade aerial-aquatic speedy
  scanner (AASS) and deep learning based super-resolution reconstruction and
  detection network <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhao, Yongying Liu, Jiaqi Wang, Yijia Chen, Dianhan Xi, Xinlei Shao, Shigeru Tabeta, Katsunori Mizuno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater litter is widely spread across aquatic environments such as lakes,
rivers, and oceans, significantly impacting natural ecosystems. Current
monitoring technologies for detecting underwater litter face limitations in
survey efficiency, cost, and environmental conditions, highlighting the need
for efficient, consumer-grade technologies for automatic detection. This
research introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with
Super-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network.
AASS enhances data acquisition efficiency over traditional methods, capturing
high-quality images that accurately identify underwater waste. SRR improves
image-resolution by mitigating motion blur and insufficient resolution, thereby
enhancing detection tasks. Specifically, the RCAN model achieved the highest
mean average precision (mAP) of 78.6% for detection accuracy on reconstructed
images among the tested SRR models. With a magnification factor of 4, the SRR
test set shows an improved mAP compared to the conventional bicubic set. These
results demonstrate the effectiveness of the proposed method in detecting
underwater litter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The earlier version of this conference paper was accepted at OCEANS
  2024-Halifax, Canada and was selected for inclusion in the Student Poster
  Competition (SPC) Program</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monitoring of Hermit Crabs Using drone-captured imagery and Deep
  Learning based Super-Resolution Reconstruction and Improved YOLOv8 <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhao, Yijia Chen, Dianhan Xi, Yongying Liu, Jiaqi Wang, Shigeru Tabeta, Katsunori Mizuno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hermit crabs play a crucial role in coastal ecosystems by dispersing seeds,
cleaning up debris, and disturbing soil. They serve as vital indicators of
marine environmental health, responding to climate change and pollution.
Traditional survey methods, like quadrat sampling, are labor-intensive,
time-consuming, and environmentally dependent. This study presents an
innovative approach combining UAV-based remote sensing with Super-Resolution
Reconstruction (SRR) and the CRAB-YOLO detection network, a modification of
YOLOv8s, to monitor hermit crabs. SRR enhances image quality by addressing
issues such as motion blur and insufficient resolution, significantly improving
detection accuracy over conventional low-resolution fuzzy images. The CRAB-YOLO
network integrates three improvements for detection accuracy, hermit crab
characteristics, and computational efficiency, achieving state-of-the-art
(SOTA) performance compared to other mainstream detection models. The RDN
networks demonstrated the best image reconstruction performance, and CRAB-YOLO
achieved a mean average precision (mAP) of 69.5% on the SRR test set, a 40%
improvement over the conventional Bicubic method with a magnification factor of
4. These results indicate that the proposed method is effective in detecting
hermit crabs, offering a cost-effective and automated solution for extensive
hermit crab monitoring, thereby aiding coastal benthos conservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The earlier version of this conference paper was presented at OCEANS
  2024-Singapore and was selected for inclusion in the Student Poster
  Competition (SPC) Program</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion
  Methods <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onkar Susladkar, Gayatri Deshmukh, Sparsh Mittal, Parth Shastri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In image processing, one of the most challenging tasks is to render an
image's semantic meaning using a variety of artistic approaches. Existing
techniques for arbitrary style transfer (AST) frequently experience
mode-collapse, over-stylization, or under-stylization due to a disparity
between the style and content images. We propose a novel framework called
D$^2$Styler (Discrete Diffusion Styler) that leverages the discrete
representational capability of VQ-GANs and the advantages of discrete
diffusion, including stable training and avoidance of mode collapse. Our method
uses Adaptive Instance Normalization (AdaIN) features as a context guide for
the reverse diffusion process. This makes it easy to move features from the
style image to the content image without bias. The proposed method
substantially enhances the visual quality of style-transferred images, allowing
the combination of content and style in a visually appealing manner. We take
style images from the WikiArt dataset and content images from the COCO dataset.
Experimental results demonstrate that D$^2$Styler produces high-quality
style-transferred images and outperforms twelve existing methods on nearly all
the metrics. The qualitative results and ablation studies provide further
insights into the efficacy of our technique. The code is available at
https://github.com/Onkarsus13/D2Styler.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at 27th International Conference on Pattern
  Recognition (ICPR), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsu Kim, Junhee Lee, Ukcheol Shin, Jean Oh, Kyungdon Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D semantic occupancy prediction is becoming important in robot
vision due to the compactness of using a single RGB camera. However, existing
methods often do not adequately account for camera perspective geometry,
resulting in information imbalance along the depth range of the image. To
address this issue, we propose a vanishing point (VP) guided monocular 3D
semantic occupancy prediction framework named VPOcc. Our framework consists of
three novel modules utilizing VP. First, in the VPZoomer module, we initially
utilize VP in feature extraction to achieve information balanced feature
extraction across the scene by generating a zoom-in image based on VP. Second,
we perform perspective geometry-aware feature aggregation by sampling points
towards VP using a VP-guided cross-attention (VPCA) module. Finally, we create
an information-balanced feature volume by effectively fusing original and
zoom-in voxel feature volumes with a balanced feature volume fusion (BVFV)
module. Experiments demonstrate that our method achieves state-of-the-art
performance for both IoU and mIoU on SemanticKITTI and SSCBench-KITTI360. These
results are obtained by effectively addressing the information imbalance in
images through the utilization of VP. Our code will be available at
www.github.com/anonymous.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP-based Point Cloud Classification via Point Cloud to Image
  Translation <span class="chip">ICPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuvozit Ghose, Manyi Li, Yiming Qian, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud understanding is an inherently challenging problem because of the
sparse and unordered structure of the point cloud in the 3D space. Recently,
Contrastive Vision-Language Pre-training (CLIP) based point cloud
classification model i.e. PointCLIP has added a new direction in the point
cloud classification research domain. In this method, at first multi-view depth
maps are extracted from the point cloud and passed through the CLIP visual
encoder. To transfer the 3D knowledge to the network, a small network called an
adapter is fine-tuned on top of the CLIP visual encoder. PointCLIP has two
limitations. Firstly, the point cloud depth maps lack image information which
is essential for tasks like classification and recognition. Secondly, the
adapter only relies on the global representation of the multi-view features.
Motivated by this observation, we propose a Pretrained Point Cloud to Image
Translation Network (PPCITNet) that produces generalized colored images along
with additional salient visual cues to the point cloud depth maps so that it
can achieve promising performance on point cloud classification and
understanding. In addition, we propose a novel viewpoint adapter that combines
the view feature processed by each viewpoint as well as the global intertwined
knowledge that exists across the multi-view features. The experimental results
demonstrate the superior performance of the proposed model over existing
state-of-the-art CLIP-based models on ModelNet10, ModelNet40, and ScanobjectNN
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic identification of the area covered by acorn trees in the
  dehesa (pastureland) Extremadura of Spain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ojeda-Magaña Benjamin, Ruelas Ruben, Quintanilla-Dominguez Joel, Gomez-Barba Leopoldo, Lopez de Herrera Juan, Robledo-Hernandez Jose, Tarquis Ana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The acorn is the fruit of the oak and is an important crop in the Spanish
dehesa extreme\~na, especially for the value it provides in the Iberian pig
food to obtain the "acorn" certification. For this reason, we want to maximise
the production of Iberian pigs with the appropriate weight. Hence the need to
know the area covered by the crowns of the acorn trees, to determine the
covered wooded area (CWA, from the Spanish Superficie Arbolada Cubierta SAC)
and thereby estimate the number of Iberian pigs that can be released per
hectare, as indicated by the royal decree 4/2014. In this work, we propose the
automatic estimation of the CWA, through aerial digital images (orthophotos) of
the pastureland of Extremadura, and with this, to offer the possibility of
determining the number of Iberian pigs to be released in a specific plot of
land. Among the main issues for automatic detection are, first, the correct
identification of acorn trees, secondly, correctly discriminating the shades of
the acorn trees and, finally, detect the arbuscles (young acorn trees not yet
productive, or shrubs that are not oaks). These difficulties represent a real
challenge, both for the automatic segmentation process and for manual
segmentation. In this work, the proposed method for automatic segmentation is
based on the clustering algorithm proposed by Gustafson-Kessel (GK) but the
modified version of Babuska (GK-B) and on the use of real orthophotos. The
obtained results are promising both in their comparison with the real images
and when compared with the images segmented by hand. The whole set of
orthophotos used in this work correspond to an approximate area of 142
hectares, and the results are of great interest to producers of certified
"acorn" pork.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 Figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoseMamba: Monocular 3D Human Pose Estimation with Bidirectional
  Global-Local Spatio-Temporal State Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Huang, Junshuo Liu, Ke Xian, Robert Caiming Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have significantly advanced the field of 3D human pose
estimation (HPE). However, existing transformer-based methods primarily use
self-attention mechanisms for spatio-temporal modeling, leading to a quadratic
complexity, unidirectional modeling of spatio-temporal relationships, and
insufficient learning of spatial-temporal correlations. Recently, the Mamba
architecture, utilizing the state space model (SSM), has exhibited superior
long-range modeling capabilities in a variety of vision tasks with linear
complexity. In this paper, we propose PoseMamba, a novel purely SSM-based
approach with linear complexity for 3D human pose estimation in monocular
video. Specifically, we propose a bidirectional global-local spatio-temporal
SSM block that comprehensively models human joint relations within individual
frames as well as temporal correlations across frames. Within this
bidirectional global-local spatio-temporal SSM block, we introduce a reordering
strategy to enhance the local modeling capability of the SSM. This strategy
provides a more logical geometric scanning order and integrates it with the
global SSM, resulting in a combined global-local spatial scan. We have
quantitatively and qualitatively evaluated our approach using two benchmark
datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments demonstrate that
PoseMamba achieves state-of-the-art performance on both datasets while
maintaining a smaller model size and reducing computational costs. The code and
models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time
  High-Quality Relighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Guo, Yuanxi Bai, Liwen Hu, Ziyi Guo, Mianzhi Liu, Yu Cai, Tiejun Huang, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a
real-time high-quality relighting method for Gaussian splats in low-frequency
lighting environments that captures soft shadows and interreflections by
precomputing 3D Gaussian splats' radiance transfer. Existing studies have
demonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields'
efficiency for dynamic lighting scenarios. However, the current relighting
method based on 3DGS still struggles to compute high-quality shadow and
indirect illumination in real time for dynamic light, leading to unrealistic
rendering results. We solve this problem by precomputing the expensive
transport simulations required for complex transfer functions like shadowing,
the resulting transfer functions are represented as dense sets of vectors or
matrices for every Gaussian splat. We introduce distinct precomputing methods
tailored for training and rendering stages, along with unique ray tracing and
indirect lighting precomputation techniques for 3D Gaussian splats to
accelerate training speed and compute accurate indirect lighting related to
environment light. Experimental analyses demonstrate that our approach achieves
state-of-the-art visual quality while maintaining competitive training times
and allows high-quality real-time (30+ fps) relighting for dynamic light and
relatively complex scenes at 1080p resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwinShadow: Shifted Window for Ambiguous Adjacent Shadow Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghui Wang, Shaokai Liu, Li Li, Wengang Zhou, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shadow detection is a fundamental and challenging task in many computer
vision applications. Intuitively, most shadows come from the occlusion of light
by the object itself, resulting in the object and its shadow being contiguous
(referred to as the adjacent shadow in this paper). In this case, when the
color of the object is similar to that of the shadow, existing methods struggle
to achieve accurate detection. To address this problem, we present SwinShadow,
a transformer-based architecture that fully utilizes the powerful shifted
window mechanism for detecting adjacent shadows. The mechanism operates in two
steps. Initially, it applies local self-attention within a single window,
enabling the network to focus on local details. Subsequently, it shifts the
attention windows to facilitate inter-window attention, enabling the capture of
a broader range of adjacent information. These combined steps significantly
improve the network's capacity to distinguish shadows from nearby objects. And
the whole process can be divided into three parts: encoder, decoder, and
feature integration. During encoding, we adopt Swin Transformer to acquire
hierarchical features. Then during decoding, for shallow layers, we propose a
deep supervision (DS) module to suppress the false positives and boost the
representation capability of shadow features for subsequent processing, while
for deep layers, we leverage a double attention (DA) module to integrate local
and shifted window in one stage to achieve a larger receptive field and enhance
the continuity of information. Ultimately, a new multi-level aggregation (MLA)
mechanism is applied to fuse the decoded features for mask prediction.
Extensive experiments on three shadow detection benchmark datasets, SBU, UCF,
and ISTD, demonstrate that our network achieves good performance in terms of
balance error rate (BER).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhosein Chahe, Lifeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method for open-vocabulary 3D scene
understanding in autonomous driving by combining Language Embedded 3D Gaussians
with Large Language Models (LLMs) for enhanced inference. We propose utilizing
LLMs to generate contextually relevant canonical phrases for segmentation and
scene interpretation. Our method leverages the contextual and semantic
capabilities of LLMs to produce a set of canonical phrases, which are then
compared with the language features embedded in the 3D Gaussians. This
LLM-guided approach significantly improves zero-shot scene understanding and
detection of objects of interest, even in the most challenging or unfamiliar
environments. Experimental results on the WayveScenes101 dataset demonstrate
that our approach surpasses state-of-the-art methods in terms of accuracy and
flexibility for open-vocabulary object detection and segmentation. This work
represents a significant advancement towards more intelligent, context-aware
autonomous driving systems, effectively bridging 3D scene representation with
high-level semantic understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoExtend: Tuning New Experts for Modality and Task Extension <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Zhong, Shanghua Gao, Zhongzhan Huang, Wushao Wen, Marinka Zitnik, Pan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in various tasks but are primarily trained
on text data, limiting their application scope. Expanding LLM capabilities to
include vision-language understanding is vital, yet training them on multimodal
data from scratch is challenging and costly. Existing instruction tuning
methods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs
via fully fine-tuning LLMs to bridge the modality gap. However, full
fine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous
knowledge, and high training costs particularly in the era of increasing tasks
and modalities. To solve this issue, we introduce MoExtend, an effective
framework designed to streamline the modality adaptation and extension of
Mixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts
into pre-trained MoE models, endowing them with novel knowledge without the
need to tune pretrained models such as MoE and vision encoders. This approach
enables rapid adaptation and extension to new modal data or tasks, effectively
addressing the challenge of accommodating new modalities within LLMs.
Furthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk
of catastrophic forgetting. Experimental results demonstrate the efficacy and
efficiency of MoExtend in enhancing the multimodal capabilities of LLMs,
contributing to advancements in multimodal AI research. Code:
https://github.com/zhongshsh/MoExtend.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 - SRW</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUI Element Detection Using SOTA YOLO Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Shayan Daneshvar, Shaowei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection of Graphical User Interface (GUI) elements is a crucial task for
automatic code generation from images and sketches, GUI testing, and GUI
search. Recent studies have leveraged both old-fashioned and modern computer
vision (CV) techniques. Oldfashioned methods utilize classic image processing
algorithms (e.g. edge detection and contour detection) and modern methods use
mature deep learning solutions for general object detection tasks. GUI element
detection, however, is a domain-specific case of object detection, in which
objects overlap more often, and are located very close to each other, plus the
number of object classes is considerably lower, yet there are more objects in
the images compared to natural images. Hence, the studies that have been
carried out on comparing various object detection models, might not apply to
GUI element detection. In this study, we evaluate the performance of the four
most recent successful YOLO models for general object detection tasks on GUI
element detection and investigate their accuracy performance in detecting
various GUI elements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opening the Black Box of 3D Reconstruction Error Analysis with VECTOR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Racquel Fygenson, Kazi Jawad, Isabel Li, Francois Ayoub, Robert G. Deen, Scott Davidoff, Dominik Moritz, Mauricio Hess-Flores
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstruction of 3D scenes from 2D images is a technical challenge that
impacts domains from Earth and planetary sciences and space exploration to
augmented and virtual reality. Typically, reconstruction algorithms first
identify common features across images and then minimize reconstruction errors
after estimating the shape of the terrain. This bundle adjustment (BA) step
optimizes around a single, simplifying scalar value that obfuscates many
possible causes of reconstruction errors (e.g., initial estimate of the
position and orientation of the camera, lighting conditions, ease of feature
detection in the terrain). Reconstruction errors can lead to inaccurate
scientific inferences or endanger a spacecraft exploring a remote environment.
To address this challenge, we present VECTOR, a visual analysis tool that
improves error inspection for stereo reconstruction BA. VECTOR provides
analysts with previously unavailable visibility into feature locations, camera
pose, and computed 3D points. VECTOR was developed in partnership with the
Perseverance Mars Rover and Ingenuity Mars Helicopter terrain reconstruction
team at the NASA Jet Propulsion Laboratory. We report on how this tool was used
to debug and improve terrain reconstruction for the Mars 2020 mission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ e-Health CSIRO at RRG24: Entropy-Augmented Self-Critical Sequence
  Training for Radiology Report Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Nicolson, Jinghui Liu, Jason Dowling, Anthony Nguyen, Bevan Koopman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Shared Task on Large-Scale Radiology Report Generation (RRG24) aims to
expedite the development of assistive systems for interpreting and reporting on
chest X-ray (CXR) images. This task challenges participants to develop models
that generate the findings and impression sections of radiology reports from
CXRs from a patient's study, using five different datasets. This paper outlines
the e-Health CSIRO team's approach, which achieved multiple first-place
finishes in RRG24. The core novelty of our approach lies in the addition of
entropy regularisation to self-critical sequence training, to maintain a higher
entropy in the token distribution. This prevents overfitting to common phrases
and ensures a broader exploration of the vocabulary during training, essential
for handling the diversity of the radiology reports in the RRG24 datasets. Our
model is available on Hugging Face https://huggingface.co/aehrc/cxrmate-rrg24.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FacialPulse: An Efficient RNN-based Depression Detection via Temporal
  Facial Landmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Wang, Jinyang Huang, Jie Zhang, Xin Liu, Xiang Zhang, Zhi Liu, Peng Zhao, Sigui Chen, Xiao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depression is a prevalent mental health disorder that significantly impacts
individuals' lives and well-being. Early detection and intervention are crucial
for effective treatment and management of depression. Recently, there are many
end-to-end deep learning methods leveraging the facial expression features for
automatic depression detection. However, most current methods overlook the
temporal dynamics of facial expressions. Although very recent 3DCNN methods
remedy this gap, they introduce more computational cost due to the selection of
CNN-based backbones and redundant facial features.
  To address the above limitations, by considering the timing correlation of
facial expressions, we propose a novel framework called FacialPulse, which
recognizes depression with high accuracy and speed. By harnessing the
bidirectional nature and proficiently addressing long-term dependencies, the
Facial Motion Modeling Module (FMMM) is designed in FacialPulse to fully
capture temporal features. Since the proposed FMMM has parallel processing
capabilities and has the gate mechanism to mitigate gradient vanishing, this
module can also significantly boost the training speed.
  Besides, to effectively use facial landmarks to replace original images to
decrease information redundancy, a Facial Landmark Calibration Module (FLCM) is
designed to eliminate facial landmark errors to further improve recognition
accuracy. Extensive experiments on the AVEC2014 dataset and MMDA dataset (a
depression dataset) demonstrate the superiority of FacialPulse on recognition
accuracy and speed, with the average MAE (Mean Absolute Error) decreased by 21%
compared to baselines, and the recognition speed increased by 100% compared to
state-of-the-art methods. Codes are released at
https://github.com/volatileee/FacialPulse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11652v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11652v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny Gupta, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a privacy-preserving approach to train models
on decentralized data. Its potential in healthcare is significant, but
challenges arise due to cross-client variations in medical image data,
exacerbated by limited annotations. This paper introduces Cross-Client
Variations Adaptive Federated Learning (CCVA-FL) to address these issues.
CCVA-FL aims to minimize cross-client variations by transforming images into a
common feature space. It involves expert annotation of a subset of images from
each client, followed by the selection of a client with the least data
complexity as the target. Synthetic medical images are then generated using
Scalable Diffusion Models with Transformers (DiT) based on the target client's
annotated images. These synthetic images, capturing diversity and representing
the original data, are shared with other clients. Each client then translates
its local images into the target image space using image-to-image translation.
The translated images are subsequently used in a federated learning setting to
develop a server model. Our results demonstrate that CCVA-FL outperforms
Vanilla Federated Averaging by effectively addressing data distribution
differences across clients without compromising privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I found critical errors in the manuscript affecting its validity. I
  need to correct these before resubmitting. Major changes to methodology and
  results are underway, significantly altering the content. I will resubmit the
  revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Power of Data Tsunami: A Comprehensive <span class="highlight-title">Survey</span> on Data
  Assessment and Selection for Instruction Tuning of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning plays a critical role in aligning large language models
(LLMs) with human preference. Despite the vast amount of open instruction
datasets, naively training a LLM on all existing instructions may not be
optimal and practical. To pinpoint the most beneficial datapoints, data
assessment and selection methods have been proposed in the fields of natural
language processing (NLP) and deep learning. However, under the context of
instruction tuning, there still exists a gap in knowledge on what kind of data
evaluation metrics can be employed and how they can be integrated into the
selection mechanism. To bridge this gap, we present a comprehensive review on
existing literature of data assessment and selection especially for instruction
tuning of LLMs. We systematically categorize all applicable methods into
quality-based, diversity-based, and importance-based ones where a unified,
fine-grained taxonomy is structured. For each category, representative methods
are elaborated to describe the landscape of relevant research. In addition,
comparison between latest methods is conducted on their officially reported
results to provide in-depth discussions on their limitations. Finally, we
summarize the open challenges and propose the promosing avenues for future
studies. All related contents are available at
https://github.com/yuleiqin/fantastic-data-engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>review, survey, 28 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pose Magic: Efficient and Temporally Consistent Human Pose Estimation
  with a Hybrid Mamba-GCN Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Zhang, Qiqi Bao, Qinpeng Cui, Wenming Yang, Qingmin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are
primarily based on Transformers. However, existing Transformer-based 3D HPE
backbones often encounter a trade-off between accuracy and computational
efficiency. To resolve the above dilemma, in this work, we leverage recent
advances in state space models and utilize Mamba for high-quality and efficient
long-range modeling. Nonetheless, Mamba still faces challenges in precisely
exploiting local dependencies between joints. To address these issues, we
propose a new attention-free hybrid spatiotemporal architecture named Hybrid
Mamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN
by capturing relationships between neighboring joints, thus producing new
representations to complement Mamba's outputs. By adaptively fusing
representations from Mamba and GCN, Pose Magic demonstrates superior capability
in learning the underlying 3D structure. To meet the requirements of real-time
inference, we also provide a fully causal version. Extensive experiments show
that Pose Magic achieves new SOTA results ($\downarrow 0.9 mm$) while saving
$74.1\%$ FLOPs. In addition, Pose Magic exhibits optimal motion consistency and
the ability to generalize to unseen sequence lengths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FourierMamba: Fourier Learning Integration with State Space Models for
  Image Deraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Li, Yidi Liu, Xueyang Fu, Senyan Xu, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image deraining aims to remove rain streaks from rainy images and restore
clear backgrounds. Currently, some research that employs the Fourier transform
has proved to be effective for image deraining, due to it acting as an
effective frequency prior for capturing rain streaks. However, despite there
exists dependency of low frequency and high frequency in images, these
Fourier-based methods rarely exploit the correlation of different frequencies
for conjuncting their learning procedures, limiting the full utilization of
frequency information for image deraining. Alternatively, the recently emerged
Mamba technique depicts its effectiveness and efficiency for modeling
correlation in various domains (e.g., spatial, temporal), and we argue that
introducing Mamba into its unexplored Fourier spaces to correlate different
frequencies would help improve image deraining. This motivates us to propose a
new framework termed FourierMamba, which performs image deraining with Mamba in
the Fourier space. Owning to the unique arrangement of frequency orders in
Fourier space, the core of FourierMamba lies in the scanning encoding of
different frequencies, where the low-high frequency order formats exhibit
differently in the spatial dimension (unarranged in axis) and channel dimension
(arranged in axis). Therefore, we design FourierMamba that correlates Fourier
space information in the spatial and channel dimensions with distinct designs.
Specifically, in the spatial dimension Fourier space, we introduce the zigzag
coding to scan the frequencies to rearrange the orders from low to high
frequencies, thereby orderly correlating the connections between frequencies;
in the channel dimension Fourier space with arranged orders of frequencies in
axis, we can directly use Mamba to perform frequency correlation and improve
the channel information representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15098v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15098v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Feng, Mohammadhossein Bahari, Kaouther Messaoud Ben Amor, Éloi Zablocki, Matthieu Cord, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle trajectory prediction has increasingly relied on data-driven
solutions, but their ability to scale to different data domains and the impact
of larger dataset sizes on their generalization remain under-explored. While
these questions can be studied by employing multiple datasets, it is
challenging due to several discrepancies, e.g., in data formats, map
resolution, and semantic annotation types. To address these challenges, we
introduce UniTraj, a comprehensive framework that unifies various datasets,
models, and evaluation criteria, presenting new opportunities for the vehicle
trajectory prediction field. In particular, using UniTraj, we conduct extensive
experiments and find that model performance significantly drops when
transferred to other datasets. However, enlarging data size and diversity can
substantially improve performance, leading to a new state-of-the-art result for
the nuScenes dataset. We provide insights into dataset characteristics to
explain these findings. The code can be found here:
https://github.com/vita-epfl/UniTraj
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subjective-Aligned <span class="highlight-title">Dataset</span> and Metric for Text-to-Video Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11956v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11956v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min, Guangtao Zhai, Ning Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of generative models, Artificial
Intelligence-Generated Contents (AIGC) have exponentially increased in daily
lives. Among them, Text-to-Video (T2V) generation has received widespread
attention. Though many T2V models have been released for generating high
perceptual quality videos, there is still lack of a method to evaluate the
quality of these videos quantitatively. To solve this issue, we establish the
largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The
dataset is composed of 10,000 videos generated by 9 different T2V models. We
also conduct a subjective study to obtain each video's corresponding mean
opinion score. Based on T2VQA-DB, we propose a novel transformer-based model
for subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model
extracts features from text-video alignment and video fidelity perspectives,
then it leverages the ability of a large language model to give the prediction
score. Experimental results show that T2VQA outperforms existing T2V metrics
and SOTA video quality assessment models. Quantitative analysis indicates that
T2VQA is capable of giving subjective-align predictions, validating its
effectiveness. The dataset and code will be released at
https://github.com/QMME/T2VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Every <span class="highlight-title">Dataset</span> Counts: Scaling up Monocular 3D Object Detection with
  Joint <span class="highlight-title">Dataset</span>s Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00920v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00920v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fulong Ma, Xiaoyang Yan, Guoyang Zhao, Xiaojie Xu, Yuxuan Liu, Ming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D object detection plays a crucial role in autonomous driving.
However, existing monocular 3D detection algorithms depend on 3D labels derived
from LiDAR measurements, which are costly to acquire for new datasets and
challenging to deploy in novel environments. Specifically, this study
investigates the pipeline for training a monocular 3D object detection model on
a diverse collection of 3D and 2D datasets. The proposed framework comprises
three components: (1) a robust monocular 3D model capable of functioning across
various camera settings, (2) a selective-training strategy to accommodate
datasets with differing class annotations, and (3) a pseudo 3D training
approach using 2D labels to enhance detection performance in scenes containing
only 2D labels. With this framework, we could train models on a joint set of
various open 3D/2D datasets to obtain models with significantly stronger
generalization capability and enhanced performance on new dataset with only 2D
labels. We conduct extensive experiments on
KITTI/nuScenes/ONCE/Cityscapes/BDD100K datasets to demonstrate the scaling
ability of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visualize and Paint GAN Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15636v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15636v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudolf Herdt, Peter Maass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate how generated structures of GANs correlate with their
activations in hidden layers, with the purpose of better understanding the
inner workings of those models and being able to paint structures with
unconditionally trained GANs. This gives us more control over the generated
images, allowing to generate them from a semantic segmentation map while not
requiring such a segmentation in the training data. To this end we introduce
the concept of tileable features, allowing us to identify activations that work
well for painting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghyun Kim, Byeongho Heo, Dongyoon Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revives Densely Connected Convolutional Networks (DenseNets) and
reveals the underrated effectiveness over predominant ResNet-style
architectures. We believe DenseNets' potential was overlooked due to untouched
training methods and traditional design elements not fully revealing their
capabilities. Our pilot study shows dense connections through concatenation are
strong, demonstrating that DenseNets can be revitalized to compete with modern
architectures. We methodically refine suboptimal components - architectural
adjustments, block redesign, and improved training recipes towards widening
DenseNets and boosting memory efficiency while keeping concatenation shortcuts.
Our models, employing simple architectural elements, ultimately surpass Swin
Transformer, ConvNeXt, and DeiT-III - key architectures in the residual
learning lineage. Furthermore, our models exhibit near state-of-the-art
performance on ImageNet-1K, competing with the very recent models and
downstream tasks, ADE20k semantic segmentation, and COCO object
detection/instance segmentation. Finally, we provide empirical analyses that
uncover the merits of the concatenation over additive shortcuts, steering a
renewed preference towards DenseNet-style designs. Our code is available at
https://github.com/naver-ai/rdnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Code at https://github.com/naver-ai/rdnet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ New Job, New Gender? Measuring the Social Bias in Image Generation
  Models <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generation models can generate or edit images from a given text. Recent
advancements in image generation technology, exemplified by DALL-E and
Midjourney, have been groundbreaking. These advanced models, despite their
impressive capabilities, are often trained on massive Internet datasets, making
them susceptible to generating content that perpetuates social stereotypes and
biases, which can lead to severe consequences. Prior research on assessing bias
within image generation models suffers from several shortcomings, including
limited accuracy, reliance on extensive human labor, and lack of comprehensive
analysis. In this paper, we propose BiasPainter, a novel evaluation framework
that can accurately, automatically and comprehensively trigger social bias in
image generation models. BiasPainter uses a diverse range of seed images of
individuals and prompts the image generation models to edit these images using
gender, race, and age-neutral queries. These queries span 62 professions, 39
activities, 57 types of objects, and 70 personality traits. The framework then
compares the edited images to the original seed images, focusing on the
significant changes related to gender, race, and age. BiasPainter adopts a key
insight that these characteristics should not be modified when subjected to
neutral prompts. Built upon this design, BiasPainter can trigger the social
bias and evaluate the fairness of image generation models. We use BiasPainter
to evaluate six widely-used image generation models, such as stable diffusion
and Midjourney. Experimental results show that BiasPainter can successfully
trigger social bias in image generation models. According to our human
evaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,
which is significantly higher than the results reported in previous work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM MM 2024 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Region Matching for Multi-Label Image Recognition with Missing
  Labels <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leilei Ma, Hongxing Xie, Lei Wang, Yanping Fu, Dengdi Sun, Haifeng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large-scale visual language pre-trained (VLP) models have
demonstrated impressive performance across various downstream tasks. Motivated
by these advancements, pioneering efforts have emerged in multi-label image
recognition with missing labels, leveraging VLP prompt-tuning technology.
However, they usually cannot match text and vision features well, due to
complicated semantics gaps and missing labels in a multi-label image. To tackle
this challenge, we propose \textbf{T}ext-\textbf{R}egion \textbf{M}atching for
optimizing \textbf{M}ulti-\textbf{L}abel prompt tuning, namely TRM-ML, a novel
method for enhancing meaningful cross-modal matching. Compared to existing
methods, we advocate exploring the information of category-aware regions rather
than the entire image or pixels, which contributes to bridging the semantic gap
between textual and visual representations in a one-to-one matching manner.
Concurrently, we further introduce multimodal contrastive learning to narrow
the semantic gap between textual and visual modalities and establish
intra-class and inter-class relationships. Additionally, to deal with missing
labels, we propose a multimodal category prototype that leverages intra- and
inter-category semantic relationships to estimate unknown labels, facilitating
pseudo-label generation. Extensive experiments on the MS-COCO, PASCAL VOC,
Visual Genome, NUS-WIDE, and CUB-200-211 benchmark datasets demonstrate that
our proposed framework outperforms the state-of-the-art methods by a
significant margin. Our code is available
here\href{https://github.com/yu-gi-oh-leilei/TRM-ML}{\raisebox{-1pt}{\faGithub}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM International Conference on Multimedia (ACM MM) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-based Human Motion Style Transfer with Semantic Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Hu, Zihao Zhang, Yongjing Ye, Yiwen Xu, Shihong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Human motion style transfer is a fundamental problem in computer graphic
and animation processing. Existing AdaIN- based methods necessitate datasets
with balanced style distribution and content/style labels to train the
clustered latent space. However, we may encounter a single unseen style example
in practical scenarios, but not in sufficient quantity to constitute a style
cluster for AdaIN-based methods. Therefore, in this paper, we propose a novel
two-stage framework for few-shot style transfer learning based on the diffusion
model. Specifically, in the first stage, we pre-train a diffusion-based
text-to-motion model as a generative prior so that it can cope with various
content motion inputs. In the second stage, based on the single style example,
we fine-tune the pre-trained diffusion model in a few-shot manner to make it
capable of style transfer. The key idea is regarding the reverse process of
diffusion as a motion-style translation process since the motion styles can be
viewed as special motion variations. During the fine-tuning for style transfer,
a simple yet effective semantic-guided style transfer loss coordinated with
style example reconstruction loss is introduced to supervise the style transfer
in CLIP semantic space. The qualitative and quantitative evaluations
demonstrate that our method can achieve state-of-the-art performance and has
practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengjie Zhu, Zhuo Chen, Jingnan Gao, Yichao Yan, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse rendering methods have achieved remarkable performance in
reconstructing high-fidelity 3D objects with disentangled geometries,
materials, and environmental light. However, they still face huge challenges in
reflective surface reconstruction. Although recent methods model the light
trace to learn specularity, the ignorance of indirect illumination makes it
hard to handle inter-reflections among multiple smooth objects. In this work,
we propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which
comprehensively computes the environmental illumination and meanwhile considers
the reflective light from object surfaces. To address the computation challenge
as the times of Monte Carlo sampling grow, we propose a specularity-adaptive
sampling strategy, significantly reducing the computational complexity. Besides
the computational resource, higher geometry accuracy is also required because
geometric errors accumulate multiple times. Therefore, we further introduce a
reflection-aware surface model to initialize the geometry and refine it during
inverse rendering. We construct a challenging dataset containing scenes with
multiple objects and inter-reflections. Experiments show that our method
outperforms other inverse rendering methods on various object groups. We also
show downstream applications, e.g., relighting and material editing, to
illustrate the disentanglement ability of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,6 figures,NeurIPS 2024 Submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP with Generative Latent Replay: a Strong Baseline for Incremental
  Learning <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Frascaroli, Aniello Panariello, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of Transformers and Vision-Language Models (VLMs) such as
CLIP, large pre-trained models have become a common strategy to enhance
performance in Continual Learning scenarios. This led to the development of
numerous prompting strategies to effectively fine-tune transformer-based models
without succumbing to catastrophic forgetting. However, these methods struggle
to specialize the model on domains significantly deviating from the
pre-training and preserving its zero-shot capabilities. In this work, we
propose Continual Generative training for Incremental prompt-Learning, a novel
approach to mitigate forgetting while adapting a VLM, which exploits generative
replay to align prompts to tasks. We also introduce a new metric to evaluate
zero-shot capabilities within CL benchmarks. Through extensive experiments on
different domains, we demonstrate the effectiveness of our framework in
adapting to new tasks while improving zero-shot capabilities. Further analysis
reveals that our approach can bridge the gap with joint prompt tuning. The
codebase is available at https://github.com/aimagelab/mammoth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figure. Accepted at the The 35th British Machine Vision
  Conference 2024 (BMVC 2024), Glasgow, UK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RCA: Region Conditioned Adaptation for Visual Abductive Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10428v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10428v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Yeo Keat Ee, Basura Fernando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual abductive reasoning aims to make likely explanations for visual
observations. We propose a simple yet effective Region Conditioned Adaptation,
a hybrid parameter-efficient fine-tuning method that equips the frozen CLIP
with the ability to infer explanations from local visual cues. We encode
``local hints'' and ``global contexts'' into visual prompts of the CLIP model
separately at fine and coarse-grained levels. Adapters are used for fine-tuning
CLIP models for downstream tasks and we design a new attention adapter, that
directly steers the focus of the attention map with trainable query and key
projections of a frozen CLIP model. Finally, we train our new model with a
modified contrastive loss to regress the visual feature simultaneously toward
features of literal description and plausible explanations. The loss enables
CLIP to maintain both perception and reasoning abilities. Experiments on the
Sherlock visual abductive reasoning benchmark show that the RCA significantly
outstands previous SOTAs, ranking the \nth{1} on the leaderboards (e.g., Human
Acc: RCA 31.74 \textit{vs} CPT-CLIP 29.58, higher =better). We also validate
the RCA is generalizable to local perception benchmarks like RefCOCO. We
open-source our project at
\textit{\color{magenta}{\url{https://github.com/LUNAProject22/RPA}}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating and Defending Shortcut Learning in Personalized Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18944v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18944v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Ruoxi Chen, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized diffusion models have gained popularity for adapting pre-trained
text-to-image models to generate images of specific topics with minimal
training data. However, these models are vulnerable to minor adversarial
perturbations, leading to degraded performance on corrupted datasets. Such
vulnerabilities are further exploited to craft protective perturbations on
sensitive images like portraits that prevent unauthorized generation. In
response, diffusion-based purification methods have been proposed to remove
these perturbations and retain generation performance. However, existing works
turn to over-purifying the images, which causes information loss. In this
paper, we take a closer look at the fine-tuning process of personalized
diffusion models through the lens of shortcut learning. And we propose a
hypothesis explaining the manipulation mechanisms of existing perturbation
methods, demonstrating that perturbed images significantly deviate from their
original prompts in the CLIP-based latent space. This misalignment during
fine-tuning causes models to associate noisy patterns with identifiers,
resulting in performance degradation. Based on these insights, we introduce a
systematic approach to maintain training performance through purification. Our
method first purifies the images to realign them with their original semantic
meanings in latent space. Then, we introduce contrastive learning with negative
tokens to decouple the learning of clean identities from noisy patterns, which
shows a strong potential capacity against adaptive perturbation. Our study
uncovers shortcut learning vulnerabilities in personalized diffusion models and
provides a firm evaluation framework for future protective perturbation
research. Code is available at https://github.com/liuyixin-louis/DiffShortcut.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Chen, Chenrui Tie, Ruihai Wu, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans perceive and interact with the world with the awareness of
equivariance, facilitating us in manipulating different objects in diverse
poses. For robotic manipulation, such equivariance also exists in many
scenarios. For example, no matter what the pose of a drawer is (translation,
rotation and tilt), the manipulation strategy is consistent (grasp the handle
and pull in a line). While traditional models usually do not have the awareness
of equivariance for robotic manipulation, which might result in more data for
training and poor performance in novel object poses, we propose our EqvAfford
framework, with novel designs to guarantee the equivariance in point-level
affordance learning for downstream robotic manipulation, with great performance
and generalization ability on representative tasks on objects in diverse poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Composed Image Retrieval via Contrastive Learning with Scaling
  Positives and Negatives <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchi Feng, Richong Zhang, Zhijie Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Composed Image Retrieval (CIR) task aims to retrieve target images using
a composed query consisting of a reference image and a modified text. Advanced
methods often utilize contrastive learning as the optimization objective, which
benefits from adequate positive and negative examples. However, the triplet for
CIR incurs high manual annotation costs, resulting in limited positive
examples. Furthermore, existing methods commonly use in-batch negative
sampling, which reduces the negative number available for the model. To address
the problem of lack of positives, we propose a data generation method by
leveraging a multi-modal large language model to construct triplets for CIR. To
introduce more negatives during fine-tuning, we design a two-stage fine-tuning
framework for CIR, whose second stage introduces plenty of static
representations of negatives to optimize the representation space rapidly. The
above two improvements can be effectively stacked and designed to be
plug-and-play, easily applied to existing CIR models without changing their
original architectures. Extensive experiments and ablation analysis demonstrate
that our method effectively scales positives and negatives and achieves
state-of-the-art results on both FashionIQ and CIRR datasets. In addition, our
method also performs well in zero-shot composed image retrieval, providing a
new CIR solution for the low-resources scenario. Our code and data are released
at https://github.com/BUAADreamer/SPN4CIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2024 Regular Papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient
  <span class="highlight-title">Dataset</span> Distillation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18381v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18381v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Yong-Lu Li, Kaitong Cui, Ziyu Wang, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-efficient learning has garnered significant attention, especially given
the current trend of large multi-modal models. Recently, dataset distillation
has become an effective approach by synthesizing data samples that are
essential for network training. However, it remains to be explored which
samples are essential for the dataset distillation process itself. In this
work, we study the data efficiency and selection for the dataset distillation
task. By re-formulating the dynamics of distillation, we provide insight into
the inherent redundancy in the real dataset, both theoretically and
empirically. We propose to use the empirical loss value as a static data
pruning criterion. To further compensate for the variation of the data value in
training, we find the most contributing samples based on their causal effects
on the distillation. The proposed selection strategy can efficiently exploit
the training dataset, outperform the previous SOTA distillation algorithms, and
consistently enhance the distillation algorithms, even on much larger-scale and
more heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We
believe this paradigm will open up new avenues in the dynamics of distillation
and pave the way for efficient dataset distillation. Our code is available on
https://github.com/silicx/GoldFromOres-BiLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Backbone for Long-Horizon Robot Task Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuai Chen, Wei Chen, Dongmyoung Lee, Yukun Ge, Nicolas Rojas, Petar Kormushev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end robot learning, particularly for long-horizon tasks, often results
in unpredictable outcomes and poor generalization. To address these challenges,
we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot
task understanding and transferability. This framework uses therbligs (basic
action elements) as the backbone to decompose high-level robot tasks into
elemental robot configurations, which are then integrated with current
foundation models to improve task understanding. The approach consists of two
stages: offline training and online testing. During the offline training stage,
we developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig
segmentation across various tasks. In the online testing stage, after a
one-shot demonstration of a new task is collected, our MGSF network extracts
high-level knowledge, which is then encoded into the image using Action
Registration (ActionREG). Additionally, the Large Language Model
(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure
precise action execution, facilitating trajectory transfer in novel robot
scenarios. Experimental results validate these methods, achieving 94.37% recall
in therblig segmentation and success rates of 94.4% and 80% in real-world
online robot testing for simple and complex scenarios, respectively.
Supplementary material is available at:
https://sites.google.com/view/therbligsbasedbackbone/home
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures. This work is intended to be submitted to IEEE
  Robotics and Automation Letters (RA-L) for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic-guided modeling of spatial relation and object co-occurrence
  for indoor scene recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12661v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12661v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanxin Song, Hanbo Wu, Xin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring the semantic context in scene images is essential for indoor scene
recognition. However, due to the diverse intra-class spatial layouts and the
coexisting inter-class objects, modeling contextual relationships to adapt
various image characteristics is a great challenge. Existing contextual
modeling methods for scene recognition exhibit two limitations: 1) They
typically model only one type of spatial relationship (order or metric) among
objects within scenes, with limited exploration of diverse spatial layouts. 2)
They often overlook the differences in coexisting objects across different
scenes, suppressing scene recognition performance. To overcome these
limitations, we propose SpaCoNet, which simultaneously models Spatial relation
and Co-occurrence of objects guided by semantic segmentation. Firstly, the
Semantic Spatial Relation Module (SSRM) is constructed to model scene spatial
features. With the help of semantic segmentation, this module decouples spatial
information from the scene image and thoroughly explores all spatial
relationships among objects in an end-to-end manner, thereby obtaining
semantic-based spatial features. Secondly, both spatial features from the SSRM
and deep features from the Image Feature Extraction Module are allocated to
each object, so as to distinguish the coexisting object across different
scenes. Finally, utilizing the discriminative features above, we design a
Global-Local Dependency Module to explore the long-range co-occurrence among
objects, and further generate a semantic-guided feature representation for
indoor scene recognition. Experimental results on three widely used scene
datasets demonstrate the effectiveness and generality of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under second review at Expert Systems with Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperKAN: Kolmogorov-Arnold Networks make Hyperspectral Image
  Classificators Smarter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valeriy Lobanov, Nikita Firsov, Evgeny Myasnikov, Roman Khabibullin, Artem Nikonorov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In traditional neural network architectures, a multilayer perceptron (MLP) is
typically employed as a classification block following the feature extraction
stage. However, the Kolmogorov-Arnold Network (KAN) presents a promising
alternative to MLP, offering the potential to enhance prediction accuracy. In
this paper, we propose the replacement of linear and convolutional layers of
traditional networks with KAN-based counterparts. These modifications allowed
us to significantly increase the per-pixel classification accuracy for
hyperspectral remote-sensing images. We modified seven different neural network
architectures for hyperspectral image classification and observed a substantial
improvement in the classification accuracy across all the networks. The
architectures considered in the paper include baseline MLP, state-of-the-art 1D
(1DCNN) and 3D convolutional (two different 3DCNN, NM3DCNN), and transformer
(SSFTT) architectures, as well as newly proposed M1DCNN. The greatest effect
was achieved for convolutional networks working exclusively on spectral data,
and the best classification quality was achieved using a KAN-based transformer
architecture. All the experiments were conducted using seven openly available
hyperspectral datasets. Our code is available at
https://github.com/f-neumann77/HyperKAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse
  Training Data <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Lin, Reinhard Heckel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based methods for image reconstruction are state-of-the-art for
a variety of imaging tasks. However, neural networks often perform worse if the
training data differs significantly from the data they are applied to. For
example, a model trained for accelerated magnetic resonance imaging (MRI) on
one scanner performs worse on another scanner. In this work, we investigate the
impact of the training data on a model's performance and robustness for
accelerated MRI. We find that models trained on the combination of various data
distributions, such as those obtained from different MRI scanners and
anatomies, exhibit robustness equal or superior to models trained on the best
single distribution for a specific target distribution. Thus training on such
diverse data tends to improve robustness. Furthermore, training on such a
diverse dataset does not compromise in-distribution performance, i.e., a model
trained on diverse data yields in-distribution performance at least as good as
models trained on the more narrow individual distributions. Our results suggest
that training a model for imaging on a variety of distributions tends to yield
a more effective and robust model than maintaining separate models for
individual distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Driving Animatronic Robot Facial Expression From Speech <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12670v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12670v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boren Li, Hang Li, Hangxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animatronic robots hold the promise of enabling natural human-robot
interaction through lifelike facial expressions. However, generating realistic,
speech-synchronized robot expressions poses significant challenges due to the
complexities of facial biomechanics and the need for responsive motion
synthesis. This paper introduces a novel, skinning-centric approach to drive
animatronic robot facial expressions from speech input. At its core, the
proposed approach employs linear blend skinning (LBS) as a unifying
representation, guiding innovations in both embodiment design and motion
synthesis. LBS informs the actuation topology, facilitates human expression
retargeting, and enables efficient speech-driven facial motion generation. This
approach demonstrates the capability to produce highly realistic facial
expressions on an animatronic face in real-time at over 4000 fps on a single
Nvidia RTX 4090, significantly advancing robots' ability to replicate nuanced
human expressions for natural interaction. To foster further research and
development in this field, the code has been made publicly available at:
\url{https://github.com/library87/OpenRoboExp}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, accepted to IROS 2024. For associated project
  page, see https://library87.github.io/animatronic-face-iros24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classifying Dry Eye Disease Patients from Healthy Controls Using Machine
  Learning and Metabolomics Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Amouei Sheshkal, Morten Gundersen, Michael Alexander Riegler, Øygunn Aass Utheim, Kjell Gunnar Gundersen, Hugo Lewi Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dry eye disease is a common disorder of the ocular surface, leading patients
to seek eye care. Clinical signs and symptoms are currently used to diagnose
dry eye disease. Metabolomics, a method for analyzing biological systems, has
been found helpful in identifying distinct metabolites in patients and in
detecting metabolic profiles that may indicate dry eye disease at early stages.
In this study, we explored using machine learning and metabolomics information
to identify which cataract patients suffered from dry eye disease. As there is
no one-size-fits-all machine learning model for metabolomics data, choosing the
most suitable model can significantly affect the quality of predictions and
subsequent metabolomics analyses. To address this challenge, we conducted a
comparative analysis of nine machine learning models on three metabolomics data
sets from cataract patients with and without dry eye disease. The models were
evaluated and optimized using nested k-fold cross-validation. To assess the
performance of these models, we selected a set of suitable evaluation metrics
tailored to the data set's challenges. The logistic regression model overall
performed the best, achieving the highest area under the curve score of 0.8378,
balanced accuracy of 0.735, Matthew's correlation coefficient of 0.5147, an
F1-score of 0.8513, and a specificity of 0.5667. Additionally, following the
logistic regression, the XGBoost and Random Forest models also demonstrated
good performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IG-SLAM: Instant Gaussian SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01126v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01126v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        F. Aykut Sarikamis, A. Aydin Alatan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting has recently shown promising results as an alternative
scene representation in SLAM systems to neural implicit representations.
However, current methods either lack dense depth maps to supervise the mapping
process or detailed training designs that consider the scale of the
environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only
SLAM system that employs robust Dense-SLAM methods for tracking and combines
them with Gaussian Splatting. A 3D map of the environment is constructed using
accurate pose and dense depth provided by tracking. Additionally, we utilize
depth uncertainty in map optimization to improve 3D reconstruction. Our decay
strategy in map optimization enhances convergence and allows the system to run
at 10 fps in a single process. We demonstrate competitive performance with
state-of-the-art RGB-only SLAM systems while achieving faster operation speeds.
We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC
datasets. The system achieves photo-realistic 3D reconstruction in large-scale
sequences, particularly in the EuRoC dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 page ref, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with
  Perturbation Strategies and Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Zhao, Chunshi Wang, Shuxue Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning for medical image segmentation presents a unique
challenge of efficiently using limited labeled data while leveraging abundant
unlabeled data. Despite advancements, existing methods often do not fully
exploit the potential of the unlabeled data for enhancing model robustness and
accuracy. In this paper, we introduce CrossMatch, a novel framework that
integrates knowledge distillation with dual perturbation strategies-image-level
and feature-level-to improve the model's learning from both labeled and
unlabeled data. CrossMatch employs multiple encoders and decoders to generate
diverse data streams, which undergo self-knowledge distillation to enhance
consistency and reliability of predictions across varied perturbations. Our
method significantly surpasses other state-of-the-art techniques in standard
benchmarks by effectively minimizing the gap between training on labeled and
unlabeled data and improving edge accuracy and generalization in medical image
segmentation. The efficacy of CrossMatch is demonstrated through extensive
experimental validations, showing remarkable performance improvements without
increasing computational costs. Code for this implementation is made available
at https://github.com/AiEson/CrossMatch.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMInstruct: A High-Quality Multi-Modal Instruction Tuning <span class="highlight-title">Dataset</span> with
  Extensive Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangzhou Liu, Yue Cao, Zhangwei Gao, Weiyun Wang, Zhe Chen, Wenhai Wang, Hao Tian, Lewei Lu, Xizhou Zhu, Tong Lu, Yu Qiao, Jifeng Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the effectiveness of vision-language supervised fine-tuning in
enhancing the performance of Vision Large Language Models (VLLMs). However,
existing visual instruction tuning datasets include the following limitations:
(1) Instruction annotation quality: despite existing VLLMs exhibiting strong
performance, instructions generated by those advanced VLLMs may still suffer
from inaccuracies, such as hallucinations. (2) Instructions and image
diversity: the limited range of instruction types and the lack of diversity in
image data may impact the model's ability to generate diversified and closer to
real-world scenarios outputs. To address these challenges, we construct a
high-quality, diverse visual instruction tuning dataset MMInstruct, which
consists of 973K instructions from 24 domains. There are four instruction
types: Judgement, Multiple-Choice, Long Visual Question Answering and Short
Visual Question Answering. To construct MMInstruct, we propose an instruction
generation data engine that leverages GPT-4V, GPT-3.5, and manual correction.
Our instruction generation engine enables semi-automatic, low-cost, and
multi-domain instruction generation at 1/6 the cost of manual construction.
Through extensive experiment validation and ablation experiments, we
demonstrate that MMInstruct could significantly improve the performance of
VLLMs, e.g., the model fine-tuning on MMInstruct achieves new state-of-the-art
performance on 10 out of 12 benchmarks. The code and data shall be available at
https://github.com/yuecao0119/MMInstruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Affect Analysis: A Protocol for Ensuring Fairness and
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyu Hu, Dimitrios Kollias, Eleni Papadopoulou, Paraskevi Tzouveli, Jie Wei, Xinyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating affect analysis methods presents challenges due to inconsistencies
in database partitioning and evaluation protocols, leading to unfair and biased
results. Previous studies claim continuous performance improvements, but our
findings challenge such assertions. Using these insights, we propose a unified
protocol for database partitioning that ensures fairness and comparability. We
provide detailed demographic annotations (in terms of race, gender and age),
evaluation metrics, and a common framework for expression recognition, action
unit detection and valence-arousal estimation. We also rerun the methods with
the new protocol and introduce a new leaderboards to encourage future research
in affect recognition with a fairer comparison. Our annotations, code, and
pre-trained models are available on
\hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.06841</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FA-Depth: Toward Fast and Accurate <span class="highlight-title">Self-supervised</span> Monocular Depth
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Jun Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing methods often rely on complex models to predict scene depth
with high accuracy, resulting in slow inference that is not conducive to
deployment. To better balance precision and speed, we first designed SmallDepth
based on sparsity. Second, to enhance the feature representation ability of
SmallDepth during training under the condition of equal complexity during
inference, we propose an equivalent transformation module(ETM). Third, to
improve the ability of each layer in the case of a fixed SmallDepth to perceive
different context information and improve the robustness of SmallDepth to the
left-right direction and illumination changes, we propose pyramid loss. Fourth,
to further improve the accuracy of SmallDepth, we utilized the proposed
function approximation loss (APX) to transfer knowledge in the pretrained
HQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in
some regions, to SmallDepth. Extensive experiments demonstrate that each
proposed component improves the precision of SmallDepth without changing the
complexity of SmallDepth during inference, and the developed approach achieves
state-of-the-art results on KITTI at an inference speed of more than 500 frames
per second and with approximately 2 M parameters. The code and models will be
publicly available at https://github.com/fwucas/FA-Depth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pose-Aware <span class="highlight-title">Self-Supervised</span> Learning with Viewpoint Trajectory
  Regularization <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayun Wang, Yubei Chen, Stella X. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning visual features from unlabeled images has proven successful for
semantic categorization, often by mapping different $views$ of the same object
to the same feature to achieve recognition invariance. However, visual
recognition involves not only identifying $what$ an object is but also
understanding $how$ it is presented. For example, seeing a car from the side
versus head-on is crucial for deciding whether to stay put or jump out of the
way. While unsupervised feature learning for downstream viewpoint reasoning is
important, it remains under-explored, partly due to the lack of a standardized
evaluation method and benchmarks.
  We introduce a new dataset of adjacent image triplets obtained from a
viewpoint trajectory, without any semantic or pose labels. We benchmark both
semantic classification and pose estimation accuracies on the same visual
feature. Additionally, we propose a viewpoint trajectory regularization loss
for learning features from unlabeled image triplets. Our experiments
demonstrate that this approach helps develop a visual representation that
encodes object identity and organizes objects by their poses, retaining
semantic classification accuracy while achieving emergent global pose awareness
and better generalization to novel objects. Our dataset and code are available
at http://pwang.pw/trajSSL/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Face of Populism: Examining Differences in Facial Emotional
  Expressions of Political Leaders Using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09914v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09914v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Major, Aleksandar Tomašević
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Populist rhetoric employed on online media is characterized as deeply
impassioned and often imbued with strong emotions. The aim of this paper is to
empirically investigate the differences in affective nonverbal communication of
political leaders. We use a deep-learning approach to process a sample of 220
YouTube videos of political leaders from 15 different countries, analyze their
facial expressions of emotion and then examine differences in average emotion
scores representing the relative presence of 6 emotional states (anger,
disgust, fear, happiness, sadness, and surprise) and a neutral expression for
each frame of the YouTube video. Based on a sample of manually coded images, we
find that this deep-learning approach has 53-60\% agreement with human labels.
We observe statistically significant differences in the average score of
negative emotions between groups of leaders with varying degrees of populist
rhetoric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 4.0: Annotation study added, supplementary information
  extended</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QClusformer: A Quantum <span class="highlight-title">Transformer</span>-based Framework for Unsupervised
  Visual Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Bac Nguyen, Hoang-Quan Nguyen, Samuel Yen-Chi Chen, Samee U. Khan, Hugh Churchill, Khoa Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised vision clustering, a cornerstone in computer vision, has been
studied for decades, yielding significant outcomes across numerous vision
tasks. However, these algorithms involve substantial computational demands when
confronted with vast amounts of unlabeled data. Conversely, quantum computing
holds promise in expediting unsupervised algorithms when handling large-scale
databases. In this study, we introduce QClusformer, a pioneering
Transformer-based framework leveraging quantum machines to tackle unsupervised
vision clustering challenges. Specifically, we design the Transformer
architecture, including the self-attention module and transformer blocks, from
a quantum perspective to enable execution on quantum hardware. In addition, we
present QClusformer, a variant based on the Transformer architecture, tailored
for unsupervised vision clustering tasks. By integrating these elements into an
end-to-end framework, QClusformer consistently outperforms previous methods
running on classical computers. Empirical evaluations across diverse
benchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior
performance of QClusformer compared to state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thermal Imaging and Radar for Remote Sleep Monitoring of Breathing and
  Apnea 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Del Regno, Alexander Vilesov, Adnan Armouti, Anirudh Bindiganavale Harish, Selim Emir Can, Ashley Kita, Achuta Kadambi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polysomnography (PSG), the current gold standard method for monitoring and
detecting sleep disorders, is cumbersome and costly. At-home testing solutions,
known as home sleep apnea testing (HSAT), exist. However, they are
contact-based, a feature which limits the ability of some patient populations
to tolerate testing and discourages widespread deployment. Previous work on
non-contact sleep monitoring for sleep apnea detection either estimates
respiratory effort using radar or nasal airflow using a thermal camera, but has
not compared the two or used them together. We conducted a study on 10
participants, ages 34 - 78, with suspected sleep disorders using a hardware
setup with a synchronized radar and thermal camera. We show the first
comparison of radar and thermal imaging for sleep monitoring, and find that our
thermal imaging method outperforms radar significantly. Our thermal imaging
method detects apneas with an accuracy of 0.99, a precision of 0.68, a recall
of 0.74, an F1 score of 0.71, and an intra-class correlation of 0.70; our radar
method detects apneas with an accuracy of 0.83, a precision of 0.13, a recall
of 0.86, an F1 score of 0.22, and an intra-class correlation of 0.13. We also
present a novel proposal for classifying obstructive and central sleep apnea by
leveraging a multimodal setup. This method could be used accurately detect and
classify apneas during sleep with non-contact sensors, thereby improving
diagnostic capacities in patient populations unable to tolerate current
technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-<span class="highlight-title">Prompt</span>ing for Automating Zero-shot Visual Recognition with LLMs <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11755v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11755v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub Micorek, Mateusz Kozinski, Hilde Kuehne, Horst Possegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt ensembling of Large Language Model (LLM) generated category-specific
prompts has emerged as an effective method to enhance zero-shot recognition
ability of Vision-Language Models (VLMs). To obtain these category-specific
prompts, the present methods rely on hand-crafting the prompts to the LLMs for
generating VLM prompts for the downstream tasks. However, this requires
manually composing these task-specific prompts and still, they might not cover
the diverse set of visual concepts and task-specific styles associated with the
categories of interest. To effectively take humans out of the loop and
completely automate the prompt generation process for zero-shot recognition, we
propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only
minimal information about the target task, in the form of its short natural
language description, and a list of associated class labels, MPVR automatically
produces a diverse set of category-specific prompts resulting in a strong
zero-shot classifier. MPVR generalizes effectively across various popular
zero-shot image recognition benchmarks belonging to widely different domains
when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot
recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on
average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV Camera Ready. Code & Data:
  https://jmiemirza.github.io/Meta-Prompting/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S^2Former-OR: Single-Stage Bi-Modal <span class="highlight-title">Transformer</span> for Scene Graph
  Generation in OR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialun Pei, Diandian Guo, Jingyang Zhang, Manxi Lin, Yueming Jin, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene graph generation (SGG) of surgical procedures is crucial in enhancing
holistically cognitive intelligence in the operating room (OR). However,
previous works have primarily relied on multi-stage learning, where the
generated semantic scene graphs depend on intermediate processes with pose
estimation and object detection. This pipeline may potentially compromise the
flexibility of learning multimodal representations, consequently constraining
the overall effectiveness. In this study, we introduce a novel single-stage
bi-modal transformer framework for SGG in the OR, termed S^2Former-OR, aimed to
complementally leverage multi-view 2D scenes and 3D point clouds for SGG in an
end-to-end manner. Concretely, our model embraces a View-Sync Transfusion
scheme to encourage multi-view visual information interaction. Concurrently, a
Geometry-Visual Cohesion operation is designed to integrate the synergic 2D
semantic features into 3D point cloud features. Moreover, based on the
augmented feature, we propose a novel relation-sensitive transformer decoder
that embeds dynamic entity-pair queries and relational trait priors, which
enables the direct prediction of entity-pair relations for graph generation
without intermediate steps. Extensive experiments have validated the superior
SGG performance and lower computational cost of S^2Former-OR on 4D-OR
benchmark, compared with current OR-SGG methods, e.g., 3 percentage points
Precision increase and 24.2M reduction in model parameters. We further compared
our method with generic single-stage SGG methods with broader metrics for a
comprehensive evaluation, with consistently better performance achieved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by TMI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EgoNav: Egocentric Scene-aware Human Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19026v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19026v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhuo Wang, C. Karen Liu, Monroe Kennedy III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wearable collaborative robots stand to assist human wearers who need fall
prevention assistance or wear exoskeletons. Such a robot needs to be able to
constantly adapt to the surrounding scene based on egocentric vision, and
predict the ego motion of the wearer. In this work, we leveraged body-mounted
cameras and sensors to anticipate the trajectory of human wearers through
complex surroundings. To facilitate research in ego-motion prediction, we have
collected a comprehensive walking scene navigation dataset centered on the
user's perspective. We then present a method to predict human motion
conditioning on the surrounding static scene. Our method leverages a diffusion
model to produce a distribution of potential future trajectories, taking into
account the user's observation of the environment. To that end, we introduce a
compact representation to encode the user's visual memory of the surroundings,
as well as an efficient sample-generating technique to speed up real-time
inference of a diffusion model. We ablate our model and compare it to
baselines, and results show that our model outperforms existing methods on key
metrics of collision avoidance and trajectory mode coverage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fingerprinting Image-to-Image Generative Adversarial Networks <span class="chip">EuroS&P 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.11760v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.11760v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanlin Li, Guowen Xu, Han Qiu, Shangwei Guo, Run Wang, Jiwei Li, Tianwei Zhang, Rongxing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have been widely used in various
application scenarios. Since the production of a commercial GAN requires
substantial computational and human resources, the copyright protection of GANs
is urgently needed. This paper presents a novel fingerprinting scheme for the
Intellectual Property (IP) protection of image-to-image GANs based on a trusted
third party. We break through the stealthiness and robustness bottlenecks
suffered by previous fingerprinting methods for classification models being
naively transferred to GANs. Specifically, we innovatively construct a
composite deep learning model from the target GAN and a classifier. Then we
generate fingerprint samples from this composite model, and embed them in the
classifier for effective ownership verification. This scheme inspires some
concrete methodologies to practically protect the modern image-to-image
translation GANs. Theoretical analysis proves that these methods can satisfy
different security requirements necessary for IP protection. We also conduct
extensive experiments to show that our solutions outperform existing
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EuroS&P 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum Learning for ab initio Deep Learned Refractive Optics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01089v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01089v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinge Yang, Qiang Fu, Wolfgang Heidrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep optical optimization has recently emerged as a new paradigm for
designing computational imaging systems using only the output image as the
objective. However, it has been limited to either simple optical systems
consisting of a single element such as a diffractive optical element (DOE) or
metalens, or the fine-tuning of compound lenses from good initial designs. Here
we present a DeepLens design method based on curriculum learning, which is able
to learn optical designs of compound lenses ab initio from randomly initialized
surfaces without human intervention, therefore overcoming the need for a good
initial design. We demonstrate the effectiveness of our approach by fully
automatically designing both classical imaging lenses and a large field-of-view
extended depth-of-field computational lens in a cellphone-style form factor,
with highly aspheric surfaces and a short back focal length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Automatically design computational lenses from scratch with
  differentiable ray tracing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoadBEV: Road Surface Reconstruction in Bird's Eye View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06605v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06605v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zhao, Lei Yang, Yichen Xie, Mingyu Ding, Masayoshi Tomizuka, Yintao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road surface conditions, especially geometry profiles, enormously affect
driving performance of autonomous vehicles. Vision-based online road
reconstruction promisingly captures road information in advance. Existing
solutions like monocular depth estimation and stereo matching suffer from
modest performance. The recent technique of Bird's-Eye-View (BEV) perception
provides immense potential to more reliable and accurate reconstruction. This
paper uniformly proposes two simple yet effective models for road elevation
reconstruction in BEV named RoadBEV-mono and RoadBEV-stereo, which estimate
road elevation with monocular and stereo images, respectively. The former
directly fits elevation values based on voxel features queried from image view,
while the latter efficiently recognizes road elevation patterns based on BEV
volume representing correlation between left and right voxel features.
Insightful analyses reveal their consistence and difference with the
perspective view. Experiments on real-world dataset verify the models'
effectiveness and superiority. Elevation errors of RoadBEV-mono and
RoadBEV-stereo achieve 1.83 cm and 0.50 cm, respectively. Our models are
promising for practical road preview, providing essential information for
promoting safety and comfort of autonomous vehicles. The code is released at
https://github.com/ztsrxh/RoadBEV
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TITS https://ieeexplore.ieee.org/document/10618926</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial-Frequency Discriminability for Revealing Adversarial
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Shuren Qi, Zhiqiu Huang, Yushu Zhang, Rushi Lan, Xiaochun Cao, Feng-Lei Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vulnerability of deep neural networks to adversarial perturbations has
been widely perceived in the computer vision community. From a security
perspective, it poses a critical risk for modern vision systems, e.g., the
popular Deep Learning as a Service (DLaaS) frameworks. For protecting deep
models while not modifying them, current algorithms typically detect
adversarial patterns through discriminative decomposition for natural and
adversarial data. However, these decompositions are either biased towards
frequency resolution or spatial resolution, thus failing to capture adversarial
patterns comprehensively. Also, when the detector relies on few fixed features,
it is practical for an adversary to fool the model while evading the detector
(i.e., defense-aware attack). Motivated by such facts, we propose a
discriminative detector relying on a spatial-frequency Krawtchouk
decomposition. It expands the above works from two aspects: 1) the introduced
Krawtchouk basis provides better spatial-frequency discriminability, capturing
the differences between natural and adversarial data comprehensively in both
spatial and frequency distributions, w.r.t. the common trigonometric or wavelet
basis; 2) the extensive features formed by the Krawtchouk decomposition allows
for adaptive feature selection and secrecy mechanism, significantly increasing
the difficulty of the defense-aware attack, w.r.t. the detector with few fixed
features. Theoretical and numerical analyses demonstrate the uniqueness and
usefulness of our detector, exhibiting competitive scores on several deep
models and image sets against a variety of adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MobileFlow: A Multimodal LLM For Mobile GUI Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songqin Nong, Jiali Zhu, Rui Wu, Jiongchao Jin, Shuo Shan, Xiutian Huang, Wenhao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, the integration of mobile Graphical User Interfaces (GUIs) is
ubiquitous in most people's daily lives. And the ongoing evolution of
multimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly
bolstered the capabilities of GUI comprehension and user action analysis,
showcasing the potentiality of intelligent GUI assistants. However, current GUI
Agents often need to access page layout information through calling system
APIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a
certain low resolution might result in the loss of fine-grained image details.
At the same time, the multimodal large models built for GUI Agents currently
have poor understanding and decision-making abilities for Chinese GUI
interfaces, making them difficult to apply to a large number of Chinese apps.
This paper introduces MobileFlow, a multimodal large language model
meticulously crafted for mobile GUI agents. Transforming from the open-source
model Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21
billion parameters and is equipped with novel hybrid visual encoders, making it
possible for variable resolutions of image inputs and good support for
multilingual GUI. By incorporating Mixture of Experts (MoE) expansions and
pioneering alignment training strategies, MobileFlow has the capacity to fully
interpret image data and comprehend user instructions for GUI interaction
tasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task
execution by GUI agents on both public and our proposed evaluation metrics, and
has been successfully deployed in real-world business contexts, proving its
effectiveness for practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18403v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18403v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional
performance across various tasks through fine-tuning. Although low-rank
adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream
tasks, their deployment is still hindered by the vast model scale and
computational costs. Post-training model pruning offers a way to compress LLMs.
However, the current pruning methods designed for LLMs are not compatible with
LoRA. This is due to their utilization of unstructured pruning on LLMs,
impeding the merging of LoRA weights, or their dependence on the gradients of
pre-trained weights to guide pruning, which can impose significant memory
overhead. To this end, we propose LoRAPrune, a new framework that delivers an
accurate structured pruned model in a highly memory-efficient manner.
Specifically, we first design a LoRA-guided pruning criterion, which uses the
weights and gradients of LoRA, rather than the gradients of pre-trained weights
for importance estimation. We subsequently integrate this criterion into an
iterative pruning process, effectively removing redundant channels and heads.
Extensive experimental results demonstrate the superior performance of our
LoRAPrune over existing approaches on the LLaMA series models. At a 50\%
compression rate, LoRAPrune demonstrates superior performance over LLM-Pruner,
achieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while
also decreasing memory usage by 52.6%. Besides, LoRAPrune also matches
semi-structural pruning across multiple LLMs, proving its wide applicability.
The code is available at https://github.com/aim-uofa/LoRAPrune.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by acl 2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cascaded Multi-path Shortcut Diffusion Model for Medical Image
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinchi Zhou, Tianqi Chen, Jun Hou, Huidong Xie, Nicha C. Dvornek, S. Kevin Zhou, David L. Wilson, James S. Duncan, Chi Liu, Bo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation is a vital component in medical imaging
processing, with many uses in a wide range of imaging modalities and clinical
scenarios. Previous methods include Generative Adversarial Networks (GANs) and
Diffusion Models (DMs), which offer realism but suffer from instability and
lack uncertainty estimation. Even though both GAN and DM methods have
individually exhibited their capability in medical image translation tasks, the
potential of combining a GAN and DM to further improve translation performance
and to enable uncertainty estimation remains largely unexplored. In this work,
we address these challenges by proposing a Cascade Multi-path Shortcut
Diffusion Model (CMDM) for high-quality medical image translation and
uncertainty estimation. To reduce the required number of iterations and ensure
robust performance, our method first obtains a conditional GAN-generated prior
image that will be used for the efficient reverse translation with a DM in the
subsequent step. Additionally, a multi-path shortcut diffusion strategy is
employed to refine translation results and estimate uncertainty. A cascaded
pipeline further enhances translation quality, incorporating residual averaging
between cascades. We collected three different medical image datasets with two
sub-tasks for each dataset to test the generalizability of our approach. Our
experimental results found that CMDM can produce high-quality translations
comparable to state-of-the-art methods while providing reasonable uncertainty
estimations that correlate well with the translation error.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Medical Image Analysis Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASR: Attention-alike Structural Re-parameterization <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The structural re-parameterization (SRP) technique is a novel deep learning
technique that achieves interconversion between different network architectures
through equivalent parameter transformations. This technique enables the
mitigation of the extra costs for performance improvement during training, such
as parameter size and inference time, through these transformations during
inference, and therefore SRP has great potential for industrial and practical
applications. The existing SRP methods have successfully considered many
commonly used architectures, such as normalizations, pooling methods, and
multi-branch convolution. However, the widely used attention modules which
drastically slow inference speed cannot be directly implemented by SRP due to
these modules usually act on the backbone network in a multiplicative manner
and the modules' output is input-dependent during inference, which limits the
application scenarios of SRP. In this paper, we conduct extensive experiments
from a statistical perspective and discover an interesting phenomenon Stripe
Observation, which reveals that channel attention values quickly approach some
constant vectors during training. This observation inspires us to propose a
simple-yet-effective attention-alike structural re-parameterization (ASR) that
allows us to achieve SRP for a given network while enjoying the effectiveness
of the attention mechanism. Extensive experiments conducted on several standard
benchmarks demonstrate the effectiveness of ASR in generally improving the
performance of existing backbone networks, attention modules, and SRP methods
without any elaborated model crafting. We also analyze the limitations and
provide experimental and theoretical evidence for the strong robustness of the
proposed ASR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imperative Learning: A <span class="highlight-title">Self-supervised</span> Neural-Symbolic Learning
  Framework for Robot Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16087v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16087v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Wang, Kaiyi Ji, Junyi Geng, Zhongqiang Ren, Taimeng Fu, Fan Yang, Yifan Guo, Haonan He, Xiangyu Chen, Zitong Zhan, Qiwei Du, Shaoshu Su, Bowen Li, Yuheng Qiu, Yi Du, Qihang Li, Yifan Yang, Xiao Lin, Zhipeng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven methods such as reinforcement and imitation learning have
achieved remarkable success in robot autonomy. However, their data-centric
nature still hinders them from generalizing well to ever-changing environments.
Moreover, collecting large datasets for robotic tasks is often impractical and
expensive. To overcome these challenges, we introduce a new self-supervised
neural-symbolic (NeSy) computational framework, imperative learning (IL), for
robot autonomy, leveraging the generalization abilities of symbolic reasoning.
The framework of IL consists of three primary components: a neural module, a
reasoning engine, and a memory system. We formulate IL as a special bilevel
optimization (BLO), which enables reciprocal learning over the three modules.
This overcomes the label-intensive obstacles associated with data-driven
approaches and takes advantage of symbolic reasoning concerning logical
reasoning, physical principles, geometric analysis, etc. We discuss several
optimization techniques for IL and verify their effectiveness in five distinct
robot autonomy tasks including path planning, rule induction, optimal control,
visual odometry, and multi-robot routing. Through various experiments, we show
that IL can significantly enhance robot autonomy capabilities and we anticipate
that it will catalyze further research across diverse domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised domain adaptation for building extraction from off-nadir
  aerial images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bipul Neupane, Jagannath Aryal, Abbas Rajabifard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building extraction $-$ needed for inventory management and planning of urban
environment $-$ is affected by the misalignment between labels and off-nadir
source imagery in training data. Teacher-Student learning of noise-tolerant
convolutional neural networks (CNNs) is the existing solution, but the Student
networks typically have lower accuracy and cannot surpass the Teacher's
performance. This paper proposes a supervised domain adaptation (SDA) of
encoder-decoder networks (EDNs) between noisy and clean datasets to tackle the
problem. EDNs are configured with high-performing lightweight encoders such as
EfficientNet, ResNeSt, and MobileViT. The proposed method is compared against
the existing Teacher-Student learning methods like knowledge distillation (KD)
and deep mutual learning (DML) with three newly developed datasets. The methods
are evaluated for different urban buildings (low-rise, mid-rise, high-rise, and
skyscrapers), where misalignment increases with the increase in building height
and spatial resolution. For a robust experimental design, 43 lightweight CNNs,
five optimisers, nine loss functions, and seven EDNs are benchmarked to obtain
the best-performing EDN for SDA. The SDA of the best-performing EDN from our
study significantly outperformed KD and DML with up to 0.943, 0.868, 0.912, and
0.697 F1 scores in the low-rise, mid-rise, high-rise, and skyscrapers
respectively. The proposed method and the experimental findings will be
beneficial in training robust CNNs for building extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to Elsevier for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable
  Primitive Assembly <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00875v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00875v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a differentiable rendering framework to learn structured 3D
abstractions in the form of primitive assemblies from sparse RGB images
capturing a 3D object. By leveraging differentiable volume rendering, our
method does not require 3D supervision. Architecturally, our network follows
the general pipeline of an image-conditioned neural radiance field (NeRF)
exemplified by pixelNeRF for color prediction. As our core contribution, we
introduce differential primitive assembly (DPA) into NeRF to output a 3D
occupancy field in place of density prediction, where the predicted occupancies
serve as opacity values for volume rendering. Our network, coined DPA-Net,
produces a union of convexes, each as an intersection of convex quadric
primitives, to approximate the target 3D object, subject to an abstraction loss
and a masking loss, both defined in the image space upon volume rendering. With
test-time adaptation and additional sampling and loss designs aimed at
improving the accuracy and compactness of the obtained assemblies, our method
demonstrates superior performance over state-of-the-art alternatives for 3D
primitive abstraction from sparse views.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, accepted to ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Augmentation Framework for Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15068v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15068v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Lin, Yaping Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation methods are commonly integrated into the training of
anomaly detection models. Previous approaches have primarily focused on
replicating real-world anomalies or enhancing diversity, without considering
that the standard of anomaly varies across different classes, potentially
leading to a biased training distribution.This paper analyzes crucial traits of
simulated anomalies that contribute to the training of reconstructive networks
and condenses them into several methods, thus creating a comprehensive
framework by selectively utilizing appropriate combinations.Furthermore, we
integrate this framework with a reconstruction-based approach and concurrently
propose a split training strategy that alleviates the issue of overfitting
while avoiding introducing interference to the reconstruction process. The
evaluations conducted on the MVTec anomaly detection dataset demonstrate that
our method outperforms the previous state-of-the-art approach, particularly in
terms of object classes. To evaluate generalizability, we generate a simulated
dataset comprising anomalies with diverse characteristics since the original
test samples only include specific types of anomalies and may lead to biased
evaluations. Experimental results demonstrate that our approach exhibits
promising potential for generalizing effectively to various unforeseen
anomalies encountered in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">104</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic
  Performance for Mercosur Common Nomenclature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinícius Di Oliveira, Yuri Façanha Bezerra, Li Weigang, Pedro Carvalho Brom, Victor Rafael R. Celestino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing (NLP) has seen significant advancements with the
advent of large language models (LLMs). However, substantial improvements are
still needed for languages other than English, especially for specific domains
like the applications of Mercosur Common Nomenclature (NCM), a Brazilian
Harmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a
foundational Portuguese LLM, as an LLM source to implement the NCM application
processing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)
technique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.
This approach retains the chain-of-thought (CoT) methodology for prompt
development in a more concise and streamlined manner, utilizing brief and
focused documents for training. The proposed model demonstrates an efficient
and cost-effective alternative for fine-tuning smaller LLMs, significantly
outperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the
research focuses on NCM applications, the methodology can be easily adapted for
HS applications worldwide.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure, to be publish in International Conference on Web
  Information Systems and Technologies - WEBIST 2024 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hard to Explain: On the Computational Hardness of In-Distribution Model
  Interpretation <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Amir, Shahaf Bassan, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to interpret Machine Learning (ML) models is becoming
increasingly essential. However, despite significant progress in the field,
there remains a lack of rigorous characterization regarding the innate
interpretability of different models. In an attempt to bridge this gap, recent
work has demonstrated that it is possible to formally assess interpretability
by studying the computational complexity of explaining the decisions of various
models. In this setting, if explanations for a particular model can be obtained
efficiently, the model is considered interpretable (since it can be explained
``easily''). However, if generating explanations over an ML model is
computationally intractable, it is considered uninterpretable. Prior research
identified two key factors that influence the complexity of interpreting an ML
model: (i) the type of the model (e.g., neural networks, decision trees, etc.);
and (ii) the form of explanation (e.g., contrastive explanations, Shapley
values, etc.). In this work, we claim that a third, important factor must also
be considered for this analysis -- the underlying distribution over which the
explanation is obtained. Considering the underlying distribution is key in
avoiding explanations that are socially misaligned, i.e., convey information
that is biased and unhelpful to users. We demonstrate the significant influence
of the underlying distribution on the resulting overall interpretation
complexity, in two settings: (i) prediction models paired with an external
out-of-distribution (OOD) detector; and (ii) prediction models designed to
inherently generate socially aligned explanations. Our findings prove that the
expressiveness of the distribution can significantly influence the overall
complexity of interpretation, and identify essential prerequisites that a model
must possess to generate socially aligned explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ECAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdapMTL: Adaptive Pruning Framework for Multitask Learning Model <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingcan Xiang, Steven Jiaxun Tang, Qizheng Yang, Hui Guan, Tongping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of multimedia and multimodal processing, the efficient handling
of diverse data streams such as images, video, and sensor data is paramount.
Model compression and multitask learning (MTL) are crucial in this field,
offering the potential to address the resource-intensive demands of processing
and interpreting multiple forms of media simultaneously. However, effectively
compressing a multitask model presents significant challenges due to the
complexities of balancing sparsity allocation and accuracy performance across
multiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive
pruning framework for MTL models. AdapMTL leverages multiple learnable soft
thresholds independently assigned to the shared backbone and the task-specific
heads to capture the nuances in different components' sensitivity to pruning.
During training, it co-optimizes the soft thresholds and MTL model weights to
automatically determine the suitable sparsity level at each component to
achieve both high task accuracy and high overall sparsity. It further
incorporates an adaptive weighting mechanism that dynamically adjusts the
importance of task-specific losses based on each task's robustness to pruning.
We demonstrate the effectiveness of AdapMTL through comprehensive experiments
on popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different
architectures, showcasing superior performance compared to state-of-the-art
pruning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, Published at ACM Multimedia (ACM MM) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaFA: Latent Feature Attacks on Non-negative Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Vu, Ben Nebgen, Erik Skau, Geigh Zollicoffer, Juan Castorena, Kim Rasmussen, Boian Alexandrov, Manish Bhattarai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Machine Learning (ML) applications rapidly grow, concerns about
adversarial attacks compromising their reliability have gained significant
attention. One unsupervised ML method known for its resilience to such attacks
is Non-negative Matrix Factorization (NMF), an algorithm that decomposes input
data into lower-dimensional latent features. However, the introduction of
powerful computational tools such as Pytorch enables the computation of
gradients of the latent features with respect to the original data, raising
concerns about NMF's reliability. Interestingly, naively deriving the
adversarial loss for NMF as in the case of ML would result in the
reconstruction loss, which can be shown theoretically to be an ineffective
attacking objective. In this work, we introduce a novel class of attacks in NMF
termed Latent Feature Attacks (LaFA), which aim to manipulate the latent
features produced by the NMF process. Our method utilizes the Feature Error
(FE) loss directly on the latent features. By employing FE loss, we generate
perturbations in the original data that significantly affect the extracted
latent features, revealing vulnerabilities akin to those found in other ML
techniques. To handle large peak-memory overhead from gradient back-propagation
in FE attacks, we develop a method based on implicit differentiation which
enables their scaling to larger datasets. We validate NMF vulnerabilities and
FE attacks effectiveness through extensive experiments on synthetic and
real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LA-UR-24-26951</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Probing for Graph Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Zhao, Xingyu Huang, Ziyu Lyu, Yanlin Wang, Lixin Cui, Lu Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning methods have been extensively applied in diverse application
areas. However, what kind of inherent graph properties e.g. graph proximity,
graph structural information has been encoded into graph representation
learning for downstream tasks is still under-explored. In this paper, we
propose a novel graph probing framework (GraphProbe) to investigate and
interpret whether the family of graph learning methods has encoded different
levels of knowledge in graph representation learning. Based on the intrinsic
properties of graphs, we design three probes to systematically investigate the
graph representation learning process from different perspectives, respectively
the node-wise level, the path-wise level, and the structural level. We
construct a thorough evaluation benchmark with nine representative graph
learning methods from random walk based approaches, basic graph neural networks
and self-supervised graph methods, and probe them on six benchmark datasets for
node classification, link prediction and graph classification. The experimental
evaluation verify that GraphProbe can estimate the capability of graph
representation learning. Remaking results have been concluded: GCN and
WeightedGCN methods are relatively versatile methods achieving better results
with respect to different tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inter-Series <span class="highlight-title">Transformer</span>: Attending to Products in Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rares Cristian, Pavithra Harsha, Clemente Ocejo, Georgia Perakis, Brian Quanz, Ioannis Spantidakis, Hamza Zerhouni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting is an important task in many fields ranging from
supply chain management to weather forecasting. Recently, Transformer neural
network architectures have shown promising results in forecasting on common
time series benchmark datasets. However, application to supply chain demand
forecasting, which can have challenging characteristics such as sparsity and
cross-series effects, has been limited.
  In this work, we explore the application of Transformer-based models to
supply chain demand forecasting. In particular, we develop a new
Transformer-based forecasting approach using a shared, multi-task per-time
series network with an initial component applying attention across time series,
to capture interactions and help address sparsity. We provide a case study
applying our approach to successfully improve demand prediction for a medical
device manufacturing company. To further validate our approach, we also apply
it to public demand forecasting datasets as well and demonstrate competitive to
superior performance compared to a variety of baseline and state-of-the-art
forecast methods across the private and public datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PackMamba: Efficient Processing of Variable-Length Sequences in Mamba
  training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Xu, Ziqian Liu, Rong Fu, Zhongling Su, Zerui Wang, Zheng Cai, Zhilin Pei, Xingcheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the evolution of large language models, traditional Transformer models
become computationally demanding for lengthy sequences due to the quadratic
growth in computation with respect to the sequence length. Mamba, emerging as a
groundbreaking architecture in the field of generative AI, demonstrates
remarkable proficiency in handling elongated sequences with reduced
computational and memory complexity. Nevertheless, the existing training
framework of Mamba presents inefficiency with variable-length sequence inputs.
Either single-sequence training results in low GPU utilization, or batched
processing of variable-length sequences to a maximum length incurs considerable
memory and computational overhead. To address this problem, we analyze the
performance of bottleneck operators in Mamba under diverse tensor shapes and
proposed PackMamba, a high-throughput Mamba that efficiently handles
variable-length sequences. Diving deep into state-space models (SSMs), we
modify the parallel operators to avoid passing information between individual
sequences while maintaining high performance. Experimental results on an NVIDIA
A100 GPU demonstrate throughput exceeding the baseline single-sequence
processing scheme: 3.06x speedup on the 1.4B model and 2.62x on the 2.8B model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hate Speech Detection and Classification in Amharic Text with Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Minale Gashe, Seid Muhie Yimam, Yaregal Assabie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech is a growing problem on social media. It can seriously impact
society, especially in countries like Ethiopia, where it can trigger conflicts
among diverse ethnic and religious groups. While hate speech detection in
resource rich languages are progressing, for low resource languages such as
Amharic are lacking. To address this gap, we develop Amharic hate speech data
and SBi-LSTM deep learning model that can detect and classify text into four
categories of hate speech: racial, religious, gender, and non-hate speech. We
have annotated 5k Amharic social media post and comment data into four
categories. The data is annotated using a custom annotation tool by a total of
100 native Amharic speakers. The model achieves a 94.8 F1-score performance.
Future improvements will include expanding the dataset and develop state-of-the
art models.
  Keywords: Amharic hate speech detection, classification, Amharic dataset,
Deep Learning, SBi-LSTM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset: https://data.mendeley.com/datasets/p74pfhz3yx/1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bi-Level Spatial and Channel-aware <span class="highlight-title">Transformer</span> for Learned Image
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Soltani, Erfan Ghasemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in learned image compression (LIC) methods have
demonstrated superior performance over traditional hand-crafted codecs. These
learning-based methods often employ convolutional neural networks (CNNs) or
Transformer-based architectures. However, these nonlinear approaches frequently
overlook the frequency characteristics of images, which limits their
compression efficiency. To address this issue, we propose a novel
Transformer-based image compression method that enhances the transformation
stage by considering frequency components within the feature map. Our method
integrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),
where a spatial-based branch independently handles high and low frequencies at
the attention layer, and a Channel-aware Self-Attention (CaSA) module captures
information across channels, significantly improving compression performance.
Additionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)
within the Transformer block to enhance the extraction of diverse and rich
information, which is crucial for effective compression. These innovations
collectively improve the transformation's ability to project data into a more
decorrelated latent space, thereby boosting overall compression efficiency.
Experimental results demonstrate that our framework surpasses state-of-the-art
LIC methods in rate-distortion performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Variation Theory in Counterfactual Data Augmentation for
  Optimized Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simret Araya Gebreegziabher, Kuangshi Ai, Zheng Zhang, Elena L. Glassman, Toby Jia-Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Learning (AL) allows models to learn interactively from user feedback.
This paper introduces a counterfactual data augmentation approach to AL,
particularly addressing the selection of datapoints for user querying, a
pivotal concern in enhancing data efficiency. Our approach is inspired by
Variation Theory, a theory of human concept learning that emphasizes the
essential features of a concept by focusing on what stays the same and what
changes. Instead of just querying with existing datapoints, our approach
synthesizes artificial datapoints that highlight potential key similarities and
differences among labels using a neuro-symbolic pipeline combining large
language models (LLMs) and rule-based models. Through an experiment in the
example domain of text classification, we show that our approach achieves
significantly higher performance when there are fewer annotated data. As the
annotated training data gets larger the impact of the generated data starts to
diminish showing its capability to address the cold start problem in AL. This
research sheds light on integrating theories of human learning into the
optimization of AL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early Prediction of Causes (not Effects) in Healthcare by Long-Term
  Clinical Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Staniek, Marius Fracarolli, Michael Hagmann, Stefan Riezler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning for early syndrome diagnosis aims to solve the intricate
task of predicting a ground truth label that most often is the outcome (effect)
of a medical consensus definition applied to observed clinical measurements
(causes), given clinical measurements observed several hours before. Instead of
focusing on the prediction of the future effect, we propose to directly predict
the causes via time series forecasting (TSF) of clinical variables and
determine the effect by applying the gold standard consensus definition to the
forecasted values. This method has the invaluable advantage of being
straightforwardly interpretable to clinical practitioners, and because model
training does not rely on a particular label anymore, the forecasted data can
be used to predict any consensus-based label. We exemplify our method by means
of long-term TSF with Transformer models, with a focus on accurate prediction
of sparse clinical variables involved in the SOFA-based Sepsis-3 definition and
the new Simplified Acute Physiology Score (SAPS-II) definition. Our experiments
are conducted on two datasets and show that contrary to recent proposals which
advocate set function encoders for time series and direct multi-step decoders,
best results are achieved by a combination of standard dense encoders with
iterative multi-step decoders. The key for success of iterative multi-step
decoding can be attributed to its ability to capture cross-variate dependencies
and to a student forcing training strategy that teaches the model to rely on
its own previous time step predictions for the next time step prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustworthy Image Semantic Communication with GenAI: Explainablity,
  Controllability, and Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xijun Wang, Dongshan Ye, Chenyuan Feng, Howard H. Yang, Xiang Chen, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image semantic communication (ISC) has garnered significant attention for its
potential to achieve high efficiency in visual content transmission. However,
existing ISC systems based on joint source-channel coding face challenges in
interpretability, operability, and compatibility. To address these limitations,
we propose a novel trustworthy ISC framework. This approach leverages text
extraction and segmentation mapping techniques to convert images into
explainable semantics, while employing Generative Artificial Intelligence
(GenAI) for multiple downstream inference tasks. We also introduce a multi-rate
ISC transmission protocol that dynamically adapts to both the received
explainable semantic content and specific task requirements at the receiver.
Simulation results demonstrate that our framework achieves explainable
learning, decoupled training, and compatible transmission in various
application scenarios. Finally, some intriguing research directions and
application scenarios are identified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable Node Similarity Matrix Guided Contrastive Graph Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhui Liu, Xinyi Gao, Tieke He, Tao Zheng, Jianhua Zhao, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph clustering, which involves the partitioning of nodes within a graph
into disjoint clusters, holds significant importance for numerous subsequent
applications. Recently, contrastive learning, known for utilizing supervisory
information, has demonstrated encouraging results in deep graph clustering.
This methodology facilitates the learning of favorable node representations for
clustering by attracting positively correlated node pairs and distancing
negatively correlated pairs within the representation space. Nevertheless, a
significant limitation of existing methods is their inadequacy in thoroughly
exploring node-wise similarity. For instance, some hypothesize that the node
similarity matrix within the representation space is identical, ignoring the
inherent semantic relationships among nodes. Given the fundamental role of
instance similarity in clustering, our research investigates contrastive graph
clustering from the perspective of the node similarity matrix. We argue that an
ideal node similarity matrix within the representation space should accurately
reflect the inherent semantic relationships among nodes, ensuring the
preservation of semantic similarities in the learned representations. In
response to this, we introduce a new framework, Reliable Node Similarity Matrix
Guided Contrastive Graph Clustering (NS4GC), which estimates an approximately
ideal node similarity matrix within the representation space to guide
representation learning. Our method introduces node-neighbor alignment and
semantic-aware sparsification, ensuring the node similarity matrix is both
accurate and efficiently sparse. Comprehensive experiments conducted on $8$
real-world datasets affirm the efficacy of learning the node similarity matrix
and the superior performance of NS4GC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Knowledge and Data Engineering
  (TKDE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Model-based Anomaly Detection in Multivariate Time Series:
  Taxonomy, <span class="highlight-title">Survey</span>, Research Challenges and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Correia, Jan-Christoph Goos, Philipp Klein, Thomas Bäck, Anna V. Kononova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-series anomaly detection plays an important role in engineering
processes, like development, manufacturing and other operations involving
dynamic systems. These processes can greatly benefit from advances in the
field, as state-of-the-art approaches may aid in cases involving, for example,
highly dimensional data. To provide the reader with understanding of the
terminology, this survey introduces a novel taxonomy where a distinction
between online and offline, and training and inference is made. Additionally,
it presents the most popular data sets and evaluation metrics used in the
literature, as well as a detailed analysis. Furthermore, this survey provides
an extensive overview of the state-of-the-art model-based online semi- and
unsupervised anomaly detection approaches for multivariate time-series data,
categorising them into different model families and other properties. The
biggest research challenge revolves around benchmarking, as currently there is
no reliable way to compare different approaches against one another. This
problem is two-fold: on the one hand, public data sets suffers from at least
one fundamental flaw, while on the other hand, there is a lack of intuitive and
representative evaluation metrics in the field. Moreover, the way most
publications choose a detection threshold disregards real-world conditions,
which hinders the application in the real world. To allow for tangible advances
in the field, these issues must be addressed in future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Engineering Applications of Artificial Intelligence
  journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion
  Posterior Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xu, Zhiqi Lin, Shigui Li, Min Chen, Junmei Yang, Delu Zeng, John Paisley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Last Layer (BLL) models focus solely on uncertainty in the output
layer of neural networks, demonstrating comparable performance to more complex
Bayesian models. However, the use of Gaussian priors for last layer weights in
Bayesian Last Layer (BLL) models limits their expressive capacity when faced
with non-Gaussian, outlier-rich, or high-dimensional datasets. To address this
shortfall, we introduce a novel approach that combines diffusion techniques and
implicit priors for variational learning of Bayesian last layer weights. This
method leverages implicit distributions for modeling weight priors in BLL,
coupled with diffusion samplers for approximating true posterior predictions,
thereby establishing a comprehensive Bayesian prior and posterior estimation
strategy. By delivering an explicit and computationally efficient variational
lower bound, our method aims to augment the expressive abilities of BLL models,
enhancing model accuracy, calibration, and out-of-distribution detection
proficiency. Through detailed exploration and experimental validation, We
showcase the method's potential for improving predictive accuracy and
uncertainty quantification while ensuring computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Multimodal Large Language Models with Quantization-Aware Scale
  Learning for Efficient Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Xie, Yuxin Zhang, Mingbao Lin, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first study to explore the potential of parameter
quantization for multimodal large language models to alleviate the significant
resource constraint encountered during vision-language instruction tuning. We
introduce a Quantization-aware Scale LeArning method based on multimodal
Warmup, termed QSLAW. This method is grounded in two key innovations: (1) The
learning of group-wise scale factors for quantized LLM weights to mitigate the
quantization error arising from activation outliers and achieve more effective
vision-language instruction tuning; (2) The implementation of a multimodal
warmup that progressively integrates linguistic and multimodal training
samples, thereby preventing overfitting of the quantized model to multimodal
data while ensuring stable adaptation of multimodal large language models to
downstream vision-language tasks. Extensive experiments demonstrate that models
quantized by QSLAW perform on par with, or even surpass, their full-precision
counterparts, while facilitating up to 1.4 times reduction in VL tuning time
and GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayes-optimal learning of an extensive-width neural network from
  quadratically many samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Maillard, Emanuele Troiani, Simon Martin, Florent Krzakala, Lenka Zdeborová
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning a target function corresponding to a
single hidden layer neural network, with a quadratic activation function after
the first layer, and random weights. We consider the asymptotic limit where the
input dimension and the network width are proportionally large. Recent work
[Cui & al '23] established that linear regression provides Bayes-optimal test
error to learn such a function when the number of available samples is only
linear in the dimension. That work stressed the open challenge of theoretically
analyzing the optimal test error in the more interesting regime where the
number of samples is quadratic in the dimension. In this paper, we solve this
challenge for quadratic activations and derive a closed-form expression for the
Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE,
which combines approximate message passing with rotationally invariant matrix
denoising, and that asymptotically achieves the optimal performance.
Technically, our result is enabled by establishing a link with recent works on
optimal denoising of extensive-rank matrices and on the ellipsoid fitting
problem. We further show empirically that, in the absence of noise,
randomly-initialized gradient descent seems to sample the space of weights,
leading to zero training loss, and averaging over initialization leads to a
test error equal to the Bayes-optimal one.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Question Rephrasing for Quantifying Uncertainty in Large Language
  Models: Applications in Molecular Chemistry Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhang Chen, Pengyu Hong, Sandeep Madireddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification enables users to assess the reliability of
responses generated by large language models (LLMs). We present a novel
Question Rephrasing technique to evaluate the input uncertainty of LLMs, which
refers to the uncertainty arising from equivalent variations of the inputs
provided to LLMs. This technique is integrated with sampling methods that
measure the output uncertainty of LLMs, thereby offering a more comprehensive
uncertainty assessment. We validated our approach on property prediction and
reaction prediction for molecular chemistry tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Convex-optimization-based Layer-wise Post-training Pruner for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Zhao, Hanyu Hu, Ping Li, Yi Zheng, Zhefeng Wang, Xiaoming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning is a critical strategy for compressing trained large language models
(LLMs), aiming at substantial memory conservation and computational
acceleration without compromising performance. However, existing pruning
methods often necessitate inefficient retraining for billion-scale LLMs or rely
on heuristic methods such as the optimal brain surgeon framework, which degrade
performance. In this paper, we introduce FISTAPruner, the first post-training
pruner based on convex optimization models and algorithms. Specifically, we
propose a convex optimization model incorporating $\ell_1$ norm to induce
sparsity and utilize the FISTA solver for optimization. FISTAPruner
incorporates an intra-layer cumulative error correction mechanism and supports
parallel pruning. We comprehensively evaluate FISTAPruner on models such as
OPT, LLaMA, LLaMA-2, and LLaMA-3 with 125M to 70B parameters under unstructured
and 2:4 semi-structured sparsity, demonstrating superior performance over
existing state-of-the-art methods across various language benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Topology Measures of Contextual Language Model Latent Spaces With
  Applications to Dialogue Term Extraction <span class="chip">SIGDIAL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Matthias Ruppik, Michael Heck, Carel van Niekerk, Renato Vukovic, Hsien-chin Lin, Shutong Feng, Marcus Zibrowius, Milica Gašić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common approach for sequence tagging tasks based on contextual word
representations is to train a machine learning classifier directly on these
embedding vectors. This approach has two shortcomings. First, such methods
consider single input sequences in isolation and are unable to put an
individual embedding vector in relation to vectors outside the current local
context of use. Second, the high performance of these models relies on
fine-tuning the embedding model in conjunction with the classifier, which may
not always be feasible due to the size or inaccessibility of the underlying
feature-generation model. It is thus desirable, given a collection of embedding
vectors of a corpus, i.e., a datastore, to find features of each vector that
describe its relation to other, similar vectors in the datastore. With this in
mind, we introduce complexity measures of the local topology of the latent
space of a contextual language model with respect to a given datastore. The
effectiveness of our features is demonstrated through their application to
dialogue term extraction. Our work continues a line of research that explores
the manifold hypothesis for word embeddings, demonstrating that local structure
in the space carved out by word embeddings can be exploited to infer semantic
properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper to SIGDIAL 2024. 9 pages, 2 figures, 3
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Blockchain-based Reliable Federated Meta-learning for Metaverse: A
  Dual Game Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emna Baccour, Aiman Erbad, Amr Mohamed, Mounir Hamdi, Mohsen Guizani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The metaverse, envisioned as the next digital frontier for avatar-based
virtual interaction, involves high-performance models. In this dynamic
environment, users' tasks frequently shift, requiring fast model
personalization despite limited data. This evolution consumes extensive
resources and requires vast data volumes. To address this, meta-learning
emerges as an invaluable tool for metaverse users, with federated meta-learning
(FML), offering even more tailored solutions owing to its adaptive
capabilities. However, the metaverse is characterized by users heterogeneity
with diverse data structures, varied tasks, and uneven sample sizes,
potentially undermining global training outcomes due to statistical difference.
Given this, an urgent need arises for smart coalition formation that accounts
for these disparities. This paper introduces a dual game-theoretic framework
for metaverse services involving meta-learners as workers to manage FML. A
blockchain-based cooperative coalition formation game is crafted, grounded on a
reputation metric, user similarity, and incentives. We also introduce a novel
reputation system based on users' historical contributions and potential
contributions to present tasks, leveraging correlations between past and new
tasks. Finally, a Stackelberg game-based incentive mechanism is presented to
attract reliable workers to participate in meta-learning, minimizing users'
energy costs, increasing payoffs, boosting FML efficacy, and improving
metaverse utility. Results show that our dual game framework outperforms
best-effort, random, and non-uniform clustering schemes - improving training
performance by up to 10%, cutting completion times by as much as 30%, enhancing
metaverse utility by more than 25%, and offering up to 5% boost in training
efficiency over non-blockchain systems, effectively countering misbehaving
users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Internet of Things Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Design of Periodic Orbits in the Restricted Three-Body
  Problem <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvaro Francisco Gil, Walther Litteri, Victor Rodriguez-Fernandez, David Camacho, Massimiliano Vasile
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Three-Body Problem has fascinated scientists for centuries and it has
been crucial in the design of modern space missions. Recent developments in
Generative Artificial Intelligence hold transformative promise for addressing
this longstanding problem. This work investigates the use of Variational
Autoencoder (VAE) and its internal representation to generate periodic orbits.
We utilize a comprehensive dataset of periodic orbits in the Circular
Restricted Three-Body Problem (CR3BP) to train deep-learning architectures that
capture key orbital characteristics, and we set up physical evaluation metrics
for the generated trajectories. Through this investigation, we seek to enhance
the understanding of how Generative AI can improve space mission planning and
astrodynamics research, leading to novel, data-driven approaches in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SPAICE Conference 2024 (7 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep
  Graph Neural Networks <span class="chip">CIKM2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Peng, Runlin Lei, Zhewei Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The drastic performance degradation of Graph Neural Networks (GNNs) as the
depth of the graph propagation layers exceeds 8-10 is widely attributed to a
phenomenon of Over-smoothing. Although recent research suggests that
Over-smoothing may not be the dominant reason for such a performance
degradation, they have not provided rigorous analysis from a theoretical view,
which warrants further investigation. In this paper, we systematically analyze
the real dominant problem in deep GNNs and identify the issues that these GNNs
towards addressing Over-smoothing essentially work on via empirical experiments
and theoretical gradient analysis. We theoretically prove that the difficult
training problem of deep MLPs is actually the main challenge, and various
existing methods that supposedly tackle Over-smoothing actually improve the
trainability of MLPs, which is the main reason for their performance gains. Our
further investigation into trainability issues reveals that properly
constrained smaller upper bounds of gradient flow notably enhance the
trainability of GNNs. Experimental results on diverse datasets demonstrate
consistency between our theoretical findings and empirical evidence. Our
analysis provides new insights in constructing deep graph models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Driven approach for sustainable extraction of earth's subsurface
  renewable energy while minimizing seismic activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Gutierrez-Oribio, Alexandros Stathas, Ioannis Stefanou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Geothermal Energy, Carbon Capture and Storage, and Hydrogen Storage hold
considerable promise for meeting the energy sector's large-scale requirements
and reducing CO$_2$ emissions. However, the injection of fluids into the
Earth's crust, essential for these activities, can induce or trigger
earthquakes. In this paper, we highlight a new approach based on Reinforcement
Learning for the control of human-induced seismicity in the highly complex
environment of an underground reservoir. This complex system poses significant
challenges in the control design due to parameter uncertainties and unmodeled
dynamics. We show that the reinforcement learning algorithm can interact
efficiently with a robust controller, by choosing the controller parameters in
real-time, reducing human-induced seismicity and allowing the consideration of
further production objectives, \textit{e.g.}, minimal control power.
Simulations are presented for a simplified underground reservoir under various
energy demand scenarios, demonstrating the reliability and effectiveness of the
proposed control-reinforcement learning approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consumer Transactions Simulation through Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergiy Tkachuk, Szymon Łukasik, Anna Wróblewska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving domain of large-scale retail data systems,
envisioning and simulating future consumer transactions has become a crucial
area of interest. It offers significant potential to fortify demand forecasting
and fine-tune inventory management. This paper presents an innovative
application of Generative Adversarial Networks (GANs) to generate synthetic
retail transaction data, specifically focusing on a novel system architecture
that combines consumer behavior modeling with stock-keeping unit (SKU)
availability constraints to address real-world assortment optimization
challenges. We diverge from conventional methodologies by integrating SKU data
into our GAN architecture and using more sophisticated embedding methods (e.g.,
hyper-graphs). This design choice enables our system to generate not only
simulated consumer purchase behaviors but also reflects the dynamic interplay
between consumer behavior and SKU availability -- an aspect often overlooked,
among others, because of data scarcity in legacy retail simulation models. Our
GAN model generates transactions under stock constraints, pioneering a
resourceful experimental system with practical implications for real-world
retail operation and strategy. Preliminary results demonstrate enhanced realism
in simulated transactions measured by comparing generated items with real ones
using methods employed earlier in related studies. This underscores the
potential for more accurate predictive modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest
  Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdou, Tasneem Mohsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)
that aims to identify and classify entities in text into predefined categories.
However, when applied to Arabic data, NER encounters unique challenges stemming
from the language's rich morphological inflections, absence of capitalization
cues, and spelling variants, where a single word can comprise multiple
morphemes. In this paper, we introduce Arabic KNN-NER, our submission to the
Wojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the
shared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained
flat-entity recognition for Arabic text, where we identify a single main entity
and possibly zero or multiple sub-entities for each word. Arabic KNN-NER
augments the probability distribution of a fine-tuned model with another label
probability distribution derived from performing a KNN search over the cached
training data. Our submission achieved 91% on the test set on the WojoodFine
dataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time is Not Enough: Time-Frequency based Explanation for Time-Series
  Black-Box Models <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunseung Chung, Sumin Jo, Yeonsu Kwon, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the massive attention given to time-series explanations due to their
extensive applications, a notable limitation in existing approaches is their
primary reliance on the time-domain. This overlooks the inherent characteristic
of time-series data containing both time and frequency features. In this work,
we present Spectral eXplanation (SpectralX), an XAI framework that provides
time-frequency explanations for time-series black-box classifiers. This easily
adaptable framework enables users to "plug-in" various perturbation-based XAI
methods for any pre-trained time-series classification models to assess their
impact on the explanation quality without having to modify the framework
architecture. Additionally, we introduce Feature Importance Approximations
(FIA), a new perturbation-based XAI method. These methods consist of feature
insertion, deletion, and combination techniques to enhance computational
efficiency and class-specific explanations in time-series classification tasks.
We conduct extensive experiments in the generated synthetic dataset and various
UCR Time-Series datasets to first compare the explanation performance of FIA
and other existing perturbation-based XAI methods in both time-domain and
time-frequency domain, and then show the superiority of our FIA in the
time-frequency domain with the SpectralX framework. Finally, we conduct a user
study to confirm the practicality of our FIA in SpectralX framework for
class-specific time-frequency based time-series explanations. The source code
is available in https://github.com/gustmd0121/Time_is_not_Enough
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CIKM 2024 (10 pages, 4 figures, 6 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the choice of the non-trainable internal weights in random feature
  maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pinak Mandal, Georg A. Gottwald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The computationally cheap machine learning architecture of random feature
maps can be viewed as a single-layer feedforward network in which the weights
of the hidden layer are random but fixed and only the outer weights are learned
via linear regression. The internal weights are typically chosen from a
prescribed distribution. The choice of the internal weights significantly
impacts the accuracy of random feature maps. We address here the task of how to
best select the internal weights. In particular, we consider the forecasting
problem whereby random feature maps are used to learn a one-step propagator map
for a dynamical system. We provide a computationally cheap hit-and-run
algorithm to select good internal weights which lead to good forecasting skill.
We show that the number of good features is the main factor controlling the
forecasting skill of random feature maps and acts as an effective feature
dimension. Lastly, we compare random feature maps with single-layer feedforward
neural networks in which the internal weights are now learned using gradient
descent. We find that random feature maps have superior forecasting
capabilities whilst having several orders of magnitude lower computational
cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Robust Generalizers Less Rigid with Soft Ascent-Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew J. Holland, Toma Hamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the traditional formulation of machine learning tasks is in terms of
performance on average, in practice we are often interested in how well a
trained model performs on rare or difficult data points at test time. To
achieve more robust and balanced generalization, methods applying
sharpness-aware minimization to a subset of worst-case examples have proven
successful for image classification tasks, but only using deep neural networks
in a scenario where the most difficult points are also the least common. In
this work, we show how such a strategy can dramatically break down under more
diverse models, and as a more robust alternative, instead of typical sharpness
we propose and evaluate a training criterion which penalizes poor loss
concentration, which can be easily combined with loss transformations such as
CVaR or DRO that control tail emphasis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Logical Fallacy-Informed Framework for Argument Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Mouchel, Debjit Paul, Shaobo Cui, Robert West, Antoine Bosselut, Boi Faltings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable performance of Large Language Models (LLMs), they
still struggle with generating logically sound arguments, resulting in
potential risks such as spreading misinformation. An important factor
contributing to LLMs' suboptimal performance in generating coherent arguments
is their oversight of logical fallacies. To address this issue, we introduce
FIPO, a fallacy-informed framework that leverages preference optimization
methods to steer LLMs toward logically sound arguments. FIPO includes a
classification loss, to capture the fine-grained information on fallacy
categories. Our results on argumentation datasets show that our method reduces
the fallacy errors by up to 17.5%. Furthermore, our human evaluation results
indicate that the quality of the generated arguments by our method
significantly outperforms the fine-tuned baselines, as well as prior preference
optimization methods, such as DPO. These findings highlight the importance of
ensuring models are aware of logical fallacies for effective argument
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Child-Directed Speech Effective Training Data for Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Y. Feng, Noah D. Goodman, Michael C. Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While high-performing language models are typically trained on hundreds of
billions of words, human children become fluent language users with a much
smaller amount of data. What are the features of the data they receive, and how
do these features support language modeling objectives? To investigate this
question, we train GPT-2 models on 29M words of English-language child-directed
speech and a new matched, synthetic dataset (TinyDialogues), comparing to a
heterogeneous blend of datasets from the BabyLM challenge. We evaluate both the
syntactic and semantic knowledge of these models using developmentally-inspired
evaluations. Through pretraining experiments, we test whether the global
developmental ordering or the local discourse ordering of children's training
data support high performance relative to other datasets. The local properties
of the data affect model results, but somewhat surprisingly, global properties
do not. Further, child language input is not uniquely valuable for training
language models. These findings support the hypothesis that, rather than
proceeding from better data, children's learning is instead substantially more
efficient than current language modeling techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code and data will be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JARViS: Detecting Actions in Video Using Unified Actor-Scene Context
  Relation Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seok Hwan Lee, Taein Son, Soo Won Seo, Jisong Kim, Jun Won Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video action detection (VAD) is a formidable vision task that involves the
localization and classification of actions within the spatial and temporal
dimensions of a video clip. Among the myriad VAD architectures, two-stage VAD
methods utilize a pre-trained person detector to extract the region of interest
features, subsequently employing these features for action detection. However,
the performance of two-stage VAD methods has been limited as they depend solely
on localized actor features to infer action semantics. In this study, we
propose a new two-stage VAD framework called Joint Actor-scene context Relation
modeling based on Visual Semantics (JARViS), which effectively consolidates
cross-modal action semantics distributed globally across spatial and temporal
dimensions using Transformer attention. JARViS employs a person detector to
produce densely sampled actor features from a keyframe. Concurrently, it uses a
video backbone to create spatio-temporal scene features from a video clip.
Finally, the fine-grained interactions between actors and scenes are modeled
through a Unified Action-Scene Context Transformer to directly output the final
set of actions in parallel. Our experimental results demonstrate that JARViS
outperforms existing methods by significant margins and achieves
state-of-the-art performance on three popular VAD datasets, including AVA,
UCF101-24, and JHMDB51-21.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InPer: Whole-Process Domain Generalization via Causal Intervention and
  Perturbation <span class="chip">BMVC2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Xinghao Ding, Yue Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable advancements achieved by deep neural networks, their
performance tends to degenerate when the test environment diverges from the
training ones. Domain generalization (DG) solves this issue by learning
representations independent of domain-related information, thus facilitating
extrapolation to unseen environments. Existing approaches typically focus on
formulating tailored training objectives to extract shared features from the
source data. However, the disjointed training and testing procedures may
compromise robustness, particularly in the face of unforeseen variations during
deployment. In this paper, we propose a novel and holistic framework based on
causality, named InPer, designed to enhance model generalization by
incorporating causal intervention during training and causal perturbation
during testing. Specifically, during the training phase, we employ
entropy-based causal intervention (EnIn) to refine the selection of causal
variables. To identify samples with anti-interference causal variables from the
target domain, we propose a novel metric, homeostatic score, through causal
perturbation (HoPer) to construct a prototype classifier in test time.
Experimental results across multiple cross-domain tasks confirm the efficacy of
InPer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BMVC2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnJa: Ensemble Jailbreak on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Zhang, Zilong Wang, Ruofan Wang, Xingjun Ma, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) are increasingly being deployed in
safety-critical applications, their vulnerability to potential jailbreaks --
malicious prompts that can disable the safety mechanism of LLMs -- has
attracted growing research attention. While alignment methods have been
proposed to protect LLMs from jailbreaks, many have found that aligned LLMs can
still be jailbroken by carefully crafted malicious prompts, producing content
that violates policy regulations. Existing jailbreak attacks on LLMs can be
categorized into prompt-level methods which make up stories/logic to circumvent
safety alignment and token-level attack methods which leverage gradient methods
to find adversarial tokens. In this work, we introduce the concept of Ensemble
Jailbreak and explore methods that can integrate prompt-level and token-level
jailbreak into a more powerful hybrid jailbreak attack. Specifically, we
propose a novel EnJa attack to hide harmful instructions using prompt-level
jailbreak, boost the attack success rate using a gradient-based attack, and
connect the two types of jailbreak attacks via a template-based connector. We
evaluate the effectiveness of EnJa on several aligned models and show that it
achieves a state-of-the-art attack success rate with fewer queries and is much
stronger than any individual jailbreak.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activations Through Extensions: A Framework To Boost Performance Of
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chandramouli Kamanchi, Sumatra Mukherjee, Kameshwaran Sampath, Pankaj Dayama, Arindam Jati, Vijay Ekambaram, Dzung Phan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation functions are non-linearities in neural networks that allow them
to learn complex mapping between inputs and outputs. Typical choices for
activation functions are ReLU, Tanh, Sigmoid etc., where the choice generally
depends on the application domain. In this work, we propose a
framework/strategy that unifies several works on activation functions and
theoretically explains the performance benefits of these works. We also propose
novel techniques that originate from the framework and allow us to obtain
``extensions'' (i.e. special generalizations of a given neural network) of
neural networks through operations on activation functions. We theoretically
and empirically show that ``extensions'' of neural networks have performance
benefits compared to vanilla neural networks with insignificant space and time
complexity costs on standard test functions. We also show the benefits of
neural network ``extensions'' in the time-series domain on real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focal Depth Estimation: A Calibration-Free, Subject- and Daytime
  Invariant Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt W. Hosp, Björn Severitt, Rajat Agarwala, Evgenia Rusak, Yannick Sauer, Siegfried Wahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where personalized technology is increasingly intertwined with
daily life, traditional eye-tracking systems and autofocal glasses face a
significant challenge: the need for frequent, user-specific calibration, which
impedes their practicality. This study introduces a groundbreaking
calibration-free method for estimating focal depth, leveraging machine learning
techniques to analyze eye movement features within short sequences. Our
approach, distinguished by its innovative use of LSTM networks and
domain-specific feature engineering, achieves a mean absolute error (MAE) of
less than 10 cm, setting a new focal depth estimation accuracy standard. This
advancement promises to enhance the usability of autofocal glasses and pave the
way for their seamless integration into extended reality environments, marking
a significant leap forward in personalized visual technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensitivity analysis using the Metamodel of Optimal Prognosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Most, Johannes Will
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real case applications within the virtual prototyping process, it is not
always possible to reduce the complexity of the physical models and to obtain
numerical models which can be solved quickly. Usually, every single numerical
simulation takes hours or even days. Although the progresses in numerical
methods and high performance computing, in such cases, it is not possible to
explore various model configurations, hence efficient surrogate models are
required. Generally the available meta-model techniques show several advantages
and disadvantages depending on the investigated problem. In this paper we
present an automatic approach for the selection of the optimal suitable
meta-model for the actual problem. Together with an automatic reduction of the
variable space using advanced filter techniques an efficient approximation is
enabled also for high dimensional problems. This filter techniques enable a
reduction of the high dimensional variable space to a much smaller subspace
where meta-model-based sensitivity analyses are carried out to assess the
influence of important variables and to identify the optimal subspace with
corresponding surrogate model which enables the most accurate probabilistic
analysis. For this purpose we investigate variance-based and moment-free
sensitivity measures in combination with advanced meta-models as moving least
squares and kriging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented at 8th Optimization and Stochastic Days, Weimar, Germany,
  24-25 November, 2011</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facing the Music: Tackling Singing Voice Separation in Cinematic Audio
  Source Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karn N. Watcharasupat, Chih-Wei Wu, Iroro Orife
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cinematic audio source separation (CASS) is a fairly new subtask of audio
source separation. A typical setup of CASS is a three-stem problem, with the
aim of separating the mixture into the dialogue stem (DX), music stem (MX), and
effects stem (FX). In practice, however, several edge cases exist as some sound
sources do not fit neatly in either of these three stems, necessitating the use
of additional auxiliary stems in production. One very common edge case is the
singing voice in film audio, which may belong in either the DX or MX, depending
heavily on the cinematic context. In this work, we demonstrate a very
straightforward extension of the dedicated-decoder Bandit and query-based
single-decoder Banquet models to a four-stem problem, treating non-musical
dialogue, instrumental music, singing voice, and effects as separate stems.
Interestingly, the query-based Banquet model outperformed the dedicated-decoder
Bandit model. We hypothesized that this is due to a better feature alignment at
the bottleneck as enforced by the band-agnostic FiLM layer. Dataset and model
implementation will be made available at
https://github.com/kwatcharasupat/source-separation-landing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the Late-Breaking Demo Session of the 25th International
  Society for Music Information Retrieval (ISMIR) Conference, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Neural Constructive Solver for Real-world TSP Scenarios <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Liang Goh, Zhiguang Cao, Yining Ma, Yanfei Dong, Mohammed Haroon Dupty, Wee Sun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing neural constructive solvers for routing problems have predominantly
employed transformer architectures, conceptualizing the route construction as a
set-to-sequence learning task. However, their efficacy has primarily been
demonstrated on entirely random problem instances that inadequately capture
real-world scenarios. In this paper, we introduce realistic Traveling Salesman
Problem (TSP) scenarios relevant to industrial settings and derive the
following insights: (1) The optimal next node (or city) to visit often lies
within proximity to the current node, suggesting the potential benefits of
biasing choices based on current locations. (2) Effectively solving the TSP
requires robust tracking of unvisited nodes and warrants succinct grouping
strategies. Building upon these insights, we propose integrating a learnable
choice layer inspired by Hypernetworks to prioritize choices based on the
current location, and a learnable approximate clustering algorithm inspired by
the Expectation-Maximization algorithm to facilitate grouping the unvisited
cities. Together, these two contributions form a hierarchical approach towards
solving the realistic TSP by considering both immediate local neighbourhoods
and learning an intermediate set of node representations. Our hierarchical
approach yields superior performance compared to both classical and recent
transformer models, showcasing the efficacy of the key designs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teach CLIP to Develop a Number Sense for Ordinal Regression <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Du, Qiang Zhai, Weihang Dai, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ordinal regression is a fundamental problem within the field of computer
vision, with customised well-trained models on specific tasks. While
pre-trained vision-language models (VLMs) have exhibited impressive performance
on various vision tasks, their potential for ordinal regression has received
less exploration. In this study, we first investigate CLIP's potential for
ordinal regression, from which we expect the model could generalise to
different ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP
fails on this task, since current VLMs have a well-documented limitation of
encapsulating compositional concepts such as number sense. We propose a simple
yet effective method called NumCLIP to improve the quantitative understanding
of VLMs. We disassemble the exact image to number-specific text matching
problem into coarse classification and fine prediction stages. We discretize
and phrase each numerical bin with common language concept to better leverage
the available pre-trained alignment in CLIP. To consider the inherent
continuous property of ordinal regression, we propose a novel fine-grained
cross-modal ranking-based regularisation loss specifically designed to keep
both semantic and ordinal alignment in CLIP's feature space. Experimental
results on three general ordinal regression tasks demonstrate the effectiveness
of NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating
and image aesthetics assessment task, respectively. Code is publicly available
at https://github.com/xmed-lab/NumCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D-OOB: Attributing Data Contribution through Joint Valuation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Sun, Jingyan Shen, Yongchan Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data valuation has emerged as a powerful framework to quantify the
contribution of each datum to the training of a particular machine learning
model. However, it is crucial to recognize that the quality of various cells
within a single data point can vary greatly in practice. For example, even in
the case of an abnormal data point, not all cells are necessarily noisy. The
single scalar valuation assigned by existing methods blurs the distinction
between noisy and clean cells of a data point, thereby compromising the
interpretability of the valuation. In this paper, we propose 2D-OOB, an
out-of-bag estimation framework for jointly determining helpful (or
detrimental) samples, as well as the particular cells that drive them. Our
comprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art
performance across multiple use cases, while being exponentially faster. 2D-OOB
excels in detecting and rectifying fine-grained outliers at the cell level, as
well as localizing backdoor triggers in data poisoning attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximum a Posteriori Estimation for Linear Structural Dynamics Models
  Using Bayesian Optimization with Rational Polynomial Chaos Expansions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Schneider, Iason Papaioannou, Bruno Sudret, Gerhard Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian analysis enables combining prior knowledge with measurement data to
learn model parameters. Commonly, one resorts to computing the maximum a
posteriori (MAP) estimate, when only a point estimate of the parameters is of
interest. We apply MAP estimation in the context of structural dynamic models,
where the system response can be described by the frequency response function.
To alleviate high computational demands from repeated expensive model calls, we
utilize a rational polynomial chaos expansion (RPCE) surrogate model that
expresses the system frequency response as a rational of two polynomials with
complex coefficients. We propose an extension to an existing sparse Bayesian
learning approach for RPCE based on Laplace's approximation for the posterior
distribution of the denominator coefficients. Furthermore, we introduce a
Bayesian optimization approach, which allows to adaptively enrich the
experimental design throughout the optimization process of MAP estimation.
Thereby, we utilize the expected improvement acquisition function as a means to
identify sample points in the input space that are possibly associated with
large objective function values. The acquisition function is estimated through
Monte Carlo sampling based on the posterior distribution of the expansion
coefficients identified in the sparse Bayesian learning process. By combining
the sparsity-inducing learning procedure with the sequential experimental
design, we effectively reduce the number of model evaluations in the MAP
estimation problem. We demonstrate the applicability of the presented methods
on the parameter updating problem of an algebraic two-degree-of-freedom system
and the finite element model of a cross-laminated timber plate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative study of generative adversarial networks for image
  recognition algorithms based on deep learning and traditional methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Zhong, Yijing Wei, Yingbin Liang, Xiqing Liu, Rongwei Ji, Yiru Cang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, an image recognition algorithm based on the combination of
deep learning and generative adversarial network (GAN) is studied, and compared
with traditional image recognition methods. The purpose of this study is to
evaluate the advantages and application prospects of deep learning technology,
especially GAN, in the field of image recognition. Firstly, this paper reviews
the basic principles and techniques of traditional image recognition methods,
including the classical algorithms based on feature extraction such as SIFT,
HOG and their combination with support vector machine (SVM), random forest, and
other classifiers. Then, the working principle, network structure, and unique
advantages of GAN in image generation and recognition are introduced. In order
to verify the effectiveness of GAN in image recognition, a series of
experiments are designed and carried out using multiple public image data sets
for training and testing. The experimental results show that compared with
traditional methods, GAN has excellent performance in processing complex
images, recognition accuracy, and anti-noise ability. Specifically, Gans are
better able to capture high-dimensional features and details of images,
significantly improving recognition performance. In addition, Gans shows unique
advantages in dealing with image noise, partial missing information, and
generating high-quality images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPC-Minimized Secure LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deevashwer Rathee, Dacheng Li, Ion Stoica, Hao Zhang, Raluca Popa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many inference services based on large language models (LLMs) pose a privacy
concern, either revealing user prompts to the service or the proprietary
weights to the user. Secure inference offers a solution to this problem through
secure multi-party computation (MPC), however, it is still impractical for
modern LLM workload due to the large overhead imposed by MPC. To address this
overhead, we propose Marill, a framework that adapts LLM fine-tuning to
minimize MPC usage during secure inference. Marill introduces high-level
architectural changes during fine-tuning that significantly reduce the number
of expensive operations needed within MPC during inference, by removing some
and relocating others outside MPC without compromising security. As a result,
Marill-generated models are more efficient across all secure inference
protocols and our approach complements MPC-friendly approximations for such
operations. Compared to standard fine-tuning, Marill results in 3.6-11.3x
better runtime and 2.4-6.9x better communication during secure inference across
various MPC settings, while typically preserving over 90% performance across
downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In2Core: Leveraging Influence Functions for Coreset Selection in
  Instruction Finetuning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayrton San Joaquin, Bin Wang, Zhengyuan Liu, Nicholas Asher, Brian Lim, Philippe Muller, Nancy Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements, fine-tuning Large Language Models (LLMs) remains costly
due to the extensive parameter count and substantial data requirements for
model generalization. Accessibility to computing resources remains a barrier
for the open-source community. To address this challenge, we propose the
In2Core algorithm, which selects a coreset by analyzing the correlation between
training and evaluation samples with a trained model. Notably, we assess the
model's internal gradients to estimate this relationship, aiming to rank the
contribution of each training point. To enhance efficiency, we propose an
optimization to compute influence functions with a reduced number of layers
while achieving similar accuracy. By applying our algorithm to instruction
fine-tuning data of LLMs, we can achieve similar performance with just 50% of
the training data. Meantime, using influence functions to analyze model
coverage to certain testing samples could provide a reliable and interpretable
signal on the training set's coverage of those test points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empirical Analysis of Large Vision-Language Models against Goal
  Hijacking via Visual <span class="highlight-title">Prompt</span> Injection <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subaru Kimura, Ryota Tanaka, Shumpei Miyawaki, Jun Suzuki, Keisuke Sakaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore visual prompt injection (VPI) that maliciously exploits the
ability of large vision-language models (LVLMs) to follow instructions drawn
onto the input image. We propose a new VPI method, "goal hijacking via visual
prompt injection" (GHVPI), that swaps the execution task of LVLMs from an
original task to an alternative task designated by an attacker. The
quantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and
demonstrates a notable attack success rate of 15.8%, which is an unignorable
security risk. Our analysis also shows that successful GHVPI requires high
character recognition capability and instruction-following ability in LVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, Accepted to NAACL 2024 SRW</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning for Robotics: A <span class="highlight-title">Survey</span> of Real-World
  Successes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Ben Abbatematteo, Jiaheng Hu, Rohan Chandra, Roberto Martín-Martín, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL), particularly its combination with deep neural
networks referred to as deep RL (DRL), has shown tremendous promise across a
wide range of applications, suggesting its potential for enabling the
development of sophisticated robotic behaviors. Robotics problems, however,
pose fundamental difficulties for the application of RL, stemming from the
complexity and cost of interacting with the physical world. This article
provides a modern survey of DRL for robotics, with a particular focus on
evaluating the real-world successes achieved with DRL in realizing several key
robotic competencies. Our analysis aims to identify the key factors underlying
those exciting successes, reveal underexplored areas, and provide an overall
characterization of the status of DRL in robotics. We highlight several
important avenues for future work, emphasizing the need for stable and
sample-efficient real-world RL paradigms, holistic approaches for discovering
and integrating various competencies to tackle complex long-horizon, open-world
tasks, and principled development and evaluation procedures. This survey is
designed to offer insights for both RL practitioners and roboticists toward
harnessing RL's power to create generally capable real-world robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors contributed equally. Accepted to Annual
  Review of Control, Robotics, and Autonomous Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimum Enclosing Ball Synthetic Minority Oversampling Technique from a
  Geometric Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Yang Shangguan, Shi-Shun Chen, Xiao-Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance refers to the significant difference in the number of samples
from different classes within a dataset, making it challenging to identify
minority class samples correctly. This issue is prevalent in real-world
classification tasks, such as software defect prediction, medical diagnosis,
and fraud detection. The synthetic minority oversampling technique (SMOTE) is
widely used to address class imbalance issue, which is based on interpolation
between randomly selected minority class samples and their neighbors. However,
traditional SMOTE and most of its variants only interpolate between existing
samples, which may be affected by noise samples in some cases and synthesize
samples that lack diversity. To overcome these shortcomings, this paper
proposes the Minimum Enclosing Ball SMOTE (MEB-SMOTE) method from a geometry
perspective. Specifically, MEB is innovatively introduced into the oversampling
method to construct a representative point. Then, high-quality samples are
synthesized by interpolation between this representative point and the existing
samples. The rationale behind constructing a representative point is discussed,
demonstrating that the center of MEB is more suitable as the representative
point. To exhibit the superiority of MEB-SMOTE, experiments are conducted on 15
real-world imbalanced datasets. The results indicate that MEB-SMOTE can
effectively improve the classification performance on imbalanced datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhosein Chahe, Lifeng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method for open-vocabulary 3D scene
understanding in autonomous driving by combining Language Embedded 3D Gaussians
with Large Language Models (LLMs) for enhanced inference. We propose utilizing
LLMs to generate contextually relevant canonical phrases for segmentation and
scene interpretation. Our method leverages the contextual and semantic
capabilities of LLMs to produce a set of canonical phrases, which are then
compared with the language features embedded in the 3D Gaussians. This
LLM-guided approach significantly improves zero-shot scene understanding and
detection of objects of interest, even in the most challenging or unfamiliar
environments. Experimental results on the WayveScenes101 dataset demonstrate
that our approach surpasses state-of-the-art methods in terms of accuracy and
flexibility for open-vocabulary object detection and segmentation. This work
represents a significant advancement towards more intelligent, context-aware
autonomous driving systems, effectively bridging 3D scene representation with
high-level semantic understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and
  Tabnet with SMOTEENN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Yu, Yixin Jin, Qianwen Xing, Ye Zhang, Shaobo Guo, Shuchen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bank credit risk is a significant challenge in modern financial transactions,
and the ability to identify qualified credit card holders among a large number
of applicants is crucial for the profitability of a bank'sbank's credit card
business. In the past, screening applicants'applicants' conditions often
required a significant amount of manual labor, which was time-consuming and
labor-intensive. Although the accuracy and reliability of previously used ML
models have been continuously improving, the pursuit of more reliable and
powerful AI intelligent models is undoubtedly the unremitting pursuit by major
banks in the financial industry. In this study, we used a dataset of over
40,000 records provided by a commercial bank as the research object. We
compared various dimensionality reduction techniques such as PCA and T-SNE for
preprocessing high-dimensional datasets and performed in-depth adaptation and
tuning of distributed models such as LightGBM and XGBoost, as well as deep
models like Tabnet. After a series of research and processing, we obtained
excellent research results by combining SMOTEENN with these techniques. The
experiments demonstrated that LightGBM combined with PCA and SMOTEENN
techniques can assist banks in accurately predicting potential high-quality
customers, showing relatively outstanding performance compared to other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pagess on IEEE ICPICS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simultaneous and Meshfree Topology Optimization with Physics-informed
  Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Yousefpour, Shirin Hosseinmardi, Carlos Mora, Ramin Bostanabad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topology optimization (TO) provides a principled mathematical approach for
optimizing the performance of a structure by designing its material spatial
distribution in a pre-defined domain and subject to a set of constraints. The
majority of existing TO approaches leverage numerical solvers for design
evaluations during the optimization and hence have a nested nature and rely on
discretizing the design variables. Contrary to these approaches, herein we
develop a new class of TO methods based on the framework of Gaussian processes
(GPs) whose mean functions are parameterized via deep neural networks.
Specifically, we place GP priors on all design and state variables to represent
them via parameterized continuous functions. These GPs share a deep neural
network as their mean function but have as many independent kernels as there
are state and design variables. We estimate all the parameters of our model in
a single for loop that optimizes a penalized version of the performance metric
where the penalty terms correspond to the state equations and design
constraints. Attractive features of our approach include $(1)$ having a
built-in continuation nature since the performance metric is optimized at the
same time that the state equations are solved, and $(2)$ being
discretization-invariant and accommodating complex domains and topologies. To
test our method against conventional TO approaches implemented in commercial
software, we evaluate it on four problems involving the minimization of
dissipated power in Stokes flow. The results indicate that our approach does
not need filtering techniques, has consistent computational costs, and is
highly robust against random initializations and problem setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Malicious Internet Entity Detection Using Local Graph Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Mandlik, Tomas Pevny, Vaclav Smidl, Lukas Bajer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection of malicious behavior in a large network is a challenging problem
for machine learning in computer security, since it requires a model with high
expressive power and scalable inference. Existing solutions struggle to achieve
this feat -- current cybersec-tailored approaches are still limited in
expressivity, and methods successful in other domains do not scale well for
large volumes of data, rendering frequent retraining impossible. This work
proposes a new perspective for learning from graph data that is modeling
network entity interactions as a large heterogeneous graph. High expressivity
of the method is achieved with neural network architecture HMILnet that
naturally models this type of data and provides theoretical guarantees. The
scalability is achieved by pursuing local graph inference, i.e., classifying
individual vertices and their neighborhood as independent samples. Our
experiments exhibit improvement over the state-of-the-art Probabilistic Threat
Propagation (PTP) algorithm, show a further threefold accuracy improvement when
additional data is used, which is not possible with the PTP algorithm, and
demonstrate the generalization capabilities of the method to new, previously
unseen entities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preprint. Full publication:
  https://ieeexplore.ieee.org/document/10418120</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StructEval: Deepen and Broaden Large Language Model Assessment via
  Structured Evaluation <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxi Cao, Mengjie Ren, Hongyu Lin, Xianpei Han, Feng Zhang, Junfeng Zhan, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation is the baton for the development of large language models. Current
evaluations typically employ a single-item assessment paradigm for each atomic
test objective, which struggles to discern whether a model genuinely possesses
the required capabilities or merely memorizes/guesses the answers to specific
questions. To this end, we propose a novel evaluation framework referred to as
StructEval. Starting from an atomic test objective, StructEval deepens and
broadens the evaluation by conducting a structured assessment across multiple
cognitive levels and critical concepts, and therefore offers a comprehensive,
robust and consistent evaluation for LLMs. Experiments on three widely-used
benchmarks demonstrate that StructEval serves as a reliable tool for resisting
the risk of data contamination and reducing the interference of potential
biases, thereby providing more reliable and consistent conclusions regarding
model capabilities. Our framework also sheds light on the design of future
principled and trustworthy LLM evaluation protocols.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024;Benchmark at https://github.com/c-box/StructEval
  ;Leaderboard at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Ma, Zhengding Luo, Thanh Vinh Vo, Kuankuan Sima, Tze-Yun Leong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward shaping addresses the challenge of sparse rewards in reinforcement
learning by constructing denser and more informative reward signals. To achieve
self-adaptive and highly efficient reward shaping, we propose a novel method
that incorporates success rates derived from historical experiences into shaped
rewards. Our approach utilizes success rates sampled from Beta distributions,
which dynamically evolve from uncertain to reliable values as more data is
collected. Initially, the self-adaptive success rates exhibit more randomness
to encourage exploration. Over time, they become more certain to enhance
exploitation, thus achieving a better balance between exploration and
exploitation. We employ Kernel Density Estimation (KDE) combined with Random
Fourier Features (RFF) to derive the Beta distributions, resulting in a
computationally efficient implementation in high-dimensional continuous state
spaces. This method provides a non-parametric and learning-free approach. The
proposed method is evaluated on a wide range of continuous control tasks with
sparse and delayed rewards, demonstrating significant improvements in sample
efficiency and convergence stability compared to relevant baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11652v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11652v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny Gupta, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a privacy-preserving approach to train models
on decentralized data. Its potential in healthcare is significant, but
challenges arise due to cross-client variations in medical image data,
exacerbated by limited annotations. This paper introduces Cross-Client
Variations Adaptive Federated Learning (CCVA-FL) to address these issues.
CCVA-FL aims to minimize cross-client variations by transforming images into a
common feature space. It involves expert annotation of a subset of images from
each client, followed by the selection of a client with the least data
complexity as the target. Synthetic medical images are then generated using
Scalable Diffusion Models with Transformers (DiT) based on the target client's
annotated images. These synthetic images, capturing diversity and representing
the original data, are shared with other clients. Each client then translates
its local images into the target image space using image-to-image translation.
The translated images are subsequently used in a federated learning setting to
develop a server model. Our results demonstrate that CCVA-FL outperforms
Vanilla Federated Averaging by effectively addressing data distribution
differences across clients without compromising privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I found critical errors in the manuscript affecting its validity. I
  need to correct these before resubmitting. Major changes to methodology and
  results are underway, significantly altering the content. I will resubmit the
  revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training a multilayer dynamical spintronic network with standard machine
  learning tools to perform time series classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erwan Plouet, Dédalo Sanz-Hernández, Aymeric Vecchiola, Julie Grollier, Frank Mizrahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to process time-series at low energy cost is critical for many
applications. Recurrent neural network, which can perform such tasks, are
computationally expensive when implementing in software on conventional
computers. Here we propose to implement a recurrent neural network in hardware
using spintronic oscillators as dynamical neurons. Using numerical simulations,
we build a multi-layer network and demonstrate that we can use backpropagation
through time (BPTT) and standard machine learning tools to train this network.
Leveraging the transient dynamics of the spintronic oscillators, we solve the
sequential digits classification task with $89.83\pm2.91~\%$ accuracy, as good
as the equivalent software network. We devise guidelines on how to choose the
time constant of the oscillators as well as hyper-parameters of the network to
adapt to different input time scales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Set Multivariate Time-Series Anomaly Detection <span class="chip">ECAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12294v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12294v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Lai, Thi Kieu Khanh Ho, Narges Armanfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous methods for time-series anomaly detection (TSAD) have emerged in
recent years, most of which are unsupervised and assume that only normal
samples are available during the training phase, due to the challenge of
obtaining abnormal data in real-world scenarios. Still, limited samples of
abnormal data are often available, albeit they are far from representative of
all possible anomalies. Supervised methods can be utilized to classify normal
and seen anomalies, but they tend to overfit to the seen anomalies present
during training, hence, they fail to generalize to unseen anomalies. We propose
the first algorithm to address the open-set TSAD problem, called Multivariate
Open-Set Time-Series Anomaly Detector (MOSAD), that leverages only a few shots
of labeled anomalies during the training phase in order to achieve superior
anomaly detection performance compared to both supervised and unsupervised TSAD
algorithms. MOSAD is a novel multi-head TSAD framework with a shared
representation space and specialized heads, including the Generative head, the
Discriminative head, and the Anomaly-Aware Contrastive head. The latter
produces a superior representation space for anomaly detection compared to
conventional supervised contrastive learning. Extensive experiments on three
real-world datasets establish MOSAD as a new state-of-the-art in the TSAD
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Adversarial Models for Extreme Geospatial Downscaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guiye Li, Guofeng Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenges of climate change requires accurate and
high-resolution mapping of geospatial data, especially climate and weather
variables. However, many existing geospatial datasets, such as the gridded
outputs of the state-of-the-art numerical climate models (e.g., general
circulation models), are only available at very coarse spatial resolutions due
to the model complexity and extremely high computational demand.
Deep-learning-based methods, particularly generative adversarial networks
(GANs) and their variants, have proved effective for refining natural images
and have shown great promise in improving geospatial datasets. This paper
describes a conditional GAN-based stochastic geospatial downscaling method that
can accommodates very high scaling factors. Compared to most existing methods,
the method can generate high-resolution accurate climate datasets from very
low-resolution inputs. More importantly, the method explicitly considers the
uncertainty inherent to the downscaling process that tends to be ignored in
existing methods. Given an input, the method can produce a multitude of
plausible high-resolution samples instead of one single deterministic result.
These samples allow for an empirical exploration and inferences of model
uncertainty and robustness. With a case study of gridded climate datasets (wind
velocity and solar irradiance), we demonstrate the performances of the
framework in downscaling tasks with large scaling factors (up to $64\times$)
and highlight the advantages of the framework with a comprehensive comparison
with commonly used and most recent downscaling methods, including area-to-point
(ATP) kriging, deep image prior (DIP), enhanced super-resolution generative
adversarial networks (ESRGAN), physics-informed resolution-enhancing GAN (PhIRE
GAN), and an efficient diffusion model for remote sensing image
super-resolution (EDiffSR).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiNR: Model Based Neural Retrieval on GPUs at LinkedIn 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13218v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13218v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fedor Borisyuk, Qingquan Song, Mingzhou Zhou, Ganesh Parameswaran, Madhu Arun, Siva Popuri, Tugrul Bingol, Zhuotao Pei, Kuang-Hsuan Lee, Lu Zheng, Qizhan Shao, Ali Naqvi, Sen Zhou, Aman Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LiNR, LinkedIn's large-scale, GPU-based retrieval
system. LiNR supports a billion-sized index on GPU models. We discuss our
experiences and challenges in creating scalable, differentiable search indexes
using TensorFlow and PyTorch at production scale. In LiNR, both items and model
weights are integrated into the model binary. Viewing index construction as a
form of model training, we describe scaling our system for large indexes,
incorporating full scans and efficient filtering. A key focus is on enabling
attribute-based pre-filtering for exhaustive GPU searches, addressing the
common challenge of post-filtering in KNN searches that often reduces system
quality. We further provide multi-embedding retrieval algorithms and strategies
for tackling cold start issues in retrieval. Our advancements in supporting
larger indexes through quantization are also discussed. We believe LiNR
represents one of the industry's first Live-updated model-based retrieval
indexes. Applied to out-of-network post recommendations on LinkedIn Feed, LiNR
has contributed to a 3% relative increase in professional daily active users.
We envisage LiNR as a step towards integrating retrieval and ranking into a
single GPU model, simplifying complex infrastructures and enabling end-to-end
optimization of the entire differentiable infrastructure through gradient
descent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic Collective Action in Machine Learning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Hardt, Eric Mazumdar, Celestine Mendler-Dünner, Tijana Zrnic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate a principled study of algorithmic collective action on digital
platforms that deploy machine learning algorithms. We propose a simple
theoretical model of a collective interacting with a firm's learning algorithm.
The collective pools the data of participating individuals and executes an
algorithmic strategy by instructing participants how to modify their own data
to achieve a collective goal. We investigate the consequences of this model in
three fundamental learning-theoretic settings: the case of a nonparametric
optimal learning algorithm, a parametric risk minimizer, and gradient-based
optimization. In each setting, we come up with coordinated algorithmic
strategies and characterize natural success criteria as a function of the
collective's size. Complementing our theory, we conduct systematic experiments
on a skill classification task involving tens of thousands of resumes from a
gig platform for freelancers. Through more than two thousand model training
runs of a BERT-like language model, we see a striking correspondence emerge
between our empirical observations and the predictions made by our theory.
Taken together, our theory and experiments broadly support the conclusion that
algorithmic collectives of exceedingly small fractional size can exert
significant control over a platform's learning algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICML 2023; Revision corrects epsilon-dependence in the
  analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiRank: Industrial Large Scale Ranking Models at LinkedIn 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fedor Borisyuk, Mingzhou Zhou, Qingquan Song, Siyu Zhu, Birjodh Tiwana, Ganesh Parameswaran, Siddharth Dangi, Lars Hertel, Qiang Xiao, Xiaochen Hou, Yunbo Ouyang, Aman Gupta, Sheallika Singh, Dan Liu, Hailing Cheng, Lei Le, Jonathan Hung, Sathiya Keerthi, Ruoyan Wang, Fengyu Zhang, Mohit Kothari, Chen Zhu, Daqi Sun, Yun Dai, Xun Luan, Sirou Zhu, Zhiwei Wang, Neil Daftary, Qianqi Shen, Chengming Jiang, Haichao Wei, Maneesh Varshney, Amol Ghoting, Souvik Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LiRank, a large-scale ranking framework at LinkedIn that brings to
production state-of-the-art modeling architectures and optimization methods. We
unveil several modeling improvements, including Residual DCN, which adds
attention and residual connections to the famous DCNv2 architecture. We share
insights into combining and tuning SOTA architectures to create a unified
model, including Dense Gating, Transformers and Residual DCN. We also propose
novel techniques for calibration and describe how we productionalized deep
learning based explore/exploit methods. To enable effective, production-grade
serving of large ranking models, we detail how to train and compress models
using quantization and vocabulary compression. We provide details about the
deployment setup for large-scale use cases of Feed ranking, Jobs
Recommendations, and Ads click-through rate (CTR) prediction. We summarize our
learnings from various A/B tests by elucidating the most effective technical
approaches. These ideas have contributed to relative metrics improvements
across the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%
qualified job applications for Jobs search and recommendations, and +4.3% for
Ads CTR. We hope this work can provide practical insights and solutions for
practitioners interested in leveraging large-scale deep ranking systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PraFFL: A Preference-Aware Scheme in Fair Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongguang Ye, Wei-Bin Kou, Ming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in federated learning has emerged as a critical concern, aiming to
develop an unbiased model for any special group (e.g., male or female) of
sensitive features. However, there is a trade-off between model performance and
fairness, i.e., improving model fairness will decrease model performance.
Existing approaches have characterized such a trade-off by introducing
hyperparameters to quantify client's preferences for model fairness and model
performance. Nevertheless, these approaches are limited to scenarios where each
client has only a single pre-defined preference, and fail to work in practical
systems where each client generally have multiple preferences. The key
challenge is to design a method that allows the model to adapt to diverse
preferences of each client in real time. To this end, we propose a
Preference-aware scheme in Fair Federated Learning paradigm (called PraFFL) to
generate preference-wise model in real time. PraFFL can adaptively adjust the
model based on each client's preferences to meet their needs. We theoretically
prove that PraFFL can offer the optimal model tailored to an arbitrary
preference of each client, and show its linear convergence. Experimental
results show that our proposed PraFFL outperforms five fair federated learning
algorithms in terms of the model's capability of adapting to clients' different
preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twins and Civil Engineering Phases: Reorienting Adoption
  Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiwo A. Adebiyi, Nafeezat A. Ajenifuja, Ruda Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital twin (DT) technology has received immense attention over the years
due to the promises it presents to various stakeholders in science and
engineering. As a result, different thematic areas of DT have been explored.
This is no different in specific fields such as manufacturing, automation, oil
and gas, and civil engineering, leading to fragmented approaches for
field-specific applications. The civil engineering industry is further
disadvantaged in this regard as it relies on external techniques by other
engineering fields for its DT adoption. A rising consequence of these
extensions is a concentrated application of DT to the operations and
maintenance phase. On another spectrum, Building Information Modeling (BIM) is
pervasively utilized in the planning/design phase, and the transient nature of
the construction phase remains a challenge for its DT adoption. In this paper,
we present a phase-based development of DT in the Architecture, Engineering,
and Construction industry. We commence by presenting succinct expositions on DT
as a concept and as a service, and establish a five-level scale system.
Furthermore, we present separately a systematic literature review of the
conventional techniques employed at each civil engineering phase. In this
regard, we identified enabling technologies such as computer vision for
extended sensing and the Internet of Things for reliable integration.
Ultimately, we attempt to reveal DT as an important tool across the entire life
cycle of civil engineering projects, and nudge researchers to think more
holistically in their quest for the integration of DT for civil engineering
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visualize and Paint GAN Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15636v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15636v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudolf Herdt, Peter Maass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate how generated structures of GANs correlate with their
activations in hidden layers, with the purpose of better understanding the
inner workings of those models and being able to paint structures with
unconditionally trained GANs. This gives us more control over the generated
images, allowing to generate them from a semantic segmentation map while not
requiring such a segmentation in the training data. To this end we introduce
the concept of tileable features, allowing us to identify activations that work
well for painting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out-of-Distribution-Aware Electric Vehicle Charging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05941v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05941v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongxin Li, Chenxi Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of learning to charge Electric Vehicles (EVs) with
Out-of-Distribution (OOD) data. Traditional scheduling algorithms typically
fail to balance near-optimal average performance with worst-case guarantees,
particularly with OOD data. Model Predictive Control (MPC) is often too
conservative and data-independent, whereas Reinforcement Learning (RL) tends to
be overly aggressive and fully trusts the data, hindering their ability to
consistently achieve the best-of-both-worlds. To bridge this gap, we introduce
a novel OOD-aware scheduling algorithm, denoted OOD-Charging. This algorithm
employs a dynamic "awareness radius", which updates in real-time based on the
Temporal Difference (TD)-error that reflects the severity of OOD. The
OOD-Charging algorithm allows for a more effective balance between consistency
and robustness in EV charging schedules, thereby significantly enhancing
adaptability and efficiency in real-world charging environments. Our results
demonstrate that this approach improves the scheduling reward reliably under
real OOD scenarios with remarkable shifts of EV charging behaviors caused by
COVID-19 in the Caltech ACN-Data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G-invariant diffusion maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07350v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07350v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eitan Rosen, Xiuyuan Cheng, Yoel Shkolnisky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion maps embedding of data lying on a manifold has shown success in
tasks such as dimensionality reduction, clustering, and data visualization. In
this work, we consider embedding data sets that were sampled from a manifold
which is closed under the action of a continuous matrix group. An example of
such a data set is images whose planar rotations are arbitrary. The G-invariant
graph Laplacian, introduced in Part I of this work, admits eigenfunctions in
the form of tensor products between the elements of the irreducible unitary
representations of the group and eigenvectors of certain matrices. We employ
these eigenfunctions to derive diffusion maps that intrinsically account for
the group action on the data. In particular, we construct both equivariant and
invariant embeddings, which can be used to cluster and align the data points.
We demonstrate the utility of our construction in the problem of random
computerized tomography.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghyun Kim, Byeongho Heo, Dongyoon Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revives Densely Connected Convolutional Networks (DenseNets) and
reveals the underrated effectiveness over predominant ResNet-style
architectures. We believe DenseNets' potential was overlooked due to untouched
training methods and traditional design elements not fully revealing their
capabilities. Our pilot study shows dense connections through concatenation are
strong, demonstrating that DenseNets can be revitalized to compete with modern
architectures. We methodically refine suboptimal components - architectural
adjustments, block redesign, and improved training recipes towards widening
DenseNets and boosting memory efficiency while keeping concatenation shortcuts.
Our models, employing simple architectural elements, ultimately surpass Swin
Transformer, ConvNeXt, and DeiT-III - key architectures in the residual
learning lineage. Furthermore, our models exhibit near state-of-the-art
performance on ImageNet-1K, competing with the very recent models and
downstream tasks, ADE20k semantic segmentation, and COCO object
detection/instance segmentation. Finally, we provide empirical analyses that
uncover the merits of the concatenation over additive shortcuts, steering a
renewed preference towards DenseNet-style designs. Our code is available at
https://github.com/naver-ai/rdnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Code at https://github.com/naver-ai/rdnet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anomalies, Representations, and Self-Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barry M. Dillon, Luigi Favaro, Friedrich Feiden, Tanmoy Modak, Tilman Plehn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a self-supervised method for density-based anomaly detection using
contrastive learning, and test it using event-level anomaly data from CMS
ADC2021. The AnomalyCLR technique is data-driven and uses augmentations of the
background data to mimic non-Standard-Model events in a model-agnostic way. It
uses a permutation-invariant Transformer Encoder architecture to map the
objects measured in a collider event to the representation space, where the
data augmentations define a representation space which is sensitive to
potential anomalous features. An AutoEncoder trained on background
representations then computes anomaly scores for a variety of signals in the
representation space. With AnomalyCLR we find significant improvements on
performance metrics for all signals when compared to the raw data baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures, journal version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Kolmogorov Arnold Networks (KAN) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntian Hou, Di Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through this comprehensive survey of Kolmogorov-Arnold Networks(KAN), we have
gained a thorough understanding of its theoretical foundation, architectural
design, application scenarios, and current research progress. KAN, with its
unique architecture and flexible activation functions, excels in handling
complex data patterns and nonlinear relationships, demonstrating wide-ranging
application potential. While challenges remain, KAN is poised to pave the way
for innovative solutions in various fields, potentially revolutionizing how we
approach complex computational problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP with Generative Latent Replay: a Strong Baseline for Incremental
  Learning <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Frascaroli, Aniello Panariello, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of Transformers and Vision-Language Models (VLMs) such as
CLIP, large pre-trained models have become a common strategy to enhance
performance in Continual Learning scenarios. This led to the development of
numerous prompting strategies to effectively fine-tune transformer-based models
without succumbing to catastrophic forgetting. However, these methods struggle
to specialize the model on domains significantly deviating from the
pre-training and preserving its zero-shot capabilities. In this work, we
propose Continual Generative training for Incremental prompt-Learning, a novel
approach to mitigate forgetting while adapting a VLM, which exploits generative
replay to align prompts to tasks. We also introduce a new metric to evaluate
zero-shot capabilities within CL benchmarks. Through extensive experiments on
different domains, we demonstrate the effectiveness of our framework in
adapting to new tasks while improving zero-shot capabilities. Further analysis
reveals that our approach can bridge the gap with joint prompt tuning. The
codebase is available at https://github.com/aimagelab/mammoth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figure. Accepted at the The 35th British Machine Vision
  Conference 2024 (BMVC 2024), Glasgow, UK</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling for Model Predictive Trajectory Planning in Autonomous Driving
  using Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09657v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09657v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Rabenstein, Lars Ullrich, Knut Graichen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alongside optimization-based planners, sampling-based approaches are often
used in trajectory planning for autonomous driving due to their simplicity.
Model predictive path integral control is a framework that builds upon
optimization principles while incorporating stochastic sampling of input
trajectories. This paper investigates several sampling approaches for
trajectory generation. In this context, normalizing flows originating from the
field of variational inference are considered for the generation of sampling
distributions, as they model transformations of simple to more complex
distributions. Accordingly, learning-based normalizing flow models are trained
for a more efficient exploration of the input domain for the task at hand. The
developed algorithm and the proposed sampling distributions are evaluated in
two simulation scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to be published as part of the 2024 IEEE Intelligent
  Vehicles Symposium (IV), Jeju Shinhwa World, Jeju Island, Korea, June 2-5,
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Chen, Chenrui Tie, Ruihai Wu, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans perceive and interact with the world with the awareness of
equivariance, facilitating us in manipulating different objects in diverse
poses. For robotic manipulation, such equivariance also exists in many
scenarios. For example, no matter what the pose of a drawer is (translation,
rotation and tilt), the manipulation strategy is consistent (grasp the handle
and pull in a line). While traditional models usually do not have the awareness
of equivariance for robotic manipulation, which might result in more data for
training and poor performance in novel object poses, we propose our EqvAfford
framework, with novel designs to guarantee the equivariance in point-level
affordance learning for downstream robotic manipulation, with great performance
and generalization ability on representative tasks on objects in diverse poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAST: Binaural Audio Spectrogram <span class="highlight-title">Transformer</span> for Binaural Sound
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.03927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.03927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Kuang, Jie Shi, Kiki van der Heijden, Siamak Mehrkanoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate sound localization in a reverberation environment is essential for
human auditory perception. Recently, Convolutional Neural Networks (CNNs) have
been utilized to model the binaural human auditory pathway. However, CNN shows
barriers in capturing the global acoustic features. To address this issue, we
propose a novel end-to-end Binaural Audio Spectrogram Transformer (BAST) model
to predict the sound azimuth in both anechoic and reverberation environments.
Two modes of implementation, i.e. BAST-SP and BAST-NSP corresponding to BAST
model with shared and non-shared parameters respectively, are explored. Our
model with subtraction interaural integration and hybrid loss achieves an
angular distance of 1.29 degrees and a Mean Square Error of 1e-3 at all
azimuths, significantly surpassing CNN based model. The exploratory analysis of
the BAST's performance on the left-right hemifields and anechoic and
reverberation environments shows its generalization ability as well as the
feasibility of binaural Transformers in sound localization. Furthermore, the
analysis of the attention maps is provided to give additional insights on the
interpretation of the localization process in a natural reverberant
environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding
  Remote Smartphone-based Consultation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Srikanth, Jayang Gurung, N Satya Deepika, Vineet Joshi, Lopamudra Giri, Pravin Vaddavalli, Soumya Jana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blindness and other eye diseases are a global health concern, particularly in
low- and middle-income countries like India. In this regard, during the
COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi
attachment for smartphone-based eye imaging gained in use. However, quality of
user-captured image often remained inadequate, requiring clinician vetting and
delays. In this backdrop, we propose an AI-based quality assessment system with
instant feedback mimicking clinicians' judgments and tested on patient-captured
images. Dividing the complex problem hierarchically, here we tackle a
nontrivial part, and demonstrate a proof of the concept.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, Presented at IEEE EMBC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Classification of Subjective Time Perception Using Multi-modal
  Physiological Data of Air Traffic Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Till Aust, Eirini Balta, Argiro Vatakis, Heiko Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-pressure environments where human individuals must simultaneously
monitor multiple entities, communicate effectively, and maintain intense focus,
the perception of time becomes a critical factor influencing performance and
well-being. One indicator of well-being can be the person's subjective time
perception. In our project $ChronoPilot$, we aim to develop a device that
modulates human subjective time perception. In this study, we present a method
to automatically assess the subjective time perception of air traffic
controllers, a group often faced with demanding conditions, using their
physiological data and eleven state-of-the-art machine learning classifiers.
The physiological data consist of photoplethysmogram, electrodermal activity,
and temperature data. We find that the support vector classifier works best
with an accuracy of 79 % and electrodermal activity provides the most
descriptive biomarker. These findings are an important step towards closing the
feedback loop of our $ChronoPilot$-device to automatically modulate the user's
subjective time perception. This technological advancement may promise
improvements in task management, stress reduction, and overall productivity in
high-stakes professions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient
  <span class="highlight-title">Dataset</span> Distillation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18381v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18381v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Yong-Lu Li, Kaitong Cui, Ziyu Wang, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-efficient learning has garnered significant attention, especially given
the current trend of large multi-modal models. Recently, dataset distillation
has become an effective approach by synthesizing data samples that are
essential for network training. However, it remains to be explored which
samples are essential for the dataset distillation process itself. In this
work, we study the data efficiency and selection for the dataset distillation
task. By re-formulating the dynamics of distillation, we provide insight into
the inherent redundancy in the real dataset, both theoretically and
empirically. We propose to use the empirical loss value as a static data
pruning criterion. To further compensate for the variation of the data value in
training, we find the most contributing samples based on their causal effects
on the distillation. The proposed selection strategy can efficiently exploit
the training dataset, outperform the previous SOTA distillation algorithms, and
consistently enhance the distillation algorithms, even on much larger-scale and
more heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We
believe this paradigm will open up new avenues in the dynamics of distillation
and pave the way for efficient dataset distillation. Our code is available on
https://github.com/silicx/GoldFromOres-BiLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonparametric Linear Feature Learning in Regression Through
  Regularisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12754v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12754v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bertille Follain, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning plays a crucial role in automated feature selection,
particularly in the context of high-dimensional data, where non-parametric
methods often struggle. In this study, we focus on supervised learning
scenarios where the pertinent information resides within a lower-dimensional
linear subspace of the data, namely the multi-index model. If this subspace
were known, it would greatly enhance prediction, computation, and
interpretation. To address this challenge, we propose a novel method for joint
linear feature learning and non-parametric function estimation, aimed at more
effectively leveraging hidden features for learning. Our approach employs
empirical risk minimisation, augmented with a penalty on function derivatives,
ensuring versatility. Leveraging the orthogonality and rotation invariance
properties of Hermite polynomials, we introduce our estimator, named RegFeaL.
By using alternative minimisation, we iteratively rotate the data to improve
alignment with leading directions. We establish that the expected risk of our
method converges in high-probability to the minimal risk under minimal
assumptions and with explicit rates. Additionally, we provide empirical results
demonstrating the performance of RegFeaL in various experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse
  Training Data <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Lin, Reinhard Heckel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based methods for image reconstruction are state-of-the-art for
a variety of imaging tasks. However, neural networks often perform worse if the
training data differs significantly from the data they are applied to. For
example, a model trained for accelerated magnetic resonance imaging (MRI) on
one scanner performs worse on another scanner. In this work, we investigate the
impact of the training data on a model's performance and robustness for
accelerated MRI. We find that models trained on the combination of various data
distributions, such as those obtained from different MRI scanners and
anatomies, exhibit robustness equal or superior to models trained on the best
single distribution for a specific target distribution. Thus training on such
diverse data tends to improve robustness. Furthermore, training on such a
diverse dataset does not compromise in-distribution performance, i.e., a model
trained on diverse data yields in-distribution performance at least as good as
models trained on the more narrow individual distributions. Our results suggest
that training a model for imaging on a variety of distributions tends to yield
a more effective and robust model than maintaining separate models for
individual distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04202v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04202v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growing concerns about safety and alignment of AI systems highlight the
importance of embedding moral capabilities in artificial agents: a promising
solution is the use of learning from experience, i.e., Reinforcement Learning.
In multi-agent (social) environments, complex population-level phenomena may
emerge from interactions between individual learning agents. Many of the
existing studies rely on simulated social dilemma environments to study the
interactions of independent learning agents; however, they tend to ignore the
moral heterogeneity that is likely to be present in societies of agents in
practice. For example, at different points in time a single learning agent may
face opponents who are consequentialist (i.e., focused on maximizing outcomes
over time), norm-based (i.e., conforming to specific norms), or virtue-based
(i.e., considering a combination of different virtues). The extent to which
agents' co-development may be impacted by such moral heterogeneity in
populations is not well understood. In this paper, we present a study of the
learning dynamics of morally heterogeneous populations interacting in a social
dilemma setting. Using an Iterated Prisoner's Dilemma environment with a
partner selection mechanism, we investigate the extent to which the prevalence
of diverse moral agents in populations affects individual agents' learning
behaviors and emergent population-level outcomes. We observe several types of
non-trivial interactions between pro-social and anti-social agents, and find
that certain types of moral agents are able to steer selfish agents towards
more cooperative behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and
  Society - San Jose, CA, USA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Lipschitz Estimation for CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf Sulehman, Tingting Mu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the Lipschitz constant of deep neural networks is of growing
interest as it is useful for informing on generalisability and adversarial
robustness. Convolutional neural networks (CNNs) in particular, underpin much
of the recent success in computer vision related applications. However,
although existing methods for estimating the Lipschitz constant can be tight,
they have limited scalability when applied to CNNs. To tackle this, we propose
a novel method to accelerate Lipschitz constant estimation for CNNs. The core
idea is to divide a large convolutional block via a joint layer and width-wise
partition, into a collection of smaller blocks. We prove an upper-bound on the
Lipschitz constant of the larger block in terms of the Lipschitz constants of
the smaller blocks. Through varying the partition factor, the resulting method
can be adjusted to prioritise either accuracy or scalability and permits
parallelisation. We demonstrate an enhanced scalability and comparable accuracy
to existing baselines through a range of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An inconsistency between the input of the flattened convolutional
  block and the flattened, partitioned input impacts the validity of the
  proposed Lipschitz bound</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Granger Causality using Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.03703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.03703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Shahid Sultan, Samuel Horvath, Hernando Ombao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dependence between nodes in a network is an important concept that pervades
many areas including finance, politics, sociology, genomics and the brain
sciences. One way to characterize dependence between components of a
multivariate time series data is via Granger Causality (GC). Standard
traditional approaches to GC estimation / inference commonly assume linear
dynamics, however such simplification does not hold in many real-world
applications where signals are inherently non-linear. In such cases, imposing
linear models such as vector autoregressive (VAR) models can lead to
mis-characterization of true Granger Causal interactions. To overcome this
limitation, Tank et al (IEEE Transactions on Pattern Analysis and Machine
Learning, 2022) proposed a solution that uses neural networks with sparse
regularization penalties. The regularization encourages learnable weights to be
sparse, which enables inference on GC. This paper overcomes the limitations of
current methods by leveraging advances in machine learning and deep learning
which have been demonstrated to learn hidden patterns in the data. We propose
novel classes of models that can handle underlying non-linearity in a
computationally efficient manner, simultaneously providing GC and lag order
selection. Firstly, we present the Learned Kernel VAR (LeKVAR) model that
learns kernel parameterized by a shared neural net followed by penalization on
learnable weights to discover GC structure. Secondly, we show one can directly
decouple lags and individual time series importance via decoupled penalties.
This is important as we want to select the lag order during the process of GC
estimation. This decoupling acts as a filtering and can be extended to any DL
model including Multi-Layer Perceptrons (MLP), Recurrent Neural Networks (RNN),
Long Short Term Memory Networks (LSTM), Transformers etc, for simultaneous GC
estimation and lag selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be Submitted to a Journal work Presented at JSM. arXiv admin note:
  text overlap with arXiv:1802.05842 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Few-shot Class Incremental Learning in Audio
  Classification using Contrastive Representation <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riyansha Singh, Parinita Nema, Vinod K Kurmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machine learning applications, gradual data ingress is common, especially
in audio processing where incremental learning is vital for real-time
analytics. Few-shot class-incremental learning addresses challenges arising
from limited incoming data. Existing methods often integrate additional
trainable components or rely on a fixed embedding extractor post-training on
base sessions to mitigate concerns related to catastrophic forgetting and the
dangers of model overfitting. However, using cross-entropy loss alone during
base session training is suboptimal for audio data. To address this, we propose
incorporating supervised contrastive learning to refine the representation
space, enhancing discriminative power and leading to better generalization
since it facilitates seamless integration of incremental classes, upon arrival.
Experimental results on NSynth and LibriSpeech datasets with 100 classes, as
well as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pose-Aware <span class="highlight-title">Self-Supervised</span> Learning with Viewpoint Trajectory
  Regularization <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayun Wang, Yubei Chen, Stella X. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning visual features from unlabeled images has proven successful for
semantic categorization, often by mapping different $views$ of the same object
to the same feature to achieve recognition invariance. However, visual
recognition involves not only identifying $what$ an object is but also
understanding $how$ it is presented. For example, seeing a car from the side
versus head-on is crucial for deciding whether to stay put or jump out of the
way. While unsupervised feature learning for downstream viewpoint reasoning is
important, it remains under-explored, partly due to the lack of a standardized
evaluation method and benchmarks.
  We introduce a new dataset of adjacent image triplets obtained from a
viewpoint trajectory, without any semantic or pose labels. We benchmark both
semantic classification and pose estimation accuracies on the same visual
feature. Additionally, we propose a viewpoint trajectory regularization loss
for learning features from unlabeled image triplets. Our experiments
demonstrate that this approach helps develop a visual representation that
encodes object identity and organizes objects by their poses, retaining
semantic classification accuracy while achieving emergent global pose awareness
and better generalization to novel objects. Our dataset and code are available
at http://pwang.pw/trajSSL/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A ripple in time: a discontinuity in American history 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01185v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01185v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kolpakov, Igor Rivin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical note we suggest a novel approach to discover temporal
(related and unrelated to language dilation) and personality (authorship
attribution) in historical datasets. We exemplify our approach on the State of
the Union speeches given by the past 42 US presidents: this dataset is known
for its relatively small amount of data, and high variability of the amount and
style of texts. Nevertheless we manage to achieve about 95\% accuracy on the
authorship attribution task, and pin down the date of writing to a single
presidential term.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures; GitHub repository
  (https://github.com/sashakolpakov/ripple_in_time); restructured manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trading Devil Final: Backdoor attack via Stock market and Bayesian
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orson Mengara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the advent of generative artificial intelligence, every company and
researcher has been rushing to develop their own generative models, whether
commercial or not. Given the large number of users of these powerful new tools,
there is currently no intrinsically verifiable way to explain from the ground
up what happens when LLMs (large language models) learn. For example, those
based on automatic speech recognition systems, which have to rely on huge and
astronomical amounts of data collected from all over the web to produce fast
and efficient results, In this article, we develop a backdoor attack called
MarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is
mainly based on modern stock market models. In order to show the possible
vulnerabilities of speech-based transformers that may rely on LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>END :jumps-Diffusion and stock market: Better quantify uncertainty in
  financial simulations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Face of Populism: Examining Differences in Facial Emotional
  Expressions of Political Leaders Using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09914v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09914v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Major, Aleksandar Tomašević
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Populist rhetoric employed on online media is characterized as deeply
impassioned and often imbued with strong emotions. The aim of this paper is to
empirically investigate the differences in affective nonverbal communication of
political leaders. We use a deep-learning approach to process a sample of 220
YouTube videos of political leaders from 15 different countries, analyze their
facial expressions of emotion and then examine differences in average emotion
scores representing the relative presence of 6 emotional states (anger,
disgust, fear, happiness, sadness, and surprise) and a neutral expression for
each frame of the YouTube video. Based on a sample of manually coded images, we
find that this deep-learning approach has 53-60\% agreement with human labels.
We observe statistically significant differences in the average score of
negative emotions between groups of leaders with varying degrees of populist
rhetoric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 4.0: Annotation study added, supplementary information
  extended</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusing <span class="highlight-title">Pretrain</span>ed ViTs with TCNet for Enhanced EEG Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Modesitt, Haicheng Yin, Williams Huang Wang, Brian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Electroencephalogram (EEG) analysis is paramount to the
development of Brain-Computer Interfaces (BCIs). However, to reach the goal of
developing robust, useful BCIs depends heavily on the speed and the accuracy at
which BCIs can understand neural dynamics. In response to that goal, this paper
details the integration of pre-trained Vision Transformers (ViTs) with Temporal
Convolutional Networks (TCNet) to enhance the precision of EEG regression. The
core of this approach lies in harnessing the sequential data processing
strengths of ViTs along with the superior feature extraction capabilities of
TCNet, to significantly improve EEG analysis accuracy. In addition, we analyze
the importance of how to construct optimal patches for the attention mechanism
to analyze, balancing both speed and accuracy tradeoffs. Our results showcase a
substantial improvement in regression accuracy, as evidenced by the reduction
of Root Mean Square Error (RMSE) from 55.4 to 51.8 on EEGEyeNet's Absolute
Position Task, outperforming existing state-of-the-art models. Without
sacrificing performance, we increase the speed of this model by an order of
magnitude (up to 4.32x faster). This breakthrough not only sets a new benchmark
in EEG regression analysis but also opens new avenues for future research in
the integration of transformer architectures with specialized feature
extraction methods for diverse EEG datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted HCI International 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transfer Learning Study of Motion <span class="highlight-title">Transformer</span>-based Trajectory
  Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08271v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08271v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Ullrich, Alex McMaster, Knut Graichen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory planning in autonomous driving is highly dependent on predicting
the emergent behavior of other road users. Learning-based methods are currently
showing impressive results in simulation-based challenges, with
transformer-based architectures technologically leading the way. Ultimately,
however, predictions are needed in the real world. In addition to the shifts
from simulation to the real world, many vehicle- and country-specific shifts,
i.e. differences in sensor systems, fusion and perception algorithms as well as
traffic rules and laws, are on the agenda. Since models that can cover all
system setups and design domains at once are not yet foreseeable, model
adaptation plays a central role. Therefore, a simulation-based study on
transfer learning techniques is conducted on basis of a transformer-based
model. Furthermore, the study aims to provide insights into possible trade-offs
between computational time and performance to support effective transfers into
the real world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 2024 IEEE Intelligent Vehicles Symposium (IV), Jeju
  Shinhwa World, Jeju Island, Korea, June 2-5, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM
  Tuning in Real-World Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alon Halfon, Shai Gretz, Ofir Arviv, Artem Spector, Orith Toledo-Ronen, Yoav Katz, Liat Ein-Dor, Michal Shmueli-Scheuer, Noam Slonim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning Large Language Models (LLMs) is an effective method to enhance
their performance on downstream tasks. However, choosing the appropriate
setting of tuning hyperparameters (HPs) is a labor-intensive and
computationally expensive process. Here, we provide recommended HP
configurations for practical use-cases that represent a better starting point
for practitioners, when considering two SOTA LLMs and two commonly used tuning
methods. We describe Coverage-based Search (CBS), a process for ranking HP
configurations based on an offline extensive grid search, such that the top
ranked configurations collectively provide a practical robust recommendation
for a wide range of datasets and domains. We focus our experiments on
Llama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a
total of > 10,000 tuning experiments. Our results suggest that, in general,
Llama-3-8B and LoRA should be preferred, when possible. Moreover, we show that
for both models and tuning methods, exploring only a few HP configurations, as
recommended by our analysis, can provide excellent results in practice, making
this work a valuable resource for practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistically Plausible Counterfactual Explanations with Normalizing
  Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patryk Wielopolski, Oleksii Furman, Jerzy Stefanowski, Maciej Zięba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PPCEF, a novel method for generating probabilistically plausible
counterfactual explanations (CFs). PPCEF advances beyond existing methods by
combining a probabilistic formulation that leverages the data distribution with
the optimization of plausibility within a unified framework. Compared to
reference approaches, our method enforces plausibility by directly optimizing
the explicit density function without assuming a particular family of
parametrized distributions. This ensures CFs are not only valid (i.e., achieve
class change) but also align with the underlying data's probability density.
For that purpose, our approach leverages normalizing flows as powerful density
estimators to capture the complex high-dimensional data distribution.
Furthermore, we introduce a novel loss that balances the trade-off between
achieving class change and maintaining closeness to the original instance while
also incorporating a probabilistic plausibility term. PPCEF's unconstrained
formulation allows for efficient gradient-based optimization with batch
processing, leading to orders of magnitude faster computation compared to prior
methods. Moreover, the unconstrained formulation of PPCEF allows for the
seamless integration of future constraints tailored to specific counterfactual
properties. Finally, extensive evaluations demonstrate PPCEF's superiority in
generating high-quality, probabilistically plausible counterfactual
explanations in high-dimensional tabular settings. This makes PPCEF a powerful
tool for not only interpreting complex machine learning models but also for
improving fairness, accountability, and trust in AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeUltraFeedback: An LLM-as-a-Judge <span class="highlight-title">Dataset</span> for Aligning Large Language
  Models to Coding Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Weyssow, Aton Kamanda, Houari Sahraoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the alignment of large language models (LLMs) with user-defined
coding preferences is a challenging endeavour that requires a deep assessment
of LLMs' outputs. Existing methods and benchmarks rely primarily on automated
metrics and static analysis tools, which often fail to capture the nuances of
user instructions and LLM outputs. To address this gap, we propose using the
LLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding
preferences. Based on this approach, we present CodeUltraFeedback, a
comprehensive dataset designed to facilitate the evaluation and improvement of
LLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each
annotated with four responses generated from a diverse pool of 14 LLMs. These
responses are ranked based on five distinct coding preferences using GPT-3.5 as
a judge, providing both numerical scores and detailed textual feedback. Our
analysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are
generally preferred over those from open-weight LLMs, highlighting significant
differences in alignment between closed and open-weight models. In turn, we
explore the usage of CodeUltraFeedback as feedback data to fine-tune and align
CodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement
learning from AI feedback (RLAIF) with direct preference optimization (DPO).
The resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in
terms of alignment with coding preferences and shows improved functional
correctness on the HumanEval+ benchmark compared to the original instruct
model. Therefore, our contributions bridge the gap in preference tuning of LLMs
for code and set the stage for further advancements in model alignment and
RLAIF in automated software engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Harmonization: Federated Clustered Batch Effect Adjustment
  and Generalization <span class="chip">KDD2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bao Hoang, Yijiang Pang, Siqi Liang, Liang Zhan, Paul Thompson, Jiayu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Independent and identically distributed (i.i.d.) data is essential to many
data analysis and modeling techniques. In the medical domain, collecting data
from multiple sites or institutions is a common strategy that guarantees
sufficient clinical diversity, determined by the decentralized nature of
medical data. However, data from various sites are easily biased by the local
environment or facilities, thereby violating the i.i.d. rule. A common strategy
is to harmonize the site bias while retaining important biological information.
The ComBat is among the most popular harmonization approaches and has recently
been extended to handle distributed sites. However, when faced with situations
involving newly joined sites in training or evaluating data from unknown/unseen
sites, ComBat lacks compatibility and requires retraining with data from all
the sites. The retraining leads to significant computational and logistic
overhead that is usually prohibitive. In this work, we develop a novel Cluster
ComBat harmonization algorithm, which leverages cluster patterns of the data in
different sites and greatly advances the usability of ComBat harmonization. We
use extensive simulation and real medical imaging data from ADNI to demonstrate
the superiority of the proposed approach. Our codes are provided in
https://github.com/illidanlab/distributed-cluster-harmonization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, accepted to KDD2024-ADS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedBChain: A Blockchain-enabled Federated Learning Framework for
  Improving DeepConvLSTM with Comparative Strategy Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaoxuan Li, Chern Hong Lim, Qiyao Ma, Xinyu Tang, Hwa Hui Tew, Fan Ding, Xuewen Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in the field of Human Activity Recognition has shown that an
improvement in prediction performance can be achieved by reducing the number of
LSTM layers. However, this kind of enhancement is only significant on
monolithic architectures, and when it runs on large-scale distributed training,
data security and privacy issues will be reconsidered, and its prediction
performance is unknown. In this paper, we introduce a novel framework:
FedBChain, which integrates the federated learning paradigm based on a modified
DeepConvLSTM architecture with a single LSTM layer. This framework performs
comparative tests of prediction performance on three different real-world
datasets based on three different hidden layer units (128, 256, and 512)
combined with five different federated learning strategies, respectively. The
results show that our architecture has significant improvements in Precision,
Recall and F1-score compared to the centralized training approach on all
datasets with all hidden layer units for all strategies: FedAvg strategy
improves on average by 4.54%, FedProx improves on average by 4.57%,
FedTrimmedAvg improves on average by 4.35%, Krum improves by 4.18% on average,
and FedAvgM improves by 4.46% on average. Based on our results, it can be seen
that FedBChain not only improves in performance, but also guarantees the
security and privacy of user data compared to centralized training methods
during the training process. The code for our experiments is publicly available
(https://github.com/Glen909/FedBChain).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-<span class="highlight-title">Prompt</span>ing for Automating Zero-shot Visual Recognition with LLMs <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11755v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11755v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub Micorek, Mateusz Kozinski, Hilde Kuehne, Horst Possegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt ensembling of Large Language Model (LLM) generated category-specific
prompts has emerged as an effective method to enhance zero-shot recognition
ability of Vision-Language Models (VLMs). To obtain these category-specific
prompts, the present methods rely on hand-crafting the prompts to the LLMs for
generating VLM prompts for the downstream tasks. However, this requires
manually composing these task-specific prompts and still, they might not cover
the diverse set of visual concepts and task-specific styles associated with the
categories of interest. To effectively take humans out of the loop and
completely automate the prompt generation process for zero-shot recognition, we
propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only
minimal information about the target task, in the form of its short natural
language description, and a list of associated class labels, MPVR automatically
produces a diverse set of category-specific prompts resulting in a strong
zero-shot classifier. MPVR generalizes effectively across various popular
zero-shot image recognition benchmarks belonging to widely different domains
when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot
recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on
average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV Camera Ready. Code & Data:
  https://jmiemirza.github.io/Meta-Prompting/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fingerprinting Image-to-Image Generative Adversarial Networks <span class="chip">EuroS&P 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.11760v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.11760v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanlin Li, Guowen Xu, Han Qiu, Shangwei Guo, Run Wang, Jiwei Li, Tianwei Zhang, Rongxing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have been widely used in various
application scenarios. Since the production of a commercial GAN requires
substantial computational and human resources, the copyright protection of GANs
is urgently needed. This paper presents a novel fingerprinting scheme for the
Intellectual Property (IP) protection of image-to-image GANs based on a trusted
third party. We break through the stealthiness and robustness bottlenecks
suffered by previous fingerprinting methods for classification models being
naively transferred to GANs. Specifically, we innovatively construct a
composite deep learning model from the target GAN and a classifier. Then we
generate fingerprint samples from this composite model, and embed them in the
classifier for effective ownership verification. This scheme inspires some
concrete methodologies to practically protect the modern image-to-image
translation GANs. Theoretical analysis proves that these methods can satisfy
different security requirements necessary for IP protection. We also conduct
extensive experiments to show that our solutions outperform existing
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EuroS&P 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum Learning for ab initio Deep Learned Refractive Optics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01089v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01089v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinge Yang, Qiang Fu, Wolfgang Heidrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep optical optimization has recently emerged as a new paradigm for
designing computational imaging systems using only the output image as the
objective. However, it has been limited to either simple optical systems
consisting of a single element such as a diffractive optical element (DOE) or
metalens, or the fine-tuning of compound lenses from good initial designs. Here
we present a DeepLens design method based on curriculum learning, which is able
to learn optical designs of compound lenses ab initio from randomly initialized
surfaces without human intervention, therefore overcoming the need for a good
initial design. We demonstrate the effectiveness of our approach by fully
automatically designing both classical imaging lenses and a large field-of-view
extended depth-of-field computational lens in a cellphone-style form factor,
with highly aspheric surfaces and a short back focal length.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Automatically design computational lenses from scratch with
  differentiable ray tracing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recovering the state and dynamics of autonomous system with partial
  states solution using neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Kag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we explore the performance of deep hidden physics model (M.
Raissi 2018) for autonomous systems. These systems are described by set of
ordinary differential equations which do not explicitly depend on time. Such
systems can be found in nature and have applications in modeling chemical
concentrations, population dynamics, n-body problems in physics etc. In this
work we consider dynamics of states, which explain how the states will evolve
are unknown to us. We approximate state and dynamics both using neural
networks. We have considered examples of 2D linear/nonlinear and Lorenz
systems. We observe that even without knowing all the states information, we
can estimate dynamics of certain states whose state information are known.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning based ECG segmentation for delineation of diverse
  arrhythmias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06237v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06237v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chankyu Joung, Mijin Kim, Taejin Paik, Seong-Ho Kong, Seung-Young Oh, Won Kyeong Jeon, Jae-hu Jeon, Joong-Sik Hong, Wan-Joong Kim, Woong Kook, Myung-Jin Cha, Otto van Koert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate delineation of key waveforms in an ECG is a critical step in
extracting relevant features to support the diagnosis and treatment of heart
conditions. Although deep learning based methods using segmentation models to
locate P, QRS, and T waves have shown promising results, their ability to
handle arrhythmias has not been studied in any detail. In this paper we
investigate the effect of arrhythmias on delineation quality and develop
strategies to improve performance in such cases. We introduce a U-Net-like
segmentation model for ECG delineation with a particular focus on diverse
arrhythmias. This is followed by a post-processing algorithm which removes
noise and automatically determines the boundaries of P, QRS, and T waves. Our
model has been trained on a diverse dataset and evaluated against the LUDB and
QTDB datasets to show strong performance, with F1-scores exceeding 99% for QRS
and T waves, and over 97% for P waves in the LUDB dataset. Furthermore, we
assess various models across a wide array of arrhythmias and observe that
models with a strong performance on standard benchmarks may still perform
poorly on arrhythmias that are underrepresented in these benchmarks, such as
tachycardias. We propose solutions to address this discrepancy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Public Large Language Models to Synthesize Data for Private
  On-device Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Wu, Zheng Xu, Yanxiang Zhang, Yuanbo Zhang, Daniel Ramage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training on public data is an effective method to improve the performance
for federated learning (FL) with differential privacy (DP). This paper
investigates how large language models (LLMs) trained on public data can
improve the quality of pre-training data for the on-device language models
trained with DP and FL. We carefully design LLM prompts to filter and transform
existing public data, and generate new data to resemble the real user data
distribution. The model pre-trained on our synthetic dataset achieves relative
improvement of 19.0% and 22.8% in next word prediction accuracy compared to the
baseline model pre-trained on a standard public dataset, when evaluated over
the real user data in Gboard (Google Keyboard, a production mobile keyboard
application). Furthermore, our method achieves evaluation accuracy better than
or comparable to the baseline during the DP FL fine-tuning over millions of
mobile devices, and our final model outperforms the baseline in production A/B
testing. Our experiments demonstrate the strengths of LLMs in synthesizing data
close to the private distribution even without accessing the private data, and
also suggest future research directions to further reduce the distribution gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18403v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18403v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional
performance across various tasks through fine-tuning. Although low-rank
adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream
tasks, their deployment is still hindered by the vast model scale and
computational costs. Post-training model pruning offers a way to compress LLMs.
However, the current pruning methods designed for LLMs are not compatible with
LoRA. This is due to their utilization of unstructured pruning on LLMs,
impeding the merging of LoRA weights, or their dependence on the gradients of
pre-trained weights to guide pruning, which can impose significant memory
overhead. To this end, we propose LoRAPrune, a new framework that delivers an
accurate structured pruned model in a highly memory-efficient manner.
Specifically, we first design a LoRA-guided pruning criterion, which uses the
weights and gradients of LoRA, rather than the gradients of pre-trained weights
for importance estimation. We subsequently integrate this criterion into an
iterative pruning process, effectively removing redundant channels and heads.
Extensive experimental results demonstrate the superior performance of our
LoRAPrune over existing approaches on the LLaMA series models. At a 50\%
compression rate, LoRAPrune demonstrates superior performance over LLM-Pruner,
achieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while
also decreasing memory usage by 52.6%. Besides, LoRAPrune also matches
semi-structural pruning across multiple LLMs, proving its wide applicability.
The code is available at https://github.com/aim-uofa/LoRAPrune.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by acl 2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imperative Learning: A <span class="highlight-title">Self-supervised</span> Neural-Symbolic Learning
  Framework for Robot Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16087v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16087v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Wang, Kaiyi Ji, Junyi Geng, Zhongqiang Ren, Taimeng Fu, Fan Yang, Yifan Guo, Haonan He, Xiangyu Chen, Zitong Zhan, Qiwei Du, Shaoshu Su, Bowen Li, Yuheng Qiu, Yi Du, Qihang Li, Yifan Yang, Xiao Lin, Zhipeng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven methods such as reinforcement and imitation learning have
achieved remarkable success in robot autonomy. However, their data-centric
nature still hinders them from generalizing well to ever-changing environments.
Moreover, collecting large datasets for robotic tasks is often impractical and
expensive. To overcome these challenges, we introduce a new self-supervised
neural-symbolic (NeSy) computational framework, imperative learning (IL), for
robot autonomy, leveraging the generalization abilities of symbolic reasoning.
The framework of IL consists of three primary components: a neural module, a
reasoning engine, and a memory system. We formulate IL as a special bilevel
optimization (BLO), which enables reciprocal learning over the three modules.
This overcomes the label-intensive obstacles associated with data-driven
approaches and takes advantage of symbolic reasoning concerning logical
reasoning, physical principles, geometric analysis, etc. We discuss several
optimization techniques for IL and verify their effectiveness in five distinct
robot autonomy tasks including path planning, rule induction, optimal control,
visual odometry, and multi-robot routing. Through various experiments, we show
that IL can significantly enhance robot autonomy capabilities and we anticipate
that it will catalyze further research across diverse domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatio-Temporal Adaptive Diffusion Models for EEG Super-Resolution in
  Epilepsy Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03089v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03089v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zhou, Shuqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalogram (EEG) technology, particularly high-density EEG (HD EEG)
devices, is widely used in fields such as neuroscience. HD EEG devices improve
the spatial resolution of EEG by placing more electrodes on the scalp, meeting
the requirements of clinical diagnostic applications such as epilepsy focus
localization. However, this technique faces challenges such as high acquisition
costs and limited usage scenarios. In this paper, spatio-temporal adaptive
diffusion models (STADMs) are proposed to pioneer the use of diffusion models
for achieving spatial SR reconstruction from low-resolution (LR, 64 channels or
fewer) EEG to high-resolution (HR, 256 channels) EEG. Specifically, a
spatio-temporal condition module is designed to extract the spatio-temporal
features of LR EEG, which then serve as conditional inputs to guide the reverse
denoising process of diffusion models. Additionally, a multi-scale Transformer
denoising module is constructed to leverage multi-scale convolution blocks and
cross-attention-based diffusion Transformer blocks for conditional guidance to
generate subject-adaptive SR EEG. Experimental results demonstrate that the
proposed method effectively enhances the spatial resolution of LR EEG and
quantitatively outperforms existing methods. Furthermore, STADMs demonstrate
their value by applying synthetic SR EEG to classification and source
localization tasks of epilepsy patients, indicating their potential to
significantly improve the spatial resolution of LR EEG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Avoiding strict saddle points of nonconvex regularized problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09274v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09274v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luwei Bai, Yaohua Hu, Hao Wang, Xiaoqi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider a class of non-convex and non-smooth sparse
optimization problems, which encompass most existing nonconvex
sparsity-inducing terms. We show the second-order optimality conditions only
depend on the nonzeros of the stationary points. We propose two damped
iterative reweighted algorithms including the iteratively reweighted $\ell_1$
algorithm (DIRL$_1$) and the iteratively reweighted $\ell_2$ (DIRL$_2$)
algorithm, to solve these problems. For DIRL$_1$, we show the reweighted
$\ell_1$ subproblem has support identification property so that DIRL$_1$
locally reverts to a gradient descent algorithm around a stationary point. For
DIRL$_2$, we show the solution map of the reweighted $\ell_2$ subproblem is
differentiable and Lipschitz continuous everywhere. Therefore, the map of
DIRL$_1$ and DIRL$_2$ and their inverse are Lipschitz continuous, and the
strict saddle points are their unstable fixed points. By applying the stable
manifold theorem, these algorithms are shown to converge only to local
minimizers with randomly initialization when the strictly saddle point property
is assumed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Practical Guide for Causal Pathways and Sub-group Disparity Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02702v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02702v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnaz Kohankhaki, Shaina Raza, Oluwanifemi Bamgbose, Deval Pandya, Elham Dolatabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce the application of causal disparity analysis to
unveil intricate relationships and causal pathways between sensitive attributes
and the targeted outcomes within real-world observational data. Our methodology
involves employing causal decomposition analysis to quantify and examine the
causal interplay between sensitive attributes and outcomes. We also emphasize
the significance of integrating heterogeneity assessment in causal disparity
analysis to gain deeper insights into the impact of sensitive attributes within
specific sub-groups on outcomes. Our two-step investigation focuses on datasets
where race serves as the sensitive attribute. The results on two datasets
indicate the benefit of leveraging causal analysis and heterogeneity assessment
not only for quantifying biases in the data but also for disentangling their
influences on outcomes. We demonstrate that the sub-groups identified by our
approach to be affected the most by disparities are the ones with the largest
ML classification errors. We also show that grouping the data only based on a
sensitive attribute is not enough, and through these analyses, we can find
sub-groups that are directly affected by disparities. We hope that our findings
will encourage the adoption of such methodologies in future ethical AI
practices and bias audits, fostering a more equitable and fair technological
landscape.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-06T00:00:00Z">2024-08-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">124</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-OneVision: Easy Visual Task Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)
developed by consolidating our insights into data, models, and visual
representations in the LLaVA-NeXT blog series. Our experimental results
demonstrate that LLaVA-OneVision is the first single model that can
simultaneously push the performance boundaries of open LMMs in three important
computer vision scenarios: single-image, multi-image, and video scenarios.
Importantly, the design of LLaVA-OneVision allows strong transfer learning
across different modalities/scenarios, yielding new emerging capabilities. In
particular, strong video understanding and cross-scenario capabilities are
demonstrated through task transfer from images to videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Homepage:
  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Anything in Medical Images and Videos: Benchmark and Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Ma, Sumin Kim, Feifei Li, Mohammed Baharoon, Reza Asakereh, Hongwei Lyu, Bo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in segmentation foundation models have enabled accurate and
efficient segmentation across a wide range of natural images and videos, but
their utility to medical data remains unclear. In this work, we first present a
comprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11
medical image modalities and videos and point out its strengths and weaknesses
by comparing it to SAM1 and MedSAM. Then, we develop a transfer learning
pipeline and demonstrate SAM2 can be quickly adapted to medical domain by
fine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio
API for efficient 3D image and video segmentation. The code has been made
publicly available at \url{https://github.com/bowang-lab/MedSAM}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MDT-A2G: Exploring Masked Diffusion <span class="highlight-title">Transformer</span>s for Co-Speech Gesture
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Mao, Zhengkai Jiang, Qilin Wang, Chencan Fu, Jiangning Zhang, Jiafu Wu, Yabiao Wang, Chengjie Wang, Wei Li, Mingmin Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in the field of Diffusion Transformers have substantially
improved the generation of high-quality 2D images, 3D videos, and 3D shapes.
However, the effectiveness of the Transformer architecture in the domain of
co-speech gesture generation remains relatively unexplored, as prior
methodologies have predominantly employed the Convolutional Neural Network
(CNNs) or simple a few transformer layers. In an attempt to bridge this
research gap, we introduce a novel Masked Diffusion Transformer for co-speech
gesture generation, referred to as MDT-A2G, which directly implements the
denoising process on gesture sequences. To enhance the contextual reasoning
capability of temporally aligned speech-driven gestures, we incorporate a novel
Masked Diffusion Transformer. This model employs a mask modeling scheme
specifically designed to strengthen temporal relation learning among sequence
gestures, thereby expediting the learning process and leading to coherent and
realistic motions. Apart from audio, Our MDT-A2G model also integrates
multi-modal information, encompassing text, emotion, and identity. Furthermore,
we propose an efficient inference strategy that diminishes the denoising
computation by leveraging previously calculated results, thereby achieving a
speedup with negligible performance degradation. Experimental results
demonstrate that MDT-A2G excels in gesture generation, boasting a learning
speed that is over 6$\times$ faster than traditional diffusion transformers and
an inference speed that is 5.7$\times$ than the standard diffusion model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks <span class="chip">ICPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Sterzinger, Christian Stippel, Robert Sablatnig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Etruscan mirrors constitute a significant category in Etruscan art,
characterized by elaborate figurative illustrations featured on their backside.
A laborious and costly aspect of their analysis and documentation is the task
of manually tracing these illustrations. In previous work, a methodology has
been proposed to automate this process, involving photometric-stereo scanning
in combination with deep neural networks. While achieving quantitative
performance akin to an expert annotator, some results still lack qualitative
precision and, thus, require annotators for inspection and potential
correction, maintaining resource intensity. In response, we propose a deep
neural network trained to interactively refine existing annotations based on
human guidance. Our human-in-the-loop approach streamlines annotation,
achieving equal quality with up to 75% less manual input required. Moreover,
during the refinement process, the relative improvement of our methodology over
pure manual labeling reaches peak values of up to 26%, attaining drastically
better quality quicker. By being tailored to the complex task of segmenting
intricate lines, specifically distinguishing it from previous methods, our
approach offers drastic improvements in efficacy, transferable to a broad
spectrum of applications beyond Etruscan mirrors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, accepted at ICPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextIM: Part-aware Interactive Motion Synthesis from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Fan, Bo Du, Xiantao Cai, Bo Peng, Longling Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose TextIM, a novel framework for synthesizing
TEXT-driven human Interactive Motions, with a focus on the precise alignment of
part-level semantics. Existing methods often overlook the critical roles of
interactive body parts and fail to adequately capture and align part-level
semantics, resulting in inaccuracies and even erroneous movement outcomes. To
address these issues, TextIM utilizes a decoupled conditional diffusion
framework to enhance the detailed alignment between interactive movements and
corresponding semantic intents from textual descriptions. Our approach
leverages large language models, functioning as a human brain, to identify
interacting human body parts and to comprehend interaction semantics to
generate complicated and subtle interactive motion. Guided by the refined
movements of the interacting parts, TextIM further extends these movements into
a coherent whole-body motion. We design a spatial coherence module to
complement the entire body movements while maintaining consistency and harmony
across body parts using a part graph convolutional network. For training and
evaluation, we carefully selected and re-labeled interactive motions from
HUMANML3D to develop a specialized dataset. Experimental results demonstrate
that TextIM produces semantically accurate human interactive motions,
significantly enhancing the realism and applicability of synthesized
interactive motions in diverse scenarios, even including interactions with
deformable and dynamically changing objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training
  Quantization for Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianwei Yang, Haisong Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have garnered significant attention for their
performance in vision tasks; however, the high computational cost and
significant latency issues have hinder widespread adoption. Post-training
quantization (PTQ), a promising method for model compression, still faces
accuracy degradation challenges with ViTs. There are two reasons for this: the
existing quantization paradigm does not fit the power-law distribution of
post-Softmax activations well, and accuracy inevitably decreases after
reparameterizing post-LayerNorm activations. We propose a Distribution-Friendly
and Outlier-Aware Post-training Quantization method for Vision Transformers,
named DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and
introduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more
on values near 1, more accurately preserving the power-law distribution of
post-Softmax activations, and achieves favorable results. Moreover, when
reparameterizing post-LayerNorm activations from channel-wise to layer-wise
quantization, the accuracy degradation is mainly due to the significant impact
of outliers in the scaling factors. Therefore, DopQ-ViT proposes a method to
Search for the Optimal Scaling Factor, denoted as SOSF, which compensates for
the influence of outliers and preserves the performance of the quantization
model. DopQ-ViT has undergone extensive validation and demonstrates significant
performance improvements in quantization models, particularly in low-bit
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biomedical SAM 2: Segment Anything in Biomedical Images and Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiling Yan, Weixiang Sun, Rong Zhou, Zhengqing Yuan, Kai Zhang, Yiwei Li, Tianming Liu, Quanzheng Li, Xiang Li, Lifang He, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation and video object segmentation are essential for
diagnosing and analyzing diseases by identifying and measuring biological
structures. Recent advances in natural domain have been driven by foundation
models like the Segment Anything Model 2 (SAM 2). To explore the performance of
SAM 2 in biomedical applications, we designed two evaluation pipelines for
single-frame image segmentation and multi-frame video segmentation with varied
prompt designs, revealing SAM 2's limitations in medical contexts.
Consequently, we developed BioSAM 2, an enhanced foundation model optimized for
biomedical data based on SAM 2. Our experiments show that BioSAM 2 not only
surpasses the performance of existing state-of-the-art foundation models but
also matches or even exceeds specialist models, demonstrating its efficacy and
potential in the medical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually
  Synced Facial Performer <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhi Guan, Zhiliang Xu, Hang Zhou, Kaisiyuan Wang, Shengyi He, Zhanwang Zhang, Borong Liang, Haocheng Feng, Errui Ding, Jingtuo Liu, Jingdong Wang, Youjian Zhao, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lip-syncing videos with given audio is the foundation for various
applications including the creation of virtual presenters or performers. While
recent studies explore high-fidelity lip-sync with different techniques, their
task-orientated models either require long-term videos for clip-specific
training or retain visible artifacts. In this paper, we propose a unified and
effective framework ReSyncer, that synchronizes generalized audio-visual facial
information. The key design is revisiting and rewiring the Style-based
generator to efficiently adopt 3D facial dynamics predicted by a principled
style-injected Transformer. By simply re-configuring the information insertion
mechanisms within the noise and style space, our framework fuses motion and
appearance with unified training. Extensive experiments demonstrate that
ReSyncer not only produces high-fidelity lip-synced videos according to audio,
but also supports multiple appealing properties that are suitable for creating
virtual presenters and performers, including fast personalized fine-tuning,
video-driven lip-syncing, the transfer of speaking styles, and even face
swapping. Resources can be found at
https://guanjz20.github.io/projects/ReSyncer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to European Conference on Computer Vision (ECCV), 2024.
  Project page: https://guanjz20.github.io/projects/ReSyncer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMES: Asymmetric and Memory-Efficient Similarity Estimation for
  Instance-level Retrieval <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Suma, Giorgos Kordopatis-Zilos, Ahmet Iscen, Giorgos Tolias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the problem of instance-level image retrieval
re-ranking with the constraint of memory efficiency, ultimately aiming to limit
memory usage to 1KB per image. Departing from the prevalent focus on
performance enhancements, this work prioritizes the crucial trade-off between
performance and memory requirements. The proposed model uses a
transformer-based architecture designed to estimate image-to-image similarity
by capturing interactions within and across images based on their local
descriptors. A distinctive property of the model is the capability for
asymmetric similarity estimation. Database images are represented with a
smaller number of descriptors compared to query images, enabling performance
improvements without increasing memory consumption. To ensure adaptability
across different applications, a universal model is introduced that adjusts to
a varying number of local descriptors during the testing phase. Results on
standard benchmarks demonstrate the superiority of our approach over both
hand-crafted and learned models. In particular, compared with current
state-of-the-art methods that overlook their memory footprint, our approach not
only attains superior performance but does so with a significantly reduced
memory footprint. The code and pretrained models are publicly available at:
https://github.com/pavelsuma/ames
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for
  Accurate Robotic Grasping Under the Occlusion <span class="chip">IROS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyu Zhang, Yongchong Gu, Jianxiong Gao, Haitao Lin, Qiang Sun, Xinwei Sun, Xiangyang Xue, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of perceiving complete object shapes
through visual perception. While prior studies have demonstrated encouraging
outcomes in segmenting the visible parts of objects within a scene, amodal
segmentation, in particular, has the potential to allow robots to infer the
occluded parts of objects. To this end, this paper introduces a new framework
that explores amodal segmentation for robotic grasping in cluttered scenes,
thus greatly enhancing robotic grasping abilities. Initially, we use a
conventional segmentation algorithm to detect the visible segments of the
target object, which provides shape priors for completing the full object mask.
Particularly, to explore how to utilize semantic features from RGB images and
geometric information from depth images, we propose a Linear-fusion
Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the
linear-fusion strategy to effectively fuse this cross-modal data, and then uses
the prior visible mask as attention map to guide the network to focus on target
feature locations for further complete mask recovery. Using the amodal mask of
the target object provides advantages in selecting more accurate and robust
grasp points compared to relying solely on the visible segments. The results on
different datasets show that our method achieves state-of-the-art performance.
Furthermore, the robot experiments validate the feasibility and robustness of
this method in the real world. Our code and demonstrations are available on the
project page: https://jrryzh.github.io/LAC-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IROS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning for Image Complexity Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shipeng Liu, Liang Zhao, Dengfeng Chen, Zhanping Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying and evaluating image complexity can be instrumental in enhancing
the performance of various computer vision tasks. Supervised learning can
effectively learn image complexity features from well-annotated datasets.
However, creating such datasets requires expensive manual annotation costs. The
models may learn human subjective biases from it. In this work, we introduce
the MoCo v2 framework. We utilize contrastive learning to represent image
complexity, named CLIC (Contrastive Learning for Image Complexity). We find
that there are complexity differences between different local regions of an
image, and propose Random Crop and Mix (RCM), which can produce positive
samples consisting of multi-scale local crops. RCM can also expand the train
set and increase data diversity without introducing additional data. We conduct
extensive experiments with CLIC, comparing it with both unsupervised and
supervised methods. The results demonstrate that the performance of CLIC is
comparable to that of state-of-the-art supervised methods. In addition, we
establish the pipelines that can apply CLIC to computer vision tasks to
effectively improve their performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Line-based 6-DoF Object Pose Estimation and Tracking With an Event
  Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zibin Liu, Banglei Guan, Yang Shang, Qifeng Yu, Laurent Kneip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose estimation and tracking of objects is a fundamental application in 3D
vision. Event cameras possess remarkable attributes such as high dynamic range,
low latency, and resilience against motion blur, which enables them to address
challenging high dynamic range scenes or high-speed motion. These features make
event cameras an ideal complement over standard cameras for object pose
estimation. In this work, we propose a line-based robust pose estimation and
tracking method for planar or non-planar objects using an event camera.
Firstly, we extract object lines directly from events, then provide an initial
pose using a globally-optimal Branch-and-Bound approach, where 2D-3D line
correspondences are not known in advance. Subsequently, we utilize event-line
matching to establish correspondences between 2D events and 3D models.
Furthermore, object poses are refined and continuously tracked by minimizing
event-line distances. Events are assigned different weights based on these
distances, employing robust estimation algorithms. To evaluate the precision of
the proposed methods in object pose estimation and tracking, we have devised
and established an event-based moving object dataset. Compared against
state-of-the-art methods, the robustness and accuracy of our methods have been
validated both on synthetic experiments and the proposed dataset. The source
code is available at https://github.com/Zibin6/LOPET.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Image Processing,2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Learn without Forgetting using Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Vettoruzzo, Joaquin Vanschoren, Mohamed-Rafik Bouguelia, Thorsteinn Rögnvaldsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) refers to the ability to continually learn over time
by accommodating new knowledge while retaining previously learned experience.
While this concept is inherent in human learning, current machine learning
methods are highly prone to overwrite previously learned patterns and thus
forget past experience. Instead, model parameters should be updated selectively
and carefully, avoiding unnecessary forgetting while optimally leveraging
previously learned patterns to accelerate future learning. Since hand-crafting
effective update mechanisms is difficult, we propose meta-learning a
transformer-based optimizer to enhance CL. This meta-learned optimizer uses
attention to learn the complex relationships between model parameters across a
stream of tasks, and is designed to generate effective weight updates for the
current task while preventing catastrophic forgetting on previously encountered
tasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and
SplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both
forward and backward transfer, even on small sets of labeled data, highlighting
the advantages of integrating a meta-learned optimizer within the continual
learning framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning
  using Instruct <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ciara Rowles, Shimon Vainer, Dante De Nigris, Slava Elizarov, Konstantin Kutsy, Simon Donné
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models continuously push the boundary of state-of-the-art image
generation, but the process is hard to control with any nuance: practice proves
that textual prompts are inadequate for accurately describing image style or
fine structural details (such as faces). ControlNet and IPAdapter address this
shortcoming by conditioning the generative process on imagery instead, but each
individual instance is limited to modeling a single conditional posterior: for
practical use-cases, where multiple different posteriors are desired within the
same workflow, training and using multiple adapters is cumbersome. We propose
IPAdapter-Instruct, which combines natural-image conditioning with ``Instruct''
prompts to swap between interpretations for the same conditioning image: style
transfer, object extraction, both, or something else still? IPAdapterInstruct
efficiently learns multiple tasks with minimal loss in quality compared to
dedicated per-task models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures, Project page:
  https://unity-research.github.io/IP-Adapter-Instruct.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalizing Federated Instrument Segmentation with Visual Trait Priors
  in Robotic Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialang Xu, Jiacheng Wang, Lequan Yu, Danail Stoyanov, Yueming Jin, Evangelos B. Mazomenos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized federated learning (PFL) for surgical instrument segmentation
(SIS) is a promising approach. It enables multiple clinical sites to
collaboratively train a series of models in privacy, with each model tailored
to the individual distribution of each site. Existing PFL methods rarely
consider the personalization of multi-headed self-attention, and do not account
for appearance diversity and instrument shape similarity, both inherent in
surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait
priors for SIS, incorporating global-personalized disentanglement (GPD),
appearance-regulation personalized enhancement (APE), and shape-similarity
global enhancement (SGE), to boost SIS performance in each site. GPD represents
the first attempt at head-wise assignment for multi-headed self-attention
personalization. To preserve the unique appearance representation of each site
and gradually leverage the inter-site difference, APE introduces appearance
regulation and provides customized layer-wise aggregation solutions via
hypernetworks for each site's personalized parameters. The mutual shape
information of instruments is maintained and shared via SGE, which enhances the
cross-style shape consistency on the image level and computes the
shape-similarity contribution of each site on the prediction level for updating
the global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%
Dice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding
code and models will be released at https://github.com/wzjialang/PFedSIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via
  Spatio-Frequency Co-Query Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoming Zheng, Yinsong Wang, Siyi Du, Chen Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI) is a leading diagnostic modality for a wide
range of exams, where multiple contrast images are often acquired for
characterizing different tissues. However, acquiring high-resolution MRI
typically extends scan time, which can introduce motion artifacts.
Super-resolution of MRI therefore emerges as a promising approach to mitigate
these challenges. Earlier studies have investigated the use of multiple
contrasts for MRI super-resolution (MCSR), whereas majority of them did not
fully exploit the rich contrast-invariant structural information. To fully
utilize such crucial prior knowledge of multi-contrast MRI, in this work, we
propose a novel structure-guided MCSR (SGSR) framework based on a new
spatio-frequency co-query attention (CQA) mechanism. Specifically, CQA performs
attention on features of multiple contrasts with a shared structural query,
which is particularly designed to extract, fuse, and refine the common
structures from different contrasts. We further propose a novel
frequency-domain CQA module in addition to the spatial domain, to enable more
fine-grained structural refinement. Extensive experiments on fastMRI knee data
and low-field brain MRI show that SGSR outperforms state-of-the-art MCSR
methods with statistical significance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 15th International Workshop on Machine Learning in Medical
  Imaging (MLMI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient NeRF Optimization -- Not All Samples Remain Equally Hard 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juuso Korhonen, Goutham Rangu, Hamed R. Tavakoli, Juho Kannala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an application of online hard sample mining for efficient training
of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality
for many 3D reconstruction and rendering tasks but require substantial
computational resources. The encoding of the scene information within the NeRF
network parameters necessitates stochastic sampling. We observe that during the
training, a major part of the compute time and memory usage is spent on
processing already learnt samples, which no longer affect the model update
significantly. We identify the backward pass on the stochastic samples as the
computational bottleneck during the optimization. We thus perform the first
forward pass in inference mode as a relatively low-cost search for hard
samples. This is followed by building the computational graph and updating the
NeRF network parameters using only the hard samples. To demonstrate the
effectiveness of the proposed approach, we apply our method to Instant-NGP,
resulting in significant improvements of the view-synthesis quality over the
baseline (1 dB improvement on average per training time, or 2x speedup to reach
the same PSNR level) along with approx. 40% memory savings coming from using
only the hard samples to build the computational graph. As our method only
interfaces with the network module, we expect it to be widely applicable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Object is Worth 64x64 Pixels: Generating 3D Object via Image
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingguang Yan, Han-Hung Lee, Ziyu Wan, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new approach for generating realistic 3D models with UV maps
through a representation termed "Object Images." This approach encapsulates
surface geometry, appearance, and patch structures within a 64x64 pixel image,
effectively converting complex 3D shapes into a more manageable 2D format. By
doing so, we address the challenges of both geometric and semantic irregularity
inherent in polygonal meshes. This method allows us to use image generation
models, such as Diffusion Transformers, directly for 3D shape generation.
Evaluated on the ABO dataset, our generated shapes with patch structures
achieve point cloud FID comparable to recent 3D generative models, while
naturally supporting PBR material generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://omages.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dilated Convolution with Learnable Spacings makes visual models more
  aligned with humans: a Grad-CAM study <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabih Chamas, Ismail Khalfaoui-Hassani, Timothee Masquelier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced
convolution method that allows enlarging the receptive fields (RF) without
increasing the number of parameters, like the dilated convolution, yet without
imposing a regular grid. DCLS has been shown to outperform the standard and
dilated convolutions on several computer vision benchmarks. Here, we show that,
in addition, DCLS increases the models' interpretability, defined as the
alignment with human visual strategies. To quantify it, we use the Spearman
correlation between the models' GradCAM heatmaps and the ClickMe dataset
heatmaps, which reflect human visual attention. We took eight reference models
- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and
36) - and drop-in replaced the standard convolution layers with DCLS ones. This
improved the interpretability score in seven of them. Moreover, we observed
that Grad-CAM generated random heatmaps for two models in our study: CAFormer
and ConvFormer models, leading to low interpretability scores. We addressed
this issue by introducing Threshold-Grad-CAM, a modification built on top of
Grad-CAM that enhanced interpretability across nearly all models. The code and
checkpoints to reproduce this study are available at:
https://github.com/rabihchamas/DCLS-GradCAM-Eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The Trustworthy AI Workshop, IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative CT Reconstruction via Latent Variable Optimization of Shallow
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sho Ozaki, Shizuo Kaji, Toshikazu Imae, Kanabu Nawa, Hideomi Yamashita, Keiichi Nakagawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generative AI has garnered significant attention in recent years. In
particular, the diffusion model, a core component of recent generative AI,
produces high-quality images with rich diversity. In this study, we propose a
novel CT reconstruction method by combining the denoising diffusion
probabilistic model with iterative CT reconstruction. In sharp contrast to
previous studies, we optimize the fidelity loss of CT reconstruction with
respect to the latent variable of the diffusion model, instead of the image and
model parameters. To suppress anatomical structure changes produced by the
diffusion model, we shallow the diffusion and reverse processes, and fix a set
of added noises in the reverse process to make it deterministic during
inference. We demonstrate the effectiveness of the proposed method through
sparse view CT reconstruction of 1/10 view projection data. Despite the
simplicity of the implementation, the proposed method shows the capability of
reconstructing high-quality images while preserving the patient's anatomical
structure, and outperforms existing methods including iterative reconstruction,
iterative reconstruction with total variation, and the diffusion model alone in
terms of quantitative indices such as SSIM and PSNR. We also explore further
sparse view CT using 1/20 view projection data with the same trained diffusion
model. As the number of iterations increases, image quality improvement
comparable to that of 1/10 sparse view CT reconstruction is achieved. In
principle, the proposed method can be widely applied not only to CT but also to
other imaging modalities such as MRI, PET, and SPECT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Entity Information for Cross-Modality Correlation Learning:
  The Entity-Guided Multimodal Summarization <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanghai Zhang, Ye Liu, Shiwei Wu, Kai Zhang, Xukai Liu, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in multimedia data has spurred advancements in Multimodal
Summarization with Multimodal Output (MSMO), which aims to produce a multimodal
summary that integrates both text and relevant images. The inherent
heterogeneity of content within multimodal inputs and outputs presents a
significant challenge to the execution of MSMO. Traditional approaches
typically adopt a holistic perspective on coarse image-text data or individual
visual objects, overlooking the essential connections between objects and the
entities they represent. To integrate the fine-grained entity knowledge, we
propose an Entity-Guided Multimodal Summarization model (EGMS). Our model,
building on BART, utilizes dual multimodal encoders with shared weights to
process text-image and entity-image information concurrently. A gating
mechanism then combines visual data for enhanced textual summary generation,
while image selection is refined through knowledge distillation from a
pre-trained vision-language model. Extensive experiments on public MSMO dataset
validate the superiority of the EGMS method, which also prove the necessity to
incorporate entity information into MSMO problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In ACL-Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast
  and Reliable Surface Defect Detection <span class="chip">ICPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blaž Rolih, Matic Fučka, Danijel Skočaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of surface defect detection is to identify and localise abnormal
regions on the surfaces of captured objects, a task that's increasingly
demanded across various industries. Current approaches frequently fail to
fulfil the extensive demands of these industries, which encompass high
performance, consistency, and fast operation, along with the capacity to
leverage the entirety of the available training data. Addressing these gaps, we
introduce SuperSimpleNet, an innovative discriminative model that evolved from
SimpleNet. This advanced model significantly enhances its predecessor's
training consistency, inference time, as well as detection performance.
SuperSimpleNet operates in an unsupervised manner using only normal training
images but also benefits from labelled abnormal training images when they are
available. SuperSimpleNet achieves state-of-the-art results in both the
supervised and the unsupervised settings, as demonstrated by experiments across
four challenging benchmark datasets. Code:
https://github.com/blaz-r/SuperSimpleNet .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking In-the-wild Multimodal Disease Recognition and A Versatile
  Baseline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqi Wei, Zhi Chen, Zi Huang, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing plant disease classification models have achieved remarkable
performance in recognizing in-laboratory diseased images. However, their
performance often significantly degrades in classifying in-the-wild images.
Furthermore, we observed that in-the-wild plant images may exhibit similar
appearances across various diseases (i.e., small inter-class discrepancy) while
the same diseases may look quite different (i.e., large intra-class variance).
Motivated by this observation, we propose an in-the-wild multimodal plant
disease recognition dataset that contains the largest number of disease classes
but also text-based descriptions for each disease. Particularly, the newly
provided text descriptions are introduced to provide rich information in
textual modality and facilitate in-the-wild disease classification with small
inter-class discrepancy and large intra-class variance issues. Therefore, our
proposed dataset can be regarded as an ideal testbed for evaluating disease
recognition methods in the real world. In addition, we further present a strong
yet versatile baseline that models text descriptions and visual data through
multiple prototypes for a given class. By fusing the contributions of
multimodal prototypes in classification, our baseline can effectively address
the small inter-class discrepancy and large intra-class variance issues.
Remarkably, our baseline model can not only classify diseases but also
recognize diseases in few-shot or training-free scenarios. Extensive
benchmarking results demonstrate that our proposed in-the-wild multimodal
dataset sets many new challenges to the plant disease recognition task and
there is a large space to improve for future works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototype Learning for Micro-gesture Classification <span class="chip">IJCAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoliang Chen, Fei Wang, Kun Li, Zhiliang Wu, Hehe Fan, Yi Yang, Meng Wang, Dan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we briefly introduce the solution developed by our team,
HFUT-VUT, for the track of Micro-gesture Classification in the MiGA challenge
at IJCAI 2024. The task of micro-gesture classification task involves
recognizing the category of a given video clip, which focuses on more
fine-grained and subtle body movements compared to typical action recognition
tasks. Given the inherent complexity of micro-gesture recognition, which
includes large intra-class variability and minimal inter-class differences, we
utilize two innovative modules, i.e., the cross-modal fusion module and
prototypical refinement module, to improve the discriminative ability of MG
features, thereby improving the classification accuracy. Our solution achieved
significant success, ranking 1st in the track of Micro-gesture Classification.
We surpassed the performance of last year's leading team by a substantial
margin, improving Top-1 accuracy by 6.13%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1st Place in Micro-gesture Classification in MiGA at IJCAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. Manni, C. Lauretti, F. Prata, R. Papalia, L. Zollo, P. Soda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endoscopic surgery relies on two-dimensional views, posing challenges for
surgeons in depth perception and instrument manipulation. While Simultaneous
Localization and Mapping (SLAM) has emerged as a promising solution to address
these limitations, its implementation in endoscopic procedures presents
significant challenges due to hardware limitations, such as the use of a
monocular camera and the absence of odometry sensors. This study presents a
robust deep learning-based SLAM approach that combines state-of-the-art and
newly developed models. It consists of three main parts: the Monocular Pose
Estimation Module that introduces a novel unsupervised method based on the
CycleGAN architecture, the Monocular Depth Estimation Module that leverages the
novel Zoe architecture, and the 3D Reconstruction Module which uses information
from the previous models to create a coherent surgical map. The performance of
the procedure was rigorously evaluated using three publicly available datasets
(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art
methods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM
demonstrated superior performance compared to state-of-the-art depth estimation
algorithms in endoscopy, whereas the novel approach in the MPEM exhibited
competitive performance and the lowest inference time. The results showcase the
robustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three
different scenarios in endoscopic surgery. The proposed SLAM approach has the
potential to improve the accuracy and efficiency of endoscopic procedures by
providing surgeons with enhanced depth perception and 3D reconstruction
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCOPE: A Synthetic Multi-Modal <span class="highlight-title">Dataset</span> for Collective Perception
  Including Physical-Correct Weather Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jörg Gamerdinger, Sven Teufel, Patrick Schulz, Stephan Amann, Jan-Patrick Kirchner, Oliver Bringmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collective perception has received considerable attention as a promising
approach to overcome occlusions and limited sensing ranges of vehicle-local
perception in autonomous driving. In order to develop and test novel collective
perception technologies, appropriate datasets are required. These datasets must
include not only different environmental conditions, as they strongly influence
the perception capabilities, but also a wide range of scenarios with different
road users as well as realistic sensor models. Therefore, we propose the
Synthetic COllective PErception (SCOPE) dataset. SCOPE is the first synthetic
multi-modal dataset that incorporates realistic camera and LiDAR models as well
as parameterized and physically accurate weather simulations for both sensor
types. The dataset contains 17,600 frames from over 40 diverse scenarios with
up to 24 collaborative agents, infrastructure sensors, and passive traffic,
including cyclists and pedestrians. In addition, recordings from two novel
digital-twin maps from Karlsruhe and T\"ubingen are included. The dataset is
available at https://ekut-es.github.io/scope
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengfei Wang, Zongqian Zhan, Rui Xia, Linxia Ji, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last few decades, image-based building surface reconstruction has
garnered substantial research interest and has been applied across various
fields, such as heritage preservation, architectural planning, etc. Compared to
the traditional photogrammetric and NeRF-based solutions, recently, Gaussian
fields-based methods have exhibited significant potential in generating surface
meshes due to their time-efficient training and detailed 3D information
preservation. However, most gaussian fields-based methods are trained with all
image pixels, encompassing building and nonbuilding areas, which results in a
significant noise for building meshes and degeneration in time efficiency. This
paper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to
generate accurate surface reconstruction for building in a time-efficient way.
The framework first applies EfficientSAM and COLMAP to generate multi-level
masks of building and the corresponding masked point clouds. Subsequently, the
masked gaussian fields are trained by integrating two innovative losses: a
multi-level perceptual masked loss focused on constructing building regions and
a boundary loss aimed at enhancing the details of the boundaries between
different masks. Finally, we improve the tetrahedral surface mesh extraction
method based on the masked gaussian spheres. Comprehensive experiments on UAV
images demonstrate that, compared to the traditional method and several
NeRF-based and Gaussian-based SOTA solutions, our approach significantly
improves both the accuracy and efficiency of building surface reconstruction.
Notably, as a byproduct, there is an additional gain in the novel view
synthesis of building.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comb, Prune, Distill: Towards Unified Pruning for Vision Model
  Compression <span class="chip">SC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Schmitt, Ruiping Liu, Junwei Zheng, Jiaming Zhang, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lightweight and effective models are essential for devices with limited
resources, such as intelligent vehicles. Structured pruning offers a promising
approach to model compression and efficiency enhancement. However, existing
methods often tie pruning techniques to specific model architectures or vision
tasks. To address this limitation, we propose a novel unified pruning framework
Comb, Prune, Distill (CPD), which addresses both model-agnostic and
task-agnostic concerns simultaneously. Our framework employs a combing step to
resolve hierarchical layer-wise dependency issues, enabling architecture
independence. Additionally, the pruning pipeline adaptively remove parameters
based on the importance scoring metrics regardless of vision tasks. To support
the model in retaining its learned information, we introduce knowledge
distillation during the pruning step. Extensive experiments demonstrate the
generalizability of our framework, encompassing both convolutional neural
network (CNN) and transformer models, as well as image classification and
segmentation tasks. In image classification we achieve a speedup of up to x4.3
with a accuracy loss of 1.8% and in semantic segmentation up to x1.89 with a
5.1% loss in mIoU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ITSC 2024. Code is publicly available at:
  https://github.com/Cranken/CPD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Targeted Visual <span class="highlight-title">Prompt</span>ing for Medical Visual Question Answering <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergio Tascon-Morales, Pablo Márquez-Neila, Raphael Sznitman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With growing interest in recent years, medical visual question answering
(Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs)
emerging as an alternative to classical model architectures. Specifically,
their ability to add visual information to the input of pre-trained LLMs brings
new capabilities for image interpretation. However, simple visual errors cast
doubt on the actual visual understanding abilities of these models. To address
this, region-based questions have been proposed as a means to assess and
enhance actual visual understanding through compositional evaluation. To
combine these two perspectives, this paper introduces targeted visual prompting
to equip MLLMs with region-based questioning capabilities. By presenting the
model with both the isolated region and the region in its context in a
customized visual prompt, we show the effectiveness of our method across
multiple datasets while comparing it to several baseline models. Our code and
data are available at https://github.com/sergiotasconmorales/locvqallm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the MICCAI AMAI Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Free Condition Video Diffusion Models for single frame
  Spatial-Semantic Echocardiogram Synthesis <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Phi Nguyen, Tri Nhan Luong Ha, Huy Hieu Pham, Quoc Long Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional video diffusion models (CDM) have shown promising results for
video synthesis, potentially enabling the generation of realistic
echocardiograms to address the problem of data scarcity. However, current CDMs
require a paired segmentation map and echocardiogram dataset. We present a new
method called Free-Echo for generating realistic echocardiograms from a single
end-diastolic segmentation map without additional training data. Our method is
based on the 3D-Unet with Temporal Attention Layers model and is conditioned on
the segmentation map using a training-free conditioning method based on SDEdit.
We evaluate our model on two public echocardiogram datasets, CAMUS and
EchoNet-Dynamic. We show that our model can generate plausible echocardiograms
that are spatially aligned with the input segmentation map, achieving
performance comparable to training-based CDMs. Our work opens up new
possibilities for generating echocardiograms from a single segmentation map,
which can be used for data augmentation, domain adaptation, and other
applications in medical imaging. Our code is available at
\url{https://github.com/gungui98/echo-free}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nighttime Pedestrian Detection Based on Fore-Background Contrast
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Yao, Yongjun Zhang, Huachun Jian, Li Zhang, Ruzhong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significance of background information is frequently overlooked in
contemporary research concerning channel attention mechanisms. This study
addresses the issue of suboptimal single-spectral nighttime pedestrian
detection performance under low-light conditions by incorporating background
information into the channel attention mechanism. Despite numerous studies
focusing on the development of efficient channel attention mechanisms, the
relevance of background information has been largely disregarded. By adopting a
contrast learning approach, we reexamine channel attention with regard to
pedestrian objects and background information for nighttime pedestrian
detection, resulting in the proposed Fore-Background Contrast Attention (FBCA).
FBCA possesses two primary attributes: (1) channel descriptors form remote
dependencies with global spatial feature information; (2) the integration of
background information enhances the distinction between channels concentrating
on low-light pedestrian features and those focusing on background information.
Consequently, the acquired channel descriptors exhibit a higher semantic level
and spatial accuracy. Experimental outcomes demonstrate that FBCA significantly
outperforms existing methods in single-spectral nighttime pedestrian detection,
achieving state-of-the-art results on the NightOwls and TJU-DHD-pedestrian
datasets. Furthermore, this methodology also yields performance improvements
for the multispectral LLVIP dataset. These findings indicate that integrating
background information into the channel attention mechanism effectively
mitigates detector performance degradation caused by illumination factors in
nighttime scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CKNN: Cleansed k-Nearest Neighbor for Unsupervised Video Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihun Yi, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the problem of unsupervised video anomaly detection
(UVAD). The task aims to detect abnormal events in test video using unlabeled
videos as training data. The presence of anomalies in the training data poses a
significant challenge in this task, particularly because they form clusters in
the feature space. We refer to this property as the "Anomaly Cluster" issue.
The condensed nature of these anomalies makes it difficult to distinguish
between normal and abnormal data in the training set. Consequently, training
conventional anomaly detection techniques using an unlabeled dataset often
leads to sub-optimal results. To tackle this difficulty, we propose a new
method called Cleansed k-Nearest Neighbor (CKNN), which explicitly filters out
the Anomaly Clusters by cleansing the training dataset. Following the k-nearest
neighbor algorithm in the feature space provides powerful anomaly detection
capability. Although the identified Anomaly Cluster issue presents a
significant challenge to applying k-nearest neighbor in UVAD, our proposed
cleansing scheme effectively addresses this problem. We evaluate the proposed
method on various benchmark datasets and demonstrate that CKNN outperforms the
previous state-of-the-art UVAD method by up to 8.5% (from 82.0 to 89.0) in
terms of AUROC. Moreover, we emphasize that the performance of the proposed
method is comparable to that of the state-of-the-art method trained using
anomaly-free data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-path Collaborative Generation Network for Emotional Video
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Ye, Weidong Chen, Jingyu Li, Lei Zhang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional Video Captioning is an emerging task that aims to describe factual
content with the intrinsic emotions expressed in videos. The essential of the
EVC task is to effectively perceive subtle and ambiguous visual emotional cues
during the caption generation, which is neglected by the traditional video
captioning. Existing emotional video captioning methods perceive global visual
emotional cues at first, and then combine them with the video features to guide
the emotional caption generation, which neglects two characteristics of the EVC
task. Firstly, their methods neglect the dynamic subtle changes in the
intrinsic emotions of the video, which makes it difficult to meet the needs of
common scenes with diverse and changeable emotions. Secondly, as their methods
incorporate emotional cues into each step, the guidance role of emotion is
overemphasized, which makes factual content more or less ignored during
generation. To this end, we propose a dual-path collaborative generation
network, which dynamically perceives visual emotional cues evolutions while
generating emotional captions by collaborative learning. Specifically, in the
dynamic emotion perception path, we propose a dynamic emotion evolution module,
which first aggregates visual features and historical caption features to
summarize the global visual emotional cues, and then dynamically selects
emotional cues required to be re-composed at each stage. Besides, in the
adaptive caption generation path, to balance the description of factual content
and emotional cues, we propose an emotion adaptive decoder. Thus, our methods
can generate emotion-related words at the necessary time step, and our caption
generation balances the guidance of factual content and emotional cues well.
Extensive experiments on three challenging datasets demonstrate the superiority
of our approach and each proposed module.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acccepted by ACM Multimedia 2024, oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multitask and Multimodal Neural Tuning for Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Yu Song, Jihong Hu, Yen-Wei Chen, Lanfen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large-scale multimodal models have demonstrated impressive
capabilities across various domains. However, enabling these models to
effectively perform multiple multimodal tasks simultaneously remains a
significant challenge. To address this, we introduce a novel tuning method
called neural tuning, designed to handle diverse multimodal tasks concurrently,
including reasoning segmentation, referring segmentation, image captioning, and
text-to-image generation. Neural tuning emulates sparse distributed
representation in human brain, where only specific subsets of neurons are
activated for each task. Additionally, we present a new benchmark, MMUD, where
each sample is annotated with multiple task labels. By applying neural tuning
to pretrained large models on the MMUD benchmark, we achieve simultaneous task
handling in a streamlined and efficient manner. All models, code, and datasets
will be publicly available after publication, facilitating further research and
development in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamLCM: Towards High-Quality Text-to-3D Generation via Latent
  Consistency Model <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhong, Xiaolin Zhang, Yao Zhao, Yunchao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the text-to-3D task has developed rapidly due to the appearance of
the SDS method. However, the SDS method always generates 3D objects with poor
quality due to the over-smooth issue. This issue is attributed to two factors:
1) the DDPM single-step inference produces poor guidance gradients; 2) the
randomness from the input noises and timesteps averages the details of the 3D
contents.In this paper, to address the issue, we propose DreamLCM which
incorporates the Latent Consistency Model (LCM). DreamLCM leverages the
powerful image generation capabilities inherent in LCM, enabling generating
consistent and high-quality guidance, i.e., predicted noises or images. Powered
by the improved guidance, the proposed method can provide accurate and detailed
gradients to optimize the target 3D models.In addition, we propose two
strategies to enhance the generation quality further. Firstly, we propose a
guidance calibration strategy, utilizing Euler Solver to calibrate the guidance
distribution to accelerate 3D models to converge. Secondly, we propose a dual
timestep strategy, increasing the consistency of guidance and optimizing 3D
models from geometry to appearance in DreamLCM. Experiments show that DreamLCM
achieves state-of-the-art results in both generation quality and training
efficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jichuan Zhang, Yali Li, Xin Liu, Shengjin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-exemplar class-incremental learning (NECIL) is to resist catastrophic
forgetting without saving old class samples. Prior methodologies generally
employ simple rules to generate features for replaying, suffering from large
distribution gap between replayed features and real ones. To address the
aforementioned issue, we propose a simple, yet effective
\textbf{Diff}usion-based \textbf{F}eature \textbf{R}eplay (\textbf{DiffFR})
method for NECIL. First, to alleviate the limited representational capacity
caused by fixing the feature extractor, we employ Siamese-based self-supervised
learning for initial generalizable features. Second, we devise diffusion models
to generate class-representative features highly similar to real features,
which provides an effective way for exemplar-free knowledge memorization.
Third, we introduce prototype calibration to direct the diffusion model's focus
towards learning the distribution shapes of features, rather than the entire
distribution. Extensive experiments on public datasets demonstrate significant
performance gains of our DiffFR, outperforming the state-of-the-art NECIL
methods by 3.0\% in average. The code will be made publicly available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-agnostic Adversarial Perturbation for Vision-Language
  <span class="highlight-title">Pre-train</span>ing Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Zheng, Wen Jiang, Xinyang Deng, Wenrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies on AI security have highlighted the vulnerability of
Vision-Language Pre-training (VLP) models to subtle yet intentionally designed
perturbations in images and texts. Investigating multimodal systems' robustness
via adversarial attacks is crucial in this field. Most multimodal attacks are
sample-specific, generating a unique perturbation for each sample to construct
adversarial samples. To the best of our knowledge, it is the first work through
multimodal decision boundaries to explore the creation of a universal,
sample-agnostic perturbation that applies to any image. Initially, we explore
strategies to move sample points beyond the decision boundaries of linear
classifiers, refining the algorithm to ensure successful attacks under the top
$k$ accuracy metric. Based on this foundation, in visual-language tasks, we
treat visual and textual modalities as reciprocal sample points and decision
hyperplanes, guiding image embeddings to traverse text-constructed decision
boundaries, and vice versa. This iterative process consistently refines a
universal perturbation, ultimately identifying a singular direction within the
input space which is exploitable to impair the retrieval performance of VLP
models. The proposed algorithms support the creation of global perturbations or
adversarial patches. Comprehensive experiments validate the effectiveness of
our method, showcasing its data, task, and model transferability across various
VLP models and datasets. Code: https://github.com/LibertazZ/MUAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, published in ACMMM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASR-enhanced Multimodal Representation Learning for Cross-Domain Product
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Zhao, Jian Jia, Yan Li, Xuehan Bai, Quan Chen, Han Li, Peng Jiang, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce is increasingly multimedia-enriched, with products exhibited in a
broad-domain manner as images, short videos, or live stream promotions. A
unified and vectorized cross-domain production representation is essential. Due
to large intra-product variance and high inter-product similarity in the
broad-domain scenario, a visual-only representation is inadequate. While
Automatic Speech Recognition (ASR) text derived from the short or live-stream
videos is readily accessible, how to de-noise the excessively noisy text for
multimodal representation learning is mostly untouched. We propose ASR-enhanced
Multimodal Product Representation Learning (AMPere). In order to extract
product-specific information from the raw ASR text, AMPere uses an
easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,
together with visual data, is then fed into a multi-branch network to generate
compact multimodal embeddings. Extensive experiments on a large-scale
tri-domain dataset verify the effectiveness of AMPere in obtaining a unified
multimodal product representation that clearly improves cross-domain product
retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Point Cloud Geometry Compression with Context-based Residual Coding
  and INR-based Refinement <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Xu, Xi Zhang, Xiaolin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compressing a set of unordered points is far more challenging than
compressing images/videos of regular sample grids, because of the difficulties
in characterizing neighboring relations in an irregular layout of points. Many
researchers resort to voxelization to introduce regularity, but this approach
suffers from quantization loss. In this research, we use the KNN method to
determine the neighborhoods of raw surface points. This gives us a means to
determine the spatial context in which the latent features of 3D points are
compressed by arithmetic coding. As such, the conditional probability model is
adaptive to local geometry, leading to significant rate reduction.
Additionally, we propose a dual-layer architecture where a non-learning base
layer reconstructs the main structures of the point cloud at low complexity,
while a learned refinement layer focuses on preserving fine details. This
design leads to reductions in model complexity and coding latency by two orders
of magnitude compared to SOTA methods. Moreover, we incorporate an implicit
neural representation (INR) into the refinement layer, allowing the decoder to
sample points on the underlying surface at arbitrary densities. This work is
the first to effectively exploit content-aware local contexts for compressing
irregular raw point clouds, achieving high rate-distortion performance, low
complexity, and the ability to function as an arbitrary-scale upsampling
network simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Temporal Action Localization with Memory-Augmented <span class="highlight-title">Transformer</span> <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngkil Song, Dongkeun Kim, Minsu Cho, Suha Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online temporal action localization (On-TAL) is the task of identifying
multiple action instances given a streaming video. Since existing methods take
as input only a video segment of fixed size per iteration, they are limited in
considering long-term context and require tuning the segment size carefully. To
overcome these limitations, we propose memory-augmented transformer (MATR).
MATR utilizes the memory queue that selectively preserves the past segment
features, allowing to leverage long-term context for inference. We also propose
a novel action localization method that observes the current input segment to
predict the end time of the ongoing action and accesses the memory queue to
estimate the start time of the action. Our method outperformed existing methods
on two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the
online setting but also some offline TAL methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024, Project page:
  https://cvlab.postech.ac.kr/research/MATR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal
  Deepfake Detection <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juho Jung, Sangyoun Lee, Jooeon Kang, Yunjin Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All current benchmarks for multimodal deepfake detection manipulate entire
frames using various generation techniques, resulting in oversaturated
detection accuracies exceeding 94% at the video-level classification. However,
these benchmarks struggle to detect dynamic deepfake attacks with challenging
frame-by-frame alterations presented in real-world scenarios. To address this
limitation, we introduce FakeMix, a novel clip-level evaluation benchmark aimed
at identifying manipulated segments within both video and audio, providing
insight into the origins of deepfakes. Furthermore, we propose novel evaluation
metrics, Temporal Accuracy (TA) and Frame-wise Discrimination Metric (FDM), to
assess the robustness of deepfake detection models. Evaluating state-of-the-art
models against diverse deepfake benchmarks, particularly FakeMix, demonstrates
the effectiveness of our approach comprehensively. Specifically, while
achieving an Average Precision (AP) of 94.2% at the video-level, the evaluation
of the existing models at the clip-level using the proposed metrics, TA and
FDM, yielded sharp declines in accuracy to 53.1%, and 52.1%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, 2 tables, Accepted as Oral Presentation at The
  Trustworthy AI Workshop @ IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segmenting Small Stroke Lesions with Novel Labeling Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Shang, Zhengyang Lou, Andrew L. Alexander, Vivek Prabhakaran, William A. Sethares, Veena A. Nair, Nagesh Adluru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have demonstrated exceptional efficacy in stroke lesion
segmentation. However, the delineation of small lesions, critical for stroke
diagnosis, remains a challenge. In this study, we propose two straightforward
yet powerful approaches that can be seamlessly integrated into a variety of
networks: Multi-Size Labeling (MSL) and Distance-Based Labeling (DBL), with the
aim of enhancing the segmentation accuracy of small lesions. MSL divides lesion
masks into various categories based on lesion volume while DBL emphasizes the
lesion boundaries. Experimental evaluations on the Anatomical Tracings of
Lesions After Stroke (ATLAS) v2.0 dataset showcase that an ensemble of MSL and
DBL achieves consistently better or equal performance on recall (3.6% and
3.7%), F1 (2.4% and 1.5%), and Dice scores (1.3% and 0.0%) compared to the
top-1 winner of the 2022 MICCAI ATLAS Challenge on both the subset only
containing small lesions and the entire dataset, respectively. Notably, on the
mini-lesion subset, a single MSL model surpasses the previous best ensemble
strategy, with enhancements of 1.0% and 0.3% on F1 and Dice scores,
respectively. Our code is available at:
https://github.com/nadluru/StrokeLesSeg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Segment Anything Model 2: The Role of SAM2 in the
  Underwater Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Lian, Hua Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With breakthroughs in large-scale modeling, the Segment Anything Model (SAM)
and its extensions have been attempted for applications in various underwater
visualization tasks in marine sciences, and have had a significant impact on
the academic community. Recently, Meta has further developed the Segment
Anything Model 2 (SAM2), which significantly improves running speed and
segmentation accuracy compared to its predecessor. This report aims to explore
the potential of SAM2 in marine science by evaluating it on the underwater
instance segmentation benchmark datasets UIIS and USIS10K. The experiments show
that the performance of SAM2 is extremely dependent on the type of
user-provided prompts. When using the ground truth bounding box as prompt, SAM2
performed excellently in the underwater instance segmentation domain. However,
when running in automatic mode, SAM2's ability with point prompts to sense and
segment underwater instances is significantly degraded. It is hoped that this
paper will inspire researchers to further explore the SAM model family in the
underwater domain. The results and evaluation codes in this paper are available
at https://github.com/LiamLian0727/UnderwaterSAM2Eval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-View Pyramid Pooling in Deep Neural Networks for Improved Medical
  Image Classification and Confidence Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqing Zhang, Qiushi Nie, Zunjie Xiao, Jilu Zhao, Xiao Wu, Pengxin Guo, Runzhi Li, Jin Liu, Yanjie Wei, Yi Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial pooling (SP) and cross-channel pooling (CCP) operators have been
applied to aggregate spatial features and pixel-wise features from feature maps
in deep neural networks (DNNs), respectively. Their main goal is to reduce
computation and memory overhead without visibly weakening the performance of
DNNs. However, SP often faces the problem of losing the subtle feature
representations, while CCP has a high possibility of ignoring salient feature
representations, which may lead to both miscalibration of confidence issues and
suboptimal medical classification results. To address these problems, we
propose a novel dual-view framework, the first to systematically investigate
the relative roles of SP and CCP by analyzing the difference between spatial
features and pixel-wise features. Based on this framework, we propose a new
pooling method, termed dual-view pyramid pooling (DVPP), to aggregate
multi-scale dual-view features. DVPP aims to boost both medical image
classification and confidence calibration performance by fully leveraging the
merits of SP and CCP operators from a dual-axis perspective. Additionally, we
discuss how to fulfill DVPP with five parameter-free implementations. Extensive
experiments on six 2D/3D medical image classification tasks show that our DVPP
surpasses state-of-the-art pooling methods in terms of medical image
classification results and confidence calibration across different DNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Intelligent Traffic Systems: A Deep Learning Method for
  Accurate Arabic License Plate Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. A. Sayedelahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel two-stage framework for accurate Egyptian
Vehicle License Plate Recognition (EVLPR). The first stage employs image
processing techniques to reliably localize license plates, while the second
stage utilizes a custom-designed deep learning model for robust Arabic
character recognition. The proposed system achieves a remarkable 99.3% accuracy
on a diverse dataset, surpassing existing approaches. Its potential
applications extend to intelligent traffic management, including traffic
violation detection and parking optimization. Future research will focus on
enhancing the system's capabilities through architectural refinements, expanded
datasets, and addressing system dependencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lighthouse: A User-Friendly Library for Reproducible Video Moment
  Retrieval and Highlight Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Nishimura, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages; library tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedTrinity-25M: A Large-scale Multimodal <span class="highlight-title">Dataset</span> with Multigranular
  Annotations for Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, Yuyin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal
dataset for medicine, covering over 25 million images across 10 modalities,
with multigranular annotations for more than 65 diseases. These enriched
annotations encompass both global textual information, such as disease/lesion
type, modality, region-specific descriptions, and inter-regional relationships,
as well as detailed local annotations for regions of interest (ROIs), including
bounding boxes, segmentation masks. Unlike existing approach which is limited
by the availability of image-text pairs, we have developed the first automated
pipeline that scales up multimodal data by generating multigranular visual and
texual annotations (in the form of image-ROI-description triplets) without the
need for any paired text descriptions. Specifically, data from over 90
different sources have been collected, preprocessed, and grounded using
domain-specific expert models to identify ROIs related to abnormal regions. We
then build a comprehensive knowledge base and prompt multimodal large language
models to perform retrieval-augmented generation with the identified ROIs as
guidance, resulting in multigranular texual descriptions. Compared to existing
datasets, MedTrinity-25M provides the most enriched annotations, supporting a
comprehensive range of multimodal tasks such as captioning and report
generation, as well as vision-centric tasks like classification and
segmentation. Pretraining on MedTrinity-25M, our model achieves
state-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodal
large language models and other representative SoTA approaches. This dataset
can also be utilized to support large-scale pre-training of multimodal medical
AI models, contributing to the development of future foundation models in the
medical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page is at https://yunfeixie233.github.io/MedTrinity-25M</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse Generation while Maintaining Semantic Coordination: A
  Diffusion-Based Data Augmentation Method for Object Detection <span class="chip">ICPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Nie, Zhuo Wang, Xinxin Wang, Kun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies emphasize the crucial role of data augmentation in enhancing
the performance of object detection models. However,existing methodologies
often struggle to effectively harmonize dataset diversity with semantic
coordination.To bridge this gap, we introduce an innovative augmentation
technique leveraging pre-trained conditional diffusion models to mediate this
balance. Our approach encompasses the development of a Category Affinity
Matrix, meticulously designed to enhance dataset diversity, and a Surrounding
Region Alignment strategy, which ensures the preservation of semantic
coordination in the augmented images. Extensive experimental evaluations
confirm the efficacy of our method in enriching dataset diversity while
seamlessly maintaining semantic coordination. Our method yields substantial
average improvements of +1.4AP, +0.9AP, and +3.4AP over existing alternatives
on three distinct object detection models, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, ICPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VizECGNet: Visual ECG Image Network for Cardiovascular Diseases
  Classification with Multi-Modal Training and Knowledge Distillation <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju-Hyeon Nam, Seo-Hyung Park, Su Jung Kim, Sang-Chul Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An electrocardiogram (ECG) captures the heart's electrical signal to assess
various heart conditions. In practice, ECG data is stored as either digitized
signals or printed images. Despite the emergence of numerous deep learning
models for digitized signals, many hospitals prefer image storage due to cost
considerations. Recognizing the unavailability of raw ECG signals in many
clinical settings, we propose VizECGNet, which uses only printed ECG graphics
to determine the prognosis of multiple cardiovascular diseases. During
training, cross-modal attention modules (CMAM) are used to integrate
information from two modalities - image and signal, while self-modality
attention modules (SMAM) capture inherent long-range dependencies in ECG data
of each modality. Additionally, we utilize knowledge distillation to improve
the similarity between two distinct predictions from each modality stream. This
innovative multi-modal deep learning architecture enables the utilization of
only ECG images during inference. VizECGNet with image input achieves higher
performance in precision, recall, and F1-Score compared to signal-based ECG
classification models, with improvements of 3.50%, 8.21%, and 7.38%,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in International Conference on Image Processing (ICIP) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Body of Her: A Preliminary Study on End-to-End Humanoid Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tenglong Ao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive virtual humanoid agent is a crucial interface with the physical
world. A relatively complete humanoid agent first needs to have face and body,
then possess both verbal and non-verbal (such as eye contact, facial
expression, lip motion, gesture, and manipulation) abilities, and finally, it
is capable of real-time duplex communication, e.g., the ability to actively
interrupt conversations. Most prior systems typically only consider a subset of
these elements, leaving a gap from realistic humanoid agent. In this work, we
propose a real-time, duplex, interactive end-to-end network capable of modeling
realistic agent behaviors, including speech, full-body movements for talking,
responding, idling, and manipulation. This system is a multimodal model
integrating audio and visual inputs, extended from a pre-trained large language
model (LLM). We collect approximately 200,000 hours of audio, around 130,000
hours of video data, and about 20,000 alignment samples to build the model. The
final model demonstrates capabilities that are difficult to achieve in previous
systems, such as generalized object manipulation. This work performs a
preliminary exploration of the end-to-end approach in this field, aiming to
inspire further research towards scaling up.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report v1; Project Page:
  https://aubrey-ao.github.io/BodyOfHer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Foundation Models in Remote Sensing: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Lu, Junlin Guo, James R Zimmer-Dauphinee, Jordan M Nieusma, Xiao Wang, Parker VanValkenburgh, Steven A Wernke, Yuankai Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) technologies have profoundly transformed the
field of remote sensing, revolutionizing data collection, processing, and
analysis. Traditionally reliant on manual interpretation and task-specific
models, remote sensing has been significantly enhanced by the advent of
foundation models--large-scale, pre-trained AI models capable of performing a
wide array of tasks with unprecedented accuracy and efficiency. This paper
provides a comprehensive survey of foundation models in the remote sensing
domain, covering models released between June 2021 and June 2024. We categorize
these models based on their applications in computer vision and domain-specific
tasks, offering insights into their architectures, pre-training datasets, and
methodologies. Through detailed performance comparisons, we highlight emerging
trends and the significant advancements achieved by these foundation models.
Additionally, we discuss the technical challenges, practical implications, and
future research directions, addressing the need for high-quality data,
computational resources, and improved model generalization. Our research also
finds that pre-training methods, particularly self-supervised learning
techniques like contrastive learning and masked autoencoders, significantly
enhance the performance and robustness of foundation models in remote sensing
tasks such as scene classification, object detection, and other applications.
This survey aims to serve as a resource for researchers and practitioners by
providing a panorama of advances and promising pathways for continued
development and application of foundation models in remote sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Post-Mortem Human Iris Segmentation Analysis with Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afzal Hossain, Tipu Sultan, Stephanie Schuckers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iris recognition is widely used in several fields such as mobile phones,
financial transactions, identification cards, airport security, international
border control, voter registration for living persons. However, the possibility
of identifying deceased individuals based on their iris patterns has emerged
recently as a supplementary or alternative method valuable in forensic
analysis. Simultaneously, it poses numerous new technological challenges and
one of the most challenging among them is the image segmentation stage as
conventional iris recognition approaches have struggled to reliably execute it.
This paper presents and compares Deep Learning (DL) models designed for
segmenting iris images collected from the deceased subjects, by training SegNet
and DeepLabV3+ semantic segmentation methods where using VGG19, ResNet18,
ResNet50, MobileNetv2, Xception, or InceptionResNetv2 as backbones. In this
study, our experiments demonstrate that our proposed method effectively learns
and identifies specific deformations inherent in post-mortem samples and
providing a significant improvement in accuracy. By employing our novel method
MobileNetv2 as the backbone of DeepLabV3+ and replacing the final layer with a
hybrid loss function combining Boundary and Dice loss, we achieve Mean
Intersection over Union of 95.54% on the Warsaw-BioBase-PostMortem-Iris-v1
dataset. To the best of our knowledge, this study provides the most extensive
evaluation of DL models for post-mortem iris segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ijcb 2024 special session</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid diffusion models: combining supervised and generative <span class="highlight-title">pretrain</span>ing
  for label-efficient fine-tuning of segmentation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Sauvalle, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are considering in this paper the task of label-efficient fine-tuning of
segmentation models: We assume that a large labeled dataset is available and
allows to train an accurate segmentation model in one domain, and that we have
to adapt this model on a related domain where only a few samples are available.
We observe that this adaptation can be done using two distinct methods: The
first method, supervised pretraining, is simply to take the model trained on
the first domain using classical supervised learning, and fine-tune it on the
second domain with the available labeled samples. The second method is to
perform self-supervised pretraining on the first domain using a generic pretext
task in order to get high-quality representations which can then be used to
train a model on the second domain in a label-efficient way. We propose in this
paper to fuse these two approaches by introducing a new pretext task, which is
to perform simultaneously image denoising and mask prediction on the first
domain. We motivate this choice by showing that in the same way that an image
denoiser conditioned on the noise level can be considered as a generative model
for the unlabeled image distribution using the theory of diffusion models, a
model trained using this new pretext task can be considered as a generative
model for the joint distribution of images and segmentation masks under the
assumption that the mapping from images to segmentation masks is deterministic.
We then empirically show on several datasets that fine-tuning a model
pretrained using this approach leads to better results than fine-tuning a
similar model trained using either supervised or unsupervised pretraining only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Set2Seq <span class="highlight-title">Transformer</span>: Learning Permutation Aware Set Representations of
  Artistic Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athanasios Efthymiou, Stevan Rudinac, Monika Kackovic, Nachoem Wijnberg, Marcel Worring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Set2Seq Transformer, a novel sequential multiple instance
architecture, that learns to rank permutation aware set representations of
sequences. First, we illustrate that learning temporal position-aware
representations of discrete timesteps can greatly improve static visual
multiple instance learning methods that do not regard temporality and
concentrate almost exclusively on visual content analysis. We further
demonstrate the significant advantages of end-to-end sequential multiple
instance learning, integrating visual content and temporal information in a
multimodal manner. As application we focus on fine art analysis related tasks.
To that end, we show that our Set2Seq Transformer can leverage visual set and
temporal position-aware representations for modelling visual artists' oeuvres
for predicting artistic success. Finally, through extensive quantitative and
qualitative evaluation using a novel dataset, WikiArt-Seq2Rank, and a visual
learning-to-rank downstream task, we show that our Set2Seq Transformer captures
essential temporal information improving the performance of strong static and
sequential multiple instance learning methods for predicting artistic success.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biomedical Image Segmentation: A Systematic Literature <span class="highlight-title">Review</span> of Deep
  Learning Based Object Detection Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fazli Wahid, Yingliang Ma, Dawar Khan, Muhammad Aamir, Syed U. K. Bukhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical image segmentation plays a vital role in diagnosis of diseases
across various organs. Deep learning-based object detection methods are
commonly used for such segmentation. There exists an extensive research in this
topic. However, there is no standard review on this topic. Existing surveys
often lack a standardized approach or focus on broader segmentation techniques.
In this paper, we conducted a systematic literature review (SLR), collected and
analysed 148 articles that explore deep learning object detection methods for
biomedical image segmentation. We critically analyzed these methods, identified
the key challenges, and discussed the future directions. From the selected
articles we extracted the results including the deep learning models, targeted
imaging modalities, targeted diseases, and the metrics for the analysis of the
methods. The results have been presented in tabular and/or charted forms. The
results are presented in three major categories including two stage detection
models, one stage detection models and point-based detection models. Each
article is individually analyzed along with its pros and cons. Finally, we
discuss open challenges, potential benefits, and future research directions.
This SLR aims to provide the research community with a quick yet deeper
understanding of these segmentation models, ultimately facilitating the
development of more powerful solutions for biomedical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Non-negative VAE:the Generalized Gamma Belief Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Duan, Tiansheng Wen, Muyao Wang, Bo Chen, Mingyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The gamma belief network (GBN), often regarded as a deep topic model, has
demonstrated its potential for uncovering multi-layer interpretable latent
representations in text data. Its notable capability to acquire interpretable
latent factors is partially attributed to sparse and non-negative
gamma-distributed latent variables. However, the existing GBN and its
variations are constrained by the linear generative model, thereby limiting
their expressiveness and applicability. To address this limitation, we
introduce the generalized gamma belief network (Generalized GBN) in this paper,
which extends the original linear generative model to a more expressive
non-linear generative model. Since the parameters of the Generalized GBN no
longer possess an analytic conditional posterior, we further propose an
upward-downward Weibull inference network to approximate the posterior
distribution of the latent variables. The parameters of both the generative
model and the inference network are jointly trained within the variational
inference framework. Finally, we conduct comprehensive experiments on both
expressivity and disentangled representation learning tasks to evaluate the
performance of the Generalized GBN against state-of-the-art Gaussian
variational autoencoders serving as baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel
  View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiable volumetric rendering-based methods made significant progress
in novel view synthesis. On one hand, innovative methods have replaced the
Neural Radiance Fields (NeRF) network with locally parameterized structures,
enabling high-quality renderings in a reasonable time. On the other hand,
approaches have used differentiable splatting instead of NeRF's ray casting to
optimize radiance fields rapidly using Gaussian kernels, allowing for fine
adaptation to the scene. However, differentiable ray casting of irregularly
spaced kernels has been scarcely explored, while splatting, despite enabling
fast rendering times, is susceptible to clearly visible artifacts.
  Our work closes this gap by providing a physically consistent formulation of
the emitted radiance c and density {\sigma}, decomposed with Gaussian functions
associated with Spherical Gaussians/Harmonics for all-frequency colorimetric
representation. We also introduce a method enabling differentiable ray casting
of irregularly distributed Gaussians using an algorithm that integrates
radiance fields slab by slab and leverages a BVH structure. This allows our
approach to finely adapt to the scene while avoiding splatting artifacts. As a
result, we achieve superior rendering quality compared to the state-of-the-art
while maintaining reasonable training times and achieving inference speeds of
25 FPS on the Blender dataset. Project page with videos and code:
https://raygauss.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page with videos and code: https://raygauss.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastEdit: Fast Text-Guided Single-Image Editing via Semantic-Aware
  Diffusion Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Chen, Zecheng Zhao, Yadan Luo, Zi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional Text-guided single-image editing approaches require a two-step
process, including fine-tuning the target text embedding for over 1K iterations
and the generative model for another 1.5K iterations. Although it ensures that
the resulting image closely aligns with both the input image and the target
text, this process often requires 7 minutes per image, posing a challenge for
practical application due to its time-intensive nature. To address this
bottleneck, we introduce FastEdit, a fast text-guided single-image editing
method with semantic-aware diffusion fine-tuning, dramatically accelerating the
editing process to only 17 seconds. FastEdit streamlines the generative model's
fine-tuning phase, reducing it from 1.5K to a mere 50 iterations. For diffusion
fine-tuning, we adopt certain time step values based on the semantic
discrepancy between the input image and target text. Furthermore, FastEdit
circumvents the initial fine-tuning step by utilizing an image-to-image model
that conditions on the feature space, rather than the text embedding space. It
can effectively align the target text prompt and input image within the same
feature space and save substantial processing time. Additionally, we apply the
parameter-efficient fine-tuning technique LoRA to U-net. With LoRA, FastEdit
minimizes the model's trainable parameters to only 0.37\% of the original size.
At the same time, we can achieve comparable editing outcomes with significantly
reduced computational overhead. We conduct extensive experiments to validate
the editing performance of our approach and show promising editing
capabilities, including content addition, style transfer, background
replacement, and posture manipulation, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infusing Environmental Captions for Long-Form Video Language Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyogun Lee, Soyeon Hong, Mujeen Sung, Jinwoo Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the problem of long-form video-language grounding
(VLG). Given a long-form video and a natural language query, a model should
temporally localize the precise moment that answers the query. Humans can
easily solve VLG tasks, even with arbitrarily long videos, by discarding
irrelevant moments using extensive and robust knowledge gained from experience.
Unlike humans, existing VLG methods are prone to fall into superficial cues
learned from small-scale datasets, even when they are within irrelevant frames.
To overcome this challenge, we propose EI-VLG, a VLG method that leverages
richer textual information provided by a Multi-modal Large Language Model
(MLLM) as a proxy for human experiences, helping to effectively exclude
irrelevant frames. We validate the effectiveness of the proposed method via
extensive experiments on a challenging EgoNLQ benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenAI Arena: An Open Evaluation Platform for Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04485v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04485v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has made remarkable strides to revolutionize fields such as
image and video generation. These advancements are driven by innovative
algorithms, architecture, and data. However, the rapid proliferation of
generative models has highlighted a critical gap: the absence of trustworthy
evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc
often fail to capture the nuanced quality and user satisfaction associated with
generative outputs. This paper proposes an open platform GenAI-Arena to
evaluate different image and video generative models, where users can actively
participate in evaluating these models. By leveraging collective user feedback
and votes, GenAI-Arena aims to provide a more democratic and accurate measure
of model performance. It covers three arenas for text-to-image generation,
text-to-video generation, and image editing respectively. Currently, we cover a
total of 27 open-source generative models. GenAI-Arena has been operating for
four months, amassing over 6000 votes from the community. We describe our
platform, analyze the data, and explain the statistical methods for ranking the
models. To further promote the research in building model-based evaluation
metrics, we release a cleaned version of our preference data for the three
tasks, namely GenAI-Bench. We prompt the existing multi-modal models like
Gemini, GPT-4o to mimic human voting. We compute the correlation between model
voting with human voting to understand their judging abilities. Our results
show existing multimodal models are still lagging in assessing the generated
visual content, even the best model GPT-4o only achieves a Pearson correlation
of 0.22 in the quality subscore, and behaves like random guessing in others.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Source-Free Domain-Invariant Performance Prediction <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Khramtsova, Mahsa Baktashmotlagh, Guido Zuccon, Xi Wang, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating model performance poses a significant challenge,
particularly in scenarios where the source and target domains follow different
data distributions. Most existing performance prediction methods heavily rely
on the source data in their estimation process, limiting their applicability in
a more realistic setting where only the trained model is accessible. The few
methods that do not require source data exhibit considerably inferior
performance. In this work, we propose a source-free approach centred on
uncertainty-based estimation, using a generative model for calibration in the
absence of source data. We establish connections between our approach for
unsupervised calibration and temperature scaling. We then employ a
gradient-based strategy to evaluate the correctness of the calibrated
predictions. Our experiments on benchmark object recognition datasets reveal
that existing source-based methods fall short with limited source sample
availability. Furthermore, our approach significantly outperforms the current
state-of-the-art source-free and source-based methods, affirming its
effectiveness in domain-invariant performance estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Generalizable Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxu Wang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Gang Han, Wen Zhao, Weining Zhang, Yecheng Shao, Yijie Guo, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An excellent representation is crucial for reinforcement learning (RL)
performance, especially in vision-based reinforcement learning tasks. The
quality of the environment representation directly influences the achievement
of the learning task. Previous vision-based RL typically uses explicit or
implicit ways to represent environments, such as images, points, voxels, and
neural radiance fields. However, these representations contain several
drawbacks. They cannot either describe complex local geometries or generalize
well to unseen scenes, or require precise foreground masks. Moreover, these
implicit neural representations are akin to a ``black box", significantly
hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit
scene representation and differentiable rendering nature, is considered a
revolutionary change for reconstruction and representation methods. In this
paper, we propose a novel Generalizable Gaussian Splatting framework to be the
representation of RL tasks, called GSRL. Through validation in the RoboMimic
environment, our method achieves better results than other baselines in
multiple tasks, improving the performance by 10%, 44%, and 15% compared with
baselines on the hardest task. This work is the first attempt to leverage
generalizable 3DGS as a representation for RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Power of Data Tsunami: A Comprehensive <span class="highlight-title">Survey</span> on Data
  Assessment and Selection for Instruction Tuning of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning plays a critical role in aligning large language models
(LLMs) with human preference. Despite the vast amount of open instruction
datasets, naively training a LLM on all existing instructions may not be
optimal and practical. To pinpoint the most beneficial datapoints, data
assessment and selection methods have been proposed in the fields of natural
language processing (NLP) and deep learning. However, under the context of
instruction tuning, there still exists a gap in knowledge on what kind of data
evaluation metrics can be employed and how they can be integrated into the
selection mechanism. To bridge this gap, we present a comprehensive review on
existing literature of data assessment and selection especially for instruction
tuning of LLMs. We systematically categorize all applicable methods into
quality-based, diversity-based, and importance-based ones where a unified,
fine-grained taxonomy is structured. For each category, representative methods
are elaborated to describe the landscape of relevant research. In addition,
comparison between latest methods is conducted on their officially reported
results to provide in-depth discussions on their limitations. Finally, we
summarize the open challenges and propose the promosing avenues for future
studies. All related contents are available at
https://github.com/yuleiqin/fantastic-data-engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>review, survey, 28 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> and Evaluation of Adversarial Attacks for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khoi Nguyen Tiet Nguyen, Wenyu Zhang, Kangkang Lu, Yuhuan Wu, Xingjian Zheng, Hui Li Tan, Liangli Zhen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models excel in various computer vision tasks but are
susceptible to adversarial examples-subtle perturbations in input data that
lead to incorrect predictions. This vulnerability poses significant risks in
safety-critical applications such as autonomous vehicles, security
surveillance, and aircraft health monitoring. While numerous surveys focus on
adversarial attacks in image classification, the literature on such attacks in
object detection is limited. This paper offers a comprehensive taxonomy of
adversarial attacks specific to object detection, reviews existing adversarial
robustness evaluation metrics, and systematically assesses open-source attack
methods and model robustness. Key observations are provided to enhance the
understanding of attack effectiveness and corresponding countermeasures.
Additionally, we identify crucial research challenges to guide future efforts
in securing automated object detection systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Getting it Right: Improving Spatial Consistency in Text-to-Image Models <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agneet Chatterjee, Gabriela Ben Melech Stan, Estelle Aflalo, Sayak Paul, Dhruba Ghosh, Tejas Gokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, Yezhou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the key shortcomings in current text-to-image (T2I) models is their
inability to consistently generate images which faithfully follow the spatial
relationships specified in the text prompt. In this paper, we offer a
comprehensive investigation of this limitation, while also developing datasets
and methods that support algorithmic solutions to improve spatial reasoning in
T2I models. We find that spatial relationships are under-represented in the
image descriptions found in current vision-language datasets. To alleviate this
data bottleneck, we create SPRIGHT, the first spatially focused, large-scale
dataset, by re-captioning 6 million images from 4 widely used vision datasets
and through a 3-fold evaluation and analysis pipeline, show that SPRIGHT
improves the proportion of spatial relationships in existing datasets. We show
the efficacy of SPRIGHT data by showing that using only $\sim$0.25% of SPRIGHT
results in a 22% improvement in generating spatially accurate images while also
improving FID and CMMD scores. We also find that training on images containing
a larger number of objects leads to substantial improvements in spatial
consistency, including state-of-the-art results on T2I-CompBench with a spatial
score of 0.2133, by fine-tuning on <500 images. Through a set of controlled
experiments and ablations, we document additional findings that could support
future work that seeks to understand factors that affect spatial consistency in
text-to-image models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024. Project Page : https://spright-t2i.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshita Gupta, Gaurav Mittal, Ahmed Magooda, Ye Yu, Graham W. Taylor, Mei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Action Localization (TAL) involves localizing and classifying action
snippets in an untrimmed video. The emergence of large video foundation models
has led RGB-only video backbones to outperform previous methods needing both
RGB and optical flow modalities. Leveraging these large models is often limited
to training only the TAL head due to the prohibitively large GPU memory
required to adapt the video backbone for TAL. To overcome this limitation, we
introduce LoSA, the first memory-and-parameter-efficient backbone adapter
designed specifically for TAL to handle untrimmed videos. LoSA specializes for
TAL by introducing Long-Short-range Adapters that adapt the intermediate layers
of the video backbone over different temporal ranges. These adapters run
parallel to the video backbone to significantly reduce memory footprint. LoSA
also includes Long-Short-range Gated Fusion that strategically combines the
output of these adapters from the video backbone layers to enhance the video
features provided to the TAL head. Experiments show that LoSA significantly
outperforms all existing methods on standard TAL benchmarks, THUMOS-14 and
ActivityNet-v1.3, by scaling end-to-end backbone adaptation to
billion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them
beyond head-only transfer learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segment anything model 2: an application to 2D and 3D medical images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Dong, Hanxue Gu, Yaqian Chen, Jichen Yang, Maciej A. Mazurowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything Model (SAM) has gained significant attention because of its
ability to segment varous objects in images given a prompt. The recently
developed SAM 2 has extended this ability to video inputs. This opens an
opportunity to apply SAM to 3D images, one of the fundamental tasks in the
medical imaging field. In this paper, we extensively evaluate SAM 2's ability
to segment both 2D and 3D medical images by first collecting 18 medical imaging
datasets, including common 3D modalities such as computed tomography (CT),
magnetic resonance imaging (MRI), and positron emission tomography (PET) as
well as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of
SAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are
provided to one or multiple slice(s) selected from the volume, and (2)
single-frame 2D segmentation, where prompts are provided to each slice. The
former is only applicable to 3D modalities, while the latter applies to both 2D
and 3D modalities. Our results show that SAM 2 exhibits similar performance as
SAM under single-frame 2D segmentation, and has variable performance under
multi-frame 3D segmentation depending on the choices of slices to annotate, the
direction of the propagation, the predictions utilized during the propagation,
etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures. An updated version with new results and
  corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness Assessment of a Runway Object Classifier for Safe Aircraft
  Taxiing <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00035v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00035v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhak Elboher, Raya Elsaleh, Omri Isac, Mélanie Ducoffe, Audrey Galametz, Guillaume Povéda, Ryma Boumazouza, Noémie Cohen, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep neural networks (DNNs) are becoming the prominent solution for many
computational problems, the aviation industry seeks to explore their potential
in alleviating pilot workload and in improving operational safety. However, the
use of DNNs in this type of safety-critical applications requires a thorough
certification process. This need can be addressed through formal verification,
which provides rigorous assurances -- e.g.,~by proving the absence of certain
mispredictions. In this case-study paper, we demonstrate this process using an
image-classifier DNN currently under development at Airbus and intended for use
during the aircraft taxiing phase. We use formal methods to assess this DNN's
robustness to three common image perturbation types: noise, brightness and
contrast, and some of their combinations. This process entails multiple
invocations of the underlying verifier, which might be computationally
expensive; and we therefore propose a method that leverages the monotonicity of
these robustness properties, as well as the results of past verification
queries, in order to reduce the overall number of verification queries required
by nearly 60%. Our results provide an indication of the level of robustness
achieved by the DNN classifier under study, and indicate that it is
considerably more vulnerable to noise than to brightness or contrast
perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint version of the paper in the proceedings of 43rd
  Digital Avionics Systems Conference (DASC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColorSwap: A Color and Word Order <span class="highlight-title">Dataset</span> for Multimodal Evaluation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jirayu Burapacheep, Ishan Gaur, Agam Bhatia, Tristan Thrush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the ColorSwap dataset, designed to assess and improve
the proficiency of multimodal models in matching objects with their colors. The
dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000
examples. Each example includes a caption-image pair, along with a
``color-swapped'' pair. We follow the Winoground schema: the two captions in an
example have the same words, but the color words have been rearranged to modify
different objects. The dataset was created through a novel blend of automated
caption and image generation with humans in the loop. We evaluate image-text
matching (ITM) and visual language models (VLMs) and find that even the latest
ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on
our main VLM metric, although they may improve with more advanced prompting
techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP
perform close to chance (at 12% and 30%, respectively), although the
non-contrastive BLIP ITM model is stronger (87%). We also find that finetuning
on fewer than 2,000 examples yields significant performance gains on this
out-of-distribution word-order understanding task. The dataset is here:
https://github.com/Top34051/colorswap and here:
https://huggingface.co/datasets/stanfordnlp/colorswap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Attribution: Inherently Explainable Vision Model with
  Feature Detector <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianren Zhang, Dongwon Lee, Suhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep vision models' popularity rapidly increases, there is a growing
emphasis on explanations for model predictions. The inherently explainable
attribution method aims to enhance the understanding of model behavior by
identifying the important regions in images that significantly contribute to
predictions. It is achieved by cooperatively training a selector (generating an
attribution map to identify important features) and a predictor (making
predictions using the identified features). Despite many advancements, existing
methods suffer from the incompleteness problem, where discriminative features
are masked out, and the interlocking problem, where the non-optimized selector
initially selects noise, causing the predictor to fit on this noise and
perpetuate the cycle. To address these problems, we introduce a new objective
that discourages the presence of discriminative features in the masked-out
regions thus enhancing the comprehensiveness of feature selection. A
pre-trained detector is introduced to detect discriminative features in the
masked-out region. If the selector selects noise instead of discriminative
features, the detector can observe and break the interlocking situation by
penalizing the selector. Extensive experiments show that our model makes
accurate predictions with higher accuracy than the regular black-box model, and
produces attribution maps with high feature coverage, localization ability,
fidelity and robustness. Our code will be available at
\href{https://github.com/Zood123/COMET}{https://github.com/Zood123/COMET}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep-learning Assisted Detection and Quantification of (oo)cysts of
  Giardia and Cryptosporidium on Smartphone Microscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suprim Nakarmi, Sanam Pudasaini, Safal Thapaliya, Pratima Upretee, Retina Shrestha, Basant Giri, Bhanu Bhakta Neupane, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The consumption of microbial-contaminated food and water is responsible for
the deaths of millions of people annually. Smartphone-based microscopy systems
are portable, low-cost, and more accessible alternatives for the detection of
Giardia and Cryptosporidium than traditional brightfield microscopes. However,
the images from smartphone microscopes are noisier and require manual cyst
identification by trained technicians, usually unavailable in resource-limited
settings. Automatic detection of (oo)cysts using deep-learning-based object
detection could offer a solution for this limitation. We evaluate the
performance of four state-of-the-art object detectors to detect (oo)cysts of
Giardia and Cryptosporidium on a custom dataset that includes both smartphone
and brightfield microscopic images from vegetable samples. Faster RCNN,
RetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer
(Deformable DETR) deep-learning models were employed to explore their efficacy
and limitations. Our results show that while the deep-learning models perform
better with the brightfield microscopy image dataset than the smartphone
microscopy image dataset, the smartphone microscopy predictions are still
comparable to the prediction performance of non-experts. Also, we publicly
release brightfield and smartphone microscopy datasets with the benchmark
results for the detection of Giardia and Cryptosporidium, independently
captured on reference (or standard lab setting) and vegetable samples. Our code
and dataset are available at
https://github.com/naamiinepal/smartphone_microscopy and
https://doi.org/10.5281/zenodo.7813183, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages (including supplementary information), 5 figures, 7 tables,
  Accepted for publication at the Journal of Machine Learning for Biomedical
  Imaging (MELBA) https://melba-journal.org/2024:014</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PT43D: A Probabilistic <span class="highlight-title">Transformer</span> for Generating 3D Shapes from Single
  Highly-Ambiguous RGB Images <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Xiong, Angela Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating 3D shapes from single RGB images is essential in various
applications such as robotics. Current approaches typically target images
containing clear and complete visual descriptions of the object, without
considering common realistic cases where observations of objects that are
largely occluded or truncated. We thus propose a transformer-based
autoregressive model to generate the probabilistic distribution of 3D shapes
conditioned on an RGB image containing potentially highly ambiguous
observations of the object. To handle realistic scenarios such as occlusion or
field-of-view truncation, we create simulated image-to-shape training pairs
that enable improved fine-tuning for real-world scenarios. We then adopt
cross-attention to effectively identify the most relevant region of interest
from the input image for shape generation. This enables inference of sampled
shapes with reasonable diversity and strong alignment with the input image. We
train and test our model on our synthetic data then fine-tune and test it on
real-world data. Experiments demonstrate that our model outperforms state of
the art in both scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures. Accepted to BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with
  Occlusion Handling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyue Wan, Zhuo Chen, Yiming Bao, Xu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of 3D Human Pose Estimation, which finds widespread daily
applications, the requirement for convenient acquisition equipment continues to
grow. To satisfy this demand, we set our sights on a short-baseline binocular
setting that offers both portability and a geometric measurement property that
radically mitigates depth ambiguity. However, as the binocular baseline
shortens, two serious challenges emerge: first, the robustness of 3D
reconstruction against 2D errors deteriorates; and second, occlusion reoccurs
due to the limited visual differences between two views. To address the first
challenge, we propose the Stereo Co-Keypoints Estimation module to improve the
view consistency of 2D keypoints and enhance the 3D robustness. In this module,
the disparity is utilized to represent the correspondence of binocular 2D
points and the Stereo Volume Feature is introduced to contain binocular
features across different disparities. Through the regression of SVF, two-view
2D keypoints are simultaneously estimated in a collaborative way which
restricts their view consistency. Furthermore, to deal with occlusions, a
Pre-trained Pose Transformer module is introduced. Through this module, 3D
poses are refined by perceiving pose coherence, a representation of joint
correlations. This perception is injected by the Pose Transformer network and
learned through a pre-training task that recovers iterative masked joints.
Comprehensive experiments carried out on H36M and MHAD datasets, complemented
by visualizations, validate the effectiveness of our approach in the
short-baseline binocular 3D Human Pose Estimation and occlusion handling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, currently under review at IEEE Transactions on
  Image Processing journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas Kühne, Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enable self-driving vehicles accurate detection and tracking of
surrounding objects is essential. While Light Detection and Ranging (LiDAR)
sensors have set the benchmark for high-performance systems, the appeal of
camera-only solutions lies in their cost-effectiveness. Notably, despite the
prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive
systems, their potential in 3D detection and tracking has been largely
disregarded due to data sparsity and measurement noise. As a recent
development, the combination of RADARs and cameras is emerging as a promising
solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a
camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking
(MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only
BEVDet architecture, CR3DT demonstrates substantial improvements in both
detection and tracking capabilities, by incorporating the spatial and velocity
information of the RADAR sensor. Experimental results demonstrate an absolute
improvement in detection performance of 5.3% in mean Average Precision (mAP)
and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the
nuScenes dataset when leveraging both modalities. CR3DT bridges the gap between
high-performance and cost-effective perception systems in autonomous driving,
by capitalizing on the ubiquitous presence of RADAR in automotive applications.
The code is available at: https://github.com/ETH-PBL/CR3DT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry
  Reconstruction in Field Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Arbab Arshad, Talukder Jubery, James Afful, Anushrut Jignasu, Aditya Balu, Baskar Ganapathysubramanian, Soumik Sarkar, Adarsh Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluate different Neural Radiance Fields (NeRFs) techniques for the 3D
reconstruction of plants in varied environments, from indoor settings to
outdoor fields. Traditional methods usually fail to capture the complex
geometric details of plants, which is crucial for phenotyping and breeding
studies. We evaluate the reconstruction fidelity of NeRFs in three scenarios
with increasing complexity and compare the results with the point cloud
obtained using LiDAR as ground truth. In the most realistic field scenario, the
NeRF models achieve a 74.6% F1 score after 30 minutes of training on the GPU,
highlighting the efficacy of NeRFs for 3D reconstruction in challenging
environments. Additionally, we propose an early stopping technique for NeRF
training that almost halves the training time while achieving only a reduction
of 7.4% in the average F1 score. This optimization process significantly
enhances the speed and efficiency of 3D reconstruction using NeRFs. Our
findings demonstrate the potential of NeRFs in detailed and realistic 3D plant
reconstruction and suggest practical approaches for enhancing the speed and
efficiency of NeRFs in the 3D reconstruction process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 'Plant Phenomics'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zong, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Large Vision-Language Models (LVLMs) have demonstrated great abilities in
image perception and language understanding. However, existing multimodal
benchmarks focus on primary perception abilities and commonsense knowledge
which are insufficient to reflect the comprehensive capabilities of LVLMs. We
propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance
Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as
diagrams, function graphs, maps and photos. GAOKAO-MM derives from native
Chinese context and sets human-level requirements for the model's abilities,
including perception, understanding, knowledge and reasoning. We evaluate 10
LVLMs and find that the accuracies of all of them are lower than 50%, with
GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking
in the top three positions. The results of our multi-dimension analysis
indicate that LVLMs have moderate distance towards Artificial General
Intelligence (AGI) and provide insights facilitating the development of
multilingual LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VCHAR:Variance-Driven Complex Human Activity Recognition framework with
  Generative Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Sun, Navid Salami Pargoo, Taqiya Ehsan, Zhao Zhang, Jorge Ortiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex human activity recognition (CHAR) remains a pivotal challenge within
ubiquitous computing, especially in the context of smart environments. Existing
studies typically require meticulous labeling of both atomic and complex
activities, a task that is labor-intensive and prone to errors due to the
scarcity and inaccuracies of available datasets. Most prior research has
focused on datasets that either precisely label atomic activities or, at
minimum, their sequence approaches that are often impractical in real world
settings.In response, we introduce VCHAR (Variance-Driven Complex Human
Activity Recognition), a novel framework that treats the outputs of atomic
activities as a distribution over specified intervals. Leveraging generative
methodologies, VCHAR elucidates the reasoning behind complex activity
classifications through video-based explanations, accessible to users without
prior machine learning expertise. Our evaluation across three publicly
available datasets demonstrates that VCHAR enhances the accuracy of complex
activity recognition without necessitating precise temporal or sequential
labeling of atomic activities. Furthermore, user studies confirm that VCHAR's
explanations are more intelligible compared to existing methods, facilitating a
broader understanding of complex activity recognition among non-experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stability-Informed Initialization of Neural Ordinary Differential
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15890v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15890v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodor Westny, Arman Mohammadi, Daniel Jung, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the training of Neural Ordinary Differential Equations
(neural ODEs), and in particular explores the interplay between numerical
integration techniques, stability regions, step size, and initialization
techniques. It is shown how the choice of integration technique implicitly
regularizes the learned model, and how the solver's corresponding stability
region affects training and prediction performance. From this analysis, a
stability-informed parameter initialization technique is introduced. The
effectiveness of the initialization method is displayed across several learning
benchmarks and industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 41 st International Conference on Machine
  Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for
  Fingerprint Presentation Attack Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Rai, Parsheel Kumar Tiwari, Jyotishna Baishya, Ram Prakash Sharma, Somnath Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic fingerprint recognition systems suffer from the threat of
presentation attacks due to their wide range of deployment in areas including
national borders and commercial applications. A presentation attack can be
performed by creating a spoof of a user's fingerprint with or without their
consent. This paper presents a dynamic ensemble of deep CNN and handcrafted
features to detect presentation attacks in known-material and unknown-material
protocols of the livness detection competition. The proposed presentation
attack detection model, in this way, utilizes the capabilities of both deep CNN
and handcrafted features techniques and exhibits better performance than their
individual performances. The proposed method is validated using benchmark
databases from the Liveness Detection Competition in 2015, 2017, and 2019,
yielding overall accuracy of 96.10\%, 96.49\%, and 94.99\% on them,
respectively. The proposed method outperforms state-of-the-art methods in terms
of classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.09397</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modality Co-Learning for Efficient Skeleton-based Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15706v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15706v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfu Liu, Chen Chen, Mengyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action recognition has garnered significant attention due to
the utilization of concise and resilient skeletons. Nevertheless, the absence
of detailed body information in skeletons restricts performance, while other
multimodal methods require substantial inference resources and are inefficient
when using multimodal data during both training and inference stages. To
address this and fully harness the complementary multimodal features, we
propose a novel multi-modality co-learning (MMCL) framework by leveraging the
multimodal large language models (LLMs) as auxiliary networks for efficient
skeleton-based action recognition, which engages in multi-modality co-learning
during the training stage and keeps efficiency by employing only concise
skeletons in inference. Our MMCL framework primarily consists of two modules.
First, the Feature Alignment Module (FAM) extracts rich RGB features from video
frames and aligns them with global skeleton features via contrastive learning.
Second, the Feature Refinement Module (FRM) uses RGB images with temporal
information and text instruction to generate instructive features based on the
powerful generalization of multimodal LLMs. These instructive text features
will further refine the classification scores and the refined scores will
enhance the model's robustness and generalization in a manner similar to soft
labels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA
benchmarks consistently verify the effectiveness of our MMCL, which outperforms
the existing skeleton-based action recognition methods. Meanwhile, experiments
on UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization
of our MMCL in zero-shot and domain-adaptive action recognition. Our code is
publicly available at: https://github.com/liujf69/MMCL-Action.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery
  Videos via Physics-embedded 3D Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00956v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00956v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenya Yang, Kai Chen, Yonghao Long, Qi Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical scene simulation plays a crucial role in surgical education and
simulator-based robot learning. Traditional approaches for creating these
environments with surgical scene involve a labor-intensive process where
designers hand-craft tissues models with textures and geometries for soft body
simulations. This manual approach is not only time-consuming but also limited
in the scalability and realism. In contrast, data-driven simulation offers a
compelling alternative. It has the potential to automatically reconstruct 3D
surgical scenes from real-world surgical video data, followed by the
application of soft body physics. This area, however, is relatively uncharted.
In our research, we introduce 3D Gaussian as a learnable representation for
surgical scene, which is learned from stereo endoscopic video. To prevent
over-fitting and ensure the geometrical correctness of these scenes, we
incorporate depth supervision and anisotropy regularization into the Gaussian
learning process. Furthermore, we apply the Material Point Method, which is
integrated with physical properties, to the 3D Gaussians to achieve realistic
scene deformations. Our method was evaluated on our collected in-house and
public surgical videos datasets. Results show that it can reconstruct and
simulate surgical scenes from endoscopic videos efficiently-taking only a few
minutes to reconstruct the surgical scene-and produce both visually and
physically plausible deformations at a speed approaching real-time. The results
demonstrate great potential of our proposed method to enhance the efficiency
and variety of simulations available for surgical education and robot learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IMAGDressing-v1: Customizable Virtual Dressing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Shen, Xin Jiang, Xin He, Hu Ye, Cong Wang, Xiaoyu Du, Zechao Li, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latest advances have achieved realistic virtual try-on (VTON) through
localized garment inpainting using latent diffusion models, significantly
enhancing consumers' online shopping experience. However, existing VTON
technologies neglect the need for merchants to showcase garments
comprehensively, including flexible control over garments, optional faces,
poses, and scenes. To address this issue, we define a virtual dressing (VD)
task focused on generating freely editable human images with fixed garments and
optional conditions. Meanwhile, we design a comprehensive affinity metric index
(CAMI) to evaluate the consistency between generated images and reference
garments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet
that captures semantic features from CLIP and texture features from VAE. We
present a hybrid attention module, including a frozen self-attention and a
trainable cross-attention, to integrate garment features from the garment UNet
into a frozen denoising UNet, ensuring users can control different scenes
through text. IMAGDressing-v1 can be combined with other extension plugins,
such as ControlNet and IP-Adapter, to enhance the diversity and controllability
of generated images. Furthermore, to address the lack of data, we release the
interactive garment pairing (IGPair) dataset, containing over 300,000 pairs of
clothing and dressed images, and establish a standard pipeline for data
assembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves
state-of-the-art human image synthesis performance under various controlled
conditions. The code and model will be available at
https://github.com/muzishen/IMAGDressing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffX: Guide Your Layout to Cross-Modal Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Wang, Jingyu Lin, Yifei Qian, Yi Huang, Shicen Tian, Bosong Chai, Juncan Deng, Lan Du, Cunjian Chen, Yufei Guo, Kejie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have made significant strides in language-driven and
layout-driven image generation. However, most diffusion models are limited to
visible RGB image generation. In fact, human perception of the world is
enriched by diverse viewpoints, such as chromatic contrast, thermal
illumination, and depth information. In this paper, we introduce a novel
diffusion model for general layout-guided cross-modal generation, called DiffX.
Notably, DiffX presents a simple yet effective cross-modal generative modeling
pipeline, which conducts diffusion and denoising processes in the
modality-shared latent space. Moreover, we introduce the Joint-Modality
Embedder (JME) to enhance interaction between layout and text conditions by
incorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP is
employed for long caption embedding for user instruction. To facilitate the
user-instructed generative training, we construct the cross-modal image
datasets with detailed text captions assisted by the Large-Multimodal Model
(LMM). Through extensive experiments, DiffX demonstrates robustness in
cross-modal generation across three ``RGB+X'' datasets: FLIR, MFNet, and
COME15K, guided by various layout conditions. It also shows the potential for
the adaptive generation of ``RGB+X+Y+Z'' images or more diverse modalities on
COME15K and MCXFace datasets. Our code and constructed cross-modal image
datasets are available at https://github.com/zeyuwang-zju/DiffX.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When a Relation Tells More Than a Concept: Exploring and Evaluating
  Classifier Decisions with CoReX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01661v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01661v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bettina Finzel, Patrick Hilme, Johannes Rabold, Ute Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations for Convolutional Neural Networks (CNNs) based on relevance of
input pixels might be too unspecific to evaluate which and how input features
impact model decisions. Especially in complex real-world domains like biology,
the presence of specific concepts and of relations between concepts might be
discriminating between classes. Pixel relevance is not expressive enough to
convey this type of information. In consequence, model evaluation is limited
and relevant aspects present in the data and influencing the model decisions
might be overlooked. This work presents a novel method to explain and evaluate
CNN models, which uses a concept- and relation-based explainer (CoReX). It
explains the predictive behavior of a model on a set of images by masking
(ir-)relevant concepts from the decision-making process and by constraining
relations in a learned interpretable surrogate model. We test our approach with
several image data sets and CNN architectures. Results show that CoReX
explanations are faithful to the CNN model in terms of predictive outcomes. We
further demonstrate through a human evaluation that CoReX is a suitable tool
for generating combined explanations that help assessing the classification
quality of CNNs. We further show that CoReX supports the identification and
re-classification of incorrect or ambiguous classifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preliminary version, submitted to Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with
  Audio2Video Diffusion Model under Weak Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the challenge of enhancing the realism and
expressiveness in talking head video generation by focusing on the dynamic and
nuanced relationship between audio cues and facial movements. We identify the
limitations of traditional techniques that often fail to capture the full
spectrum of human expressions and the uniqueness of individual facial styles.
To address these issues, we propose EMO, a novel framework that utilizes a
direct audio-to-video synthesis approach, bypassing the need for intermediate
3D models or facial landmarks. Our method ensures seamless frame transitions
and consistent identity preservation throughout the video, resulting in highly
expressive and lifelike animations. Experimental results demonsrate that EMO is
able to produce not only convincing speaking videos but also singing videos in
various styles, significantly outperforming existing state-of-the-art
methodologies in terms of expressiveness and realism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMTrail: A Multimodal Trailer Video <span class="highlight-title">Dataset</span> with Language and Music
  Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowei Chi, Yatian Wang, Aosong Cheng, Pengjun Fang, Zeyue Tian, Yingqing He, Zhaoyang Liu, Xingqun Qi, Jiahao Pan, Rongyu Zhang, Mengfei Li, Ruibin Yuan, Yanbing Jiang, Wei Xue, Wenhan Luo, Qifeng Chen, Shanghang Zhang, Qifeng Liu, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive multi-modality datasets play a significant role in facilitating the
success of large video-language models. However, current video-language
datasets primarily provide text descriptions for visual frames, considering
audio to be weakly related information. They usually overlook exploring the
potential of inherent audio-visual correlation, leading to monotonous
annotation within each modality instead of comprehensive and precise
descriptions. Such ignorance results in the difficulty of multiple
cross-modality studies. To fulfill this gap, we present MMTrail, a large-scale
multi-modality video-language dataset incorporating more than 20M trailer clips
with visual captions, and 2M high-quality clips with multimodal captions.
Trailers preview full-length video works and integrate context, visual frames,
and background music. In particular, the trailer has two main advantages: (1)
the topics are diverse, and the content characters are of various types, e.g.,
film, news, and gaming. (2) the corresponding background music is
custom-designed, making it more coherent with the visual context. Upon these
insights, we propose a systemic captioning framework, achieving various
modality annotations with more than 27.1k hours of trailer videos. Here, to
ensure the caption retains music perspective while preserving the authority of
visual context, we leverage the advanced LLM to merge all annotations
adaptively. In this fashion, our MMtrail dataset potentially paves the path for
fine-grained large multimodal-language model training. In experiments, we
provide evaluation metrics and benchmark results on our dataset, demonstrating
the high quality of our annotation and its effectiveness for model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages. Dataset report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features
  for Highly Controllable Text-Driven Image Translation <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Gao, Jiaying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models have been a revolutionary
milestone in the evolution of generative AI and multimodal technology, allowing
wonderful image generation with natural-language text prompt. However, the
issue of lacking controllability of such models restricts their practical
applicability for real-life content creation. Thus, attention has been focused
on leveraging a reference image to control text-to-image synthesis, which is
also regarded as manipulating (or editing) a reference image as per a text
prompt, namely, text-driven image-to-image translation. This paper contributes
a novel, concise, and efficient approach that adapts pre-trained large-scale
text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a
plug-and-play manner, realizing high-quality and versatile text-driven I2I
translation without any model training, model fine-tuning, or online
optimization process. To guide T2I generation with a reference image, we
propose to decompose diverse guiding factors with different frequency bands of
diffusion features in the DCT spectral space, and accordingly devise a novel
frequency band substitution layer which realizes dynamic control of the
reference image to the T2I generation result in a plug-and-play manner. We
demonstrate that our method allows flexible control over both guiding factor
and guiding intensity of the reference image simply by tuning the type and
bandwidth of the substituted frequency band, respectively. Extensive
qualitative and quantitative experiments verify superiority of our approach
over related methods in I2I translation visual quality, versatility, and
controllability. The code is publicly available at:
https://github.com/XiangGao1102/FBSDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted conference paper of ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity
  with Free-Flying Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02558v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02558v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Holly Dinkel, Julia Di, Jamie Santos, Keenan Albee, Paulo Borges, Marina Moreira, Oleg Alexandrov, Brian Coltin, Trey Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive free-flyer robots autonomously caring for future crewed outposts --
such as NASA's Astrobee robots on the International Space Station (ISS) -- must
be able to detect day-to-day interior changes to track inventory, detect and
diagnose faults, and monitor the outpost status. This work presents a framework
for multi-agent cooperative mapping and change detection to enable robotic
maintenance of space outposts. One agent is used to reconstruct a 3D model of
the environment from sequences of images and corresponding depth information.
Another agent is used to periodically scan the environment for inconsistencies
against the 3D model. Change detection is validated after completing the
surveys using real image and pose data collected by Astrobee robots in a ground
testing environment and from microgravity aboard the ISS. This work outlines
the objectives, requirements, and algorithmic modules for the multi-agent
reconstruction system, including recommendations for its use by assistive
free-flyers aboard future microgravity outposts.
  *Denotes Equal Contribution
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, Manuscript presented at the 74th International
  Astronautical Congress, IAC 2023, Baku, Azerbaijan, 2 - 6 October 2023. Video
  presentation: [https://www.youtube.com/watch?v=VfjV-zwFEtU]. Code:
  [https://github.com/hollydinkel/astrobeecd]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Retrieval Robustness for Retrieval-Augmented Image
  Captioning <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02265v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02265v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyan Li, Jiaang Li, Rita Ramos, Raphael Tang, Desmond Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in retrieval-augmented models for image captioning highlight
the benefit of retrieving related captions for efficient, lightweight models
with strong domain-transfer capabilities. While these models demonstrate the
success of retrieval augmentation, retrieval models are still far from perfect
in practice: the retrieved information can sometimes mislead the model,
resulting in incorrect generation and worse performance. In this paper, we
analyze the robustness of a retrieval-augmented captioning model SmallCap. Our
analysis shows that the model is sensitive to tokens that appear in the
majority of the retrieved captions, and the input attribution shows that those
tokens are likely copied into the generated output. Given these findings, we
propose to train the model by sampling retrieved captions from more diverse
sets. This decreases the chance that the model learns to copy majority tokens,
and improves both in-domain and cross-domain performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, long paper at ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Correlation Meets Embedding: Towards a 2nd Generation of
  JDE-based Real-Time Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfei Zhang, Chao Liang, Jin Gao, Zhipeng Zhang, Weiming Hu, Stephen Maybank, Xue Zhou, Liang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint Detection and Embedding (JDE) trackers have demonstrated excellent
performance in Multi-Object Tracking (MOT) tasks by incorporating the
extraction of appearance features as auxiliary tasks through embedding
Re-Identification task (ReID) into the detector, achieving a balance between
inference speed and tracking performance. However, solving the competition
between the detector and the feature extractor has always been a challenge.
Meanwhile, the issue of directly embedding the ReID task into MOT has remained
unresolved. The lack of high discriminability in appearance features results in
their limited utility. In this paper, a new learning approach using
cross-correlation to capture temporal information of objects is proposed. The
feature extraction network is no longer trained solely on appearance features
from each frame but learns richer motion features by utilizing feature heatmaps
from consecutive frames, which addresses the challenge of inter-class feature
similarity. Furthermore, our learning approach is applied to a more lightweight
feature extraction network, and treat the feature matching scores as strong
cues rather than auxiliary cues, with an appropriate weight calculation to
reflect the compatibility between our obtained features and the MOT task. Our
tracker, named TCBTrack, achieves state-of-the-art performance on multiple
public benchmarks, i.e., MOT17, MOT20, and DanceTrack datasets. Specifically,
on the DanceTrack test set, we achieve 56.8 HOTA, 58.1 IDF1 and 92.5 MOTA,
making it the best online tracker capable of achieving real-time performance.
Comparative evaluations with other trackers prove that our tracker achieves the
best balance between speed, robustness and accuracy. Code is available at
https://github.com/yfzhang1214/TCBTrack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A submission to IJCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Feedback Helps CLIP See Better 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP), which excels at abstracting
open-world representations across domains and modalities, has become a
foundation for a variety of vision and multimodal tasks. However, recent
studies reveal that CLIP has severe visual shortcomings, such as which can
hardly distinguish orientation, quantity, color, structure, etc. These visual
shortcomings also limit the perception capabilities of multimodal large
language models (MLLMs) built on CLIP. The main reason could be that the
image-text pairs used to train CLIP are inherently biased, due to the lack of
the distinctiveness of the text and the diversity of images. In this work, we
present a simple post-training approach for CLIP models, which largely
overcomes its visual shortcomings via a self-supervised diffusion process. We
introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP.
Specifically, DIVA leverages generative feedback from text-to-image diffusion
models to optimize CLIP representations, with only images (without
corresponding text). We demonstrate that DIVA improves CLIP's performance on
the challenging MMVP-VLM benchmark which assesses fine-grained visual abilities
to a large extent (e.g., 3-7%), and enhances the performance of MLLMs and
vision models on multimodal understanding and segmentation tasks. Extensive
evaluation on 29 image classification and retrieval benchmarks confirms that
our framework preserves CLIP's strong zero-shot capabilities. The code is
available at https://github.com/baaivision/DIVA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenECAD: An Efficient Visual Language Model for Editable 3D-CAD Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09913v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09913v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Yuan, Jianqi Shi, Yanhong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-aided design (CAD) tools are utilized in the manufacturing industry
for modeling everything from cups to spacecraft. These programs are complex to
use and typically require years of training and experience to master.
Structured and well-constrained 2D sketches and 3D constructions are crucial
components of CAD modeling. A well-executed CAD model can be seamlessly
integrated into the manufacturing process, thereby enhancing production
efficiency. Deep generative models of 3D shapes and 3D object reconstruction
models have garnered significant research interest. However, most of these
models produce discrete forms of 3D objects that are not editable. Moreover,
the few models based on CAD operations often have substantial input
restrictions. In this work, we fine-tuned pre-trained models to create OpenECAD
models (0.55B, 0.89B, 2.4B and 3.1B), leveraging the visual, logical, coding,
and general capabilities of visual language models. OpenECAD models can process
images of 3D designs as input and generate highly structured 2D sketches and 3D
construction commands, ensuring that the designs are editable. These outputs
can be directly used with existing CAD tools' APIs to generate project files.
To train our network, we created a series of OpenECAD datasets. These datasets
are derived from existing public CAD datasets, adjusted and augmented to meet
the specific requirements of vision language model (VLM) training.
Additionally, we have introduced an approach that utilizes dependency
relationships to define and generate sketches, further enriching the content
and functionality of the datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CityX: Controllable Procedural Content Generation for Unbounded 3D
  Cities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17572v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17572v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shougao Zhang, Mengqi Zhou, Yuxi Wang, Chuanchen Luo, Rongyu Wang, Yiwei Li, Xucheng Yin, Zhaoxiang Zhang, Junran Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating a realistic, large-scale 3D virtual city remains a complex
challenge due to the involvement of numerous 3D assets, various city styles,
and strict layout constraints. Existing approaches provide promising attempts
at procedural content generation to create large-scale scenes using Blender
agents. However, they face crucial issues such as difficulties in scaling up
generation capability and achieving fine-grained control at the semantic layout
level. To address these problems, we propose a novel multi-modal controllable
procedural content generation method, named CityX, which enhances realistic,
unbounded 3D city generation guided by multiple layout conditions, including
OSM, semantic maps, and satellite images. Specifically, the proposed method
contains a general protocol for integrating various PCG plugins and a
multi-agent framework for transforming instructions into executable Blender
actions. Through this effective framework, CityX shows the potential to build
an innovative ecosystem for 3D scene generation by bridging the gap between the
quality of generated assets and industrial requirements. Extensive experiments
have demonstrated the effectiveness of our method in creating high-quality,
diverse, and unbounded cities guided by multi-modal conditions. Our project
page: https://cityx-lab.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen
  Domains by Intrinsic Learning from Redundant LLM Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14362v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14362v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Yue, Jiancheng Zhao, Chunhui Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen
classes against domain shift problem (DSP) where data of unseen classes may be
misclassified as seen classes. However, existing GZSL is still limited to seen
domains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which
addresses GZSL towards unseen domains. Different from existing GZSL methods
which alleviate DSP by generating features of unseen classes with semantics,
CDGZSL needs to construct a common feature space across domains and acquire the
corresponding intrinsic semantics shared among domains to transfer from seen to
unseen domains. Considering the information asymmetry problem caused by
redundant class semantics annotated with large language models (LLMs), we
present Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR
consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates
the non-intrinsic semantics not shared across all domains under the guidance of
inter-class feature relationships, and Unseen-class Meta Generation (UMG),
which preserves intrinsic semantics to maintain connectivity between seen and
unseen classes by simulating feature generation. MDASR effectively aligns the
redundant semantic space with the common feature space, mitigating the
information asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on
the Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics
for these datasets as the benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Posterior Proximal Sampling for Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjie Wu, Linchao He, Mingqin Zhang, Dongdong Chen, Kunming Luo, Mengting Luo, Ji-Zhe Zhou, Hu Chen, Jiancheng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable efficacy in generating
high-quality samples. Existing diffusion-based image restoration algorithms
exploit pre-trained diffusion models to leverage data priors, yet they still
preserve elements inherited from the unconditional generation paradigm. These
strategies initiate the denoising process with pure white noise and incorporate
random noise at each generative step, leading to over-smoothed results. In this
paper, we present a refined paradigm for diffusion-based image restoration.
Specifically, we opt for a sample consistent with the measurement identity at
each generative step, exploiting the sampling selection as an avenue for output
stability and enhancement. The number of candidate samples used for selection
is adaptively determined based on the signal-to-noise ratio of the timestep.
Additionally, we start the restoration process with an initialization combined
with the measurement signal, providing supplementary information to better
align the generative process. Extensive experimental results and analyses
validate that our proposed method significantly enhances image restoration
performance while consuming negligible additional computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2024 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InceptionHuman: Controllable <span class="highlight-title">Prompt</span>-to-NeRF for Photorealistic 3D Human
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiu-hong Kao, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents InceptionHuman, a prompt-to-NeRF framework that allows
easy control via a combination of prompts in different modalities (e.g., text,
poses, edge, segmentation map, etc) as inputs to generate photorealistic 3D
humans. While many works have focused on generating 3D human models, they
suffer one or more of the following: lack of distinctive features, unnatural
shading/shadows, unnatural poses/clothes, limited views, etc. InceptionHuman
achieves consistent 3D human generation within a progressively refined NeRF
space with two novel modules, Iterative Pose-Aware Refinement (IPAR) and
Progressive-Augmented Reconstruction (PAR). IPAR iteratively refines the
diffusion-generated images and synthesizes high-quality 3D-aware views
considering the close-pose RGB values. PAR employs a pretrained diffusion prior
to augment the generated synthetic views and adds regularization for
view-independent appearance. Overall, the synthesis of photorealistic novel
views empowers the resulting 3D human NeRF from 360-degree perspectives.
Extensive qualitative and quantitative experimental comparison show that our
InceptionHuman models achieve state-of-the-art application quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafePaint: Anti-forensic Image Inpainting with Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dunyun Chen, Xin Liao, Xiaoshuai Wu, Shiwei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing image inpainting methods have achieved remarkable accomplishments in
generating visually appealing results, often accompanied by a trend toward
creating more intricate structural textures. However, while these models excel
at creating more realistic image content, they often leave noticeable traces of
tampering, posing a significant threat to security. In this work, we take the
anti-forensic capabilities into consideration, firstly proposing an end-to-end
training framework for anti-forensic image inpainting named SafePaint.
Specifically, we innovatively formulated image inpainting as two major tasks:
semantically plausible content completion and region-wise optimization. The
former is similar to current inpainting methods that aim to restore the missing
regions of corrupted images. The latter, through domain adaptation, endeavors
to reconcile the discrepancies between the inpainted region and the unaltered
area to achieve anti-forensic goals. Through comprehensive theoretical
analysis, we validate the effectiveness of domain adaptation for anti-forensic
performance. Furthermore, we meticulously crafted a region-wise separated
attention (RWSA) module, which not only aligns with our objective of
anti-forensics but also enhances the performance of the model. Extensive
qualitative and quantitative evaluations show our approach achieves comparable
results to existing image inpainting methods while offering anti-forensic
capabilities not available in other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pathology Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mieko Ochi, Daisuke Komura, Shumpei Ishikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology has played a crucial role in the diagnosis and evaluation of
patient tissue samples obtained from surgeries and biopsies for many years. The
advent of Whole Slide Scanners and the development of deep learning
technologies have significantly advanced the field, leading to extensive
research and development in pathology AI (Artificial Intelligence). These
advancements have contributed to reducing the workload of pathologists and
supporting decision-making in treatment plans. Recently, large-scale AI models
known as Foundation Models (FMs), which are more accurate and applicable to a
wide range of tasks compared to traditional AI, have emerged, and expanded
their application scope in the healthcare field. Numerous FMs have been
developed in pathology, and there are reported cases of their application in
various tasks, such as disease diagnosis, rare cancer diagnosis, patient
survival prognosis prediction, biomarker expression prediction, and the scoring
of immunohistochemical expression intensity. However, several challenges remain
for the clinical application of FMs, which healthcare professionals, as users,
must be aware of. Research is ongoing to address these challenges. In the
future, it is expected that the development of Generalist Medical AI, which
integrates pathology FMs with FMs from other medical domains, will progress,
leading to the effective utilization of AI in real clinical settings to promote
precision and personalized medicine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frame Interpolation with Consecutive Brownian Bridge Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05953v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05953v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a
diffusion-based conditional image generation problem, synthesizing the
intermediate frame given a random noise and neighboring frames. Due to the
relatively high resolution of videos, Latent Diffusion Models (LDMs) are
employed as the conditional generation model, where the autoencoder compresses
images into latent representations for diffusion and then reconstructs images
from these latent representations. Such a formulation poses a crucial
challenge: VFI expects that the output is deterministically equal to the ground
truth intermediate frame, but LDMs randomly generate a diverse set of different
images when the model runs multiple times. The reason for the diverse
generation is that the cumulative variance (variance accumulated at each step
of generation) of generated latent representations in LDMs is large. This makes
the sampling trajectory random, resulting in diverse rather than deterministic
generations. To address this problem, we propose our unique solution: Frame
Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we
propose consecutive Brownian Bridge diffusion that takes a deterministic
initial value as input, resulting in a much smaller cumulative variance of
generated latent representations. Our experiments suggest that our method can
improve together with the improvement of the autoencoder and achieve
state-of-the-art performance in VFI, leaving strong potential for further
enhancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>corrected typo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarks and Challenges in Pose Estimation for Egocentric Hand
  Interactions with Objects <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Zheng Liu, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We interact with the world with our hands and see it through our own
(egocentric) perspective. A holistic 3Dunderstanding of such interactions from
egocentric views is important for tasks in robotics, AR/VR, action recognition
and motion generation. Accurately reconstructing such interactions in 3D is
challenging due to heavy occlusion, viewpoint bias, camera distortion, and
motion blur from the head movement. To this end, we designed the HANDS23
challenge based on the AssemblyHands and ARCTIC datasets with carefully
designed training and testing splits. Based on the results of the top submitted
methods and more recent baselines on the leaderboards, we perform a thorough
analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates
the effectiveness of addressing distortion specific to egocentric cameras,
adopting high-capacity transformers to learn complex hand-object interactions,
and fusing predictions from different views. Our study further reveals
challenging scenarios intractable with state-of-the-art methods, such as fast
hand motion, object reconstruction from narrow egocentric views, and close
contact between two hands and objects. Our efforts will enrich the community's
knowledge foundation and facilitate future hand studies on egocentric
hand-object interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Zhang, Haoning Wu, Yingjie Zhou, Chunyi Li, Wei Sun, Chaofeng Chen, Xiongkuo Min, Xiaohong Liu, Weisi Lin, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large multi-modality models (LMMs) have seen extensive exploration
and application in various quality assessment studies, their integration into
Point Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs'
exceptional performance and robustness in low-level vision and quality
assessment tasks, this study aims to investigate the feasibility of imparting
PCQA knowledge to LMMs through text supervision. To achieve this, we transform
quality labels into textual descriptions during the fine-tuning phase, enabling
LMMs to derive quality rating logits from 2D projections of point clouds. To
compensate for the loss of perception in the 3D domain, structural features are
extracted as well. These quality logits and structural features are then
combined and regressed into quality scores. Our experimental results affirm the
effectiveness of our approach, showcasing a novel integration of LMMs into PCQA
that enhances model understanding and assessment accuracy. We hope our
contributions can inspire subsequent investigations into the fusion of LMMs
with PCQA, fostering advancements in 3D visual quality analysis and beyond. The
code is available at https://github.com/zzc-1998/LMM-PCQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware
  State Space Model <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Zeng, Hao Shi, Jiacheng Lin, Siyu Li, Jintao Cheng, Kaiwei Wang, Zhiyong Li, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment
moving objects in point clouds of the current scan using motion information
from previous scans. Despite the promising results achieved by previous MOS
methods, several key issues, such as the weak coupling of temporal and spatial
information, still need further study. In this paper, we propose a novel
LiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model,
termed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue
Bootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial
information in point clouds and alleviate the issue of overlooked temporal
clues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to
endow the model with the capacity to understand the temporal correlations of
the same object across different time steps. Specifically, MSSM emphasizes the
motion states of the same object at different time steps through two distinct
temporal modeling and correlation steps. We utilize an improved state space
model to represent these motion differences, significantly modeling the motion
states. Finally, extensive experiments on the SemanticKITTI-MOS and KITTI-Road
benchmarks demonstrate that the proposed MambaMOS achieves state-of-the-art
performance. The source code is publicly available at
https://github.com/Terminal-K/MambaMOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2024. The source code is publicly available at
  https://github.com/Terminal-K/MambaMOS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adv3D: Generating 3D Adversarial Examples for 3D Object Detection in
  Driving Scenarios with NeRF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leheng Li, Qing Lian, Ying-Cong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have been proven extremely susceptible to
adversarial examples, which raises special safety-critical concerns for
DNN-based autonomous driving stacks (i.e., 3D object detection). Although there
are extensive works on image-level attacks, most are restricted to 2D pixel
spaces, and such attacks are not always physically realistic in our 3D world.
Here we present Adv3D, the first exploration of modeling adversarial examples
as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic
appearances and 3D accurate generation, yielding a more realistic and
realizable adversarial example. We train our adversarial NeRF by minimizing the
surrounding objects' confidence predicted by 3D detectors on the training set.
Then we evaluate Adv3D on the unseen validation set and show that it can cause
a large performance reduction when rendering NeRF in any sampled pose. To
generate physically realizable adversarial examples, we propose primitive-aware
sampling and semantic-guided regularization that enable 3D patch attacks with
camouflage adversarial texture. Experimental results demonstrate that the
trained adversarial NeRF generalizes well to different poses, scenes, and 3D
detectors. Finally, we provide a defense method to our attacks that involves
adversarial training through data augmentation. Project page:
https://len-li.github.io/adv3d-web
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image
  Relational Association Capabilities in Large Visual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwei Wu, Kang Zhu, Yu Bai, Yiming Liang, Yizhi Li, Haoning Wu, J. H. Liu, Ruibo Liu, Xingwei Qu, Xuxin Cheng, Ge Zhang, Wenhao Huang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the remarkable success that large visual language models (LVLMs) have
achieved in image perception tasks, the endeavor to make LVLMs perceive the
world like humans is drawing increasing attention. Current multi-modal
benchmarks primarily focus on facts or specific topic-related knowledge
contained within individual images. However, they often overlook the
associative relations between multiple images, which require the identification
and analysis of similarities among entities or content present in different
images. Therefore, we propose the multi-image relation association task and a
meticulously curated Multi-granularity Multi-image Relational Association
(MMRA) benchmark, comprising 1,024 samples. In order to systematically and
comprehensively evaluate current LVLMs, we establish an associational relation
system among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)
at two granularity levels (i.e., image and entity) according to the relations
in ConceptNet. Our experiments reveal that on the MMRA benchmark, current
multi-image LVLMs exhibit distinct advantages and disadvantages across various
subtasks. Notably, fine-grained, entity-level multi-image perception tasks pose
a greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs
perform poorly on spatial-related tasks, indicating that LVLMs still have
limited spatial awareness. Additionally, our findings indicate that while LVLMs
demonstrate a strong capability to perceive image details, enhancing their
ability to associate information across multiple images hinges on improving the
reasoning capabilities of their language model component. Moreover, we explored
the ability of LVLMs to perceive image sequences within the context of our
multi-image association task. Our experiments show that the majority of current
LVLMs do not adequately model image sequences during the pre-training process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VLMs, Multi-Image Association</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Activated Muscle Group Estimation in the Wild <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00952v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00952v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyu Peng, David Schneider, Alina Roitberg, Kailun Yang, Jiaming Zhang, Chen Deng, Kaiyu Zhang, M. Saquib Sarfraz, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the new task of video-based Activated Muscle Group
Estimation (AMGE) aiming at identifying active muscle regions during physical
activity in the wild. To this intent, we provide the MuscleMap dataset
featuring >15K video clips with 135 different activities and 20 labeled muscle
groups. This dataset opens the vistas to multiple video-based applications in
sports and rehabilitation medicine under flexible environment constraints. The
proposed MuscleMap dataset is constructed with YouTube videos, specifically
targeting High-Intensity Interval Training (HIIT) physical exercise in the
wild. To make the AMGE model applicable in real-life situations, it is crucial
to ensure that the model can generalize well to numerous types of physical
activities not present during training and involving new combinations of
activated muscles. To achieve this, our benchmark also covers an evaluation
setting where the model is exposed to activity types excluded from the training
set. Our experiments reveal that the generalizability of existing architectures
adapted for the AMGE task remains a challenge. Therefore, we also propose a new
approach, TransM3E, which employs a multi-modality feature fusion mechanism
between both the video transformer model and the skeleton-based graph
convolution model with novel cross-modal knowledge distillation executed on
multi-classification tokens. The proposed method surpasses all popular video
classification models when dealing with both, previously seen and new types of
physical activities. The database and code can be found at
https://github.com/KPeng9510/MuscleMap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2024. The database and code can be found at
  https://github.com/KPeng9510/MuscleMap</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automation of Quantum Dot Measurement Analysis via Explainable Machine
  Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13699v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13699v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of quantum dot (QD) devices for quantum computing has
necessitated more efficient and automated methods for device characterization
and tuning. Many of the measurements acquired during the tuning process come in
the form of images that need to be properly analyzed to guide the subsequent
tuning steps. By design, features present in such images capture certain
behaviors or states of the measured QD devices. When considered carefully, such
features can aid the control and calibration of QD devices. An important
example of such images are so-called \textit{triangle plots}, which visually
represent current flow and reveal characteristics important for QD device
calibration. While image-based classification tools, such as convolutional
neural networks (CNNs), can be used to verify whether a given measurement is
\textit{good} and thus warrants the initiation of the next phase of tuning,
they do not provide any insights into how the device should be adjusted in the
case of \textit{bad} images. This is because CNNs sacrifice prediction and
model intelligibility for high accuracy. To ameliorate this trade-off, a recent
study introduced an image vectorization approach that relies on the Gabor
wavelet transform [1]. Here we propose an alternative vectorization method that
involves mathematical modeling of synthetic triangles to mimic the experimental
data. Using explainable boosting machines, we show that this new method offers
superior explainability of model prediction without sacrificing accuracy. This
work demonstrates the feasibility and advantages of applying explainable
machine learning techniques to the analysis of quantum dot measurements, paving
the way for further advances in automated and transparent QD device tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures, abbreviated version published in Proceedings of
  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,
  (Vancouver, Canada)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Holistic Evaluation for Interleaved Text-and-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minqian Liu, Zhiyang Xu, Zihao Lin, Trevor Ashby, Joy Rimchala, Jiaxin Zhang, Lifu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interleaved text-and-image generation has been an intriguing research
direction, where the models are required to generate both images and text
pieces in an arbitrary order. Despite the emerging advancements in interleaved
generation, the progress in its evaluation still significantly lags behind.
Existing evaluation benchmarks do not support arbitrarily interleaved images
and text for both inputs and outputs, and they only cover a limited number of
domains and use cases. Also, current works predominantly use similarity-based
metrics which fall short in assessing the quality in open-ended scenarios. To
this end, we introduce InterleavedBench, the first benchmark carefully curated
for the evaluation of interleaved text-and-image generation. InterleavedBench
features a rich array of tasks to cover diverse real-world use cases. In
addition, we present InterleavedEval, a strong reference-free metric powered by
GPT-4o to deliver accurate and explainable evaluation. We carefully define five
essential evaluation aspects for InterleavedEval, including text quality,
perceptual quality, image coherence, text-image coherence, and helpfulness, to
ensure a comprehensive and fine-grained assessment. Through extensive
experiments and rigorous human evaluation, we show that our benchmark and
metric can effectively evaluate the existing models with a strong correlation
with human judgments surpassing previous reference-based metrics. We also
provide substantial findings and insights to foster future research in
interleaved generation and its evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 6 tables. Website:
  https://vt-nlp.github.io/InterleavedEval/. Dataset:
  https://huggingface.co/mqliu/InterleavedBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Art of Camouflage: Few-Shot Learning for Animal Detection and
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07444v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07444v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh-Danh Nguyen, Anh-Khoa Nguyen Vu, Nhat-Duy Nguyen, Vinh-Tiep Nguyen, Thanh Duc Ngo, Thanh-Toan Do, Minh-Triet Tran, Tam V. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflaged object detection and segmentation is a new and challenging
research topic in computer vision. There is a serious issue of lacking data on
concealed objects such as camouflaged animals in natural scenes. In this paper,
we address the problem of few-shot learning for camouflaged object detection
and segmentation. To this end, we first collect a new dataset, CAMO-FS, for the
benchmark. As camouflaged instances are challenging to recognize due to their
similarity compared to the surroundings, we guide our models to obtain
camouflaged features that highly distinguish the instances from the background.
In this work, we propose FS-CDIS, a framework to efficiently detect and segment
camouflaged instances via two loss functions contributing to the training
process. Firstly, the instance triplet loss with the characteristic of
differentiating the anchor, which is the mean of all camouflaged foreground
points, and the background points are employed to work at the instance level.
Secondly, to consolidate the generalization at the class level, we present
instance memory storage with the scope of storing camouflaged features of the
same category, allowing the model to capture further class-level information
during the learning process. The extensive experiments demonstrated that our
proposed method achieves state-of-the-art performance on the newly collected
dataset. Code is available at https://github.com/danhntd/FS-CDIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Access 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skeleton-Based Human Action Recognition with Noisy Labels <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xu, Kunyu Peng, Di Wen, Ruiping Liu, Junwei Zheng, Yufan Chen, Jiaming Zhang, Alina Roitberg, Kailun Yang, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human actions from body poses is critical for assistive robots
sharing space with humans in order to make informed and safe decisions about
the next interaction. However, precise temporal localization and annotation of
activity sequences is time-consuming and the resulting labels are often noisy.
If not effectively addressed, label noise negatively affects the model's
training, resulting in lower recognition quality. Despite its importance,
addressing label noise for skeleton-based action recognition has been
overlooked so far. In this study, we bridge this gap by implementing a
framework that augments well-established skeleton-based human action
recognition methods with label-denoising strategies from various research areas
to serve as the initial benchmark. Observations reveal that these baselines
yield only marginal performance when dealing with sparse skeleton data.
Consequently, we introduce a novel methodology, NoiseEraSAR, which integrates
global sample selection, co-teaching, and Cross-Modal Mixture-of-Experts
(CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise.
Our proposed approach demonstrates better performance on the established
benchmark, setting new state-of-the-art standards. The source code for this
study is accessible at https://github.com/xuyizdby/NoiseEraSAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2024. The source code for this study is accessible
  at https://github.com/xuyizdby/NoiseEraSAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Visual Localization for Multi-Agent Collaboration: A Data-Driven
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Hanlon, Boyang Sun, Marc Pollefeys, Hermann Blum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rather than having each newly deployed robot create its own map of its
surroundings, the growing availability of SLAM-enabled devices provides the
option of simply localizing in a map of another robot or device. In cases such
as multi-robot or human-robot collaboration, localizing all agents in the same
map is even necessary. However, localizing e.g. a ground robot in the map of a
drone or head-mounted MR headset presents unique challenges due to viewpoint
changes. This work investigates how active visual localization can be used to
overcome such challenges of viewpoint changes. Specifically, we focus on the
problem of selecting the optimal viewpoint at a given location. We compare
existing approaches in the literature with additional proposed baselines and
propose a novel data-driven approach. The result demonstrates the superior
performance of the data-driven approach when compared to existing methods, both
in controlled simulation experiments and real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with
  Diffusion-Controllable Adversaries <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00391v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00391v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Jer Chang, Francesco Pittaluga, Masayoshi Tomizuka, Wei Zhan, Manmohan Chandraker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the performance of autonomous vehicle planning algorithms
necessitates simulating long-tail safety-critical traffic scenarios. However,
traditional methods for generating such scenarios often fall short in terms of
controllability and realism; they also neglect the dynamics of agent
interactions. To address these limitations, we introduce SAFE-SIM, a novel
diffusion-based controllable closed-loop safety-critical simulation framework.
Our approach yields two distinct advantages: 1) generating realistic long-tail
safety-critical scenarios that closely reflect real-world conditions, and 2)
providing controllable adversarial behavior for more comprehensive and
interactive evaluations. We develop a novel approach to simulate
safety-critical scenarios through an adversarial term in the denoising process
of diffusion models, which allows an adversarial agent to challenge a planner
with plausible maneuvers while all agents in the scene exhibit reactive and
realistic behaviors. Furthermore, we propose novel guidance objectives and a
partial diffusion process that enables users to control key aspects of the
scenarios, such as the collision type and aggressiveness of the adversarial
agent, while maintaining the realism of the behavior. We validate our framework
empirically using the nuScenes and nuPlan datasets across multiple planners,
demonstrating improvements in both realism and controllability. These findings
affirm that diffusion models provide a robust and versatile foundation for
safety-critical, interactive traffic simulation, extending their utility across
the broader autonomous driving landscape. Project website:
https://safe-sim.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024; Project website: https://safe-sim.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Tree Type Detection in Forest Fire Risk Assessment:
  Multi-Stage Approach and Color Encoding with Forest Fire Risk Evaluation
  Framework for UAV Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinda Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forest fires pose a significant threat to ecosystems, economies, and human
health worldwide. Early detection and assessment of forest fires are crucial
for effective management and conservation efforts. Unmanned Aerial Vehicles
(UAVs) equipped with advanced computer vision algorithms offer a promising
solution for forest fire detection and assessment. In this paper, we optimize
an integrated forest fire risk assessment framework using UAVs and multi-stage
object detection algorithms. We introduce improvements to our previous
framework, including the adoption of Faster R-CNN, Grid R-CNN, Sparse R-CNN,
Cascade R-CNN, Dynamic R-CNN, and Libra R-CNN detectors, and explore
optimizations such as CBAM for attention enhancement, random erasing for
preprocessing, and different color space representations. We evaluate these
enhancements through extensive experimentation using aerial image footage from
various regions in British Columbia, Canada. Our findings demonstrate the
effectiveness of multi-stage detectors and optimizations in improving the
accuracy of forest fire risk assessment. This research contributes to the
advancement of UAV-based forest fire detection and assessment systems,
enhancing their efficiency and effectiveness in supporting sustainable forest
management and conservation efforts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Efficacy of Text-Based Input Modalities for Action Anticipation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apoorva Beedu, Karan Samel, Irfan Essa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anticipating future actions is a highly challenging task due to the diversity
and scale of potential future actions; yet, information from different
modalities help narrow down plausible action choices. Each modality can provide
diverse and often complementary context for the model to learn from. While
previous multi-modal methods leverage information from modalities such as video
and audio, we primarily explore how text descriptions of actions and objects
can also lead to more accurate action anticipation by providing additional
contextual cues, e.g., about the environment and its contents. We propose a
Multi-modal Contrastive Anticipative Transformer (M-CAT), a video transformer
architecture that jointly learns from multi-modal features and text
descriptions of actions and objects. We train our model in two stages, where
the model first learns to align video clips with descriptions of future
actions, and is subsequently fine-tuned to predict future actions. Compared to
existing methods, M-CAT has the advantage of learning additional context from
two types of text inputs: rich descriptions of future actions during
pre-training, and, text descriptions for detected objects and actions during
modality feature fusion. Through extensive experimental evaluation, we
demonstrate that our model outperforms previous methods on the EpicKitchens
datasets, and show that using simple text descriptions of actions and objects
aid in more effective action anticipation. In addition, we examine the impact
of object and action information obtained via text, and perform extensive
ablations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinaya Sree Katamneni, Ajita Rattani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the digital age, the emergence of deepfakes and synthetic media presents a
significant threat to societal and political integrity. Deepfakes based on
multi-modal manipulation, such as audio-visual, are more realistic and pose a
greater threat. Current multi-modal deepfake detectors are often based on the
attention-based fusion of heterogeneous data streams from multiple modalities.
However, the heterogeneous nature of the data (such as audio and visual
signals) creates a distributional modality gap and poses a significant
challenge in effective fusion and hence multi-modal deepfake detection. In this
paper, we propose a novel multi-modal attention framework based on recurrent
neural networks (RNNs) that leverages contextual information for audio-visual
deepfake detection. The proposed approach applies attention to multi-modal
multi-sequence representations and learns the contributing features among them
for deepfake detection and localization. Thorough experimental validations on
audio-visual deepfake datasets, namely FakeAVCeleb, AV-Deepfake1M, TVIL, and
LAV-DF datasets, demonstrate the efficacy of our approach. Cross-comparison
with the published studies demonstrates superior performance of our approach
with an improved accuracy and precision by 3.47% and 2.05% in deepfake
detection and localization, respectively. Thus, obtaining state-of-the-art
performance. To facilitate reproducibility, the code and the datasets
information is available at https://github.com/vcbsl/audiovisual-deepfake/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COVID-19 Detection Based on Blood Test Parameters using Various
  Artificial Intelligence Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02348v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02348v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavian Khanjani, Seyed Rasoul Hosseini, Hamid Taheri, Shahrzad Shashaani, Mohammad Teshnehlab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 2019, the world faced a new challenge: a COVID-19 disease caused by the
novel coronavirus, SARS-CoV-2. The virus rapidly spread across the globe,
leading to a high rate of mortality, which prompted health organizations to
take measures to control its transmission. Early disease detection is crucial
in the treatment process, and computer-based automatic detection systems have
been developed to aid in this effort. These systems often rely on artificial
intelligence (AI) approaches such as machine learning, neural networks, fuzzy
systems, and deep learning to classify diseases. This study aimed to
differentiate COVID-19 patients from others using self-categorizing classifiers
and employing various AI methods. This study used two datasets: the blood test
samples and radiography images. The best results for the blood test samples
obtained from San Raphael Hospital, which include two classes of individuals,
those with COVID-19 and those with non-COVID diseases, were achieved through
the use of the Ensemble method (a combination of a neural network and two
machines learning methods). The results showed that this approach for COVID-19
diagnosis is cost-effective and provides results in a shorter amount of time
than other methods. The proposed model achieved an accuracy of 94.09% on the
dataset used. Secondly, the radiographic images were divided into four classes:
normal, viral pneumonia, ground glass opacity, and COVID-19 infection. These
were used for segmentation and classification. The lung lobes were extracted
from the images and then categorized into specific classes. We achieved an
accuracy of 91.1% on the image dataset. Generally, this study highlights the
potential of AI in detecting and managing COVID-19 and underscores the
importance of continued research and development in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under review by Int. J. of Computational Science and
  Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ENet-21: An Optimized light CNN Structure for Lane Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Rasoul Hosseini, Hamid Taheri, Mohammad Teshnehlab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane detection for autonomous vehicles is an important concept, yet it is a
challenging issue of driver assistance systems in modern vehicles. The
emergence of deep learning leads to significant progress in self-driving cars.
Conventional deep learning-based methods handle lane detection problems as a
binary segmentation task and determine whether a pixel belongs to a line. These
methods rely on the assumption of a fixed number of lanes, which does not
always work. This study aims to develop an optimal structure for the lane
detection problem, offering a promising solution for driver assistance features
in modern vehicles by utilizing a machine learning method consisting of binary
segmentation and Affinity Fields that can manage varying numbers of lanes and
lane change scenarios. In this approach, the Convolutional Neural Network
(CNN), is selected as a feature extractor, and the final output is obtained
through clustering of the semantic segmentation and Affinity Field outputs. Our
method uses less complex CNN architecture than existing ones. Experiments on
the TuSimple dataset support the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is under review by Int. J. of Mechatronics and Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GP-VLS: A general-purpose vision language model for surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Schmidgall, Joseph Cho, Cyril Zakka, William Hiesinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgery requires comprehensive medical knowledge, visual assessment skills,
and procedural expertise. While recent surgical AI models have focused on
solving task-specific problems, there is a need for general-purpose systems
that can understand surgical scenes and interact through natural language. This
paper introduces GP-VLS, a general-purpose vision language model for surgery
that integrates medical and surgical knowledge with visual scene understanding.
For comprehensively evaluating general-purpose surgical models, we propose
SurgiQual, which evaluates across medical and surgical knowledge benchmarks as
well as surgical vision-language questions. To train GP-VLS, we develop six new
datasets spanning medical knowledge, surgical textbooks, and vision-language
pairs for tasks like phase recognition and tool identification. We show that
GP-VLS significantly outperforms existing open- and closed-source models on
surgical vision-language tasks, with 8-21% improvements in accuracy across
SurgiQual benchmarks. GP-VLS also demonstrates strong performance on medical
and surgical knowledge tests compared to open-source alternatives. Overall,
GP-VLS provides an open-source foundation for developing AI assistants to
support surgeons across a wide range of tasks and scenarios. The code and data
for this work is publicly available at gpvls-surgery-vlm.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Capsule Network Projectors are Equivariant and Invariant Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miles Everett, Aiden Durrant, Mingjun Zhong, Georgios Leontidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning invariant representations has been the longstanding approach to
self-supervised learning. However, recently progress has been made in
preserving equivariant properties in representations, yet do so with highly
prescribed architectures. In this work, we propose an invariant-equivariant
self-supervised architecture that employs Capsule Networks (CapsNets) which
have been shown to capture equivariance with respect to novel viewpoints. We
demonstrate that the use of CapsNets in equivariant self-supervised
architectures achieves improved downstream performance on equivariant tasks
with higher efficiency and fewer network parameters. To accommodate the
architectural changes of CapsNets, we introduce a new objective function based
on entropy minimisation. This approach which we name CapsIE (Capsule Invariant
Equivariant Network) achieves state-of-the-art performance across invariant and
equivariant tasks on the 3DIEBench dataset compared to prior equivariant SSL
methods, while outperforming supervised baselines. Our results demonstrate the
ability of CapsNets to learn complex and generalised representations for
large-scale, multi-task datasets compared to previous CapsNet benchmarks. Code
is available at https://github.com/AberdeenML/CapsIE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 10 Tables; code to be released at:
  https://github.com/AberdeenML/CapsIE V2: corrected typos, added a new Table 3
  and additional results in Table 1 and Table 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction
  using Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Saha, Zekai Liang, Shan Lin, Jingpei Lu, Michael Yip, Sainan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstruction of deformable scenes from endoscopic videos is important for
many applications such as intraoperative navigation, surgical visual
perception, and robotic surgery. It is a foundational requirement for realizing
autonomous robotic interventions for minimally invasive surgery. However,
previous approaches in this domain have been limited by their modular nature
and are confined to specific camera and scene settings. Our work adopts the
Neural Radiance Fields (NeRF) approach to learning 3D implicit representations
of scenes that are both dynamic and deformable over time, and furthermore with
unknown camera poses. We demonstrate this approach on endoscopic surgical
scenes from robotic surgery. This work removes the constraints of known camera
poses and overcomes the drawbacks of the state-of-the-art unstructured dynamic
scene reconstruction technique, which relies on the static part of the scene
for accurate reconstruction. Through several experimental datasets, we
demonstrate the versatility of our proposed model to adapt to diverse camera
and scene settings, and show its promise for both current and future robotic
surgical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RICA2: Rubric-Informed, Calibrated Assessment of Actions <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02138v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02138v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abrar Majeedi, Viswanatha Reddy Gajjala, Satya Sai Srinath Namburi GNVV, Yin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to quantify how well an action is carried out, also known as
action quality assessment (AQA), has attracted recent interest in the vision
community. Unfortunately, prior methods often ignore the score rubric used by
human experts and fall short of quantifying the uncertainty of the model
prediction. To bridge the gap, we present RICA^2 - a deep probabilistic model
that integrates score rubric and accounts for prediction uncertainty for AQA.
Central to our method lies in stochastic embeddings of action steps, defined on
a graph structure that encodes the score rubric. The embeddings spread
probabilistic density in the latent space and allow our method to represent
model uncertainty. The graph encodes the scoring criteria, based on which the
quality scores can be decoded. We demonstrate that our method establishes new
state of the art on public benchmarks, including FineDiving, MTL-AQA, and
JIGSAWS, with superior performance in score prediction and uncertainty
calibration. Our code is available at https://abrarmajeedi.github.io/rica2_aqa/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at European Conference on Computer Vision (ECCV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative
  Generation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Lu, Ryan Teehan, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose ProCreate, a simple and easy-to-implement method to
improve sample diversity and creativity of diffusion-based image generative
models and to prevent training data reproduction. ProCreate operates on a set
of reference images and actively propels the generated image embedding away
from the reference embeddings during the generation process. We propose FSCG-8
(Few-Shot Creative Generation 8), a few-shot creative generation dataset on
eight different categories -- encompassing different concepts, styles, and
settings -- in which ProCreate achieves the highest sample diversity and
fidelity. Furthermore, we show that ProCreate is effective at preventing
replicating training data in a large-scale evaluation using training text
prompts. Code and FSCG-8 are available at
https://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The
project page is available at https://procreate-diffusion.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024. Project page:
  https://procreate-diffusion.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bilateral Guided Radiance Field Processing <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuehao Wang, Chaoyi Wang, Bingchen Gong, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) achieves unprecedented performance in
synthesizing novel view synthesis, utilizing multi-view consistency. When
capturing multiple inputs, image signal processing (ISP) in modern cameras will
independently enhance them, including exposure adjustment, color correction,
local tone mapping, etc. While these processings greatly improve image quality,
they often break the multi-view consistency assumption, leading to "floaters"
in the reconstructed radiance fields. To address this concern without
compromising visual aesthetics, we aim to first disentangle the enhancement by
ISP at the NeRF training stage and re-apply user-desired enhancements to the
reconstructed radiance fields at the finishing stage. Furthermore, to make the
re-applied enhancements consistent between novel views, we need to perform
imaging signal processing in 3D space (i.e. "3D ISP"). For this goal, we adopt
the bilateral grid, a locally-affine model, as a generalized representation of
ISP processing. Specifically, we optimize per-view 3D bilateral grids with
radiance fields to approximate the effects of camera pipelines for each input
view. To achieve user-adjustable 3D finishing, we propose to learn a low-rank
4D bilateral grid from a given single view edit, lifting photo enhancements to
the whole 3D scene. We demonstrate our approach can boost the visual quality of
novel view synthesis by effectively removing floaters and performing
enhancements from user retouching. The source code and our data are available
at: https://bilarfpro.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH (ACM TOG), 2024. Project page: https://bilarfpro.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisit <span class="highlight-title">Self-supervised</span> Depth Estimation with Local
  Structure-from-Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Zhu, Xiaoming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both self-supervised depth estimation and Structure-from-Motion (SfM) recover
scene depth from RGB videos. Despite sharing a similar objective, the two
approaches are disconnected. Prior works of self-supervision backpropagate
losses defined within immediate neighboring frames. Instead of
learning-through-loss, this work proposes an alternative scheme by performing
local SfM. First, with calibrated RGB or RGB-D images, we employ a depth and
correspondence estimator to infer depthmaps and pair-wise correspondence maps.
Then, a novel bundle-RANSAC-adjustment algorithm jointly optimizes camera poses
and one depth adjustment for each depthmap. Finally, we fix camera poses and
employ a NeRF, however, without a neural network, for dense triangulation and
geometric verification. Poses, depth adjustments, and triangulated sparse
depths are our outputs. For the first time, we show self-supervision within $5$
frames already benefits SoTA supervised depth and correspondence models. The
project page is held in the link (https://shngjz.github.io/SSfM.github.io/).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for
  Upper-Body Pose Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhong Catherine Yu, Manru Mary Zhang, Peter He, Chi-Jung Lee, Cassidy Cheesman, Saif Mahmud, Ruidong Zhang, François Guimbretière, Cheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seams are areas of overlapping fabric formed by stitching two or more pieces
of fabric together in the cut-and-sew apparel manufacturing process. In
SeamPose, we repurposed seams as capacitive sensors in a shirt for continuous
upper-body pose estimation. Compared to previous all-textile motion-capturing
garments that place the electrodes on the clothing surface, our solution
leverages existing seams inside of a shirt by machine-sewing insulated
conductive threads over the seams. The unique invisibilities and placements of
the seams afford the sensing shirt to look and wear similarly as a conventional
shirt while providing exciting pose-tracking capabilities. To validate this
approach, we implemented a proof-of-concept untethered shirt with 8 capacitive
sensing seams. With a 12-participant user study, our customized deep-learning
pipeline accurately estimates the relative (to the pelvis) upper-body 3D joint
positions with a mean per joint position error (MPJPE) of 6.0 cm. SeamPose
represents a step towards unobtrusive integration of smart clothing for
everyday pose estimation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">133</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClassiFIM: An Unsupervised Method To Detect Phase Transitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Kasatkin, Evgeny Mozgunov, Nicholas Ezzell, Utkarsh Mishra, Itay Hen, Daniel Lidar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimation of the Fisher Information Metric (FIM-estimation) is an important
task that arises in unsupervised learning of phase transitions, a problem
proposed by physicists. This work completes the definition of the task by
defining rigorous evaluation metrics distMSE, distMSEPS, and distRE and
introduces ClassiFIM, a novel machine learning method designed to solve the
FIM-estimation task. Unlike existing methods for unsupervised learning of phase
transitions, ClassiFIM directly estimates a well-defined quantity (the FIM),
allowing it to be rigorously compared to any present and future other methods
that estimate the same. ClassiFIM transforms a dataset for the FIM-estimation
task into a dataset for an auxiliary binary classification task and involves
selecting and training a model for the latter. We prove that the output of
ClassiFIM approaches the exact FIM in the limit of infinite dataset size and
under certain regularity conditions. We implement ClassiFIM on multiple
datasets, including datasets describing classical and quantum phase
transitions, and find that it achieves a good ground truth approximation with
modest computational resources. Furthermore, we independently implement two
alternative state-of-the-art methods for unsupervised estimation of phase
transition locations on the same datasets and find that ClassiFIM predicts such
locations at least as well as these other methods. To emphasize the generality
of our method, we also propose and generate the MNIST-CNN dataset, which
consists of the output of CNNs trained on MNIST for different hyperparameter
choices. Using ClassiFIM on this dataset suggests there is a phase transition
in the distribution of image-prediction pairs for CNNs trained on MNIST,
demonstrating the broad scope of FIM-estimation beyond physics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hedge Fund Portfolio Construction Using PolyModel Theory and
  i<span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqiao Zhao, Zhikang Dong, Zeyu Cao, Raphael Douady
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When constructing portfolios, a key problem is that a lot of financial time
series data are sparse, making it challenging to apply machine learning
methods. Polymodel theory can solve this issue and demonstrate superiority in
portfolio construction from various aspects. To implement the PolyModel theory
for constructing a hedge fund portfolio, we begin by identifying an asset pool,
utilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theory
also involves choosing a wide-ranging set of risk factors, which includes
various financial indices, currencies, and commodity prices. This comprehensive
selection mirrors the complexities of the real-world environment. Leveraging on
the PolyModel theory, we create quantitative measures such as Long-term Alpha,
Long-term Ratio, and SVaR. We also use more classical measures like the Sharpe
ratio or Morningstar's MRAR. To enhance the performance of the constructed
portfolio, we also employ the latest deep learning techniques (iTransformer) to
capture the upward trend, while efficiently controlling the downside, using all
the features. The iTransformer model is specifically designed to address the
challenges in high-dimensional time series forecasting and could largely
improve our strategies. More precisely, our strategies achieve better Sharpe
ratio and annualized return. The above process enables us to create multiple
portfolio strategies aiming for high returns and low risks when compared to
various benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling LLM Test-Time Compute Optimally can be More Effective than
  Scaling Model Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling LLMs to improve their outputs by using more test-time computation is
a critical step towards building generally self-improving agents that can
operate on open-ended natural language. In this paper, we study the scaling of
inference-time computation in LLMs, with a focus on answering the question: if
an LLM is allowed to use a fixed but non-trivial amount of inference-time
compute, how much can it improve its performance on a challenging prompt?
Answering this question has implications not only on the achievable performance
of LLMs, but also on the future of LLM pretraining and how one should tradeoff
inference-time and pre-training compute. Despite its importance, little
research attempted to understand the scaling behaviors of various test-time
inference methods. Moreover, current work largely provides negative results for
a number of these strategies. In this work, we analyze two primary mechanisms
to scale test-time computation: (1) searching against dense, process-based
verifier reward models; and (2) updating the model's distribution over a
response adaptively, given the prompt at test time. We find that in both cases,
the effectiveness of different approaches to scaling test-time compute
critically varies depending on the difficulty of the prompt. This observation
motivates applying a "compute-optimal" scaling strategy, which acts to most
effectively allocate test-time compute adaptively per prompt. Using this
compute-optimal strategy, we can improve the efficiency of test-time compute
scaling by more than 4x compared to a best-of-N baseline. Additionally, in a
FLOPs-matched evaluation, we find that on problems where a smaller base model
attains somewhat non-trivial success rates, test-time compute can be used to
outperform a 14x larger model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ing and in-context learning IS Bayesian inference a la De
  Finetti 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naimeng Ye, Hanming Yang, Andrew Siah, Hongseok Namkoong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately gauging uncertainty on the underlying environment is a
longstanding goal of intelligent systems. We characterize which latent concepts
pre-trained sequence models are naturally able to reason with. We go back to De
Finetti's predictive view of Bayesian reasoning: instead of modeling latent
parameters through priors and likelihoods like topic models do, De Finetti has
long advocated for modeling exchangeable (permutation invariant) sequences of
observables. According to this view, pre-training autoregressive models
formulates informed beliefs based on prior observations ("empirical Bayes"),
and forward generation is a simulated instantiation of an environment
("posterior inference"). This connection allows extending in-context learning
(ICL) beyond predictive settings, highlighting sequence models' ability to
perform explicit statistical inference. In particular, we show the sequence
prediction loss over exchangeable documents controls performance on downstream
tasks where uncertainty quantification is key. Empirically, we propose and
demonstrate several approaches for encoding exchangeability in sequence model
architectures: data augmentation, regularization, and causal masking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks <span class="chip">ICPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Sterzinger, Christian Stippel, Robert Sablatnig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Etruscan mirrors constitute a significant category in Etruscan art,
characterized by elaborate figurative illustrations featured on their backside.
A laborious and costly aspect of their analysis and documentation is the task
of manually tracing these illustrations. In previous work, a methodology has
been proposed to automate this process, involving photometric-stereo scanning
in combination with deep neural networks. While achieving quantitative
performance akin to an expert annotator, some results still lack qualitative
precision and, thus, require annotators for inspection and potential
correction, maintaining resource intensity. In response, we propose a deep
neural network trained to interactively refine existing annotations based on
human guidance. Our human-in-the-loop approach streamlines annotation,
achieving equal quality with up to 75% less manual input required. Moreover,
during the refinement process, the relative improvement of our methodology over
pure manual labeling reaches peak values of up to 26%, attaining drastically
better quality quicker. By being tailored to the complex task of segmenting
intricate lines, specifically distinguishing it from previous methods, our
approach offers drastic improvements in efficacy, transferable to a broad
spectrum of applications beyond Etruscan mirrors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, accepted at ICPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SARA: Singular-Value Based Adaptive Low-Rank Adaption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Gu, Shuai Chen, Zelin Wang, Yibo Zhang, Ping Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing number of parameters in large pre-trained models, LoRA as
a parameter-efficient fine-tuning(PEFT) method is widely used for not adding
inference overhead. The LoRA method assumes that weight changes during
fine-tuning can be approximated by low-rank matrices. However, the rank values
need to be manually verified to match different downstream tasks, and they
cannot accommodate the varying importance of different layers in the model. In
this work, we first analyze the relationship between the performance of
different layers and their ranks using SVD. Based on this, we design the
Singular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds
the rank during initialization by performing SVD on the pre-trained weights.
Additionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly
reduces the number of parameters by fine-tuning only multiple parallel sets of
singular values controlled by a router. Extensive experiments on various
complex tasks demonstrate the simplicity and parameter efficiency of our
methods. They can effectively and adaptively find the most suitable rank for
each layer of each model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compress and Compare: Interactively Evaluating Efficiency and Behavior
  Across ML Model Compression Experiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angie Boggust, Venkatesh Sivaraman, Yannick Assogba, Donghao Ren, Dominik Moritz, Fred Hohman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To deploy machine learning models on-device, practitioners use compression
algorithms to shrink and speed up models while maintaining their high-quality
output. A critical aspect of compression in practice is model comparison,
including tracking many compression experiments, identifying subtle changes in
model behavior, and negotiating complex accuracy-efficiency trade-offs.
However, existing compression tools poorly support comparison, leading to
tedious and, sometimes, incomplete analyses spread across disjoint tools. To
support real-world comparative workflows, we develop an interactive visual
system called Compress and Compare. Within a single interface, Compress and
Compare surfaces promising compression strategies by visualizing provenance
relationships between compressed models and reveals compression-induced
behavior changes by comparing models' predictions, weights, and activations. We
demonstrate how Compress and Compare supports common compression analysis tasks
through two case studies, debugging failed compression on generative language
models and identifying compression artifacts in image classification models. We
further evaluate Compress and Compare in a user study with eight compression
experts, illustrating its potential to provide structure to compression
workflows, help practitioners build intuition about compression, and encourage
thorough analysis of compression's effect on model behavior. Through these
evaluations, we identify compression-specific challenges that future visual
analytics tools should consider and Compress and Compare visualizations that
may generalize to broader model comparison tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to VIS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding
  with Extended Degrees of Freedom 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        W. S. Leite, R. C. de Lamare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problem of direction-of-arrival (DOA) estimation
using multiple partially-calibrated sparse subarrays. In particular, we present
the Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA
estimation algorithm to scenarios with partially-calibrated sparse subarrays.
The proposed GCA-MUSIC algorithm exploits the difference coarray for each
subarray, followed by a specific pseudo-spectrum merging rule that is based on
the intersection of the signal subspaces associated to each subarray. This rule
assumes that there is no a priori knowledge about the cross-covariance between
subarrays. In that way, only the second-order statistics of each subarray are
used to estimate the directions with increased degrees of freedom, i.e., the
estimation procedure preserves the coarray Multiple Signal Classification and
sparse arrays properties to estimate more sources than the number of physical
sensors in each subarray. Numerical simulations show that the proposed
GCA-MUSIC has better performance than other similar strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Think It Twice: Exploit Shift Invariance for Efficient Online
  Streaming Inference of CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning time-series processing often relies on convolutional neural
networks with overlapping windows. This overlap allows the network to produce
an output faster than the window length. However, it introduces additional
computations. This work explores the potential to optimize computational
efficiency during inference by exploiting convolution's shift-invariance
properties to skip the calculation of layer activations between successive
overlapping windows. Although convolutions are shift-invariant, zero-padding
and pooling operations, widely used in such networks, are not efficient and
complicate efficient streaming inference. We introduce StreamiNNC, a strategy
to deploy Convolutional Neural Networks for online streaming inference. We
explore the adverse effects of zero padding and pooling on the accuracy of
streaming inference, deriving theoretical error upper bounds for pooling during
streaming. We address these limitations by proposing signal padding and pooling
alignment and provide guidelines for designing and deploying models for
StreamiNNC. We validate our method in simulated data and on three real-world
biomedical signal processing applications. StreamiNNC achieves a low deviation
between streaming output and normal inference for all three networks (2.03 -
3.55% NRMSE). This work demonstrates that it is possible to linearly speed up
the inference of streaming CNNs processing overlapping windows, negating the
additional computation typically incurred by overlapping windows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Random Noise for Communication Efficient Federaetd Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwei Li, Yingyi Cheng, Haozhao Wang, Xing Tang, Shijie Xu, Weihong Luo, Yuhua Li, Dugang Liu, Xiuqiang He, and Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a promising distributed training paradigm that
effectively safeguards data privacy. However, it may involve significant
communication costs, which hinders training efficiency. In this paper, we aim
to enhance communication efficiency from a new perspective. Specifically, we
request the distributed clients to find optimal model updates relative to
global model parameters within predefined random noise. For this purpose, we
propose Federated Masked Random Noise (FedMRN), a novel framework that enables
clients to learn a 1-bit mask for each model parameter and apply masked random
noise (i.e., the Hadamard product of random noise and masks) to represent model
updates. To make FedMRN feasible, we propose an advanced mask training
strategy, called progressive stochastic masking (PSM). After local training,
each client only need to transmit local masks and a random seed to the server.
Additionally, we provide theoretical guarantees for the convergence of FedMRN
under both strongly convex and non-convex assumptions. Extensive experiments
are conducted on four popular datasets. The results show that FedMRN exhibits
superior convergence speed and test accuracy compared to relevant baselines,
while attaining a similar level of accuracy as FedAvg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Learn without Forgetting using Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Vettoruzzo, Joaquin Vanschoren, Mohamed-Rafik Bouguelia, Thorsteinn Rögnvaldsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) refers to the ability to continually learn over time
by accommodating new knowledge while retaining previously learned experience.
While this concept is inherent in human learning, current machine learning
methods are highly prone to overwrite previously learned patterns and thus
forget past experience. Instead, model parameters should be updated selectively
and carefully, avoiding unnecessary forgetting while optimally leveraging
previously learned patterns to accelerate future learning. Since hand-crafting
effective update mechanisms is difficult, we propose meta-learning a
transformer-based optimizer to enhance CL. This meta-learned optimizer uses
attention to learn the complex relationships between model parameters across a
stream of tasks, and is designed to generate effective weight updates for the
current task while preventing catastrophic forgetting on previously encountered
tasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and
SplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both
forward and backward transfer, even on small sets of labeled data, highlighting
the advantages of integrating a meta-learned optimizer within the continual
learning framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedBAT: Communication-Efficient Federated Learning via Learnable
  Binarization <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwei Li, Wenchao Xu, Haozhao Wang, Xing Tang, Yining Qi, Shijie Xu, Weihong Luo, Yuhua Li, Xiuqiang He, Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a promising distributed machine learning paradigm that
can effectively exploit large-scale data without exposing users' privacy.
However, it may incur significant communication overhead, thereby potentially
impairing the training efficiency. To address this challenge, numerous studies
suggest binarizing the model updates. Nonetheless, traditional methods usually
binarize model updates in a post-training manner, resulting in significant
approximation errors and consequent degradation in model accuracy. To this end,
we propose Federated Binarization-Aware Training (FedBAT), a novel framework
that directly learns binary model updates during the local training process,
thus inherently reducing the approximation errors. FedBAT incorporates an
innovative binarization operator, along with meticulously designed derivatives
to facilitate efficient learning. In addition, we establish theoretical
guarantees regarding the convergence of FedBAT. Extensive experiments are
conducted on four popular datasets. The results show that FedBAT significantly
accelerates the convergence and exceeds the accuracy of baselines by up to 9\%,
even surpassing that of FedAvg in some cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence Conditions for Stochastic Line Search Based Optimization of
  Over-parametrized Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Lapucci, Davide Pucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we deal with algorithms to solve the finite-sum problems
related to fitting over-parametrized models, that typically satisfy the
interpolation condition. In particular, we focus on approaches based on
stochastic line searches and employing general search directions. We define
conditions on the sequence of search directions that guarantee finite
termination and bounds for the backtracking procedure. Moreover, we shed light
on the additional property of directions needed to prove fast (linear)
convergence of the general class of algorithms when applied to PL functions in
the interpolation regime. From the point of view of algorithms design, the
proposed analysis identifies safeguarding conditions that could be employed in
relevant algorithmic framework. In particular, it could be of interest to
integrate stochastic line searches within momentum, conjugate gradient or
adaptive preconditioning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RELIEF: Reinforcement Learning Empowered Graph Feature <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiapeng Zhu, Zichen Ding, Jianxiang Yu, Jiaqi Tan, Xiang Li, Weining Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of the "pre-train, prompt" paradigm has recently extended its
generalization ability and data efficiency to graph representation learning,
following its achievements in Natural Language Processing (NLP). Initial graph
prompt tuning approaches tailored specialized prompting functions for Graph
Neural Network (GNN) models pre-trained with specific strategies, such as edge
prediction, thus limiting their applicability. In contrast, another pioneering
line of research has explored universal prompting via adding prompts to the
input graph's feature space, thereby removing the reliance on specific
pre-training strategies. However, the necessity to add feature prompts to all
nodes remains an open question. Motivated by findings from prompt tuning
research in the NLP domain, which suggest that highly capable pre-trained
models need less conditioning signal to achieve desired behaviors, we advocate
for strategically incorporating necessary and lightweight feature prompts to
certain graph nodes to enhance downstream task performance. This introduces a
combinatorial optimization problem, requiring a policy to decide 1) which nodes
to prompt and 2) what specific feature prompts to attach. We then address the
problem by framing the prompt incorporation process as a sequential
decision-making problem and propose our method, RELIEF, which employs
Reinforcement Learning (RL) to optimize it. At each step, the RL agent selects
a node (discrete action) and determines the prompt content (continuous action),
aiming to maximize cumulative performance gain. Extensive experiments on graph
and node-level tasks with various pre-training strategies in few-shot scenarios
demonstrate that our RELIEF outperforms fine-tuning and other prompt-based
approaches in classification performance and data efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Object is Worth 64x64 Pixels: Generating 3D Object via Image
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingguang Yan, Han-Hung Lee, Ziyu Wan, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new approach for generating realistic 3D models with UV maps
through a representation termed "Object Images." This approach encapsulates
surface geometry, appearance, and patch structures within a 64x64 pixel image,
effectively converting complex 3D shapes into a more manageable 2D format. By
doing so, we address the challenges of both geometric and semantic irregularity
inherent in polygonal meshes. This method allows us to use image generation
models, such as Diffusion Transformers, directly for 3D shape generation.
Evaluated on the ABO dataset, our generated shapes with patch structures
achieve point cloud FID comparable to recent 3D generative models, while
naturally supporting PBR material generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://omages.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Parameter Efficient Training Methods for Low Resource Text
  Classification: A Case Study in Marathi 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranita Deshmukh, Nikita Kulkarni, Sanhita Kulkarni, Kareena Manghani, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the surge in digital content in low-resource languages, there is an
escalating demand for advanced Natural Language Processing (NLP) techniques
tailored to these languages. BERT (Bidirectional Encoder Representations from
Transformers), serving as the foundational framework for numerous NLP
architectures and language models, is increasingly employed for the development
of low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method
for fine-tuning Large Language Models (LLMs) and reducing the training
parameters to some extent to decrease the computational costs needed for
training the model and achieve results comparable to a fully fine-tuned model.
In this work, we present a study of PEFT methods for the Indic low-resource
language Marathi. We conduct a comprehensive analysis of PEFT methods applied
to various monolingual and multilingual Marathi BERT models. These approaches
are evaluated on prominent text classification datasets like MahaSent,
MahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to
significantly expedite the training speed of the models, addressing a critical
aspect of model development and deployment. In this study, we explore Low-Rank
Adaptation of Large Language Models (LoRA) and adapter methods for low-resource
text classification. We show that these methods are competitive with full
fine-tuning and can be used without loss in accuracy. This study contributes
valuable insights into the effectiveness of Marathi BERT models, offering a
foundation for the continued advancement of NLP capabilities in Marathi and
similar Indic languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at I2CT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative CT Reconstruction via Latent Variable Optimization of Shallow
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sho Ozaki, Shizuo Kaji, Toshikazu Imae, Kanabu Nawa, Hideomi Yamashita, Keiichi Nakagawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generative AI has garnered significant attention in recent years. In
particular, the diffusion model, a core component of recent generative AI,
produces high-quality images with rich diversity. In this study, we propose a
novel CT reconstruction method by combining the denoising diffusion
probabilistic model with iterative CT reconstruction. In sharp contrast to
previous studies, we optimize the fidelity loss of CT reconstruction with
respect to the latent variable of the diffusion model, instead of the image and
model parameters. To suppress anatomical structure changes produced by the
diffusion model, we shallow the diffusion and reverse processes, and fix a set
of added noises in the reverse process to make it deterministic during
inference. We demonstrate the effectiveness of the proposed method through
sparse view CT reconstruction of 1/10 view projection data. Despite the
simplicity of the implementation, the proposed method shows the capability of
reconstructing high-quality images while preserving the patient's anatomical
structure, and outperforms existing methods including iterative reconstruction,
iterative reconstruction with total variation, and the diffusion model alone in
terms of quantitative indices such as SSIM and PSNR. We also explore further
sparse view CT using 1/20 view projection data with the same trained diffusion
model. As the number of iterations increases, image quality improvement
comparable to that of 1/10 sparse view CT reconstruction is achieved. In
principle, the proposed method can be widely applied not only to CT but also to
other imaging modalities such as MRI, PET, and SPECT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TSC: A Simple Two-Sided Constraint against Over-Smoothing <span class="chip">KDD2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furong Peng, Kang Liu, Xuan Lu, Yuhua Qian, Hongren Yan, Chao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolutional Neural Network (GCN), a widely adopted method for
analyzing relational data, enhances node discriminability through the
aggregation of neighboring information. Usually, stacking multiple layers can
improve the performance of GCN by leveraging information from high-order
neighbors. However, the increase of the network depth will induce the
over-smoothing problem, which can be attributed to the quality and quantity of
neighbors changing: (a) neighbor quality, node's neighbors become overlapping
in high order, leading to aggregated information becoming indistinguishable,
(b) neighbor quantity, the exponentially growing aggregated neighbors submerges
the node's initial feature by recursively aggregating operations. Current
solutions mainly focus on one of the above causes and seldom consider both at
once.
  Aiming at tackling both causes of over-smoothing in one shot, we introduce a
simple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet
potent techniques: random masking and contrastive constraint. The random
masking acts on the representation matrix's columns to regulate the degree of
information aggregation from neighbors, thus preventing the convergence of node
representations. Meanwhile, the contrastive constraint, applied to the
representation matrix's rows, enhances the discriminability of the nodes.
Designed as a plug-in module, TSC can be easily coupled with GCN or SGC
architectures. Experimental analyses on diverse real-world graph datasets
verify that our approach markedly reduces the convergence of node's
representation and the performance degradation in deeper GCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accept by KDD2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditioning LLMs with Emotion in Neural Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Brazier, Jean-Luc Rouas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable performance in Natural
Language Processing tasks, including Machine Translation (MT). In this work, we
propose a novel MT pipeline that integrates emotion information extracted from
a Speech Emotion Recognition (SER) model into LLMs to enhance translation
quality. We first fine-tune five existing LLMs on the Libri-trans dataset and
select the most performant model. Subsequently, we augment LLM prompts with
different dimensional emotions and train the selected LLM under these different
configurations. Our experiments reveal that integrating emotion information,
especially arousal, into LLM prompts leads to notable improvements in
translation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, In Proceedings of the 21st International Conference on
  Spoken Language Translation (IWSLT), Bangkok, Thailand, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Learning for Level Set Estimation Using Randomized Straddle
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Inatsu, Shion Takeno, Kentaro Kutsukake, Ichiro Takeuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Level set estimation (LSE), the problem of identifying the set of input
points where a function takes value above (or below) a given threshold, is
important in practical applications. When the function is expensive-to-evaluate
and black-box, the \textit{straddle} algorithm, which is a representative
heuristic for LSE based on Gaussian process models, and its extensions having
theoretical guarantees have been developed. However, many of existing methods
include a confidence parameter $\beta^{1/2}_t$ that must be specified by the
user, and methods that choose $\beta^{1/2}_t$ heuristically do not provide
theoretical guarantees. In contrast, theoretically guaranteed values of
$\beta^{1/2}_t$ need to be increased depending on the number of iterations and
candidate points, and are conservative and not good for practical performance.
In this study, we propose a novel method, the \textit{randomized straddle}
algorithm, in which $\beta_t$ in the straddle algorithm is replaced by a random
sample from the chi-squared distribution with two degrees of freedom. The
confidence parameter in the proposed method has the advantages of not needing
adjustment, not depending on the number of iterations and candidate points, and
not being conservative. Furthermore, we show that the proposed method has
theoretical guarantees that depend on the sample complexity and the number of
iterations. Finally, we confirm the usefulness of the proposed method through
numerical experiments using synthetic and real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topic Modeling with Fine-tuning LLMs and Bag of Sentences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM)'s are increasingly used for topic modeling
outperforming classical topic models such as LDA. Commonly, pre-trained LLM
encoders such as BERT are used out-of-the-box despite the fact that fine-tuning
is known to improve LLMs considerably. The challenge lies in obtaining a
suitable (labeled) dataset for fine-tuning. In this paper, we use the recent
idea to use bag of sentences as the elementary unit in computing topics. In
turn, we derive an approach FT-Topic to perform unsupervised fine-tuning
relying primarily on two steps for constructing a training dataset in an
automatic fashion. First, a heuristic method to identifies pairs of sentence
groups that are either assumed to be of the same or different topics. Second,
we remove sentence pairs that are likely labeled incorrectly. The dataset is
then used to fine-tune an encoder LLM, which can be leveraged by any topic
modeling approach using embeddings. However, in this work, we demonstrate its
effectiveness by deriving a novel state-of-the-art topic modeling method called
SenClu, which achieves fast inference through an expectation-maximization
algorithm and hard assignments of sentence groups to a single topic, while
giving users the possibility to encode prior knowledge on the topic-document
distribution. Code is at \url{https://github.com/JohnTailor/FT-Topic}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the submitted journal version of enhanced with the novel
  fine-tuning part of "Efficient and Flexible Topic Modeling using Pretrained
  Embeddings and Bag of Sentences'' which appeared at the International
  Conference on Agents and Artificial Intelligence(ICAART) in 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Provably Robust Policies in Uncertain Parametric Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannik Schnitzer, Alessandro Abate, David Parker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a data-driven approach for learning MDP policies that are robust
across stochastic environments whose transition probabilities are defined by
parameters with an unknown distribution. We produce probably approximately
correct (PAC) guarantees for the performance of these learned policies in a
new, unseen environment over the unknown distribution. Our approach is based on
finite samples of the MDP environments, for each of which we build an
approximation of the model as an interval MDP, by exploring a set of generated
trajectories. We use the built approximations to synthesise a single policy
that performs well (meets given requirements) across the sampled environments,
and furthermore bound its risk (of not meeting the given requirements) when
deployed in an unseen environment. Our procedure offers a trade-off between the
guaranteed performance of the learned policy and the risk of not meeting the
guarantee in an unseen environment. Our approach exploits knowledge of the
environment's state space and graph structure, and we show how additional
knowledge of its parametric structure can be leveraged to optimize learning and
to obtain tighter guarantees from less samples. We evaluate our approach on a
diverse range of established benchmarks, demonstrating that we can generate
highly performing and robust policies, along with guarantees that tightly
quantify their performance and the associated risk.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Dutta, Nouhaila Innan, Alberto Marchisio, Sadok Ben Yahia, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial market prediction and optimal trading strategy development remain
challenging due to market complexity and volatility. Our research in quantum
finance and reinforcement learning for decision-making demonstrates the
approach of quantum-classical hybrid algorithms to tackling real-world
financial challenges. In this respect, we corroborate the concept with rigorous
backtesting and validate the framework's performance under realistic market
conditions, by including fixed transaction cost per trade. This paper
introduces a Quantum Attention Deep Q-Network (QADQN) approach to address these
challenges through quantum-enhanced reinforcement learning. Our QADQN
architecture uses a variational quantum circuit inside a traditional deep
Q-learning framework to take advantage of possible quantum advantages in
decision-making. We gauge the QADQN agent's performance on historical data from
major market indices, including the S&P 500. We evaluate the agent's learning
process by examining its reward accumulation and the effectiveness of its
experience replay mechanism. Our empirical results demonstrate the QADQN's
superior performance, achieving better risk-adjusted returns with Sortino
ratios of 1.28 and 1.19 for non-overlapping and overlapping test periods
respectively, indicating effective downside risk management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2024 IEEE International Conference on Quantum
  Computing and Engineering (QCE24), QCRL, September 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matrix Multiplication on Quantum Computer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Yao, Ding Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative and practical approach to universal
quantum matrix multiplication. We designed optimized quantum adders and
multipliers based on Quantum Fourier Transform (QFT), which significantly
reduced the number of gates used compared to classical adders and multipliers.
Subsequently, we construct a basic universal quantum matrix multiplication and
extend it to the Strassen algorithm. We conduct comparative experiments to
analyze the performance of the quantum matrix multiplication and evaluate the
acceleration provided by the optimized quantum adder and multiplier.
Furthermore, we investigate the advantages and disadvantages of the quantum
Strassen algorithm compared to basic quantum matrix multiplication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on Autonomous Driving Decision-making Strategies based Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Wang, Hao Yan, Changsong Wei, Junyu Wang, Shi Bo, Minheng Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The behavior decision-making subsystem is a key component of the autonomous
driving system, which reflects the decision-making ability of the vehicle and
the driver, and is an important symbol of the high-level intelligence of the
vehicle. However, the existing rule-based decision-making schemes are limited
by the prior knowledge of designers, and it is difficult to cope with complex
and changeable traffic scenarios. In this work, an advanced deep reinforcement
learning model is adopted, which can autonomously learn and optimize driving
strategies in a complex and changeable traffic environment by modeling the
driving decision-making process as a reinforcement learning problem.
Specifically, we used Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO) for comparative experiments. DQN guides the agent to choose the best
action by approximating the state-action value function, while PPO improves the
decision-making quality by optimizing the policy function. We also introduce
improvements in the design of the reward function to promote the robustness and
adaptability of the model in real-world driving situations. Experimental
results show that the decision-making strategy based on deep reinforcement
learning has better performance than the traditional rule-based method in a
variety of driving tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeurDB: On the Design and Implementation of an AI-powered Autonomous
  Database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanhao Zhao, Shaofeng Cai, Haotian Gao, Hexiang Pan, Siqi Xiang, Naili Xing, Gang Chen, Beng Chin Ooi, Yanyan Shen, Yuncheng Wu, Meihui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Databases are increasingly embracing AI to provide autonomous system
optimization and intelligent in-database analytics, aiming to relieve end-user
burdens across various industry sectors. Nonetheless, most existing approaches
fail to account for the dynamic nature of databases, which renders them
ineffective for real-world applications characterized by evolving data and
workloads. This paper introduces NeurDB, an AI-powered autonomous database that
deepens the fusion of AI and databases with adaptability to data and workload
drift. NeurDB establishes a new in-database AI ecosystem that seamlessly
integrates AI workflows within the database. This integration enables efficient
and effective in-database AI analytics and fast-adaptive learned system
components. Empirical evaluations demonstrate that NeurDB substantially
outperforms existing solutions in managing AI analytics tasks, with the
proposed learned components more effectively handling environmental dynamism
than state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning Architectures: A Performance Evaluation with Crop
  Yield Prediction Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwesha Mukherjee, Rajkumar Buyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has become an emerging technology for data analysis for
IoT applications. This paper implements centralized and decentralized federated
learning frameworks for crop yield prediction based on Long Short-Term Memory
Network. For centralized federated learning, multiple clients and one server is
considered, where the clients exchange their model updates with the server that
works as the aggregator to build the global model. For the decentralized
framework, a collaborative network is formed among the devices either using
ring topology or using mesh topology. In this network, each device receives
model updates from the neighbour devices, and performs aggregation to build the
upgraded model. The performance of the centralized and decentralized federated
learning frameworks are evaluated in terms of prediction accuracy, precision,
recall, F1-Score, and training time. The experimental results present that
$\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized
and decentralized federated learning-based frameworks respectively. The results
also show that the using centralized federated learning the response time can
be reduced by $\sim$75% than the cloud-only framework. Finally, the future
research directions of the use of federated learning in crop yield prediction
are explored in this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Differential Smoothness-based Compact-Dynamic Graph Convolutional
  Network for Spatiotemporal Signal Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Gao, Zicheng Gao, Ye Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High quality spatiotemporal signal is vitally important for real application
scenarios like energy management, traffic planning and cyber security. Due to
the uncontrollable factors like abrupt sensors breakdown or communication
fault, the spatiotemporal signal collected by sensors is always incomplete. A
dynamic graph convolutional network (DGCN) is effective for processing
spatiotemporal signal recovery. However, it adopts a static GCN and a sequence
neural network to explore the spatial and temporal patterns, separately. Such a
separated two-step processing is loose spatiotemporal, thereby failing to
capture the complex inner spatiotemporal correlation. To address this issue,
this paper proposes a Compact-Dynamic Graph Convolutional Network (CDGCN) for
spatiotemporal signal recovery with the following two-fold ideas: a) leveraging
the tensor M-product to build a unified tensor graph convolution framework,
which considers both spatial and temporal patterns simultaneously; and b)
constructing a differential smoothness-based objective function to reduce the
noise interference in spatiotemporal signal, thereby further improve the
recovery accuracy. Experiments on real-world spatiotemporal datasets
demonstrate that the proposed CDGCN significantly outperforms the
state-of-the-art models in terms of recovery accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wave Interpolation Neural Operator: Interpolated Prediction of Electric
  Fields Across Untrained Wavelengths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joonhyuk Seo, Chanik Kang, Dongjin Seo, Haejun Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing photonic structures requires electromagnetic simulations, which
often require high computational costs. Researchers have developed surrogate
solvers for predicting electric fields to alleviate the computational issues.
However, existing surrogate solvers are limited to performing inference at
fixed simulation conditions and require retraining for different conditions. To
address this, we propose Wave Interpolation Neural Operator (WINO), a novel
surrogate solver enabling simulation condition interpolation across a
continuous spectrum of broadband wavelengths. WINO introduces the Fourier Group
Convolution Shuffling operator and a new conditioning method to efficiently
predict electric fields from both trained and untrained wavelength data,
achieving significant improvements in parameter efficiency and spectral
interpolation performance. Our model demonstrates approximately 100 times
faster performance than traditional finite-difference frequency-domain
simulations. Moreover, compared to the state-of-the-art model, we achieve a 74%
reduction in parameters and 80.5% improvements in prediction accuracy for
untrained wavelengths, and 13.2% improvements for trained wavelengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 4 tables / Appendix: 4 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model
  and Neural Operator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Dong, Chuanqi Chen, Jin-Long Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Closure models are widely used in simulating complex multiscale dynamical
systems such as turbulence and the earth system, for which direct numerical
simulation that resolves all scales is often too expensive. For those systems
without a clear scale separation, deterministic and local closure models often
lack enough generalization capability, which limits their performance in many
real-world applications. In this work, we propose a data-driven modeling
framework for constructing stochastic and non-local closure models via
conditional diffusion model and neural operator. Specifically, the Fourier
neural operator is incorporated into a score-based diffusion model, which
serves as a data-driven stochastic closure model for complex dynamical systems
governed by partial differential equations (PDEs). We also demonstrate how
accelerated sampling methods can improve the efficiency of the data-driven
stochastic closure model. The results show that the proposed methodology
provides a systematic approach via generative machine learning techniques to
construct data-driven stochastic closure models for multiscale dynamical
systems with continuous spatiotemporal fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synaptic Modulation using Interspike Intervals Increases Energy
  Efficiency of Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Adams, Magda Zajaczkowska, Ashiq Anjum, Andrea Soltoggio, Shirin Dora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite basic differences between Spiking Neural Networks (SNN) and
Artificial Neural Networks (ANN), most research on SNNs involve adapting
ANN-based methods for SNNs. Pruning (dropping connections) and quantization
(reducing precision) are often used to improve energy efficiency of SNNs. These
methods are very effective for ANNs whose energy needs are determined by
signals transmitted on synapses. However, the event-driven paradigm in SNNs
implies that energy is consumed by spikes. In this paper, we propose a new
synapse model whose weights are modulated by Interspike Intervals (ISI) i.e.
time difference between two spikes. SNNs composed of this synapse model, termed
ISI Modulated SNNs (IMSNN), can use gradient descent to estimate how the ISI of
a neuron changes after updating its synaptic parameters. A higher ISI implies
fewer spikes and vice-versa. The learning algorithm for IMSNNs exploits this
information to selectively propagate gradients such that learning is achieved
by increasing the ISIs resulting in a network that generates fewer spikes. The
performance of IMSNNs with dense and convolutional layers have been evaluated
in terms of classification accuracy and the number of spikes using the MNIST
and FashionMNIST datasets. The performance comparison with conventional SNNs
shows that IMSNNs exhibit upto 90% reduction in the number of spikes while
maintaining similar classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kolmogorov-Arnold PointNet: Deep learning for prediction of fluid fields
  on irregular geometries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Kashefi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Kolmogorov-Arnold PointNet (KA-PointNet) as a novel supervised
deep learning framework for the prediction of incompressible steady-state fluid
flow fields in irregular domains, where the predicted fields are a function of
the geometry of the domains. In KA-PointNet, we implement shared
Kolmogorov-Arnold Networks (KANs) in the segmentation branch of the PointNet
architecture. We utilize Jacobi polynomials to construct shared KANs. As a
benchmark test case, we consider incompressible laminar steady-state flow over
a cylinder, where the geometry of its cross-section varies over the data set.
We investigate the performance of Jacobi polynomials with different degrees as
well as special cases of Jacobi polynomials such as Legendre polynomials,
Chebyshev polynomials of the first and second kinds, and Gegenbauer
polynomials, in terms of the computational cost of training and accuracy of
prediction of the test set. Additionally, we compare the performance of
PointNet with shared KANs (i.e., KA-PointNet) and PointNet with shared
Multilayer Perceptrons (MLPs). It is observed that when the number of trainable
parameters is approximately equal, PointNet with shared KANs (i.e.,
KA-PointNet) outperforms PointNet with shared MLPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws for Data Poisoning in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work shows that LLMs are vulnerable to data poisoning, in which they
are trained on partially corrupted or harmful data. Poisoned data is hard to
detect, breaks guardrails, and leads to undesirable and harmful behavior. Given
the intense efforts by leading labs to train and deploy increasingly larger and
more capable LLMs, it is critical to ask if the risk of data poisoning will be
naturally mitigated by scale, or if it is an increasing threat. We consider
three threat models by which data poisoning can occur: malicious fine-tuning,
imperfect data curation, and intentional data contamination. Our experiments
evaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72
billion parameters on three datasets which speak to each of our threat models.
We find that larger LLMs are increasingly vulnerable, learning harmful behavior
-- including sleeper agent behavior -- significantly more quickly than smaller
LLMs with even minimal data poisoning. These results underscore the need for
robust safeguards against data poisoning in larger LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achieving More with Less: A Tensor-Optimization-Powered Ensemble Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghui Yuan, Weijin Jiang, Zhe Cao, Fangyuan Xie, Rong Wang, Feiping Nie, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble learning is a method that leverages weak learners to produce a
strong learner. However, obtaining a large number of base learners requires
substantial time and computational resources. Therefore, it is meaningful to
study how to achieve the performance typically obtained with many base learners
using only a few. We argue that to achieve this, it is essential to enhance
both classification performance and generalization ability during the ensemble
process. To increase model accuracy, each weak base learner needs to be more
efficiently integrated. It is observed that different base learners exhibit
varying levels of accuracy in predicting different classes. To capitalize on
this, we introduce confidence tensors $\tilde{\mathbf{\Theta}}$ and
$\tilde{\mathbf{\Theta}}_{rst}$ signifies that the $t$-th base classifier
assigns the sample to class $r$ while it actually belongs to class $s$. To the
best of our knowledge, this is the first time an evaluation of the performance
of base classifiers across different classes has been proposed. The proposed
confidence tensor compensates for the strengths and weaknesses of each base
classifier in different classes, enabling the method to achieve superior
results with a smaller number of base learners. To enhance generalization
performance, we design a smooth and convex objective function that leverages
the concept of margin, making the strong learner more discriminative.
Furthermore, it is proved that in gradient matrix of the loss function, the sum
of each column's elements is zero, allowing us to solve a constrained
optimization problem using gradient-based methods. We then compare our
algorithm with random forests of ten times the size and other classical methods
across numerous datasets, demonstrating the superiority of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghui Yuan, Chusheng Zeng, Fangyuan Xie, Zhe Cao, Rong Wang, Feiping Nie, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering is a fundamental task in machine learning and data science, and
similarity graph-based clustering is an important approach within this domain.
Doubly stochastic symmetric similarity graphs provide numerous benefits for
clustering problems and downstream tasks, yet learning such graphs remains a
significant challenge. Marcus theorem states that a strictly positive symmetric
matrix can be transformed into a doubly stochastic symmetric matrix by diagonal
matrices. However, in clustering, learning sparse matrices is crucial for
computational efficiency. We extend Marcus theorem by proposing the Marcus
mapping, which indicates that certain sparse matrices can also be transformed
into doubly stochastic symmetric matrices via diagonal matrices. Additionally,
we introduce rank constraints into the clustering problem and propose the
Doubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus
Mapping (ANCMM). This ensures that the learned graph naturally divides into the
desired number of clusters. We validate the effectiveness of our algorithm
through extensive comparisons with state-of-the-art algorithms. Finally, we
explore the relationship between the Marcus mapping and optimal transport. We
prove that the Marcus mapping solves a specific type of optimal transport
problem and demonstrate that solving this problem through Marcus mapping is
more efficient than directly applying optimal transport methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Need for a Big World Simulator: A Scientific Challenge for Continual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Kumar, Hong Jun Jeon, Alex Lewandowski, Benjamin Van Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The "small agent, big world" frame offers a conceptual view that motivates
the need for continual learning. The idea is that a small agent operating in a
much bigger world cannot store all information that the world has to offer. To
perform well, the agent must be carefully designed to ingest, retain, and eject
the right information. To enable the development of performant continual
learning agents, a number of synthetic environments have been proposed.
However, these benchmarks suffer from limitations, including unnatural
distribution shifts and a lack of fidelity to the "small agent, big world"
framing. This paper aims to formalize two desiderata for the design of future
simulated environments. These two criteria aim to reflect the objectives and
complexity of continual learning in practical settings while enabling rapid
prototyping of algorithms on a smaller scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Finding the Frame Workshop at RLC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy
  Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Wang, Duanyu Feng, Yongfu Dai, Zhengyu Chen, Jimin Huang, Sophia Ananiadou, Qianqian Xie, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data serves as the fundamental foundation for advancing deep learning,
particularly tabular data presented in a structured format, which is highly
conducive to modeling. However, even in the era of LLM, obtaining tabular data
from sensitive domains remains a challenge due to privacy or copyright
concerns. Hence, exploring how to effectively use models like LLMs to generate
realistic and privacy-preserving synthetic tabular data is urgent. In this
paper, we take a step forward to explore LLMs for tabular data synthesis and
privacy protection, by introducing a new framework HARMONIC for tabular data
generation and evaluation. In the tabular data generation of our framework,
unlike previous small-scale LLM-based methods that rely on continued
pre-training, we explore the larger-scale LLMs with fine-tuning to generate
tabular data and enhance privacy. Based on idea of the k-nearest neighbors
algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to
discover inter-row relationships. Then, with fine-tuning, LLMs are trained to
remember the format and connections of the data rather than the data itself,
which reduces the risk of privacy leakage. In the evaluation part of our
framework, we develop specific privacy risk metrics DLT for LLM synthetic data
generation, as well as performance evaluation metrics LLE for downstream LLM
tasks. Our experiments find that this tabular data generation framework
achieves equivalent performance to existing methods with better privacy, which
also demonstrates our evaluation framework for the effectiveness of synthetic
data and privacy risks in LLM scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Metric Driven Approach to Mixed Precision Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitchelle Rasquinha, Gil Tabak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep learning methodologies have developed, it has been generally agreed
that increasing neural network size improves model quality. However, this is at
the expense of memory and compute requirements, which also need to be
increased. Various efficiency techniques have been proposed to rein in hardware
costs, one being the use of low precision numerics. Recent accelerators have
introduced several different 8-bit data types to help accommodate DNNs in terms
of numerics. In this paper, we identify a metric driven methodology to aid in
the choice of numerics. We demonstrate how such a methodology can help scale
training of a language representation model. The technique can be generalized
to other model architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compromising Embodied Agents with Contextual Backdoor Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishan Liu, Yuguang Zhou, Xianglong Liu, Tianyuan Zhang, Siyuan Liang, Jiakai Wang, Yanjun Pu, Tianlin Li, Junqi Zhang, Wenbo Zhou, Qing Guo, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have transformed the development of embodied
intelligence. By providing a few contextual demonstrations, developers can
utilize the extensive internal knowledge of LLMs to effortlessly translate
complex tasks described in abstract language into sequences of code snippets,
which will serve as the execution logic for embodied agents. However, this
paper uncovers a significant backdoor security threat within this process and
introduces a novel method called \method{}. By poisoning just a few contextual
demonstrations, attackers can covertly compromise the contextual environment of
a black-box LLM, prompting it to generate programs with context-dependent
defects. These programs appear logically sound but contain defects that can
activate and induce unintended behaviors when the operational agent encounters
specific triggers in its interactive environment. To compromise the LLM's
contextual environment, we employ adversarial in-context generation to optimize
poisoned demonstrations, where an LLM judge evaluates these poisoned prompts,
reporting to an additional LLM that iteratively optimizes the demonstration in
a two-player adversarial game using chain-of-thought reasoning. To enable
context-dependent behaviors in downstream agents, we implement a dual-modality
activation strategy that controls both the generation and execution of program
defects through textual and visual triggers. We expand the scope of our attack
by developing five program defect modes that compromise key aspects of
confidentiality, integrity, and availability in embodied agents. To validate
the effectiveness of our approach, we conducted extensive experiments across
various tasks, including robot planning, robot manipulation, and compositional
visual reasoning. Additionally, we demonstrate the potential impact of our
approach by successfully attacking real-world autonomous driving systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing EEG-Based Gaze Prediction Using Depthwise Separable
  Convolution and Enhanced Pre-Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew L Key, Tural Mehtiyev, Xiaodong Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of EEG-based gaze prediction, the application of deep learning
to interpret complex neural data poses significant challenges. This study
evaluates the effectiveness of pre-processing techniques and the effect of
additional depthwise separable convolution on EEG vision transformers (ViTs) in
a pretrained model architecture. We introduce a novel method, the EEG Deeper
Clustered Vision Transformer (EEG-DCViT), which combines depthwise separable
convolutional neural networks (CNNs) with vision transformers, enriched by a
pre-processing strategy involving data clustering. The new approach
demonstrates superior performance, establishing a new benchmark with a Root
Mean Square Error (RMSE) of 51.6 mm. This achievement underscores the impact of
pre-processing and model refinement in enhancing EEG-based applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effect of Kernel Size on CNN-Vision-<span class="highlight-title">Transformer</span>-Based Gaze Prediction
  Using Electroencephalography Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhui Qiu, Bugao Liang, Matthew L Key
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an algorithm of gaze prediction from
Electroencephalography (EEG) data. EEG-based gaze prediction is a new research
topic that can serve as an alternative to traditional video-based eye-tracking.
Compared to the existing state-of-the-art (SOTA) method, we improved the root
mean-squared-error of EEG-based gaze prediction to 53.06 millimeters, while
reducing the training time to less than 33% of its original duration. Our
source code can be found at https://github.com/AmCh-Q/CSCI6907Project
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Human-Computer Interaction (HCII 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Serve As Time Series Anomaly Detectors? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manqing Dong, Hao Huang, Longbing Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An emerging topic in large language models (LLMs) is their application to
time series forecasting, characterizing mainstream and patternable
characteristics of time series. A relevant but rarely explored and more
challenging question is whether LLMs can detect and explain time series
anomalies, a critical task across various real-world applications. In this
paper, we investigate the capabilities of LLMs, specifically GPT-4 and LLaMA3,
in detecting and explaining anomalies in time series. Our studies reveal that:
1) LLMs cannot be directly used for time series anomaly detection. 2) By
designing prompt strategies such as in-context learning and chain-of-thought
prompting, GPT-4 can detect time series anomalies with results competitive to
baseline methods. 3) We propose a synthesized dataset to automatically generate
time series anomalies with corresponding explanations. By applying instruction
fine-tuning on this dataset, LLaMA3 demonstrates improved performance in time
series anomaly detection tasks. In summary, our exploration shows the promising
potential of LLMs as time series anomaly detectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating HCI <span class="highlight-title">Dataset</span>s in Project-Based Machine Learning Courses: A
  College-Level <span class="highlight-title">Review</span> and Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Qu, Matthew Key, Eric Luo, Chuhui Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the integration of real-world machine learning (ML)
projects using human-computer interfaces (HCI) datasets in college-level
courses to enhance both teaching and learning experiences. Employing a
comprehensive literature review, course websites analysis, and a detailed case
study, the research identifies best practices for incorporating HCI datasets
into project-based ML education. Key f indings demonstrate increased student
engagement, motivation, and skill development through hands-on projects, while
instructors benefit from effective tools for teaching complex concepts. The
study also addresses challenges such as data complexity and resource
allocation, offering recommendations for future improvements. These insights
provide a valuable framework for educators aiming to bridge the gap between
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Foundation Models in Remote Sensing: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Lu, Junlin Guo, James R Zimmer-Dauphinee, Jordan M Nieusma, Xiao Wang, Parker VanValkenburgh, Steven A Wernke, Yuankai Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) technologies have profoundly transformed the
field of remote sensing, revolutionizing data collection, processing, and
analysis. Traditionally reliant on manual interpretation and task-specific
models, remote sensing has been significantly enhanced by the advent of
foundation models--large-scale, pre-trained AI models capable of performing a
wide array of tasks with unprecedented accuracy and efficiency. This paper
provides a comprehensive survey of foundation models in the remote sensing
domain, covering models released between June 2021 and June 2024. We categorize
these models based on their applications in computer vision and domain-specific
tasks, offering insights into their architectures, pre-training datasets, and
methodologies. Through detailed performance comparisons, we highlight emerging
trends and the significant advancements achieved by these foundation models.
Additionally, we discuss the technical challenges, practical implications, and
future research directions, addressing the need for high-quality data,
computational resources, and improved model generalization. Our research also
finds that pre-training methods, particularly self-supervised learning
techniques like contrastive learning and masked autoencoders, significantly
enhance the performance and robustness of foundation models in remote sensing
tasks such as scene classification, object detection, and other applications.
This survey aims to serve as a resource for researchers and practitioners by
providing a panorama of advances and promising pathways for continued
development and application of foundation models in remote sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When does the mean network capture the topology of a sample of networks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        François G Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The notion of Fr\'echet mean (also known as "barycenter") network is the
workhorse of most machine learning algorithms that require the estimation of a
"location" parameter to analyse network-valued data. In this context, it is
critical that the network barycenter inherits the topological structure of the
networks in the training dataset. The metric - which measures the proximity
between networks - controls the structural properties of the barycenter. This
work is significant because it provides for the first time analytical estimates
of the sample Fr\'echet mean for the stochastic blockmodel, which is at the
cutting edge of rigorous probabilistic analysis of random networks. We show
that the mean network computed with the Hamming distance is unable to capture
the topology of the networks in the training sample, whereas the mean network
computed using the effective resistance distance recovers the correct
partitions and associated edge density. From a practical standpoint, our work
informs the choice of metrics in the context where the sample Fr\'echet mean
network is used to characterise the topology of networks for network-valued
machine learning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Generalization of Preference Learning with DPO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities but
often struggle to align with human preferences, leading to harmful or
undesirable outputs. Preference learning, which trains models to distinguish
between preferred and non-preferred responses based on human feedback, has
become a crucial component for ensuring that LLMs align with human values.
Despite the widespread adoption in real-world systems, a thorough theoretical
understanding of the generalization guarantees for these models remain lacking.
This paper bridges that gap by introducing a new theoretical framework to
analyze the generalization guarantees of models trained with direct preference
optimization (DPO). While existing generalization theory often focuses on
overparameterized models achieving near-optimal loss or models independent of
the training process, our framework rigorously assesses how well models
generalize after a finite number of gradient steps, reflecting real-world LLM
training practices. By analyzing the reward margin associated with each sample
and its trajectory throughout training, we can effectively bound the
generalization error. We derive learning guarantees showing that, under
specific conditions, models trained with DPO can correctly discern preferred
responses on unseen data with high probability. These insights are empirically
validated on contemporary LLMs, underscoring the practical relevance of our
theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Surrogate Model for Accelerating the Design of Electric
  Vehicle Battery Enclosures for Crash Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadab Anwar Shaikh, Harish Cherukuri, Kranthi Balusu, Ram Devanathan, Ayoub Soulami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a probabilistic surrogate model for the accelerated
design of electric vehicle battery enclosures with a focus on crash
performance. The study integrates high-throughput finite element simulations
and Gaussian Process Regression to develop a surrogate model that predicts
crash parameters with high accuracy while providing uncertainty estimates. The
model was trained using data generated from thermoforming and crash simulations
over a range of material and process parameters. Validation against new
simulation data demonstrated the model's predictive accuracy with mean absolute
percentage errors within 8.08% for all output variables. Additionally, a Monte
Carlo uncertainty propagation study revealed the impact of input variability on
outputs. The results highlight the efficacy of the Gaussian Process Regression
model in capturing complex relationships within the dataset, offering a robust
and efficient tool for the design optimization of composite battery enclosures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EEGMobile: Enhancing Speed and Accuracy in EEG-Based Gaze Prediction
  with Advanced Mobile Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Liang, Andrews Damoah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) analysis is an important domain in the realm of
Brain-Computer Interface (BCI) research. To ensure BCI devices are capable of
providing practical applications in the real world, brain signal processing
techniques must be fast, accurate, and resource-conscious to deliver
low-latency neural analytics. This study presents a model that leverages a
pre-trained MobileViT alongside Knowledge Distillation (KD) for EEG regression
tasks. Our results showcase that this model is capable of performing at a level
comparable (only 3% lower) to the previous State-Of-The-Art (SOTA) on the
EEGEyeNet Absolute Position Task while being 33% faster and 60% smaller. Our
research presents a cost-effective model applicable to resource-constrained
devices and contributes to expanding future research on lightweight,
mobile-friendly models for EEG regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted HCI International 2024 - Late Breaking Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spacecraft inertial parameters estimation using time series clustering
  and reinforcement learning <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Platanitis, Miguel Arana-Catania, Leonardo Capicchiano, Saurabh Upadhyay, Leonard Felicetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a machine learning approach to estimate the inertial
parameters of a spacecraft in cases when those change during operations, e.g.
multiple deployments of payloads, unfolding of appendages and booms, propellant
consumption as well as during in-orbit servicing and active debris removal
operations. The machine learning approach uses time series clustering together
with an optimised actuation sequence generated by reinforcement learning to
facilitate distinguishing among different inertial parameter sets. The
performance of the proposed strategy is assessed against the case of a
multi-satellite deployment system showing that the algorithm is resilient
towards common disturbances in such kinds of operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 1 table. To be presented in ESA - AI for Space
  (SPAICE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple Perturbations Subvert Ethereum Phishing Transactions Detection:
  An Empirical Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahod Alghureid, David Mohaisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the vulnerability of machine learning models,
specifically Random Forest, Decision Tree, and K-Nearest Neighbors, to very
simple single-feature adversarial attacks in the context of Ethereum fraudulent
transaction detection. Through comprehensive experimentation, we investigate
the impact of various adversarial attack strategies on model performance
metrics, such as accuracy, precision, recall, and F1-score. Our findings,
highlighting how prone those techniques are to simple attacks, are alarming,
and the inconsistency in the attacks' effect on different algorithms promises
ways for attack mitigation. We examine the effectiveness of different
mitigation strategies, including adversarial training and enhanced feature
selection, in enhancing model robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 5 tables, accepted for presentation at WISA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid diffusion models: combining supervised and generative <span class="highlight-title">pretrain</span>ing
  for label-efficient fine-tuning of segmentation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Sauvalle, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are considering in this paper the task of label-efficient fine-tuning of
segmentation models: We assume that a large labeled dataset is available and
allows to train an accurate segmentation model in one domain, and that we have
to adapt this model on a related domain where only a few samples are available.
We observe that this adaptation can be done using two distinct methods: The
first method, supervised pretraining, is simply to take the model trained on
the first domain using classical supervised learning, and fine-tune it on the
second domain with the available labeled samples. The second method is to
perform self-supervised pretraining on the first domain using a generic pretext
task in order to get high-quality representations which can then be used to
train a model on the second domain in a label-efficient way. We propose in this
paper to fuse these two approaches by introducing a new pretext task, which is
to perform simultaneously image denoising and mask prediction on the first
domain. We motivate this choice by showing that in the same way that an image
denoiser conditioned on the noise level can be considered as a generative model
for the unlabeled image distribution using the theory of diffusion models, a
model trained using this new pretext task can be considered as a generative
model for the joint distribution of images and segmentation masks under the
assumption that the mapping from images to segmentation masks is deterministic.
We then empirically show on several datasets that fine-tuning a model
pretrained using this approach leads to better results than fine-tuning a
similar model trained using either supervised or unsupervised pretraining only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Conditional Transport on Probabilistic Graphs for
  Interpretable Counterfactual Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agathe Fernandes Machado, Arthur Charpentier, Ewen Gallic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we link two existing approaches to derive counterfactuals:
adaptations based on a causal graph, as suggested in Ple\v{c}ko and Meinshausen
(2020) and optimal transport, as in De Lara et al. (2024). We extend "Knothe's
rearrangement" Bonnotte (2013) and "triangular transport" Zech and Marzouk
(2022a) to probabilistic graphical models, and use this counterfactual
approach, referred to as sequential transport, to discuss individual fairness.
After establishing the theoretical foundations of the proposed method, we
demonstrate its application through numerical experiments on both synthetic and
real datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Scores of Classifiers, Calibration is not Enough 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agathe Fernandes Machado, Arthur Charpentier, Emmanuel Flachaire, Ewen Gallic, François Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In binary classification tasks, accurate representation of probabilistic
predictions is essential for various real-world applications such as predicting
payment defaults or assessing medical risks. The model must then be
well-calibrated to ensure alignment between predicted probabilities and actual
outcomes. However, when score heterogeneity deviates from the underlying data
probability distribution, traditional calibration metrics lose reliability,
failing to align score distribution with actual probabilities. In this study,
we highlight approaches that prioritize optimizing the alignment between
predicted scores and true probability distributions over minimizing traditional
performance or calibration metrics. When employing tree-based models such as
Random Forest and XGBoost, our analysis emphasizes the flexibility these models
offer in tuning hyperparameters to minimize the Kullback-Leibler (KL)
divergence between predicted and true distributions. Through extensive
empirical analysis across 10 UCI datasets and simulations, we demonstrate that
optimizing tree-based models based on KL divergence yields superior alignment
between predicted scores and actual probabilities without significant
performance loss. In real-world scenarios, the reference probability is
determined a priori as a Beta distribution estimated through maximum
likelihood. Conversely, minimizing traditional calibration metrics may lead to
suboptimal results, characterized by notable performance declines and inferior
KL values. Our findings reveal limitations in traditional calibration metrics,
which could undermine the reliability of predictive models for critical
decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logistic Regression makes small LLMs strong and explainable
  "tens-of-shot" classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcus Buckmann, Edward Hill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For simple classification tasks, we show that users can benefit from the
advantages of using small, local, generative language models instead of large
commercial models without a trade-off in performance or introducing extra
labelling costs. These advantages, including those around privacy,
availability, cost, and explainability, are important both in commercial
applications and in the broader democratisation of AI. Through experiments on
17 sentence classification tasks (2-4 classes), we show that penalised logistic
regression on the embeddings from a small LLM equals (and usually betters) the
performance of a large LLM in the "tens-of-shot" regime. This requires no more
labelled instances than are needed to validate the performance of the large
LLM. Finally, we extract stable and sensible explanations for classification
decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A TVD neural network closure and application to turbulent combustion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seung Won Suh, Jonathan F MacArt, Luke N Olson, Jonathan B Freund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trained neural networks (NN) have attractive features for closing governing
equations, but in the absence of additional constraints, they can stray from
physical reality. A NN formulation is introduced to preclude spurious
oscillations that violate solution boundedness or positivity. It is embedded in
the discretized equations as a machine learning closure and strictly
constrained, inspired by total variation diminishing (TVD) methods for
hyperbolic conservation laws. The constraint is exactly enforced during
gradient-descent training by rescaling the NN parameters, which maps them onto
an explicit feasible set. Demonstrations show that the constrained NN closure
model usefully recovers linear and nonlinear hyperbolic phenomena and
anti-diffusion while enforcing the non-oscillatory property. Finally, the model
is applied to subgrid-scale (SGS) modeling of a turbulent reacting flow, for
which it suppresses spurious oscillations in scalar fields that otherwise
violate the solution boundedness. It outperforms a simple penalization of
oscillations in the loss function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Aided Compilation for Tensor Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Hong, Sahil Bhatia, Altan Haan, Shengjun Kris Dong, Dima Nikiforov, Alvin Cheung, Yakun Sophia Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hardware accelerators, in particular accelerators for tensor processing, have
many potential application domains. However, they currently lack the software
infrastructure to support the majority of domains outside of deep learning.
Furthermore, a compiler that can easily be updated to reflect changes at both
application and hardware levels would enable more agile development and design
space exploration of accelerators, allowing hardware designers to realize
closer-to-optimal performance. In this work, we discuss how large language
models (LLMs) could be leveraged to build such a compiler. Specifically, we
demonstrate the ability of GPT-4 to achieve high pass rates in translating code
to the Gemmini accelerator, and prototype a technique for decomposing
translation into smaller, more LLM-friendly steps. Additionally, we propose a
2-phase workflow for utilizing LLMs to generate hardware-optimized code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 page workshop paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Clustering via Distribution Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanfang Dong, Zijie Tan, Chenqiu Zhao, Anup Basu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distribution learning finds probability density functions from a set of data
samples, whereas clustering aims to group similar data points to form clusters.
Although there are deep clustering methods that employ distribution learning
methods, past work still lacks theoretical analysis regarding the relationship
between clustering and distribution learning. Thus, in this work, we provide a
theoretical analysis to guide the optimization of clustering via distribution
learning. To achieve better results, we embed deep clustering guided by a
theoretical analysis. Furthermore, the distribution learning method cannot
always be directly applied to data. To overcome this issue, we introduce a
clustering-oriented distribution learning method called Monte-Carlo
Marginalization for Clustering. We integrate Monte-Carlo Marginalization for
Clustering into Deep Clustering, resulting in Deep Clustering via Distribution
Learning (DCDL). Eventually, the proposed DCDL achieves promising results
compared to state-of-the-art methods on popular datasets. Considering a
clustering task, the new distribution learning method outperforms previous
methods as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Diverse Information for Coordinated Action: Stochastic Bandit
  Algorithms for Heterogeneous Agents <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucia Gordon, Esther Rolf, Milind Tambe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic multi-agent multi-armed bandits typically assume that the rewards
from each arm follow a fixed distribution, regardless of which agent pulls the
arm. However, in many real-world settings, rewards can depend on the
sensitivity of each agent to their environment. In medical screening, disease
detection rates can vary by test type; in preference matching, rewards can
depend on user preferences; and in environmental sensing, observation quality
can vary across sensors. Since past work does not specify how to allocate
agents of heterogeneous but known sensitivity of these types in a stochastic
bandit setting, we introduce a UCB-style algorithm, Min-Width, which aggregates
information from diverse agents. In doing so, we address the joint challenges
of (i) aggregating the rewards, which follow different distributions for each
agent-arm pair, and (ii) coordinating the assignments of agents to arms.
Min-Width facilitates efficient collaboration among heterogeneous agents,
exploiting the known structure in the agents' reward functions to weight their
rewards accordingly. We analyze the regret of Min-Width and conduct
pseudo-synthetic and fully synthetic experiments to study the performance of
different levels of information sharing. Our results confirm that the gains to
modeling agent heterogeneity tend to be greater when the sensitivities are more
varied across agents, while combining more information does not always improve
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, to be published in ECAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Set2Seq <span class="highlight-title">Transformer</span>: Learning Permutation Aware Set Representations of
  Artistic Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athanasios Efthymiou, Stevan Rudinac, Monika Kackovic, Nachoem Wijnberg, Marcel Worring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Set2Seq Transformer, a novel sequential multiple instance
architecture, that learns to rank permutation aware set representations of
sequences. First, we illustrate that learning temporal position-aware
representations of discrete timesteps can greatly improve static visual
multiple instance learning methods that do not regard temporality and
concentrate almost exclusively on visual content analysis. We further
demonstrate the significant advantages of end-to-end sequential multiple
instance learning, integrating visual content and temporal information in a
multimodal manner. As application we focus on fine art analysis related tasks.
To that end, we show that our Set2Seq Transformer can leverage visual set and
temporal position-aware representations for modelling visual artists' oeuvres
for predicting artistic success. Finally, through extensive quantitative and
qualitative evaluation using a novel dataset, WikiArt-Seq2Rank, and a visual
learning-to-rank downstream task, we show that our Set2Seq Transformer captures
essential temporal information improving the performance of strong static and
sequential multiple instance learning methods for predicting artistic success.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attacks and Defenses for Generative Diffusion Models: A Comprehensive
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vu Tuan Truong, Luan Ba Dang, Long Bao Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have achieved state-of-the-art performance on various
generative tasks such as image synthesis, text-to-image, and text-guided
image-to-image generation. However, the more powerful the DMs, the more harmful
they potentially are. Recent studies have shown that DMs are prone to a wide
range of attacks, including adversarial attacks, membership inference, backdoor
injection, and various multi-modal threats. Since numerous pre-trained DMs are
published widely on the Internet, potential threats from these attacks are
especially detrimental to the society, making DM-related security a worth
investigating topic. Therefore, in this paper, we conduct a comprehensive
survey on the security aspect of DMs, focusing on various attack and defense
methods for DMs. First, we present crucial knowledge of DMs with five main
types of DMs, including denoising diffusion probabilistic models, denoising
diffusion implicit models, noise conditioned score networks, stochastic
differential equations, and multi-modal conditional DMs. We further survey a
variety of recent studies investigating different types of attacks that exploit
the vulnerabilities of DMs. Then, we thoroughly review potential
countermeasures to mitigate each of the presented threats. Finally, we discuss
open challenges of DM-related security and envision certain research directions
for this topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting
  Algorithms <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Roque, Carlos Soares, Luís Torgo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Robustness of Hierarchically Organized Time Series (RHiOTS)
framework, designed to assess the robustness of hierarchical time series
forecasting models and algorithms on real-world datasets. Hierarchical time
series, where lower-level forecasts must sum to upper-level ones, are prevalent
in various contexts, such as retail sales across countries. Current empirical
evaluations of forecasting methods are often limited to a small set of
benchmark datasets, offering a narrow view of algorithm behavior. RHiOTS
addresses this gap by systematically altering existing datasets and modifying
the characteristics of individual series and their interrelations. It uses a
set of parameterizable transformations to simulate those changes in the data
distribution. Additionally, RHiOTS incorporates an innovative visualization
component, turning complex, multidimensional robustness evaluation results into
intuitive, easily interpretable visuals. This approach allows an in-depth
analysis of algorithm and model behavior under diverse conditions. We
illustrate the use of RHiOTS by analyzing the predictive performance of several
algorithms. Our findings show that traditional statistical methods are more
robust than state-of-the-art deep learning algorithms, except when the
transformation effect is highly disruptive. Furthermore, we found no
significant differences in the robustness of the algorithms when applying
specific reconciliation methods, such as MinT. RHiOTS provides researchers with
a comprehensive tool for understanding the nuanced behavior of forecasting
algorithms, offering a more reliable basis for selecting the most appropriate
method for a given problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining (KDD '24), August 25--29, 2024, Barcelona, Spain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for
  <span class="highlight-title">Transformer</span> Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyush Dhingra, Janardhan Rao Doppa, Partha Pratim Pande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have revolutionized deep learning and generative modeling to
enable unprecedented advancements in natural language processing tasks and
beyond. However, designing hardware accelerators for executing transformer
models is challenging due to the wide variety of computing kernels involved in
the transformer architecture. Existing accelerators are either inadequate to
accelerate end-to-end transformer models or suffer notable thermal limitations.
In this paper, we propose the design of a three-dimensional heterogeneous
architecture referred to as HeTraX specifically optimized to accelerate
end-to-end transformer models. HeTraX employs hardware resources aligned with
the computational kernels of transformers and optimizes both performance and
energy. Experimental results show that HeTraX outperforms existing
state-of-the-art by up to 5.6x in speedup and improves EDP by 14.5x while
ensuring thermally feasibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ACM/IEEE International Symposium on Low Power
  Electronics and Design (ISLPED-24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Non-negative VAE:the Generalized Gamma Belief Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Duan, Tiansheng Wen, Muyao Wang, Bo Chen, Mingyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The gamma belief network (GBN), often regarded as a deep topic model, has
demonstrated its potential for uncovering multi-layer interpretable latent
representations in text data. Its notable capability to acquire interpretable
latent factors is partially attributed to sparse and non-negative
gamma-distributed latent variables. However, the existing GBN and its
variations are constrained by the linear generative model, thereby limiting
their expressiveness and applicability. To address this limitation, we
introduce the generalized gamma belief network (Generalized GBN) in this paper,
which extends the original linear generative model to a more expressive
non-linear generative model. Since the parameters of the Generalized GBN no
longer possess an analytic conditional posterior, we further propose an
upward-downward Weibull inference network to approximate the posterior
distribution of the latent variables. The parameters of both the generative
model and the inference network are jointly trained within the variational
inference framework. Finally, we conduct comprehensive experiments on both
expressivity and disentangled representation learning tasks to evaluate the
performance of the Generalized GBN against state-of-the-art Gaussian
variational autoencoders serving as baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditioning of Banach Space Valued Gaussian Random Variables: An
  Approximation Approach Based on Martingales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03453v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03453v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ingo Steinwart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we investigate the conditional distributions of two Banach
space valued, jointly Gaussian random variables. We show that these conditional
distributions are again Gaussian and that their means and covariances are
determined by a general finite dimensional approximation scheme based upon a
martingale approach. In particular, it turns out that the covariance operators
occurring in this scheme converge with respect to the nuclear norm and that the
conditional probabilities converge weakly. Moreover, we discuss in detail, how
our approximation scheme can be implemented in several classes of important
Banach spaces such as (reproducing kernel) Hilbert spaces and spaces of
continuous functions. As an example, we then apply our general results to the
case of Gaussian processes with continuous paths conditioned to partial but
infinite observations of their paths. Here we show that conditioning on
sufficiently rich, increasing sets of finitely many observations leads to
consistent approximations, that is, both the mean and covariance functions
converge uniformly and the conditional probabilities converge weakly. Moreover,
we discuss how these results improve our understanding of the popular Gaussian
processes for machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages plus 22 pages of supplemental material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SIGMA: Similarity-based Efficient Global Aggregation for Heterophilous
  Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09958v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09958v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Liu, Ningyi Liao, Siqiang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) realize great success in graph learning but
suffer from performance loss when meeting heterophily, i.e. neighboring nodes
are dissimilar, due to their local and uniform aggregation. Existing attempts
of heterophilous GNNs incorporate long-range or global aggregations to
distinguish nodes in the graph. However, these aggregations usually require
iteratively maintaining and updating full-graph information, which limits their
efficiency when applying to large-scale graphs. In this paper, we propose
SIGMA, an efficient global heterophilous GNN aggregation integrating the
structural similarity measurement SimRank. Our theoretical analysis illustrates
that SIGMA inherently captures distant global similarity even under
heterophily, that conventional approaches can only achieve after iterative
aggregations. Furthermore, it enjoys efficient one-time computation with a
complexity only linear to the node set size $\mathcal{O}(n)$. Comprehensive
evaluation demonstrates that SIGMA achieves state-of-the-art performance with
superior aggregation and overall efficiency. Notably, it obtains 5$\times$
acceleration on the large-scale heterophily dataset \emph{pokec} with over 30
million edges compared to the best baseline aggregation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential
  Recommenders <span class="chip">CIKM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02354v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02354v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danil Gusak, Gleb Mezentsev, Ivan Oseledets, Evgeny Frolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalability is a major challenge in modern recommender systems. In sequential
recommendations, full Cross-Entropy (CE) loss achieves state-of-the-art
recommendation quality but consumes excessive GPU memory with large item
catalogs, limiting its practicality. Using a GPU-efficient locality-sensitive
hashing-like algorithm for approximating large tensor of logits, this paper
introduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly
reduces memory consumption while allowing one to enjoy the state-of-the-art
performance of full CE loss. Experimental results on various datasets show that
RECE cuts training peak memory usage by up to 12 times compared to existing
methods while retaining or exceeding performance metrics of CE loss. The
approach also opens up new possibilities for large-scale applications in other
domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted for CIKM'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infusing Environmental Captions for Long-Form Video Language Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyogun Lee, Soyeon Hong, Mujeen Sung, Jinwoo Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we tackle the problem of long-form video-language grounding
(VLG). Given a long-form video and a natural language query, a model should
temporally localize the precise moment that answers the query. Humans can
easily solve VLG tasks, even with arbitrarily long videos, by discarding
irrelevant moments using extensive and robust knowledge gained from experience.
Unlike humans, existing VLG methods are prone to fall into superficial cues
learned from small-scale datasets, even when they are within irrelevant frames.
To overcome this challenge, we propose EI-VLG, a VLG method that leverages
richer textual information provided by a Multi-modal Large Language Model
(MLLM) as a proxy for human experiences, helping to effectively exclude
irrelevant frames. We validate the effectiveness of the proposed method via
extensive experiments on a challenging EgoNLQ benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Generalizable Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxu Wang, Qiang Zhang, Jingkai Sun, Jiahang Cao, Gang Han, Wen Zhao, Weining Zhang, Yecheng Shao, Yijie Guo, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An excellent representation is crucial for reinforcement learning (RL)
performance, especially in vision-based reinforcement learning tasks. The
quality of the environment representation directly influences the achievement
of the learning task. Previous vision-based RL typically uses explicit or
implicit ways to represent environments, such as images, points, voxels, and
neural radiance fields. However, these representations contain several
drawbacks. They cannot either describe complex local geometries or generalize
well to unseen scenes, or require precise foreground masks. Moreover, these
implicit neural representations are akin to a ``black box", significantly
hindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit
scene representation and differentiable rendering nature, is considered a
revolutionary change for reconstruction and representation methods. In this
paper, we propose a novel Generalizable Gaussian Splatting framework to be the
representation of RL tasks, called GSRL. Through validation in the RoboMimic
environment, our method achieves better results than other baselines in
multiple tasks, improving the performance by 10%, 44%, and 15% compared with
baselines on the hardest task. This work is the first attempt to leverage
generalizable 3DGS as a representation for RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoMa: Efficient Early-Fusion <span class="highlight-title">Pre-train</span>ing with Mixture of Modality-Aware
  Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, Armen Aghajanyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)
architecture designed for pre-training mixed-modal, early-fusion language
models. MoMa processes images and text in arbitrary sequences by dividing
expert modules into modality-specific groups. These groups exclusively process
designated tokens while employing learned routing within each group to maintain
semantically informed adaptivity. Our empirical results reveal substantial
pre-training efficiency gains through this modality-specific parameter
allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,
featuring 4 text experts and 4 image experts, achieves impressive FLOPs
savings: 3.7x overall, with 2.6x for text and 5.2x for image processing
compared to a compute-equivalent dense baseline, measured by pre-training loss.
This outperforms the standard expert-choice MoE with 8 mixed-modal experts,
which achieves 3x overall FLOPs savings (3x for text, 2.8x for image).
Combining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs
savings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination
hurts performance in causal inference due to increased sensitivity to router
accuracy. These results demonstrate MoMa's potential to significantly advance
the efficiency of mixed-modal, early-fusion language model pre-training, paving
the way for more resource-efficient and capable multimodal AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2 -> update related work section</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximal Volume Matrix Cross Approximation for Image Compression and
  Least Squares Solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Allen, Ming-Jun Lai, Zhaiming Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the classic matrix cross approximation based on the maximal volume
submatrices. Our main results consist of an improvement of the classic estimate
for matrix cross approximation and a greedy approach for finding the maximal
volume submatrices. More precisely, we present a new proof of the classic
estimate of the inequality with an improved constant. Also, we present a family
of greedy maximal volume algorithms to improve the computational efficiency of
matrix cross approximation. The proposed algorithms are shown to have
theoretical guarantees of convergence. Finally, we present two applications:
image compression and the least squares approximation of continuous functions.
Our numerical results at the end of the paper demonstrate the effective
performance of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness Assessment of a Runway Object Classifier for Safe Aircraft
  Taxiing <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00035v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00035v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhak Elboher, Raya Elsaleh, Omri Isac, Mélanie Ducoffe, Audrey Galametz, Guillaume Povéda, Ryma Boumazouza, Noémie Cohen, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep neural networks (DNNs) are becoming the prominent solution for many
computational problems, the aviation industry seeks to explore their potential
in alleviating pilot workload and in improving operational safety. However, the
use of DNNs in this type of safety-critical applications requires a thorough
certification process. This need can be addressed through formal verification,
which provides rigorous assurances -- e.g.,~by proving the absence of certain
mispredictions. In this case-study paper, we demonstrate this process using an
image-classifier DNN currently under development at Airbus and intended for use
during the aircraft taxiing phase. We use formal methods to assess this DNN's
robustness to three common image perturbation types: noise, brightness and
contrast, and some of their combinations. This process entails multiple
invocations of the underlying verifier, which might be computationally
expensive; and we therefore propose a method that leverages the monotonicity of
these robustness properties, as well as the results of past verification
queries, in order to reduce the overall number of verification queries required
by nearly 60%. Our results provide an indication of the level of robustness
achieved by the DNN classifier under study, and indicate that it is
considerably more vulnerable to noise than to brightness or contrast
perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint version of the paper in the proceedings of 43rd
  Digital Avionics Systems Conference (DASC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SteP: Stacked LLM Policies for Web Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03720v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03720v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, Ryan McDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing tasks on the web presents fundamental challenges to large language
models (LLMs), including combinatorially large open-world tasks and variations
across web interfaces. Simply specifying a large prompt to handle all possible
behaviors and states is extremely complex, and results in behavior leaks
between unrelated behaviors. Decomposition to distinct policies can address
this challenge, but requires carefully handing off control between policies. We
propose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically
compose policies to solve a diverse set of web tasks. SteP defines a Markov
Decision Process where the state is a stack of policies representing the
control state, i.e., the chain of policy calls. Unlike traditional methods that
are restricted to static hierarchies, SteP enables dynamic control that adapts
to the complexity of the task. We evaluate SteP against multiple baselines and
web environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP
improves (14.9\% to 33.5\%) over SOTA that use GPT-4 policies, while on
MiniWob++, SteP is competitive with prior works while using significantly less
data. Our code and data are available at
https://asappresearch.github.io/webagents-step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Conference on Language Modeling (COLM) 2024. 30 pages, 15
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep-learning Assisted Detection and Quantification of (oo)cysts of
  Giardia and Cryptosporidium on Smartphone Microscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suprim Nakarmi, Sanam Pudasaini, Safal Thapaliya, Pratima Upretee, Retina Shrestha, Basant Giri, Bhanu Bhakta Neupane, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The consumption of microbial-contaminated food and water is responsible for
the deaths of millions of people annually. Smartphone-based microscopy systems
are portable, low-cost, and more accessible alternatives for the detection of
Giardia and Cryptosporidium than traditional brightfield microscopes. However,
the images from smartphone microscopes are noisier and require manual cyst
identification by trained technicians, usually unavailable in resource-limited
settings. Automatic detection of (oo)cysts using deep-learning-based object
detection could offer a solution for this limitation. We evaluate the
performance of four state-of-the-art object detectors to detect (oo)cysts of
Giardia and Cryptosporidium on a custom dataset that includes both smartphone
and brightfield microscopic images from vegetable samples. Faster RCNN,
RetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer
(Deformable DETR) deep-learning models were employed to explore their efficacy
and limitations. Our results show that while the deep-learning models perform
better with the brightfield microscopy image dataset than the smartphone
microscopy image dataset, the smartphone microscopy predictions are still
comparable to the prediction performance of non-experts. Also, we publicly
release brightfield and smartphone microscopy datasets with the benchmark
results for the detection of Giardia and Cryptosporidium, independently
captured on reference (or standard lab setting) and vegetable samples. Our code
and dataset are available at
https://github.com/naamiinepal/smartphone_microscopy and
https://doi.org/10.5281/zenodo.7813183, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages (including supplementary information), 5 figures, 7 tables,
  Accepted for publication at the Journal of Machine Learning for Biomedical
  Imaging (MELBA) https://melba-journal.org/2024:014</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: Topological Deep Learning is the New Frontier for Relational
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08871v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08871v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodore Papamarkou, Tolga Birdal, Michael Bronstein, Gunnar Carlsson, Justin Curry, Yue Gao, Mustafa Hajij, Roland Kwitt, Pietro Liò, Paolo Di Lorenzo, Vasileios Maroulas, Nina Miolane, Farzana Nasrin, Karthikeyan Natesan Ramamurthy, Bastian Rieck, Simone Scardapane, Michael T. Schaub, Petar Veličković, Bei Wang, Yusu Wang, Guo-Wei Wei, Ghada Zamzmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological deep learning (TDL) is a rapidly evolving field that uses
topological features to understand and design deep learning models. This paper
posits that TDL is the new frontier for relational learning. TDL may complement
graph representation learning and geometric deep learning by incorporating
topological concepts, and can thus provide a natural choice for various machine
learning settings. To this end, this paper discusses open problems in TDL,
ranging from practical benefits to theoretical foundations. For each problem,
it outlines potential solutions and future research opportunities. At the same
time, this paper serves as an invitation to the scientific community to
actively participate in TDL research to unlock the potential of this emerging
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 41st International Conference on Machine Learning,
  Vienna, Austria. PMLR 235, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00809v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00809v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, José Miguel Hernández-Lobato, Aliaksandr Hubin, Alexander Immer, Theofanis Karaletsos, Mohammad Emtiyaz Khan, Agustinus Kristiadi, Yingzhen Li, Stephan Mandt, Christopher Nemeth, Michael A. Osborne, Tim G. J. Rudner, David Rügamer, Yee Whye Teh, Max Welling, Andrew Gordon Wilson, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current landscape of deep learning research, there is a predominant
emphasis on achieving high predictive accuracy in supervised tasks involving
large image and language datasets. However, a broader perspective reveals a
multitude of overlooked metrics, tasks, and data types, such as uncertainty,
active and continual learning, and scientific data, that demand attention.
Bayesian deep learning (BDL) constitutes a promising avenue, offering
advantages across these diverse settings. This paper posits that BDL can
elevate the capabilities of deep learning. It revisits the strengths of BDL,
acknowledges existing challenges, and highlights some exciting research avenues
aimed at addressing these obstacles. Looking ahead, the discussion focuses on
possible ways to combine large-scale foundation models with BDL to unlock their
full potential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 41st International Conference on Machine Learning,
  Vienna, Austria. PMLR 235, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaloQVAE : Simulating high-energy particle-calorimeter interactions
  using hybrid quantum-classical generative models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03179v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03179v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehmimul Hoque, Hao Jia, Abhishek Abhishek, Mojde Fadaie, J. Quetzalcoatl Toledo-Marín, Tiago Vale, Roger G. Melko, Maximilian Swiatlowski, Wojciech T. Fedorko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Large Hadron Collider's high luminosity era presents major computational
challenges in the analysis of collision events. Large amounts of Monte Carlo
(MC) simulation will be required to constrain the statistical uncertainties of
the simulated datasets below these of the experimental data. Modelling of
high-energy particles propagating through the calorimeter section of the
detector is the most computationally intensive MC simulation task. We introduce
a technique combining recent advancements in generative models and quantum
annealing for fast and efficient simulation of high-energy particle-calorimeter
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphLearner: Graph Node Clustering with Fully Learnable Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xihong Yang, Erxue Min, Ke Liang, Yue Liu, Siwei Wang, Sihang Zhou, Huijun Wu, Xinwang Liu, En Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive deep graph clustering (CDGC) leverages the power of contrastive
learning to group nodes into different clusters. The quality of contrastive
samples is crucial for achieving better performance, making augmentation
techniques a key factor in the process. However, the augmentation samples in
existing methods are always predefined by human experiences, and agnostic from
the downstream task clustering, thus leading to high human resource costs and
poor performance. To overcome these limitations, we propose a Graph Node
Clustering with Fully Learnable Augmentation, termed GraphLearner. It
introduces learnable augmentors to generate high-quality and task-specific
augmented samples for CDGC. GraphLearner incorporates two learnable augmentors
specifically designed for capturing attribute and structural information.
Moreover, we introduce two refinement matrices, including the high-confidence
pseudo-label matrix and the cross-view sample similarity matrix, to enhance the
reliability of the learned affinity matrix. During the training procedure, we
notice the distinct optimization goals for training learnable augmentors and
contrastive learning networks. In other words, we should both guarantee the
consistency of the embeddings as well as the diversity of the augmented
samples. To address this challenge, we propose an adversarial learning
mechanism within our method. Besides, we leverage a two-stage training strategy
to refine the high-confidence matrices. Extensive experimental results on six
benchmark datasets validate the effectiveness of GraphLearner.The code and
appendix of GraphLearner are available at
https://github.com/xihongyang1999/GraphLearner on Github.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Compute Thresholds: Features and Functions in AI Regulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Heim, Leonie Koessler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulators in the US and EU are using thresholds based on training
compute--the number of computational operations used in training--to identify
general-purpose artificial intelligence (GPAI) models that may pose risks of
large-scale societal harm. We argue that training compute currently is the most
suitable metric to identify GPAI models that deserve regulatory oversight and
further scrutiny. Training compute correlates with model capabilities and
risks, is quantifiable, can be measured early in the AI lifecycle, and can be
verified by external actors, among other advantageous features. These features
make compute thresholds considerably more suitable than other proposed metrics
to serve as an initial filter to trigger additional regulatory requirements and
scrutiny. However, training compute is an imperfect proxy for risk. As such,
compute thresholds should not be used in isolation to determine appropriate
mitigation measures. Instead, they should be used to detect potentially risky
GPAI models that warrant regulatory oversight, such as through notification
requirements, and further scrutiny, such as via model evaluations and risk
assessments, the results of which may inform which mitigation measures are
appropriate. In fact, this appears largely consistent with how compute
thresholds are used today. As GPAI technology and market structures evolve,
regulators should update compute thresholds and complement them with other
metrics into regulatory review processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Major revision of earlier working paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calo-VQ: Vector-Quantized Two-Stage Generative Model in Calorimeter
  Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06605v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06605v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibin Liu, Chase Shimmin, Xiulong Liu, Eli Shlizerman, Shu Li, Shih-Chieh Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel machine learning method developed for the fast
simulation of calorimeter detector response, adapting vector-quantized
variational autoencoder (VQ-VAE). Our model adopts a two-stage generation
strategy: initially compressing geometry-aware calorimeter data into a discrete
latent space, followed by the application of a sequence model to learn and
generate the latent tokens. Extensive experimentation on the Calo-challenge
dataset underscores the efficiency of our approach, showcasing a remarkable
improvement in the generation speed compared with conventional method by a
factor of 2000. Remarkably, our model achieves the generation of calorimeter
showers within milliseconds. Furthermore, comprehensive quantitative
evaluations across various metrics are performed to validate physics
performance of generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tool Learning with Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08354v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08354v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess an extraordinary ability to create and utilize tools, allowing
them to overcome physical limitations and explore new frontiers. With the
advent of foundation models, AI systems have the potential to be equally adept
in tool use as humans. This paradigm, i.e., tool learning with foundation
models, combines the strengths of specialized tools and foundation models to
achieve enhanced accuracy, efficiency, and automation in problem-solving.
Despite its immense potential, there is still a lack of a comprehensive
understanding of key challenges, opportunities, and future endeavors in this
field. To this end, we present a systematic investigation of tool learning in
this paper. We first introduce the background of tool learning, including its
cognitive origins, the paradigm shift of foundation models, and the
complementary roles of tools and models. Then we recapitulate existing tool
learning research into tool-augmented and tool-oriented learning. We formulate
a general tool learning framework: starting from understanding the user
instruction, models should learn to decompose a complex task into several
subtasks, dynamically adjust their plan through reasoning, and effectively
conquer each sub-task by selecting appropriate tools. We also discuss how to
train models for improved tool-use capabilities and facilitate the
generalization in tool learning. Considering the lack of a systematic tool
learning evaluation in prior works, we experiment with 18 representative tools
and show the potential of current foundation models in skillfully utilizing
tools. Finally, we discuss several open problems that require further
investigation for tool learning. In general, we hope this paper could inspire
future research in integrating tools with foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Adaptive Second-order Method for a Class of Nonconvex Nonsmooth
  Composite Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Xiangyu Yang, Yichen Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores a specific type of nonconvex sparsity-promoting
regularization problems, namely those involving $\ell_p$-norm regularization,
in conjunction with a twice continuously differentiable loss function. We
propose a novel second-order algorithm designed to effectively address this
class of challenging nonconvex and nonsmooth problems, showcasing several
innovative features: (i) The use of an alternating strategy to solve a
reweighted $\ell_1$ regularized subproblem and the subspace approximate Newton
step. (ii) The reweighted $\ell_1$ regularized subproblem relies on a convex
approximation to the nonconvex regularization term, enabling a closed-form
solution characterized by the soft-thresholding operator. This feature allows
our method to be applied to various nonconvex regularization problems. (iii)
Our algorithm ensures that the iterates maintain their sign values and that
nonzero components are kept away from 0 for a sufficient number of iterations,
eventually transitioning to a perturbed Newton method. (iv) We provide
theoretical guarantees of global convergence, local superlinear convergence in
the presence of the Kurdyka-\L ojasiewicz (KL) property, and local quadratic
convergence when employing the exact Newton step in our algorithm. We also
showcase the effectiveness of our approach through experiments on a diverse set
of model prediction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-Explainer: A Model-Agnostic Explainability Framework Based on
  Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evandro S. Ortigossa, Fábio F. Dias, Brian Barr, Claudio T. Silva, Luis Gustavo Nonato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of machine learning applications has increased significantly
in recent years, motivated by the remarkable ability of learning-powered
systems to discover and generalize intricate patterns hidden in massive
datasets. Modern learning models, while powerful, often have a level of
complexity that renders them opaque black boxes, resulting in a notable lack of
transparency that hinders our ability to decipher their reasoning. Opacity
challenges the interpretability and practical application of machine learning,
especially in critical domains where understanding the underlying reasons is
essential for informed decision-making. Explainable Artificial Intelligence
(XAI) rises to address that challenge, unraveling the complexity of black boxes
by providing elucidating explanations. Among the various XAI approaches,
feature attribution/importance stands out for its capacity to delineate the
significance of input features in the prediction process. However, most
existing attribution methods have limitations, such as instability, when
divergent explanations may result from similar or even the same instance. This
work introduces T-Explainer, a novel local additive attribution explainer based
on Taylor expansion. It has desirable properties, such as local accuracy and
consistency, making T-Explainer stable over multiple runs. We demonstrate
T-Explainer's effectiveness in quantitative benchmark experiments against
well-known attribution methods. Additionally, we provide several tools to
evaluate and visualize explanations, turning T-Explainer into a comprehensive
XAI framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages -- 2 figures and 20 tables -- Under review. This work has
  been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-guided Active Sample Reweighting for Urban Flow Prediction <span class="chip">CIKM '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Tong Chen, Guanhua Ye, Wentao Zhang, Lizhen Cui, Zi Huang, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban flow prediction is a spatio-temporal modeling task that estimates the
throughput of transportation services like buses, taxis, and ride-sharing,
where data-driven models have become the most popular solution in the past
decade. Meanwhile, the implicitly learned mapping between historical
observations to the prediction targets tend to over-simplify the dynamics of
real-world urban flows, leading to suboptimal predictions. Some recent
spatio-temporal prediction solutions bring remedies with the notion of
physics-guided machine learning (PGML), which describes spatio-temporal data
with nuanced and principled physics laws, thus enhancing both the prediction
accuracy and interpretability. However, these spatio-temporal PGML methods are
built upon a strong assumption that the observed data fully conforms to the
differential equations that define the physical system, which can quickly
become ill-posed in urban flow prediction tasks. The observed urban flow data,
especially when sliced into time-dependent snapshots to facilitate predictions,
is typically incomplete and sparse, and prone to inherent noise incurred in the
collection process. As a result, such physical inconsistency between the data
and PGML model significantly limits the predictive power and robustness of the
solution. Moreover, due to the interval-based predictions and intermittent
nature of data filing in many transportation services, the instantaneous
dynamics of urban flows can hardly be captured, rendering differential
equation-based continuous modeling a loose fit for this setting. To overcome
the challenges, we develop a discretized physics-guided network (PN), and
propose a data-aware framework Physics-guided Active Sample Reweighting
(P-GASR) to enhance PN. Experimental results in four real-world datasets
demonstrate that our method achieves state-of-the-art performance with a
demonstrable improvement in robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by Proceedings of the 33nd ACM International
  Conference on Information and Knowledge Management (CIKM '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09146v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09146v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Kashif, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Baumgärtner, Yang Gao, Dana Alon, Donald Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is a popular method for
aligning Language Models (LM) with human values and preferences. RLHF requires
a large number of preference pairs as training data, which are often used in
both the Supervised Fine-Tuning and Reward Model training and therefore
publicly available datasets are commonly used. In this work, we study to what
extent a malicious actor can manipulate the LMs generations by poisoning the
preferences, i.e., injecting poisonous preference pairs into these datasets and
the RLHF training process. We propose strategies to build poisonous preference
pairs and test their performance by poisoning two widely used preference
datasets. Our results show that preference poisoning is highly effective:
injecting a small amount of poisonous data (1-5\% of the original dataset), we
can effectively manipulate the LM to generate a target entity in a target
sentiment (positive or negative). The findings from our experiments also shed
light on strategies to defend against the preference poisoning attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hummer: Towards Limited Competitive Preference <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Jiang, Yusen Wu, Junwu Xiong, Jingqing Ruan, Yichuan Ding, Qingpei Guo, Zujie Wen, Jun Zhou, Xiaotie Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference datasets are essential for incorporating human preferences into
pre-trained language models, playing a key role in the success of Reinforcement
Learning from Human Feedback. However, these datasets often demonstrate
conflicting alignment objectives, leading to increased vulnerability to
jailbreak attacks and challenges in adapting downstream tasks to prioritize
specific alignment objectives without negatively impacting others. In this
work, we introduce a novel statistical metric, Alignment Dimension Conflict, to
quantify the degree of conflict within preference datasets. We then present
\texttt{Hummer} and its fine-grained variant, \texttt{Hummer-F}, as innovative
pairwise preference datasets with reduced-conflict alignment objectives.
\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback
from GPT-4, marking as the first preference dataset aimed at reducing the
competition between alignment objectives. Furthermore, we develop reward
models, HummerRM and HummerRM-F, which employ a hybrid sampling approach to
balance diverse alignment objectives effectively. This sampling method
positions HummerRM as an ideal model for domain-specific further fine-tuning
and reducing vulnerabilities to attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized
  Least-Squares Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07186v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07186v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Li, Dimitri Meunier, Mattes Mollenhauer, Arthur Gretton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first optimal rates for infinite-dimensional vector-valued
ridge regression on a continuous scale of norms that interpolate between $L_2$
and the hypothesis space, which we consider as a vector-valued reproducing
kernel Hilbert space. These rates allow to treat the misspecified case in which
the true regression function is not contained in the hypothesis space. We
combine standard assumptions on the capacity of the hypothesis space with a
novel tensor product construction of vector-valued interpolation spaces in
order to characterize the smoothness of the regression function. Our upper
bound not only attains the same rate as real-valued kernel ridge regression,
but also removes the assumption that the target regression function is bounded.
For the lower bound, we reduce the problem to the scalar setting using a
projection argument. We show that these rates are optimal in most cases and
independent of the dimension of the output space. We illustrate our results for
the special case of vector-valued Sobolev spaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published JMLR version. arXiv admin note: text overlap with
  arXiv:2208.01711</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stability-Informed Initialization of Neural Ordinary Differential
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15890v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15890v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodor Westny, Arman Mohammadi, Daniel Jung, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the training of Neural Ordinary Differential Equations
(neural ODEs), and in particular explores the interplay between numerical
integration techniques, stability regions, step size, and initialization
techniques. It is shown how the choice of integration technique implicitly
regularizes the learned model, and how the solver's corresponding stability
region affects training and prediction performance. From this analysis, a
stability-informed parameter initialization technique is introduced. The
effectiveness of the initialization method is displayed across several learning
benchmarks and industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 41 st International Conference on Machine
  Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When a Relation Tells More Than a Concept: Exploring and Evaluating
  Classifier Decisions with CoReX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01661v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01661v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bettina Finzel, Patrick Hilme, Johannes Rabold, Ute Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations for Convolutional Neural Networks (CNNs) based on relevance of
input pixels might be too unspecific to evaluate which and how input features
impact model decisions. Especially in complex real-world domains like biology,
the presence of specific concepts and of relations between concepts might be
discriminating between classes. Pixel relevance is not expressive enough to
convey this type of information. In consequence, model evaluation is limited
and relevant aspects present in the data and influencing the model decisions
might be overlooked. This work presents a novel method to explain and evaluate
CNN models, which uses a concept- and relation-based explainer (CoReX). It
explains the predictive behavior of a model on a set of images by masking
(ir-)relevant concepts from the decision-making process and by constraining
relations in a learned interpretable surrogate model. We test our approach with
several image data sets and CNN architectures. Results show that CoReX
explanations are faithful to the CNN model in terms of predictive outcomes. We
further demonstrate through a human evaluation that CoReX is a suitable tool
for generating combined explanations that help assessing the classification
quality of CNNs. We further show that CoReX supports the identification and
re-classification of incorrect or ambiguous classifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preliminary version, submitted to Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence Analysis of Natural Gradient Descent for Over-parameterized
  Physics-Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianliang Xu, Ting Du, Wang Kong, Ye Li, Zhongyi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  First-order methods, such as gradient descent (GD) and stochastic gradient
descent (SGD), have been proven effective in training neural networks. In the
context of over-parameterization, there is a line of work demonstrating that
randomly initialized (stochastic) gradient descent converges to a globally
optimal solution at a linear convergence rate for the quadratic loss function.
However, the learning rate of GD for training two-layer neural networks
exhibits poor dependence on the sample size and the Gram matrix, leading to a
slow training process. In this paper, we show that for the $L^2$ regression
problems, the learning rate can be improved from $\mathcal{O}(\lambda_0/n^2)$
to $\mathcal{O}(1/\|\bm{H}^{\infty}\|_2)$, which implies that GD actually
enjoys a faster convergence rate. Furthermore, we generalize the method to GD
in training two-layer Physics-Informed Neural Networks (PINNs), showing a
similar improvement for the learning rate. Although the improved learning rate
has a mild dependence on the Gram matrix, we still need to set it small enough
in practice due to the unknown eigenvalues of the Gram matrix. More
importantly, the convergence rate is tied to the least eigenvalue of the Gram
matrix, which can lead to slow convergence. In this work, we provide the
convergence analysis of natural gradient descent (NGD) in training two-layer
PINNs, demonstrating that the learning rate can be $\mathcal{O}(1)$, and at
this rate, the convergence rate is independent of the Gram matrix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Clock: High-Dimensional Effects in Two-Dimensional Plots <span class="chip">IEEE VIS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Ovcharenko, Rita Sevastjanova, Valentina Boeva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans struggle to perceive and interpret high-dimensional data. Therefore,
high-dimensional data are often projected into two dimensions for
visualization. Many applications benefit from complex nonlinear dimensionality
reduction techniques, but the effects of individual high-dimensional features
are hard to explain in the two-dimensional space. Most visualization solutions
use multiple two-dimensional plots, each showing the effect of one
high-dimensional feature in two dimensions; this approach creates a need for a
visual inspection of k plots for a k-dimensional input space. Our solution,
Feature Clock, provides a novel approach that eliminates the need to inspect
these k plots to grasp the influence of original features on the data structure
depicted in two dimensions. Feature Clock enhances the explainability and
compactness of visualizations of embedded data and is available in an
open-source Python library.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE VIS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal <span class="highlight-title">Dataset</span> Creation for Federated Learning with DICOM
  Structured Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Tölle, Lukas Burger, Halvar Kelm, Florian André, Peter Bannas, Gerhard Diller, Norbert Frey, Philipp Garthe, Stefan Groß, Anja Hennemuth, Lars Kaderali, Nina Krüger, Andreas Leha, Simon Martin, Alexander Meyer, Eike Nagel, Stefan Orwat, Clemens Scherer, Moritz Seiffert, Jan Moritz Seliger, Stefan Simm, Tim Friede, Tim Seidler, Sandy Engelhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Federated training is often hindered by heterogeneous datasets due
to divergent data storage options, inconsistent naming schemes, varied
annotation procedures, and disparities in label quality. This is particularly
evident in the emerging multi-modal learning paradigms, where dataset
harmonization including a uniform data representation and filtering options are
of paramount importance.
  Methods: DICOM structured reports enable the standardized linkage of
arbitrary information beyond the imaging domain and can be used within Python
deep learning pipelines with highdicom. Building on this, we developed an open
platform for data integration and interactive filtering capabilities that
simplifies the process of assembling multi-modal datasets.
  Results: In this study, we extend our prior work by showing its applicability
to more and divergent data types, as well as streamlining datasets for
federated training within an established consortium of eight university
hospitals in Germany. We prove its concurrent filtering ability by creating
harmonized multi-modal datasets across all locations for predicting the outcome
after minimally invasive heart valve replacement. The data includes DICOM data
(i.e. computed tomography images, electrocardiography scans) as well as
annotations (i.e. calcification segmentations, pointsets and pacemaker
dependency), and metadata (i.e. prosthesis and diagnoses).
  Conclusion: Structured reports bridge the traditional gap between imaging
systems and information systems. Utilizing the inherent DICOM reference system
arbitrary data types can be queried concurrently to create meaningful cohorts
for clinical studies. The graphical interface as well as example structured
report templates will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preservation of Feature Stability in Machine Learning Under Data
  Uncertainty for Decision Support in Critical Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karol Capała, Paulina Tworek, Jose Sousa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a world where Machine Learning (ML) is increasingly deployed to support
decision-making in critical domains, providing decision-makers with
explainable, stable, and relevant inputs becomes fundamental. Understanding how
machine learning works under missing data and how this affects feature
variability is paramount. This is even more relevant as machine learning
approaches focus on standardising decision-making approaches that rely on an
idealised set of features. However, decision-making in human activities often
relies on incomplete data, even in critical domains. This paper addresses this
gap by conducting a set of experiments using traditional machine learning
methods that look for optimal decisions in comparison to a recently deployed
machine learning method focused on a classification that is more descriptive
and mimics human decision making, allowing for the natural integration of
explainability. We found that the ML descriptive approach maintains higher
classification accuracy while ensuring the stability of feature selection as
data incompleteness increases. This suggests that descriptive classification
methods can be helpful in uncertain decision-making scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 figures, supplementary materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Evaluation as a Defense Against Adversarial Attacks on LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03234v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03234v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Brown, Leon Lin, Kenji Kawaguchi, Michael Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a defense against adversarial attacks on LLMs utilizing
self-evaluation. Our method requires no model fine-tuning, instead using
pre-trained models to evaluate the inputs and outputs of a generator model,
significantly reducing the cost of implementation in comparison to other,
finetuning-based methods. Our method can significantly reduce the attack
success rate of attacks on both open and closed-source LLMs, beyond the
reductions demonstrated by Llama-Guard2 and commonly used content moderation
APIs. We present an analysis of the effectiveness of our method, including
attempts to attack the evaluator in various settings, demonstrating that it is
also more resilient to attacks than existing methods. Code and data will be
made available at https://github.com/Linlt-leon/self-eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closing the gap between SVRG and TD-SVRG with Gradient Splitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16237v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16237v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsenii Mustafin, Alex Olshevsky, Ioannis Ch. Paschalidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal difference (TD) learning is a policy evaluation in reinforcement
learning whose performance can be enhanced by variance reduction methods.
Recently, multiple works have sought to fuse TD learning with Stochastic
Variance Reduced Gradient (SVRG) method to achieve a geometric rate of
convergence. However, the resulting convergence rate is significantly weaker
than what is achieved by SVRG in the setting of convex optimization. In this
work we utilize a recent interpretation of TD-learning as the splitting of the
gradient of an appropriately chosen function, thus simplifying the algorithm
and fusing TD with SVRG. Our main result is a geometric convergence bound with
predetermined learning rate of $1/8$, which is identical to the convergence
bound available for SVRG in the convex setting. Our theoretical findings are
supported by a set of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 8 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape Constraints in Symbolic Regression using Penalized Least Squares 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Martinek, Julia Reuter, Ophelia Frotscher, Sanaz Mostaghim, Markus Richter, Roland Herzog
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the addition of shape constraints (SC) and their consideration
during the parameter identification step of symbolic regression (SR). SC serve
as a means to introduce prior knowledge about the shape of the otherwise
unknown model function into SR. Unlike previous works that have explored SC in
SR, we propose minimizing SC violations during parameter identification using
gradient-based numerical optimization. We test three algorithm variants to
evaluate their performance in identifying three symbolic expressions from
synthetically generated data sets. This paper examines two benchmark scenarios:
one with varying noise levels and another with reduced amounts of training
data. The results indicate that incorporating SC into the expression search is
particularly beneficial when data is scarce. Compared to using SC only in the
selection process, our approach of minimizing violations during parameter
identification shows a statistically significant benefit in some of our test
cases, without being significantly worse in any instance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variance reduction techniques for stochastic proximal point algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheik Traoré, Vassilis Apidopoulos, Saverio Salzo, Silvia Villa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of finite sums minimization, variance reduction techniques are
widely used to improve the performance of state-of-the-art stochastic gradient
methods. Their practical impact is clear, as well as their theoretical
properties. Stochastic proximal point algorithms have been studied as an
alternative to stochastic gradient algorithms since they are more stable with
respect to the choice of the step size. However, their variance-reduced
versions are not as well studied as the gradient ones. In this work, we propose
the first unified study of variance reduction techniques for stochastic
proximal point algorithms. We introduce a generic stochastic proximal-based
algorithm that can be specified to give the proximal version of SVRG, SAGA, and
some of their variants. For this algorithm, in the smooth setting, we provide
several convergence rates for the iterates and the objective function values,
which are faster than those of the vanilla stochastic proximal point algorithm.
More specifically, for convex functions, we prove a sublinear convergence rate
of $O(1/k)$. In addition, under the Polyak-{\L}ojasiewicz (PL) condition, we
obtain linear convergence rates. Finally, our numerical experiments demonstrate
the advantages of the proximal variance reduction methods over their gradient
counterparts in terms of the stability with respect to the choice of the step
size in most cases, especially for difficult problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equivariance and partial observations in Koopman operator theory for
  partial differential equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Peitz, Hans Harder, Feliks Nüske, Friedrich Philipp, Manuel Schaller, Karl Worthmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Koopman operator has become an essential tool for data-driven analysis,
prediction and control of complex systems. The main reason is the enormous
potential of identifying linear function space representations of nonlinear
dynamics from measurements. This equally applies to ordinary, stochastic, and
partial differential equations (PDEs). Until now, with a few exceptions only,
the PDE case is mostly treated rather superficially, and the specific structure
of the underlying dynamics is largely ignored. In this paper, we show that
symmetries in the system dynamics can be carried over to the Koopman operator,
which allows us to massively increase the model efficacy. Moreover, the
situation where we only have access to partial observations (i.e.,
measurements, as is very common for experimental data) has not been treated to
its full extent, either. Moreover, we address the highly-relevant case where we
cannot measure the full state, such that alternative approaches such as delay
coordinates have to be considered. We derive rigorous statements on the
required number of observables in this situation, based on embedding theory. We
present numerical evidence using various numerical examples including the wave
equation and the Kuramoto-Sivashinsky equation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A finite element-based physics-informed operator learning framework for
  spatiotemporal partial differential equations on arbitrary domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12465v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12465v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Yamazaki, Ali Harandi, Mayu Muramatsu, Alexandre Viardin, Markus Apel, Tim Brepols, Stefanie Reese, Shahed Rezaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel finite element-based physics-informed operator learning
framework that allows for predicting spatiotemporal dynamics governed by
partial differential equations (PDEs). The proposed framework employs a loss
function inspired by the finite element method (FEM) with the implicit Euler
time integration scheme. A transient thermal conduction problem is considered
to benchmark the performance. The proposed operator learning framework takes a
temperature field at the current time step as input and predicts a temperature
field at the next time step. The Galerkin discretized weak formulation of the
heat equation is employed to incorporate physics into the loss function, which
is coined finite operator learning (FOL). Upon training, the networks
successfully predict the temperature evolution over time for any initial
temperature field at high accuracy compared to the FEM solution. The framework
is also confirmed to be applicable to a heterogeneous thermal conductivity and
arbitrary geometry. The advantages of FOL can be summarized as follows: First,
the training is performed in an unsupervised manner, avoiding the need for a
large data set prepared from costly simulations or experiments. Instead, random
temperature patterns generated by the Gaussian random process and the Fourier
series, combined with constant temperature fields, are used as training data to
cover possible temperature cases. Second, shape functions and backward
difference approximation are exploited for the domain discretization, resulting
in a purely algebraic equation. This enhances training efficiency, as one
avoids time-consuming automatic differentiation when optimizing weights and
biases while accepting possible discretization errors. Finally, thanks to the
interpolation power of FEM, any arbitrary geometry can be handled with FOL,
which is crucial to addressing various engineering application scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparison analysis between standard polysomnographic data and
  in-ear-EEG signals: A preliminary study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10107v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10107v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianpaolo Palo, Luigi Fiorillo, Giuliana Monachino, Michal Bechny, Michel Walti, Elias Meier, Francesca Pentimalli Biscaretti di Ruffia, Mark Melnykowycz, Athina Tzovara, Valentina Agostini, Francesca Dalia Faraci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Study Objectives: Polysomnography (PSG) currently serves as the benchmark for
evaluating sleep disorders. Its discomfort makes long-term monitoring
unfeasible, leading to bias in sleep quality assessment. Hence, less invasive,
cost-effective, and portable alternatives need to be explored. One promising
contender is the in-ear-EEG sensor. This study aims to establish a methodology
to assess the similarity between the single-channel in-ear-EEG and standard PSG
derivations.
  Methods: The study involves four-hour signals recorded from ten healthy
subjects aged 18 to 60 years. Recordings are analyzed following two
complementary approaches: (i) a hypnogram-based analysis aimed at assessing the
agreement between PSG and in-ear-EEG-derived hypnograms; and (ii) a
feature-based analysis based on time- and frequency- domain feature extraction,
unsupervised feature selection, and definition of Feature-based Similarity
Index via Jensen-Shannon Divergence (JSD-FSI).
  Results: We find large variability between PSG and in-ear-EEG hypnograms
scored by the same sleep expert according to Cohen's kappa metric, with
significantly greater agreements for PSG scorers than for in-ear-EEG scorers (p
< 0.001) based on Fleiss' kappa metric. On average, we demonstrate a high
similarity between PSG and in-ear-EEG signals in terms of JSD-FSI (0.79 +/-
0.06 -awake, 0.77 +/- 0.07 -NREM, and 0.67 +/- 0.10 -REM) and in line with the
similarity values computed independently on standard PSG-channel-combinations.
  Conclusions: In-ear-EEG is a valuable solution for home-based sleep
monitoring, however further studies with a larger and more heterogeneous
dataset are needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Generated Faces in the Real World: A Large-Scale Case Study of
  Twitter Profile Images <span class="chip">RAID 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Ricker, Dennis Assenmacher, Thorsten Holz, Asja Fischer, Erwin Quiring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in the field of generative artificial intelligence (AI) have
blurred the lines between authentic and machine-generated content, making it
almost impossible for humans to distinguish between such media. One notable
consequence is the use of AI-generated images for fake profiles on social
media. While several types of disinformation campaigns and similar incidents
have been reported in the past, a systematic analysis has been lacking. In this
work, we conduct the first large-scale investigation of the prevalence of
AI-generated profile pictures on Twitter. We tackle the challenges of a
real-world measurement study by carefully integrating various data sources and
designing a multi-stage detection pipeline. Our analysis of nearly 15 million
Twitter profile pictures shows that 0.052% were artificially generated,
confirming their notable presence on the platform. We comprehensively examine
the characteristics of these accounts and their tweet content, and uncover
patterns of coordinated inauthentic behavior. The results also reveal several
motives, including spamming and political amplification campaigns. Our research
reaffirms the need for effective detection and mitigation strategies to cope
with the potential negative effects of generative AI in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RAID 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Posterior Proximal Sampling for Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjie Wu, Linchao He, Mingqin Zhang, Dongdong Chen, Kunming Luo, Mengting Luo, Ji-Zhe Zhou, Hu Chen, Jiancheng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable efficacy in generating
high-quality samples. Existing diffusion-based image restoration algorithms
exploit pre-trained diffusion models to leverage data priors, yet they still
preserve elements inherited from the unconditional generation paradigm. These
strategies initiate the denoising process with pure white noise and incorporate
random noise at each generative step, leading to over-smoothed results. In this
paper, we present a refined paradigm for diffusion-based image restoration.
Specifically, we opt for a sample consistent with the measurement identity at
each generative step, exploiting the sampling selection as an avenue for output
stability and enhancement. The number of candidate samples used for selection
is adaptively determined based on the signal-to-noise ratio of the timestep.
Additionally, we start the restoration process with an initialization combined
with the measurement signal, providing supplementary information to better
align the generative process. Extensive experimental results and analyses
validate that our proposed method significantly enhances image restoration
performance while consuming negligible additional computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2024 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network Emulator for Atmospheric Chemical ODE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Song Liu, Petri Clusius, Michael Boy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling atmospheric chemistry is complex and computationally intense. Given
the recent success of Deep neural networks in digital signal processing, we
propose a Neural Network Emulator for fast chemical concentration modeling. We
consider atmospheric chemistry as a time-dependent Ordinary Differential
Equation. To extract the hidden correlations between initial states and future
time evolution, we propose ChemNNE, an Attention based Neural Network Emulator
(NNE) that can model the atmospheric chemistry as a neural ODE process. To
efficiently simulate the chemical changes, we propose the sinusoidal time
embedding to estimate the oscillating tendency over time. More importantly, we
use the Fourier neural operator to model the ODE process for efficient
computation. We also propose three physical-informed losses to supervise the
training optimization. To evaluate our model, we propose a large-scale chemical
dataset that can be used for neural network training and evaluation. The
extensive experiments show that our approach achieves state-of-the-art
performance in modeling accuracy and computational speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-Based Optimal Control with Performance Guarantees for Unknown
  Systems with Latent States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17963v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17963v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Lefringhausen, Supitsana Srithasan, Armin Lederer, Sandra Hirche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As control engineering methods are applied to increasingly complex systems,
data-driven approaches for system identification appear as a promising
alternative to physics-based modeling. While the Bayesian approaches prevalent
for safety-critical applications usually rely on the availability of state
measurements, the states of a complex system are often not directly measurable.
It may then be necessary to jointly estimate the dynamics and the latent state,
making the quantification of uncertainties and the design of controllers with
formal performance guarantees considerably more challenging. This paper
proposes a novel method for the computation of an optimal input trajectory for
unknown nonlinear systems with latent states based on a combination of particle
Markov chain Monte Carlo methods and scenario theory. Probabilistic performance
guarantees are derived for the resulting input trajectory, and an approach to
validate the performance of arbitrary control laws is presented. The
effectiveness of the proposed method is demonstrated in a numerical simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted version submitted to the 2024 European Control Conference
  (ECC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence of Batch Asynchronous Stochastic Approximation With
  Applications to Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.03445v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.03445v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajeeva L. Karandikar, M. Vidyasagar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We begin by briefly surveying some results on the convergence of the
Stochastic Gradient Descent (SGD) Method, proved in a companion paper by the
present authors. These results are based on viewing SGD as a version of
Stochastic Approximation (SA). Ever since its introduction in the classic paper
of Robbins and Monro in 1951, SA has become a standard tool for finding a
solution of an equation of the form $f(\theta) = 0$, when only noisy
measurements of $f(\cdot)$ are available. In most situations, \textit{every
component} of the putative solution $\theta_t$ is updated at each step $t$. In
some applications in Reinforcement Learning (RL), \textit{only one component}
of $\theta_t$ is updated at each $t$. This is known as \textbf{asynchronous}
SA. In this paper, we study \textbf{Block Asynchronous SA (BASA)}, in which, at
each step $t$, \textit{some but not necessarily all} components of $\theta_t$
are updated. The theory presented here embraces both conventional (synchronous)
SA as well as asynchronous SA, and all in-between possibilities. We provide
sufficient conditions for the convergence of BASA, and also prove bounds on the
\textit{rate} of convergence of $\theta_t$ to the solution. For the case of
conventional SGD, these results reduce to those proved in our companion paper.
Then we apply these results to the problem of finding a fixed point of a map
with only noisy measurements. This problem arises frequently in RL. We prove
sufficient conditions for convergence as well as estimates for the rate of
convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NetLLM: Adapting Large Language Models for Networking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02338v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02338v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duo Wu, Xianda Wang, Yaqi Qiao, Zhi Wang, Junchen Jiang, Shuguang Cui, Fangxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many networking tasks now employ deep learning (DL) to solve complex
prediction and optimization problems. However, current design philosophy of
DL-based algorithms entails intensive engineering overhead due to the manual
design of deep neural networks (DNNs) for different networking tasks. Besides,
DNNs tend to achieve poor generalization performance on unseen data
distributions/environments.
  Motivated by the recent success of large language models (LLMs), this work
studies the LLM adaptation for networking to explore a more sustainable design
philosophy. With the powerful pre-trained knowledge, the LLM is promising to
serve as the foundation model to achieve "one model for all tasks" with even
better performance and stronger generalization. In pursuit of this vision, we
present NetLLM, the first framework that provides a coherent design to harness
the powerful capabilities of LLMs with low efforts to solve networking
problems. Specifically, NetLLM empowers the LLM to effectively process
multimodal data in networking and efficiently generate task-specific answers.
Besides, NetLLM drastically reduces the costs of fine-tuning the LLM to acquire
domain knowledge for networking. Across three networking-related use cases -
viewport prediction, adaptive bitrate streaming and cluster job scheduling, we
showcase that the NetLLM-adapted LLM significantly outperforms state-of-the-art
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ACM SIGCOMM 2024. DOI:
  https://doi.org/10.1145/3651890.3672268</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuron Patching: Semantic-based Neuron-level Language Model Repair for
  Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05356v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05356v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have already gained widespread adoption in
software engineering, particularly in code generation tasks. However, updating
these models with new knowledge can be prohibitively expensive, yet it is
essential to maximize their utility, such as implementing a hotfix technique to
address urgent or critical LLM errors. In this paper, we propose \textsc{MENT},
a novel and effective model editing approach to repair LLMs in coding tasks.
\textsc{MENT} is effective, efficient, and reliable, capable of correcting a
neural model by patching just one or two neurons. As pioneering work on
neuron-level model editing of generative models, we formalize the editing
process and introduce the involved concepts. We also introduce new measures to
evaluate its generalization ability and establish a benchmark for further
study. Our approach is evaluated on three coding tasks: line-level code
generation, shellcode generation, and intent-to-bash translation. The
experimental results demonstrate that the proposed approach significantly
outperforms the state-of-the-art in both effectiveness and efficiency measures.
Furthermore, we showcase the applications of \textsc{MENT} for LLM reasoning in
software engineering. By editing LLM knowledge, the directly or indirectly
dependent behaviors of API invocation in the chain-of-thought change
accordingly. This illustrates the significance of repairing LLMs in the context
of software engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, 7 tables, under peer-review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Logistic Regression Training with A Faster Gradient
  Variant <span class="chip">ICANN 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.10838v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.10838v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logistic regression training over encrypted data has been an attractive idea
to security concerns for years. In this paper, we propose a faster gradient
variant called $\texttt{quadratic gradient}$ for privacy-preserving logistic
regression training. The core of $\texttt{quadratic gradient}$ can be seen as
an extension of the simplified fixed Hessian. We enhance Nesterov's accelerated
gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with
$\texttt{quadratic gradient}$ and evaluate the enhanced algorithms on several
datasets. Experiments show that the enhanced methods have a state-of-the-art
performance in convergence speed compared to the raw first-order gradient
methods. We then adopt the enhanced NAG method to implement homomorphic
logistic regression training, obtaining a comparable result by only $3$
iterations. There is a promising chance that $\texttt{quadratic gradient}$
could be used to enhance other first-order gradient methods for general
numerical optimization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The basic work of this paper, $\texttt{quadratic gradient}$ and the
  enhanced full batch NAG, was nearly finished in September 2019. The initial
  version of this paper was written in April 2020, rejected by ICANN 2020. The
  enhanced mini-batch NAG was introduced into this paper in September 2020 and
  later rejected by a special issue on the journal FGCS 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Large Language Models for Text-Attributed Graph Learning <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Pan, Zheng Zhang, Yifei Zhang, Yuntong Hu, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-Attributed Graphs (TAGs) are graphs of connected textual documents.
Graph models can efficiently learn TAGs, but their training heavily relies on
human-annotated labels, which are scarce or even unavailable in many
applications. Large language models (LLMs) have recently demonstrated
remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer
from scalability, cost, and privacy issues. Therefore, in this work, we focus
on synergizing LLMs and graph models with their complementary strengths by
distilling the power of LLMs to a local graph model on TAG learning. To address
the inherent gaps between LLMs (generative models for texts) and graph models
(discriminative models for graphs), we propose first to let LLMs teach an
interpreter with rich textual rationale and then let a student model mimic the
interpreter's reasoning without LLMs' textual rationale. Extensive experiments
validate the efficacy of our proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Cross-Layer Energy Optimizations in AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae-Won Chung, Nishil Talati, Mosharaf Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The "AI for Science, Energy, and Security" report from DOE outlines a
significant focus on developing and optimizing artificial intelligence
workflows for a foundational impact on a broad range of DOE missions. With the
pervasive usage of artificial intelligence (AI) and machine learning (ML) tools
and techniques, their energy efficiency is likely to become the gating factor
toward adoption. This is because generative AI (GenAI) models are massive
energy hogs: for instance, training a 200-billion parameter large language
model (LLM) at Amazon is estimated to have taken 11.9 GWh, which is enough to
power more than a thousand average U.S. households for a year. Inference
consumes even more energy, because a model trained once serve millions. Given
this scale, high energy efficiency is key to addressing the power delivery
problem of constructing and operating new supercomputers and datacenters
specialized for AI workloads. In that regard, we outline software- and
architecture-level research challenges and opportunities, setting the stage for
creating cross-layer energy optimizations in AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 Energy-Efficient Computing for Science Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Uncertainty-Based Explore for Index Construction and Retrieval in
  Recommendation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Jiang, Kaiqiang Wang, Yinlong Wang, Fengchang Lv, Taiyang Peng, Shuai Yang, Xianteng Wu, Pengye Zhang, Shuo Yuan, Yifan Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, the relevance and novelty of the final results are
selected through a cascade system of Matching -> Ranking -> Strategy. The
matching model serves as the starting point of the pipeline and determines the
upper bound of the subsequent stages. Balancing the relevance and novelty of
matching results is a crucial step in the design and optimization of
recommendation systems, contributing significantly to improving recommendation
quality. However, the typical matching algorithms have not simultaneously
addressed the relevance and novelty perfectly. One main reason is that deep
matching algorithms exhibit significant uncertainty when estimating items in
the long tail (e.g., due to insufficient training samples) items.The
uncertainty not only affects the training of the models but also influences the
confidence in the index construction and beam search retrieval process of these
models. This paper proposes the UICR (Uncertainty-based explore for Index
Construction and Retrieval) algorithm, which introduces the concept of
uncertainty modeling in the matching stage and achieves multi-task modeling of
model uncertainty and index uncertainty. The final matching results are
obtained by combining the relevance score and uncertainty score infered by the
model. Experimental results demonstrate that the UICR improves novelty without
sacrificing relevance on realworld industrial productive environments and
multiple open-source datasets. Remarkably, online A/B test results of display
advertising in Shopee demonstrates the effectiveness of the proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by cikm2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automation of Quantum Dot Measurement Analysis via Explainable Machine
  Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13699v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13699v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of quantum dot (QD) devices for quantum computing has
necessitated more efficient and automated methods for device characterization
and tuning. Many of the measurements acquired during the tuning process come in
the form of images that need to be properly analyzed to guide the subsequent
tuning steps. By design, features present in such images capture certain
behaviors or states of the measured QD devices. When considered carefully, such
features can aid the control and calibration of QD devices. An important
example of such images are so-called \textit{triangle plots}, which visually
represent current flow and reveal characteristics important for QD device
calibration. While image-based classification tools, such as convolutional
neural networks (CNNs), can be used to verify whether a given measurement is
\textit{good} and thus warrants the initiation of the next phase of tuning,
they do not provide any insights into how the device should be adjusted in the
case of \textit{bad} images. This is because CNNs sacrifice prediction and
model intelligibility for high accuracy. To ameliorate this trade-off, a recent
study introduced an image vectorization approach that relies on the Gabor
wavelet transform [1]. Here we propose an alternative vectorization method that
involves mathematical modeling of synthetic triangles to mimic the experimental
data. Using explainable boosting machines, we show that this new method offers
superior explainability of model prediction without sacrificing accuracy. This
work demonstrates the feasibility and advantages of applying explainable
machine learning techniques to the analysis of quantum dot measurements, paving
the way for further advances in automated and transparent QD device tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures, abbreviated version published in Proceedings of
  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,
  (Vancouver, Canada)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Local Explainability and Trust Scores with Random Forest
  Proximities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12428v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12428v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Rosaler, Dhruv Desai, Bhaskarjit Sarmah, Dimitrios Vamvourellis, Deran Onay, Dhagash Mehta, Stefano Pasquali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate a novel approach to explain the predictions and out of sample
performance of random forest (RF) regression and classification models by
exploiting the fact that any RF can be mathematically formulated as an adaptive
weighted K nearest-neighbors model. Specifically, we employ a recent result
that, for both regression and classification tasks, any RF prediction can be
rewritten exactly as a weighted sum of the training targets, where the weights
are RF proximities between the corresponding pairs of data points. We show that
this linearity facilitates a local notion of explainability of RF predictions
that generates attributions for any model prediction across observations in the
training set, and thereby complements established feature-based methods like
SHAP, which generate attributions for a model prediction across input features.
We show how this proximity-based approach to explainability can be used in
conjunction with SHAP to explain not just the model predictions, but also
out-of-sample performance, in the sense that proximities furnish a novel means
of assessing when a given model prediction is more or less likely to be
correct. We demonstrate this approach in the modeling of US corporate bond
prices and returns in both regression and classification cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Graph Topology-Aware <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Jiaxuan Zhao, Lingling Li, Licheng Jiao, Fang Liu, Shuyuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing efforts are dedicated to designing many topologies and graph-aware
strategies for the graph Transformer, which greatly improve the model's
representation capabilities. However, manually determining the suitable
Transformer architecture for a specific graph dataset or task requires
extensive expert knowledge and laborious trials. This paper proposes an
evolutionary graph Transformer architecture search framework (EGTAS) to
automate the construction of strong graph Transformers. We build a
comprehensive graph Transformer search space with the micro-level and
macro-level designs. EGTAS evolves graph Transformer topologies at the macro
level and graph-aware strategies at the micro level. Furthermore, a surrogate
model based on generic architectural coding is proposed to directly predict the
performance of graph Transformers, substantially reducing the evaluation cost
of evolutionary search. We demonstrate the efficacy of EGTAS across a range of
graph-level and node-level tasks, encompassing both small-scale and large-scale
graph datasets. Experimental results and ablation studies show that EGTAS can
construct high-performance architectures that rival state-of-the-art manual and
automated baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by IEEE Transactions on Neural Networks
  and Learning Systems. Copyright may be transferred without notice, after
  which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UnifiedNN: Efficient Neural Network Training on the Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sifat Ut Taki, Arthi Padmanabhan, Spyridon Mastorakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, cloud-based services are widely favored over the traditional
approach of locally training a Neural Network (NN) model. Oftentimes, a cloud
service processes multiple requests from users--thus training multiple NN
models concurrently. However, training NN models concurrently is a challenging
process, which typically requires significant amounts of available computing
resources and takes a long time to complete. In this paper, we present
UnifiedNN to effectively train multiple NN models concurrently on the cloud.
UnifiedNN effectively "combines" multiple NN models and features several memory
and time conservation mechanisms to train multiple NN models simultaneously
without impacting the accuracy of the training process. Specifically, UnifiedNN
merges multiple NN models and creates a large singular unified model in order
to efficiently train all models at once. We have implemented a prototype of
UnifiedNN in PyTorch and we have compared its performance with relevant
state-of-the-art frameworks. Our experimental results demonstrate that
UnifiedNN can reduce memory consumption by up to 53% and training time by up to
81% when compared with vanilla PyTorch without impacting the model training and
testing accuracy. Finally, our results indicate that UnifiedNN can reduce
memory consumption by up to 52% and training time by up to 41% when compared to
state-of-the-art frameworks when training multiple models concurrently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitation Learning from Purified Demonstrations <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunke Wang, Minjing Dong, Yukun Zhao, Bo Du, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning has emerged as a promising approach for addressing
sequential decision-making problems, with the assumption that expert
demonstrations are optimal. However, in real-world scenarios, most
demonstrations are often imperfect, leading to challenges in the effectiveness
of imitation learning. While existing research has focused on optimizing with
imperfect demonstrations, the training typically requires a certain proportion
of optimal demonstrations to guarantee performance. To tackle these problems,
we propose to purify the potential noises in imperfect demonstrations first,
and subsequently conduct imitation learning from these purified demonstrations.
Motivated by the success of diffusion model, we introduce a two-step
purification via diffusion process. In the first step, we apply a forward
diffusion process to smooth potential noises in imperfect demonstrations by
introducing additional noise. Subsequently, a reverse generative process is
utilized to recover the optimal demonstration from the diffused ones. We
provide theoretical evidence supporting our approach, demonstrating that the
distance between the purified and optimal demonstration can be bounded.
Empirical results on MuJoCo and RoboSuite demonstrate the effectiveness of our
method from different aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Variable-Block Chain with Adaptive Variable Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1912.03573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1912.03573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixiang Zhang, Lin Lin, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The architectures of deep neural networks (DNN) rely heavily on the
underlying grid structure of variables, for instance, the lattice of pixels in
an image. For general high dimensional data with variables not associated with
a grid, the multi-layer perceptron and deep belief network are often used.
However, it is frequently observed that those networks do not perform
competitively and they are not helpful for identifying important variables. In
this paper, we propose a framework that imposes on blocks of variables a chain
structure obtained by step-wise greedy search so that the DNN architecture can
leverage the constructed grid. We call this new neural network Deep
Variable-Block Chain (DVC). Because the variable blocks are used for
classification in a sequential manner, we further develop the capacity of
selecting variables adaptively according to a number of regions trained by a
decision tree. Our experiments show that DVC outperforms other generic DNNs and
other strong classifiers. Moreover, DVC can achieve high accuracy at much
reduced dimensionality and sometimes reveals drastically different sets of
relevant variables for different regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Over-Memorization During Natural, Robust and Catastrophic
  Overfitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runqi Lin, Chaojian Yu, Bo Han, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overfitting negatively impacts the generalization ability of deep neural
networks (DNNs) in both natural and adversarial training. Existing methods
struggle to consistently address different types of overfitting, typically
designing strategies that focus separately on either natural or adversarial
patterns. In this work, we adopt a unified perspective by solely focusing on
natural patterns to explore different types of overfitting. Specifically, we
examine the memorization effect in DNNs and reveal a shared behaviour termed
over-memorization, which impairs their generalization capacity. This behaviour
manifests as DNNs suddenly becoming high-confidence in predicting certain
training patterns and retaining a persistent memory for them. Furthermore, when
DNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit
high-confidence prediction for the corresponding natural pattern. These
findings motivate us to holistically mitigate different types of overfitting by
hindering the DNNs from over-memorization training patterns. To this end, we
propose a general framework, Distraction Over-Memorization (DOM), which
explicitly prevents over-memorization by either removing or augmenting the
high-confidence natural patterns. Extensive experiments demonstrate the
effectiveness of our proposed method in mitigating overfitting across various
training paradigms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with
  Diffusion-Controllable Adversaries <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00391v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00391v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Jer Chang, Francesco Pittaluga, Masayoshi Tomizuka, Wei Zhan, Manmohan Chandraker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the performance of autonomous vehicle planning algorithms
necessitates simulating long-tail safety-critical traffic scenarios. However,
traditional methods for generating such scenarios often fall short in terms of
controllability and realism; they also neglect the dynamics of agent
interactions. To address these limitations, we introduce SAFE-SIM, a novel
diffusion-based controllable closed-loop safety-critical simulation framework.
Our approach yields two distinct advantages: 1) generating realistic long-tail
safety-critical scenarios that closely reflect real-world conditions, and 2)
providing controllable adversarial behavior for more comprehensive and
interactive evaluations. We develop a novel approach to simulate
safety-critical scenarios through an adversarial term in the denoising process
of diffusion models, which allows an adversarial agent to challenge a planner
with plausible maneuvers while all agents in the scene exhibit reactive and
realistic behaviors. Furthermore, we propose novel guidance objectives and a
partial diffusion process that enables users to control key aspects of the
scenarios, such as the collision type and aggressiveness of the adversarial
agent, while maintaining the realism of the behavior. We validate our framework
empirically using the nuScenes and nuPlan datasets across multiple planners,
demonstrating improvements in both realism and controllability. These findings
affirm that diffusion models provide a robust and versatile foundation for
safety-critical, interactive traffic simulation, extending their utility across
the broader autonomous driving landscape. Project website:
https://safe-sim.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024; Project website: https://safe-sim.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic
  Spatio-Temporal Traffic Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08119v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08119v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lequan Lin, Dai Shi, Andi Han, Junbin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic forecasting, a crucial application of spatio-temporal graph (STG)
learning, has traditionally relied on deterministic models for accurate point
estimations. Yet, these models fall short of quantifying future uncertainties.
Recently, many probabilistic methods, especially variants of diffusion models,
have been proposed to fill this gap. However, existing diffusion methods
typically deal with individual sensors separately when generating future time
series, resulting in limited usage of spatial information in the probabilistic
learning process. In this work, we propose SpecSTG, a novel spectral diffusion
framework, to better leverage spatial dependencies and systematic patterns
inherent in traffic data. More specifically, our method generates the Fourier
representation of future time series, transforming the learning process into
the spectral domain enriched with spatial information. Additionally, our
approach incorporates a fast spectral graph convolution designed for Fourier
input, alleviating the computational burden associated with existing models.
Compared with state-of-the-arts, SpecSTG achieves up to 8% improvements on
point estimations and up to 0.78% improvements on quantifying future
uncertainties. Furthermore, SpecSTG's training and validation speed is 3.33X of
the most efficient existing diffusion method for STG forecasting. The source
code for SpecSTG is available at https://anonymous.4open.science/r/SpecSTG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs
  for Reduced hERG Liability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregory W. Kyro, Matthew T. Martin, Eric D. Watt, Victor S. Batista
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The link between in vitro hERG ion channel inhibition and subsequent in vivo
QT interval prolongation, a critical risk factor for the development of
arrythmias such as Torsade de Pointes, is so well established that in vitro
hERG activity alone is often sufficient to end the development of an otherwise
promising drug candidate. It is therefore of tremendous interest to develop
advanced methods for identifying hERG-active compounds in the early stages of
drug development, as well as for proposing redesigned compounds with reduced
hERG liability and preserved on-target potency. In this work, we present
CardioGenAI, a machine learning-based framework for re-engineering both
developmental and commercially available drugs for reduced hERG activity while
preserving their pharmacological activity. The framework incorporates novel
state-of-the-art discriminative models for predicting hERG channel activity, as
well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to
their potential implications in modulating the arrhythmogenic potential induced
by hERG channel blockade. We applied the complete framework to pimozide, an
FDA-approved antipsychotic agent that demonstrates high affinity to the hERG
channel, and generated 100 refined candidates. Remarkably, among the candidates
is fluspirilene, a compound which is of the same class of drugs
(diphenylmethanes) as pimozide and therefore has similar pharmacological
activity, yet exhibits over 700-fold weaker binding to hERG. We envision that
this method can effectively be applied to developmental compounds exhibiting
hERG liabilities to provide a means of rescuing drug development programs that
have stalled due to hERG-related safety concerns. We have made all of our
software open-source to facilitate integration of the CardioGenAI framework for
molecular hypothesis generation into drug discovery workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nemotron-4 340B Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Nvidia,  :, Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base,
Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open
access under the NVIDIA Open Model License Agreement, a permissive model
license that allows distribution, modification, and use of the models and its
outputs. These models perform competitively to open access models on a wide
range of evaluation benchmarks, and were sized to fit on a single DGX H100 with
8 GPUs when deployed in FP8 precision. We believe that the community can
benefit from these models in various research studies and commercial
applications, especially for generating synthetic data to train smaller
language models. Notably, over 98% of data used in our model alignment process
is synthetically generated, showcasing the effectiveness of these models in
generating synthetic data. To further support open research and facilitate
model development, we are also open-sourcing the synthetic data generation
pipeline used in our model alignment process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Filtered Corpus Training (FiCT) Shows that Language Models can
  Generalize from Indirect Evidence <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Patil, Jaap Jumelet, Yu Ying Chiu, Andy Lapastora, Peter Shen, Lexie Wang, Clevis Willrich, Shane Steinert-Threlkeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Filtered Corpus Training, a method that trains language
models (LMs) on corpora with certain linguistic constructions filtered out from
the training data, and uses it to measure the ability of LMs to perform
linguistic generalization on the basis of indirect evidence. We apply the
method to both LSTM and Transformer LMs (of roughly comparable size),
developing filtered corpora that target a wide range of linguistic phenomena.
Our results show that while transformers are better qua LMs (as measured by
perplexity), both models perform equally and surprisingly well on linguistic
generalization measures, suggesting that they are capable of generalizing from
indirect evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming in Transactions of the Association for Computational
  Linguistics (TACL). This is a pre-MIT Press publication version. For code and
  trained models, see http://github.com/CLMBRs/corpus-filtering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Efficacy of Text-Based Input Modalities for Action Anticipation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apoorva Beedu, Karan Samel, Irfan Essa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anticipating future actions is a highly challenging task due to the diversity
and scale of potential future actions; yet, information from different
modalities help narrow down plausible action choices. Each modality can provide
diverse and often complementary context for the model to learn from. While
previous multi-modal methods leverage information from modalities such as video
and audio, we primarily explore how text descriptions of actions and objects
can also lead to more accurate action anticipation by providing additional
contextual cues, e.g., about the environment and its contents. We propose a
Multi-modal Contrastive Anticipative Transformer (M-CAT), a video transformer
architecture that jointly learns from multi-modal features and text
descriptions of actions and objects. We train our model in two stages, where
the model first learns to align video clips with descriptions of future
actions, and is subsequently fine-tuned to predict future actions. Compared to
existing methods, M-CAT has the advantage of learning additional context from
two types of text inputs: rich descriptions of future actions during
pre-training, and, text descriptions for detected objects and actions during
modality feature fusion. Through extensive experimental evaluation, we
demonstrate that our model outperforms previous methods on the EpicKitchens
datasets, and show that using simple text descriptions of actions and objects
aid in more effective action anticipation. In addition, we examine the impact
of object and action information obtained via text, and perform extensive
ablations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Collaborative Fine-Tuning for On-Device Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Wagner, Dongyang Fan, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore on-device self-supervised collaborative fine-tuning of large
language models with limited local data availability. Taking inspiration from
the collaborative learning community, we introduce three distinct
trust-weighted gradient aggregation schemes: weight similarity-based,
prediction similarity-based and validation performance-based. To minimize
communication overhead, we integrate Low-Rank Adaptation (LoRA) and only
exchange LoRA weight updates. Our protocols, driven by prediction and
performance metrics, surpass both FedAvg and local fine-tuning methods, which
is particularly evident in realistic scenarios with more diverse local data
distributions. The results underscore the effectiveness of our approach in
addressing heterogeneity and scarcity within local datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Reinforcement Learning with Enhanced PPO for Safe Mobile Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamid Taheri, Seyed Rasoul Hosseini, Mohammad Ali Nekoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collision-free motion is essential for mobile robots. Most approaches to
collision-free and efficient navigation with wheeled robots require parameter
tuning by experts to obtain good navigation behavior. This study investigates
the application of deep reinforcement learning to train a mobile robot for
autonomous navigation in a complex environment. The robot utilizes LiDAR sensor
data and a deep neural network to generate control signals guiding it toward a
specified target while avoiding obstacles. We employ two reinforcement learning
algorithms in the Gazebo simulation environment: Deep Deterministic Policy
Gradient and proximal policy optimization. The study introduces an enhanced
neural network structure in the Proximal Policy Optimization algorithm to boost
performance, accompanied by a well-designed reward function to improve
algorithm efficacy. Experimental results conducted in both obstacle and
obstacle-free environments underscore the effectiveness of the proposed
approach. This research significantly contributes to the advancement of
autonomous robotics in complex environments through the application of deep
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under review by Int. J. of Intelligent Machines and
  Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Practical and Robust Safety Guarantees for Advanced Counterfactual
  Learning to Rank <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Gupta, Harrie Oosterhuis, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual learning to rank (CLTR) can be risky and, in various
circumstances, can produce sub-optimal models that hurt performance when
deployed. Safe CLTR was introduced to mitigate these risks when using inverse
propensity scoring to correct for position bias. However, the existing safety
measure for CLTR is not applicable to state-of-the-art CLTR methods, cannot
handle trust bias, and relies on specific assumptions about user behavior. Our
contributions are two-fold. First, we generalize the existing safe CLTR
approach to make it applicable to state-of-the-art doubly robust CLTR and trust
bias. Second, we propose a novel approach, proximal ranking policy optimization
(PRPO), that provides safety in deployment without assumptions about user
behavior. PRPO removes incentives for learning ranking behavior that is too
dissimilar to a safe ranking model. Thereby, PRPO imposes a limit on how much
learned models can degrade performance metrics, without relying on any specific
user assumptions. Our experiments show that both our novel safe doubly robust
method and PRPO provide higher performance than the existing safe inverse
propensity scoring approach. However, in unexpected circumstances, the safe
doubly robust approach can become unsafe and bring detrimental performance. In
contrast, PRPO always maintains safety, even in maximally adversarial
situations. By avoiding assumptions, PRPO is the first method with
unconditional safety in deployment that translates to robust safety for
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as full paper at CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Solve Robot Routing? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehui Huang, Guangyao Shi, Gaurav S. Sukhatme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Routing problems are common in mobile robotics, encompassing tasks such as
inspection, surveillance, and coverage. Depending on the objective and
constraints, these problems often reduce to variants of the Traveling Salesman
Problem (TSP), with solutions traditionally derived by translating high-level
objectives into an optimization formulation and using modern solvers to arrive
at a solution. Here, we explore the potential of Large Language Models (LLMs)
to replace the entire pipeline from tasks described in natural language to the
generation of robot routes. We systematically investigate the performance of
LLMs in robot routing by constructing a dataset with 80 unique robot routing
problems across 8 variants in both single and multi-robot settings. We evaluate
LLMs through three frameworks: single attempt, self-debugging, and
self-debugging with self-verification and various contexts, including
mathematical formulations, pseudo-code, and related research papers. Our
findings reveal that both self-debugging and self-verification enhance success
rates without significantly lowering the optimality gap. We observe
context-sensitive behavior - providing mathematical formulations as context
decreases the optimality gap but significantly decreases success rates and
providing pseudo-code and related research papers as context does not
consistently improve success rates or decrease the optimality gap. We identify
key challenges and propose future directions to enhance LLM performance in
solving robot routing problems. Our source code is available on the project
website: https://sites.google.com/view/words-to-routes/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Symposium of Robotics Research (ISRR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GP-VLS: A general-purpose vision language model for surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Schmidgall, Joseph Cho, Cyril Zakka, William Hiesinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgery requires comprehensive medical knowledge, visual assessment skills,
and procedural expertise. While recent surgical AI models have focused on
solving task-specific problems, there is a need for general-purpose systems
that can understand surgical scenes and interact through natural language. This
paper introduces GP-VLS, a general-purpose vision language model for surgery
that integrates medical and surgical knowledge with visual scene understanding.
For comprehensively evaluating general-purpose surgical models, we propose
SurgiQual, which evaluates across medical and surgical knowledge benchmarks as
well as surgical vision-language questions. To train GP-VLS, we develop six new
datasets spanning medical knowledge, surgical textbooks, and vision-language
pairs for tasks like phase recognition and tool identification. We show that
GP-VLS significantly outperforms existing open- and closed-source models on
surgical vision-language tasks, with 8-21% improvements in accuracy across
SurgiQual benchmarks. GP-VLS also demonstrates strong performance on medical
and surgical knowledge tests compared to open-source alternatives. Overall,
GP-VLS provides an open-source foundation for developing AI assistants to
support surgeons across a wide range of tasks and scenarios. The code and data
for this work is publicly available at gpvls-surgery-vlm.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Regional Explainability by Automatic and Model-agnostic Rule
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Chen, Tianyu Cui, Alexander Capstick, Nan Fletcher-Loyd, Payam Barnaghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Explainable AI, rule extraction translates model knowledge into logical
rules, such as IF-THEN statements, crucial for understanding patterns learned
by black-box models. This could significantly aid in fields like disease
diagnosis, disease progression estimation, or drug discovery. However, such
application domains often contain imbalanced data, with the class of interest
underrepresented. Existing methods inevitably compromise the performance of
rules for the minor class to maximise the overall performance. As the first
attempt in this field, we propose a model-agnostic approach for extracting
rules from specific subgroups of data, featuring automatic rule generation for
numerical features. This method enhances the regional explainability of machine
learning models and offers wider applicability compared to existing methods. We
additionally introduce a new method for selecting features to compose rules,
reducing computational costs in high-dimensional spaces. Experiments across
various datasets and models demonstrate the effectiveness of our methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding Language Model Reasoning with Planning Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05707v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05707v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, Alessandro Sordoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently attracted considerable interest
for their ability to perform complex reasoning tasks, such as chain-of-thought
(CoT) reasoning. However, most of the existing approaches to enhance this
ability rely heavily on data-driven methods, while neglecting the structural
aspects of the model's reasoning capacity. To encourage a more structural
generation of CoT steps, we propose a hierarchical generation scheme: we let
the LM generate a planning token at the start of each reasoning step,
intuitively serving as a high-level plan of the current step, and add their
embeddings to the model parameters. Our approach requires a negligible increase
in trainable parameters (0.001%) and can be applied through either full
fine-tuning or a more parameter-efficient scheme. We demonstrate our method's
effectiveness by applying it to three different LLMs, showing notable accuracy
improvements across three math word problem datasets and one multihop QA
dataset with respect to standard fine-tuning baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INSIGHT: Universal Neural Simulator for Analog Circuits Harnessing
  Autoregressive <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07346v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07346v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souradip Poddar, Youngmin Oh, Yao Lai, Hanqing Zhu, Bosun Hwang, David Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analog front-end design heavily relies on specialized human expertise and
costly trial-and-error simulations, which motivated many prior works on analog
design automation. However, efficient and effective exploration of the vast and
complex design space remains constrained by the time-consuming nature of SPICE
simulations, making effective design automation a challenging endeavor. In this
paper, we introduce INSIGHT, a GPU-powered, technology-agnostic, effective
universal neural simulator in the analog front-end design automation loop.
INSIGHT accurately predicts the performance metrics of analog circuits across
various technologies with just a few microseconds of inference time. Notably,
its autoregressive capabilities enable INSIGHT to accurately predict
simulation-costly critical transient specifications leveraging less expensive
performance metric information. The low cost and high fidelity feature make
INSIGHT a good substitute for standard simulators in analog front-end
optimization frameworks. INSIGHT is compatible with any optimization framework,
facilitating enhanced design space exploration for sample efficiency through
sophisticated offline learning and adaptation techniques. Our experiments
demonstrate that INSIGHT-M, a model-based batch reinforcement learning sizing
framework with INSIGHT as the accurate surrogate, only requires < 20 real-time
simulations with 100-1000x lower simulation costs and significant speedup over
existing sizing methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-05T00:00:00Z">2024-08-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">119</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent-INR: A Flexible Framework for Implicit Representations of Videos
  with Discriminative Semantics <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shishira R Maiya, Anubhav Gupta, Matthew Gwilliam, Max Ehrlich, Abhinav Shrivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Networks (INRs) have emerged as powerful representations to
encode all forms of data, including images, videos, audios, and scenes. With
video, many INRs for video have been proposed for the compression task, and
recent methods feature significant improvements with respect to encoding time,
storage, and reconstruction quality. However, these encoded representations
lack semantic meaning, so they cannot be used for any downstream tasks that
require such properties, such as retrieval. This can act as a barrier for
adoption of video INRs over traditional codecs as they do not offer any
significant edge apart from compression. To alleviate this, we propose a
flexible framework that decouples the spatial and temporal aspects of the video
INR. We accomplish this with a dictionary of per-frame latents that are learned
jointly with a set of video specific hypernetworks, such that given a latent,
these hypernetworks can predict the INR weights to reconstruct the given frame.
This framework not only retains the compression efficiency, but the learned
latents can be aligned with features from large vision models, which grants
them discriminative properties. We align these latents with CLIP and show good
performance for both compression and video retrieval tasks. By aligning with
VideoLlama, we are able to perform open-ended chat with our learned latents as
the visual inputs. Additionally, the learned latents serve as a proxy for the
underlying weights, allowing us perform tasks like video interpolation. These
semantic properties and applications, existing simultaneously with ability to
perform compression, interpolation, and superresolution properties, are a first
in this field of work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>equal contribution for first two authors; accepted to ECCV2024; 14
  pages, 4 tables, 10 figures in main paper, supplementary after bibliography</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lumina-m<span class="highlight-title">GPT</span>: Illuminate Flexible Photorealistic Text-to-Image Generation
  with Multimodal Generative <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, Peng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Lumina-mGPT, a family of multimodal autoregressive models capable
of various vision and language tasks, particularly excelling in generating
flexible photorealistic images from text descriptions. Unlike existing
autoregressive image generation approaches, Lumina-mGPT employs a pretrained
decoder-only transformer as a unified framework for modeling multimodal token
sequences. Our key insight is that a simple decoder-only transformer with
multimodal Generative PreTraining (mGPT), utilizing the next-token prediction
objective on massive interleaved text-image sequences, can learn broad and
general multimodal capabilities, thereby illuminating photorealistic
text-to-image generation. Building on these pretrained models, we propose
Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text
pairs to fully unlock their potential for high-aesthetic image synthesis at any
resolution while maintaining their general multimodal capabilities.
Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),
transforming Lumina-mGPT into a foundation model that seamlessly achieves
omnipotent task unification. The resulting model demonstrates versatile
multimodal capabilities, including visual generation tasks like flexible
text-to-image generation and controllable generation, visual recognition tasks
like segmentation and depth estimation, and vision-language tasks like
multiturn visual question answering. Additionally, we analyze the differences
and similarities between diffusion-based and autoregressive methods in a direct
comparison.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/Alpha-VLLM/Lumina-mGPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Using Quasirandom Sequences in Machine Learning for Model Weight
  Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andriy Miranskyy, Adam Sorrenti, Viral Thakar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of training neural networks directly impacts computational
costs, resource allocation, and model development timelines in machine learning
applications. An optimizer's ability to train the model adequately (in terms of
trained model performance) depends on the model's initial weights. Model weight
initialization schemes use pseudorandom number generators (PRNGs) as a source
of randomness.
  We investigate whether substituting PRNGs for low-discrepancy quasirandom
number generators (QRNGs) -- namely Sobol' sequences -- as a source of
randomness for initializers can improve model performance. We examine
Multi-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long
Short-Term Memory (LSTM), and Transformer architectures trained on MNIST,
CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses
ten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);
Orthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with
weights set using PRNG- and QRNG-based initializers are compared pairwise for
each combination of dataset, architecture, optimizer, and initialization
scheme.
  Our findings indicate that QRNG-based neural network initializers either
reach a higher accuracy or achieve the same accuracy more quickly than
PRNG-based initializers in 60% of the 120 experiments conducted. Thus, using
QRNG-based initializers instead of PRNG-based initializers can speed up and
improve model training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive 3D Medical Image Segmentation with SAM 2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyun Shen, Wenhao Li, Yuhang Shi, Xiangfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive medical image segmentation (IMIS) has shown significant potential
in enhancing segmentation accuracy by integrating iterative feedback from
medical professionals. However, the limited availability of enough 3D medical
data restricts the generalization and robustness of most IMIS methods. The
Segment Anything Model (SAM), though effective for 2D images, requires
expensive semi-auto slice-by-slice annotations for 3D medical images. In this
paper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta
SAM model trained on videos, for 3D medical image segmentation. By treating
sequential 2D slices of 3D images as video frames, SAM 2 can fully
automatically propagate annotations from a single frame to the entire 3D
volume. We propose a practical pipeline for using SAM 2 in 3D medical image
segmentation and present key findings highlighting its efficiency and potential
for further optimization. Concretely, numerical experiments on the BraTS2020
and the medical segmentation decathlon datasets demonstrate that SAM 2 still
has a gap with supervised methods but can narrow the gap in specific settings
and organ types, significantly reducing the annotation burden on medical
professionals. Our code will be open-sourced and available at
https://github.com/Chuyun-Shen/SAM_2_Medical_3D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VidGen-1M: A Large-Scale <span class="highlight-title">Dataset</span> for Text-to-video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, Hao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of video-text pairs fundamentally determines the upper bound of
text-to-video models. Currently, the datasets used for training these models
suffer from significant shortcomings, including low temporal consistency,
poor-quality captions, substandard video quality, and imbalanced data
distribution. The prevailing video curation process, which depends on image
models for tagging and manual rule-based curation, leads to a high
computational load and leaves behind unclean data. As a result, there is a lack
of appropriate training datasets for text-to-video models. To address this
problem, we present VidGen-1M, a superior training dataset for text-to-video
models. Produced through a coarse-to-fine curation strategy, this dataset
guarantees high-quality videos and detailed captions with excellent temporal
consistency. When used to train the video generation model, this dataset has
led to experimental results that surpass those obtained with other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://sais-fuxi.github.io/projects/vidgen-1m</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOWOv3: An Efficient and Generalized Framework for Human Action
  Detection and Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc Manh Nguyen Dang, Viet Hang Duong, Jia Ching Wang, Nhan Bui Duc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new framework called YOWOv3, which is an improved
version of YOWOv2, designed specifically for the task of Human Action Detection
and Recognition. This framework is designed to facilitate extensive
experimentation with different configurations and supports easy customization
of various components within the model, reducing efforts required for
understanding and modifying the code. YOWOv3 demonstrates its superior
performance compared to YOWOv2 on two widely used datasets for Human Action
Detection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessor
model YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2,
respectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model -
YOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33%
and 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate that
YOWOv3 significantly reduces the number of parameters and GFLOPs while still
achieving comparable performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local
  Attention and Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Fu, Chaoqi Chen, Yizhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Transformer-based diffusion models have shown remarkable performance,
largely attributed to the ability of the self-attention mechanism to accurately
capture both global and local contexts by computing all-pair interactions among
input tokens. However, their quadratic complexity poses significant
computational challenges for long-sequence inputs. Conversely, a recent state
space model called Mamba offers linear complexity by compressing a filtered
global context into a hidden state. Despite its efficiency, compression
inevitably leads to information loss of fine-grained local dependencies among
tokens, which are crucial for effective visual generative modeling. Motivated
by these observations, we introduce Local Attentional Mamba (LaMamba) blocks
that combine the strengths of self-attention and Mamba, capturing both global
contexts and local details with linear complexity. Leveraging the efficient
U-Net architecture, our model exhibits exceptional scalability and surpasses
the performance of DiT across various model scales on ImageNet at 256x256
resolution, all while utilizing substantially fewer GFLOPs and a comparable
number of parameters. Compared to state-of-the-art diffusion models on ImageNet
256x256 and 512x512, our largest model presents notable advantages, such as a
reduction of up to 62\% GFLOPs compared to DiT-XL/2, while achieving superior
performance with comparable or fewer parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelling Visual Semantics via Image Captioning to extract Enhanced
  Multi-Level Cross-Modal Semantic Incongruity Representation with Attention
  for Multimodal Sarcasm Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajal Aggarwal, Ananya Pandey, Dinesh Kumar Vishwakarma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sarcasm is a type of irony, characterized by an inherent mismatch between the
literal interpretation and the intended connotation. Though sarcasm detection
in text has been extensively studied, there are situations in which textual
input alone might be insufficient to perceive sarcasm. The inclusion of
additional contextual cues, such as images, is essential to recognize sarcasm
in social media data effectively. This study presents a novel framework for
multimodal sarcasm detection that can process input triplets. Two components of
these triplets comprise the input text and its associated image, as provided in
the datasets. Additionally, a supplementary modality is introduced in the form
of descriptive image captions. The motivation behind incorporating this visual
semantic representation is to more accurately capture the discrepancies between
the textual and visual content, which are fundamental to the sarcasm detection
task. The primary contributions of this study are: (1) a robust textual feature
extraction branch that utilizes a cross-lingual language model; (2) a visual
feature extraction branch that incorporates a self-regulated residual ConvNet
integrated with a lightweight spatially aware attention module; (3) an
additional modality in the form of image captions generated using an
encoder-decoder architecture capable of reading text embedded in images; (4)
distinct attention modules to effectively identify the incongruities between
the text and two levels of image representations; (5) multi-level cross-domain
semantic incongruity representation achieved through feature fusion. Compared
with cutting-edge baselines, the proposed model achieves the best accuracy of
92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and
MultiBully datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning-based Multi Modal Architecture for Emoticon
  Prediction by Employing Image-Text Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananya Pandey, Dinesh Kumar Vishwakarma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emoticons are symbolic representations that generally accompany the
textual content to visually enhance or summarize the true intention of a
written message. Although widely utilized in the realm of social media, the
core semantics of these emoticons have not been extensively explored based on
multiple modalities. Incorporating textual and visual information within a
single message develops an advanced way of conveying information. Hence, this
research aims to analyze the relationship among sentences, visuals, and
emoticons. For an orderly exposition, this paper initially provides a detailed
examination of the various techniques for extracting multimodal features,
emphasizing the pros and cons of each method. Through conducting a
comprehensive examination of several multimodal algorithms, with specific
emphasis on the fusion approaches, we have proposed a novel contrastive
learning based multimodal architecture. The proposed model employs the joint
training of dual-branch encoder along with the contrastive learning to
accurately map text and images into a common latent space. Our key finding is
that by integrating the principle of contrastive learning with that of the
other two branches yields superior results. The experimental results
demonstrate that our suggested methodology surpasses existing multimodal
approaches in terms of accuracy and robustness. The proposed model attained an
accuracy of 91% and an MCC-score of 90% while assessing emoticons using the
Multimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidence
that deep features acquired by contrastive learning are more efficient,
suggesting that the proposed fusion technique also possesses strong
generalisation capabilities for recognising emoticons across several modes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modality Clustering-based Self-Labeling for Multimodal Data
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paweł Zyblewski, Leandro L. Minku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technological advances facilitate the ability to acquire multimodal data,
posing a challenge for recognition systems while also providing an opportunity
to use the heterogeneous nature of the information to increase the
generalization capability of models. An often overlooked issue is the cost of
the labeling process, which is typically high due to the need for a significant
investment in time and money associated with human experts. Existing
semi-supervised learning methods often focus on operating in the feature space
created by the fusion of available modalities, neglecting the potential for
cross-utilizing complementary information available in each modality. To
address this problem, we propose Cross-Modality Clustering-based Self-Labeling
(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances
belonging to each modality in the deep feature space and then propagates known
labels within the resulting clusters. Next, information about the instances'
class membership in each modality is exchanged based on the Euclidean distance
to ensure more accurate labeling. Experimental evaluation conducted on 20
datasets derived from the MM-IMDb dataset indicates that cross-propagation of
labels between modalities -- especially when the number of pre-labeled
instances is small -- can allow for more reliable labeling and thus increase
the classification performance in each modality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HQOD: Harmonious Quantization for Object Detection <span class="chip">ICME</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Huang, Zhiwei Dong, Song-Lu Chen, Ruiyao Zhang, Shutong Ti, Feng Chen, Xu-Cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task inharmony problem commonly occurs in modern object detectors, leading to
inconsistent qualities between classification and regression tasks. The
predicted boxes with high classification scores but poor localization positions
or low classification scores but accurate localization positions will worsen
the performance of detectors after Non-Maximum Suppression. Furthermore, when
object detectors collaborate with Quantization-Aware Training (QAT), we observe
that the task inharmony problem will be further exacerbated, which is
considered one of the main causes of the performance degradation of quantized
detectors. To tackle this issue, we propose the Harmonious Quantization for
Object Detection (HQOD) framework, which consists of two components. Firstly,
we propose a task-correlated loss to encourage detectors to focus on improving
samples with lower task harmony quality during QAT. Secondly, a harmonious
Intersection over Union (IoU) loss is incorporated to balance the optimization
of the regression branch across different IoU levels. The proposed HQOD can be
easily integrated into different QAT algorithms and detectors. Remarkably, on
the MS COCO dataset, our 4-bit ATSS with ResNet-50 backbone achieves a
state-of-the-art mAP of 39.6%, even surpassing the full-precision one.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Multimedia and Expo (ICME),
  July 15 - July 19, 2024, Niagra Falls, Ontario, Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh
  Tokenization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MeshAnything V2, an autoregressive transformer that generates
Artist-Created Meshes (AM) aligned to given shapes. It can be integrated with
various 3D asset production pipelines to achieve high-quality, highly
controllable AM generation. MeshAnything V2 surpasses previous methods in both
efficiency and performance using models of the same size. These improvements
are due to our newly proposed mesh tokenization method: Adjacent Mesh
Tokenization (AMT). Different from previous methods that represent each face
with three vertices, AMT uses a single vertex whenever possible. Compared to
previous methods, AMT requires about half the token sequence length to
represent the same mesh in average. Furthermore, the token sequences from AMT
are more compact and well-structured, fundamentally benefiting AM generation.
Our extensive experiments show that AMT significantly improves the efficiency
and performance of AM generation. Project Page:
https://buaacyw.github.io/meshanything-v2/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://buaacyw.github.io/meshanything-v2/ Github:
  https://github.com/buaacyw/MeshAnythingV2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Pore Location of PBF-LB/M Processes with Segmentation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans Aoyang Zhou, Jan Theunissen, Marco Kemmerling, Anas Abdelrazeq, Johannes Henrich Schleifenbaum, Robert H. Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliably manufacturing defect free products is still an open challenge for
Laser Powder Bed Fusion processes. Particularly, pores that occur frequently
have a negative impact on mechanical properties like fatigue performance.
Therefore, an accurate localisation of pores is mandatory for quality
assurance, but requires time-consuming post-processing steps like computer
tomography scans. Although existing solutions using in-situ monitoring data can
detect pore occurrence within a layer, they are limited in their localisation
precision. Therefore, we propose a pore localisation approach that estimates
their position within a single layer using a Gaussian kernel density
estimation. This allows segmentation models to learn the correlation between
in-situ monitoring data and the derived probability distribution of pore
occurrence. Within our experiments, we compare the prediction performance of
different segmentation models depending on machine parameter configuration and
geometry features. From our results, we conclude that our approach allows a
precise localisation of pores that requires minimal data preprocessing. Our
research extends the literature by providing a foundation for more precise pore
detection systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, This work has been submitted to the Journal
  Progress in Additive Manufacturing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic rating of incomplete hippocampal inversions evaluated across
  multiple cohorts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Hemforth, Baptiste Couvy-Duchesne, Kevin De Matos, Camille Brianceau, Matthieu Joulot, Tobias Banaschewski, Arun L. W. Bokde, Sylvane Desrivières, Herta Flor, Antoine Grigis, Hugh Garavan, Penny Gowland, Andreas Heinz, Rüdiger Brühl, Jean-Luc Martinot, Marie-Laure Paillère Martinot, Eric Artiges, Dimitri Papadopoulos, Herve Lemaitre, Tomas Paus, Luise Poustka, Sarah Hohmann, Nathalie Holz, Juliane H. Fröhner, Michael N. Smolka, Nilakshi Vaidya, Henrik Walter, Robert Whelan, Gunter Schumann, Christian Büchel, JB Poline, Bernd Itterman, Vincent Frouin, Alexandre Martin, IMAGEN study group, Claire Cury, Olivier Colliot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal
malrotation, is an atypical anatomical pattern of the hippocampus found in
about 20% of the general population. IHI can be visually assessed on coronal
slices of T1 weighted MR images, using a composite score that combines four
anatomical criteria. IHI has been associated with several brain disorders
(epilepsy, schizophrenia). However, these studies were based on small samples.
Furthermore, the factors (genetic or environmental) that contribute to the
genesis of IHI are largely unknown. Large-scale studies are thus needed to
further understand IHI and their potential relationships to neurological and
psychiatric disorders. However, visual evaluation is long and tedious,
justifying the need for an automatic method. In this paper, we propose, for the
first time, to automatically rate IHI. We proceed by predicting four anatomical
criteria, which are then summed up to form the IHI score, providing the
advantage of an interpretable score. We provided an extensive experimental
investigation of different machine learning methods and training strategies. We
performed automatic rating using a variety of deep learning models (conv5-FC3,
ResNet and SECNN) as well as a ridge regression. We studied the generalization
of our models using different cohorts and performed multi-cohort learning. We
relied on a large population of 2,008 participants from the IMAGEN study, 993
and 403 participants from the QTIM/QTAB studies as well as 985 subjects from
the UKBiobank. We showed that deep learning models outperformed a ridge
regression. We demonstrated that the performances of the conv5-FC3 network were
at least as good as more complex networks while maintaining a low complexity
and computation time. We showed that training on a single cohort may lack in
variability while training on several cohorts improves generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:016</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiranjeev Chiranjeev, Muskan Dosi, Kartik Thakral, Mayank Vatsa, Richa Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional deep learning models rely on methods such as softmax
cross-entropy and ArcFace loss for tasks like classification and face
recognition. These methods mainly explore angular features in a hyperspherical
space, often resulting in entangled inter-class features due to dense angular
data across many classes. In this paper, a new field of feature exploration is
proposed known as HyperSpaceX which enhances class discrimination by exploring
both angular and radial dimensions in multi-hyperspherical spaces, facilitated
by a novel DistArc loss. The proposed DistArc loss encompasses three feature
arrangement components: two angular and one radial, enforcing intra-class
binding and inter-class separation in multi-radial arrangement, improving
feature discriminability. Evaluation of HyperSpaceX framework for the novel
representation utilizes a proposed predictive measure that accounts for both
angular and radial elements, providing a more comprehensive assessment of model
accuracy beyond standard metrics. Experiments across seven object
classification and six face recognition datasets demonstrate state-of-the-art
(SoTA) results obtained from HyperSpaceX, achieving up to a 20% performance
improvement on large-scale object datasets in lower dimensions and up to 6%
gain in higher dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Conditional Multi-Modal <span class="highlight-title">Prompt</span>s for Zero-shot HOI Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Lei, Shaofeng Yin, Yuxin Peng, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier
topic due to its capability to detect HOIs beyond a predefined set of
categories. This task entails not only identifying the interactiveness of
human-object pairs and localizing them but also recognizing both seen and
unseen interaction categories. In this paper, we introduce a novel framework
for zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP.
This approach enhances the generalization of large foundation models, such as
CLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learning
methods, we propose learning decoupled vision and language prompts for
interactiveness-aware visual feature extraction and generalizable interaction
classification, respectively. Specifically, we integrate prior knowledge of
different granularity into conditional vision prompts, including an
input-conditioned instance prior and a global spatial pattern prior. The former
encourages the image encoder to treat instances belonging to seen or
potentially unseen HOI concepts equally while the latter provides
representative plausible spatial configuration of the human and object under
interaction. Besides, we employ language-aware prompt learning with a
consistency constraint to preserve the knowledge of the large foundation model
to enable better generalization in the text branch. Extensive experiments
demonstrate the efficacy of our detector with conditional multi-modal prompts,
outperforming previous state-of-the-art on unseen classes of various zero-shot
settings. The code and models are available at
\url{https://github.com/ltttpku/CMMP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness and Bias Mitigation in Computer Vision: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Dehdashtian, Ruozhen He, Yi Li, Guha Balakrishnan, Nuno Vasconcelos, Vicente Ordonez, Vishnu Naresh Boddeti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision systems have witnessed rapid progress over the past two
decades due to multiple advances in the field. As these systems are
increasingly being deployed in high-stakes real-world applications, there is a
dire need to ensure that they do not propagate or amplify any discriminatory
tendencies in historical or human-curated data or inadvertently learn biases
from spurious correlations. This paper presents a comprehensive survey on
fairness that summarizes and sheds light on ongoing trends and successes in the
context of computer vision. The topics we discuss include 1) The origin and
technical definitions of fairness drawn from the wider fair machine learning
literature and adjacent disciplines. 2) Work that sought to discover and
analyze biases in computer vision systems. 3) A summary of methods proposed to
mitigate bias in computer vision systems in recent years. 4) A comprehensive
summary of resources and datasets produced by researchers to measure, analyze,
and mitigate bias and enhance fairness. 5) Discussion of the field's success,
continuing trends in the context of multimodal foundation and generative
models, and gaps that still need to be addressed. The presented
characterization should help researchers understand the importance of
identifying and mitigating bias in computer vision and the state of the field
and identify potential directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An investigation into the causes of race bias in AI-based cine CMR
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiarna Lee, Esther Puyol-Anton, Bram Ruijsink, Sebastien Roujol, Theodore Barfoot, Shaheim Ogbomo-Harmitt, Miaojing Shi, Andrew P. King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) methods are being used increasingly for the
automated segmentation of cine cardiac magnetic resonance (CMR) imaging.
However, these methods have been shown to be subject to race bias, i.e. they
exhibit different levels of performance for different races depending on the
(im)balance of the data used to train the AI model. In this paper we
investigate the source of this bias, seeking to understand its root cause(s) so
that it can be effectively mitigated. We perform a series of classification and
segmentation experiments on short-axis cine CMR images acquired from Black and
White subjects from the UK Biobank and apply AI interpretability methods to
understand the results. In the classification experiments, we found that race
can be predicted with high accuracy from the images alone, but less accurately
from ground truth segmentations, suggesting that the distributional shift
between races, which is often the cause of AI bias, is mostly image-based
rather than segmentation-based. The interpretability methods showed that most
attention in the classification models was focused on non-heart regions, such
as subcutaneous fat. Cropping the images tightly around the heart reduced
classification accuracy to around chance level. Similarly, race can be
predicted from the latent representations of a biased segmentation model,
suggesting that race information is encoded in the model. Cropping images
tightly around the heart reduced but did not eliminate segmentation bias. We
also investigate the influence of possible confounders on the bias observed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attenuation-adjusted deep learning of pore defects in 2D radiographs of
  additive manufacturing powders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Bjerregaard, David Schumacher, Jon Sporring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The presence of gas pores in metal feedstock powder for additive
manufacturing greatly affects the final AM product. Since current porosity
analysis often involves lengthy X-ray computed tomography (XCT) scans with a
full rotation around the sample, motivation exists to explore methods that
allow for high throughput -- possibly enabling in-line porosity analysis during
manufacturing. Through labelling pore pixels on single 2D radiographs of
powders, this work seeks to simulate such future efficient setups. High
segmentation accuracy is achieved by combining a model of X-ray attenuation
through particles with a variant of the widely applied UNet architecture;
notably, F1-score increases by $11.4\%$ compared to the baseline UNet. The
proposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)
making tight particle cutouts, and 3) subtracting an ideal particle without
pores generated from a distance map inspired by Lambert-Beers law. This paper
explores four image processing methods, where the fastest (yet still
unoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,
and the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalable
nature, these strategies can be involved in making high throughput porosity
analysis of metal feedstock powder for additive manufacturing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Implementation on https://github.com/yhsure/porosity</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FPT+: A Parameter and Memory Efficient Transfer Learning Method for
  High-resolution Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of large-scale pre-trained models has established fine-tuning as
a standard method for achieving significant improvements in downstream tasks.
However, fine-tuning the entire parameter set of a pre-trained model is costly.
Parameter-efficient transfer learning (PETL) has recently emerged as a
cost-effective alternative for adapting pre-trained models to downstream tasks.
Despite its advantages, the increasing model size and input resolution present
challenges for PETL, as the training memory consumption is not reduced as
effectively as the parameter usage. In this paper, we introduce Fine-grained
Prompt Tuning plus (FPT+), a PETL method designed for high-resolution medical
image classification, which significantly reduces memory consumption compared
to other PETL methods. FPT+ performs transfer learning by training a
lightweight side network and accessing pre-trained knowledge from a large
pre-trained model (LPM) through fine-grained prompts and fusion modules.
Specifically, we freeze the LPM and construct a learnable lightweight side
network. The frozen LPM processes high-resolution images to extract
fine-grained features, while the side network employs the corresponding
down-sampled low-resolution images to minimize the memory usage. To enable the
side network to leverage pre-trained knowledge, we propose fine-grained prompts
and fusion modules, which collaborate to summarize information through the
LPM's intermediate activations. We evaluate FPT+ on eight medical image
datasets of varying sizes, modalities, and complexities. Experimental results
demonstrate that FPT+ outperforms other PETL methods, using only 1.03% of the
learnable parameters and 3.18% of the memory required for fine-tuning an entire
ViT-B model. Our code is available at https://github.com/YijinHuang/FPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FE-Adapter: Adapting Image-based Emotion Classifiers to Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyank N Gowda, Boyan Gao, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing large pre-trained models for specific tasks has yielded impressive
results. However, fully fine-tuning these increasingly large models is becoming
prohibitively resource-intensive. This has led to a focus on more
parameter-efficient transfer learning, primarily within the same modality. But
this approach has limitations, particularly in video understanding where
suitable pre-trained models are less common. Addressing this, our study
introduces a novel cross-modality transfer learning approach from images to
videos, which we call parameter-efficient image-to-video transfer learning. We
present the Facial-Emotion Adapter (FE-Adapter), designed for efficient
fine-tuning in video tasks. This adapter allows pre-trained image models, which
traditionally lack temporal processing capabilities, to analyze dynamic video
content efficiently. Notably, it uses about 15 times fewer parameters than
previous methods, while improving accuracy. Our experiments in video emotion
recognition demonstrate that the FE-Adapter can match or even surpass existing
fine-tuning and video emotion models in both performance and efficiency. This
breakthrough highlights the potential for cross-modality approaches in
enhancing the capabilities of AI models, particularly in fields like video
emotion analysis where the demand for efficiency and accuracy is constantly
rising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-weather Cross-view Geo-localization Using Denoising Diffusion
  Models <span class="chip">ACM MM24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongtong Feng, Qing Li, Xin Wang, Mingzi Wang, Guangyao Li, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-view geo-localization in GNSS-denied environments aims to determine an
unknown location by matching drone-view images with the correct geo-tagged
satellite-view images from a large gallery. Recent research shows that learning
discriminative image representations under specific weather conditions can
significantly enhance performance. However, the frequent occurrence of unseen
extreme weather conditions hinders progress. This paper introduces MCGF, a
Multi-weather Cross-view Geo-localization Framework designed to dynamically
adapt to unseen weather conditions. MCGF establishes a joint optimization
between image restoration and geo-localization using denoising diffusion
models. For image restoration, MCGF incorporates a shared encoder and a
lightweight restoration module to help the backbone eliminate weather-specific
information. For geo-localization, MCGF uses EVA-02 as a backbone for feature
extraction, with cross-entropy loss for training and cosine distance for
testing. Extensive experiments on University160k-WX demonstrate that MCGF
achieves competitive results for geo-localization in varying weather
conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM24 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensorial template matching for fast cross-correlation with rotations
  and its application for tomography <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Martinez-Sanchez, Ulrike Homberg, José María Almira, Harold Phelippeau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection is a main task in computer vision. Template matching is the
reference method for detecting objects with arbitrary templates. However,
template matching computational complexity depends on the rotation accuracy,
being a limiting factor for large 3D images (tomograms). Here, we implement a
new algorithm called tensorial template matching, based on a mathematical
framework that represents all rotations of a template with a tensor field.
Contrary to standard template matching, the computational complexity of the
presented algorithm is independent of the rotation accuracy. Using both,
synthetic and real data from tomography, we demonstrate that tensorial template
matching is much faster than template matching and has the potential to improve
its accuracy
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in The 18th European Conference on Computer Vision ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point
  Cloud Registration <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongxin Yao, Yixin Xuan, Xinyang Li, Yu Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-point cloud registration aims to determine the relative camera pose
of an RGB image with respect to a point cloud. It plays an important role in
camera localization within pre-built LiDAR maps. Despite the modality gaps,
most learning-based methods establish 2D-3D point correspondences in feature
space without any feedback mechanism for iterative optimization, resulting in
poor accuracy and interpretability. In this paper, we propose to reformulate
the registration procedure as an iterative Markov decision process, allowing
for incremental adjustments to the camera pose based on each intermediate
state. To achieve this, we employ reinforcement learning to develop a
cross-modal registration agent (CMR-Agent), and use imitation learning to
initialize its registration policy for stability and quick-start of the
training. According to the cross-modal observations, we propose a 2D-3D hybrid
state representation that fully exploits the fine-grained features of RGB
images while reducing the useless neutral states caused by the spatial
truncation of camera frustum. Additionally, the overall framework is
well-designed to efficiently reuse one-shot cross-modal embeddings, avoiding
repetitive and time-consuming feature extraction. Extensive experiments on the
KITTI-Odometry and NuScenes datasets demonstrate that CMR-Agent achieves
competitive accuracy and efficiency in registration. Once the one-shot
embeddings are completed, each iteration only takes a few milliseconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MaFreeI2P: A Matching-Free Image-to-Point Cloud Registration Paradigm
  with Active Camera Pose Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongxin Yao, Xinyang Li, Yixin Xuan, Yu Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-point cloud registration seeks to estimate their relative camera
pose, which remains an open question due to the data modality gaps. The recent
matching-based methods tend to tackle this by building 2D-3D correspondences.
In this paper, we reveal the information loss inherent in these methods and
propose a matching-free paradigm, named MaFreeI2P. Our key insight is to
actively retrieve the camera pose in SE(3) space by contrasting the geometric
features between the point cloud and the query image. To achieve this, we first
sample a set of candidate camera poses and construct their cost volume using
the cross-modal features. Superior to matching, cost volume can preserve more
information and its feature similarity implicitly reflects the confidence level
of the sampled poses. Afterwards, we employ a convolutional network to
adaptively formulate a similarity assessment function, where the input cost
volume is further improved by filtering and pose-based weighting. Finally, we
update the camera pose based on the similarity scores, and adopt a heuristic
strategy to iteratively shrink the pose sampling space for convergence. Our
MaFreeI2P achieves a very competitive registration accuracy and recall on the
KITTI-Odometry and Apollo-DaoxiangLake datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Conference on Multimedia Expo 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross Psuedo Supervision Framework for Sparsely Labelled Geo-spatial
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Dixit, Naman Srivastava, Joel D Joy, Rohan Olikara, Swarup E, Rakshit Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Land Use Land Cover (LULC) mapping is essential for urban and resource
planning and is one of the key elements in developing smart and sustainable
cities. This study introduces a semi-supervised segmentation model for LULC
prediction using high-resolution satellite images with a huge diversity in data
distributions in different areas from the country of India. Our approach
ensures a robust generalization across different types of buildings, roads,
trees, and water bodies within these distinct areas. We propose a modified
Cross Pseudo Supervision framework to train image segmentation models on
sparsely labelled data. The proposed framework addresses the limitations of the
popular "Cross Pseudo Supervision" technique for semi-supervised learning.
Specifically, it tackles the challenges of training segmentation models on
noisy satellite image data with sparse and inaccurate labels. This
comprehensive approach enhances the accuracy and utility of LULC mapping for
various urban planning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC
  2024 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Wang, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper delineates the visual speech recognition (VSR) system introduced
by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech
Recognition Challenge (CNVSRC 2024), engaging in all four tracks, including the
fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In
terms of data processing, we leverage the lip motion extractor from the
baseline1 to produce multiscale video data. Besides, various augmentation
techniques are applied during training, encompassing speed perturbation, random
rotation, horizontal flipping, and color transformation. The VSR model adopts
an end-to-end architecture with joint CTC/attention loss, introducing Enhanced
ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional
Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker
Task and 34.30% CER for the Multi-Speaker Task, securing second place in the
open track of the Single-Speaker Task and first place in the other three
tracks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, 2 figures, CNVSRC 2024 System Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StoDIP: Efficient 3D MRF image reconstruction with deep image priors and
  stochastic iterations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Perla Mayo, Matteo Cencini, Carolin M. Pirkl, Marion I. Menzel, Michela Tosetti, Bjoern H. Menze, Mohammad Golbabaee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to
quantitative MRI for multiparametric tissue mapping. The reconstruction of
quantitative maps requires tailored algorithms for removing aliasing artefacts
from the compressed sampled MRF acquisitions. Within approaches found in the
literature, many focus solely on two-dimensional (2D) image reconstruction,
neglecting the extension to volumetric (3D) scans despite their higher
relevance and clinical value. A reason for this is that transitioning to 3D
imaging without appropriate mitigations presents significant challenges,
including increased computational cost and storage requirements, and the need
for large amount of ground-truth (artefact-free) data for training. To address
these issues, we introduce StoDIP, a new algorithm that extends the
ground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging.
StoDIP employs memory-efficient stochastic updates across the multicoil MRF
data, a carefully selected neural network architecture, as well as faster
nonuniform FFT (NUFFT) transformations. This enables a faster convergence
compared against a conventional DIP implementation without these features.
Tested on a dataset of whole-brain scans from healthy volunteers, StoDIP
demonstrated superior performance over the ground-truth-free reconstruction
baselines, both quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 1 table, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Earth System Data Cubes: Avenues for advancing Earth system research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Montero, Guido Kraemer, Anca Anghelea, César Aybar, Gunnar Brandt, Gustau Camps-Valls, Felix Cremer, Ida Flik, Fabian Gans, Sarah Habershon, Chaonan Ji, Teja Kattenborn, Laura Martínez-Ferrer, Francesco Martinuzzi, Martin Reinhardt, Maximilian Söchting, Khalil Teber, Miguel D. Mahecha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Earth system science have been marked by the
exponential increase in the availability of diverse, multivariate datasets
characterised by moderate to high spatio-temporal resolutions. Earth System
Data Cubes (ESDCs) have emerged as one suitable solution for transforming this
flood of data into a simple yet robust data structure. ESDCs achieve this by
organising data into an analysis-ready format aligned with a spatio-temporal
grid, facilitating user-friendly analysis and diminishing the need for
extensive technical data processing knowledge. Despite these significant
benefits, the completion of the entire ESDC life cycle remains a challenging
task. Obstacles are not only of a technical nature but also relate to
domain-specific problems in Earth system research. There exist barriers to
realising the full potential of data collections in light of novel cloud-based
technologies, particularly in curating data tailored for specific application
domains. These include transforming data to conform to a spatio-temporal grid
with minimum distortions and managing complexities such as spatio-temporal
autocorrelation issues. Addressing these challenges is pivotal for the
effective application of Artificial Intelligence (AI) approaches. Furthermore,
adhering to open science principles for data dissemination, reproducibility,
visualisation, and reuse is crucial for fostering sustainable research.
Overcoming these challenges offers a substantial opportunity to advance
data-driven Earth system research, unlocking the full potential of an
integrated, multidimensional view of Earth system processes. This is
particularly true when such research is coupled with innovative research
paradigms and technological progress.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Cost Self-Ensembles Based on Multi-Branch Transformation and Grouped
  Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojung Lee, Jong-Seok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in low-cost ensemble learning have demonstrated improved
efficiency for image classification. However, the existing low-cost ensemble
methods show relatively lower accuracy compared to conventional ensemble
learning. In this paper, we propose a new low-cost ensemble learning, which can
simultaneously achieve high efficiency and classification performance. A CNN is
transformed into a multi-branch structure without introduction of additional
components, which maintains the computational complexity as that of the
original single model and also enhances diversity among the branches' outputs
via sufficient separation between different pathways of the branches. In
addition, we propose a new strategy that applies grouped convolution in the
branches with different numbers of groups in different branches, which boosts
the diversity of the branches' outputs. For training, we employ knowledge
distillation using the ensemble of the outputs as the teacher signal. The high
diversity among the outputs enables to form a powerful teacher, enhancing the
individual branch's classification performance and consequently the overall
ensemble performance. Experimental results show that our method achieves
state-of-the-art classification accuracy and higher uncertainty estimation
performance compared to previous low-cost ensemble methods. The code is
available at https://github.com/hjdw2/SEMBG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture-of-Noises Enhanced Forgery-Aware Predictor for Multi-Face
  Manipulation Detection and Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changtao Miao, Qi Chu, Tao Gong, Zhentao Tan, Zhenchao Jin, Wanyi Zhuang, Man Luo, Honggang Hu, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of face manipulation technology, forgery images in
multi-face scenarios are gradually becoming a more complex and realistic
challenge. Despite this, detection and localization methods for such multi-face
manipulations remain underdeveloped. Traditional manipulation localization
methods either indirectly derive detection results from localization masks,
resulting in limited detection performance, or employ a naive two-branch
structure to simultaneously obtain detection and localization results, which
cannot effectively benefit the localization capability due to limited
interaction between two tasks. This paper proposes a new framework, namely
MoNFAP, specifically tailored for multi-face manipulation detection and
localization. The MoNFAP primarily introduces two novel modules: the
Forgery-aware Unified Predictor (FUP) Module and the Mixture-of-Noises Module
(MNM). The FUP integrates detection and localization tasks using a token
learning strategy and multiple forgery-aware transformers, which facilitates
the use of classification information to enhance localization capability.
Besides, motivated by the crucial role of noise information in forgery
detection, the MNM leverages multiple noise extractors based on the concept of
the mixture of experts to enhance the general RGB features, further boosting
the performance of our framework. Finally, we establish a comprehensive
benchmark for multi-face detection and localization and the proposed
\textit{MoNFAP} achieves significant performance. The codes will be made
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network Fission Ensembles for Low-Cost Self-Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojung Lee, Jong-Seok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent ensemble learning methods for image classification have been shown to
improve classification accuracy with low extra cost. However, they still
require multiple trained models for ensemble inference, which eventually
becomes a significant burden when the model size increases. In this paper, we
propose a low-cost ensemble learning and inference, called Network Fission
Ensembles (NFE), by converting a conventional network itself into a multi-exit
structure. Starting from a given initial network, we first prune some of the
weights to reduce the training burden. We then group the remaining weights into
several sets and create multiple auxiliary paths using each set to construct
multi-exits. We call this process Network Fission. Through this, multiple
outputs can be obtained from a single network, which enables ensemble learning.
Since this process simply changes the existing network structure to multi-exits
without using additional networks, there is no extra computational burden for
ensemble learning and inference. Moreover, by learning from multiple losses of
all exits, the multi-exits improve performance via regularization, and high
performance can be achieved even with increased network sparsity. With our
simple yet effective method, we achieve significant improvement compared to
existing ensemble methods. The code is available at
https://github.com/hjdw2/NFE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perception Matters: Enhancing Embodied AI with Uncertainty-Aware
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Prasanna, Daniel Honerkamp, Kshitij Sirohi, Tim Welschehold, Wolfram Burgard, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied AI has made significant progress acting in unexplored environments.
However, tasks such as object search have largely focused on efficient policy
learning. In this work, we identify several gaps in current search methods:
They largely focus on dated perception models, neglect temporal aggregation,
and transfer from ground truth directly to noisy perception at test time,
without accounting for the resulting overconfidence in the perceived state. We
address the identified problems through calibrated perception probabilities and
uncertainty across aggregation and found decisions, thereby adapting the models
for sequential tasks. The resulting methods can be directly integrated with
pretrained models across a wide family of existing search approaches at no
additional training cost. We perform extensive evaluations of aggregation
methods across both different semantic perception models and policies,
confirming the importance of calibrated uncertainties in both the aggregation
and found decisions. We make the code and trained models available at
http://semantic-search.cs.uni-freiburg.de.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfGeo: <span class="highlight-title">Self-supervised</span> and Geodesic-consistent Estimation of Keypoints
  on Deformable Shapes <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Zohaib, Luca Cosmo, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex
task, even more challenging when an object shape is deforming. As keypoints
should be semantically and geometrically consistent across all the 3D frames -
each keypoint should be anchored to a specific part of the deforming shape
irrespective of intrinsic and extrinsic motion. This paper presents, "SelfGeo",
a self-supervised method that computes persistent 3D keypoints of non-rigid
objects from arbitrary PCDs without the need of human annotations. The gist of
SelfGeo is to estimate keypoints between frames that respect invariant
properties of deforming bodies. Our main contribution is to enforce that
keypoints deform along with the shape while keeping constant geodesic distances
among them. This principle is then propagated to the design of a set of losses
which minimization let emerge repeatable keypoints in specific semantic
locations of the non-rigid shape. We show experimentally that the use of
geodesic has a clear advantage in challenging dynamic scenes and with different
classes of deforming shapes (humans and animals). Code and data are available
at: https://github.com/IIT-PAVIS/SelfGeo
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted in ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint-Motion Mutual Learning for Pose Estimation in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sifan Wu, Haipeng Chen, Yifang Yin, Sihao Hu, Runyang Feng, Yingying Jiao, Ziqi Yang, Zhenguang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human pose estimation in videos has long been a compelling yet challenging
task within the realm of computer vision. Nevertheless, this task remains
difficult because of the complex video scenes, such as video defocus and
self-occlusion. Recent methods strive to integrate multi-frame visual features
generated by a backbone network for pose estimation. However, they often ignore
the useful joint information encoded in the initial heatmap, which is a
by-product of the backbone generation. Comparatively, methods that attempt to
refine the initial heatmap fail to consider any spatio-temporal motion
features. As a result, the performance of existing methods for pose estimation
falls short due to the lack of ability to leverage both local joint (heatmap)
information and global motion (feature) dynamics.
  To address this problem, we propose a novel joint-motion mutual learning
framework for pose estimation, which effectively concentrates on both local
joint dependency and global pixel-level motion dynamics. Specifically, we
introduce a context-aware joint learner that adaptively leverages initial
heatmaps and motion flow to retrieve robust local joint feature. Given that
local joint feature and global motion flow are complementary, we further
propose a progressive joint-motion mutual learning that synergistically
exchanges information and interactively learns between joint feature and motion
flow to improve the capability of the model. More importantly, to capture more
diverse joint and motion cues, we theoretically analyze and propose an
information orthogonality objective to avoid learning redundant information
from multi-cues. Empirical experiments show our method outperforms prior arts
on three challenging benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascading Refinement Video Denoising with Uncertainty Adaptivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate alignment is crucial for video denoising. However, estimating
alignment in noisy environments is challenging. This paper introduces a
cascading refinement video denoising method that can refine alignment and
restore images simultaneously. Better alignment enables restoration of more
detailed information in each frame. Furthermore, better image quality leads to
better alignment. This method has achieved SOTA performance by a large margin
on the CRVD dataset. Simultaneously, aiming to deal with multi-level noise, an
uncertainty map was created after each iteration. Because of this, redundant
computation on the easily restored videos was avoided. By applying this method,
the entire computation was reduced by 25% on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric Algebra Meets Large Language Models: Instruction-Based
  Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitris Angelis, Prodromos Kolyvakis, Manos Kamarianakis, George Papagiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel integration of Large Language Models (LLMs)
with Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene
editing, particularly for object repositioning tasks, which traditionally
requires intricate manual processes and specialized expertise. These
conventional methods typically suffer from reliance on large training datasets
or lack a formalized language for precise edits. Utilizing CGA as a robust
formal language, our system, shenlong, precisely models spatial transformations
necessary for accurate object repositioning. Leveraging the zero-shot learning
capabilities of pre-trained LLMs, shenlong translates natural language
instructions into CGA operations which are then applied to the scene,
facilitating exact spatial transformations within 3D scenes without the need
for specialized pre-training. Implemented in a realistic simulation
environment, shenlong ensures compatibility with existing graphics pipelines.
To accurately assess the impact of CGA, we benchmark against robust Euclidean
Space baselines, evaluating both latency and accuracy. Comparative performance
evaluations indicate that shenlong significantly reduces LLM response times by
16% and boosts success rates by 9.6% on average compared to the traditional
methods. Notably, shenlong achieves a 100% perfect success rate in common
practical queries, a benchmark where other systems fall short. These
advancements underscore shenlong's potential to democratize 3D scene editing,
enhancing accessibility and fostering innovation across sectors such as
education, digital entertainment, and virtual reality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COM Kitchens: An Unedited Overhead-view Video <span class="highlight-title">Dataset</span> as a
  Vision-Language Benchmark <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koki Maeda, Tosho Hirasawa, Atsushi Hashimoto, Jun Harashima, Leszek Rybicki, Yusuke Fukasawa, Yoshitaka Ushiku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural video understanding is gaining attention in the vision and
language community. Deep learning-based video analysis requires extensive data.
Consequently, existing works often use web videos as training resources, making
it challenging to query instructional contents from raw video observations. To
address this issue, we propose a new dataset, COM Kitchens. The dataset
consists of unedited overhead-view videos captured by smartphones, in which
participants performed food preparation based on given recipes. Fixed-viewpoint
video datasets often lack environmental diversity due to high camera setup
costs. We used modern wide-angle smartphone lenses to cover cooking counters
from sink to cooktop in an overhead view, capturing activity without in-person
assistance. With this setup, we collected a diverse dataset by distributing
smartphones to participants. With this dataset, we propose the novel
video-to-text retrieval task Online Recipe Retrieval (OnRR) and new video
captioning domain Dense Video Captioning on unedited Overhead-View videos
(DVC-OV). Our experiments verified the capabilities and limitations of current
web-video-based SOTA methods in handling these tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary
  Concepts <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Tan, Fengtao Zhou, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept bottleneck model (CBM) is an interpretable-by-design framework
that makes decisions by first predicting a set of interpretable concepts, and
then predicting the class label based on the given concepts. Existing CBMs are
trained with a fixed set of concepts (concepts are either annotated by the
dataset or queried from language models). However, this closed-world assumption
is unrealistic in practice, as users may wonder about the role of any desired
concept in decision-making after the model is deployed. Inspired by the large
success of recent vision-language pre-trained models such as CLIP in zero-shot
classification, we propose "OpenCBM" to equip the CBM with open vocabulary
concepts via: (1) Aligning the feature space of a trainable image feature
extractor with that of a CLIP's image encoder via a prototype based feature
alignment; (2) Simultaneously training an image classifier on the downstream
dataset; (3) Reconstructing the trained classification head via any set of
user-desired textual concepts encoded by CLIP's text encoder. To reveal
potentially missing concepts from users, we further propose to iteratively find
the closest concept embedding to the residual parameters during the
reconstruction until the residual is small enough. To the best of our
knowledge, our "OpenCBM" is the first CBM with concepts of open vocabularies,
providing users the unique benefit such as removing, adding, or replacing any
desired concept to explain the model's prediction even after a model is
trained. Moreover, our model significantly outperforms the previous
state-of-the-art CBM by 9% in the classification accuracy on the benchmark
dataset CUB-200-2011.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Lu, Jiahao Nie, Zhiwei He, Hongjie Gu, Xudong Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current LiDAR point cloud-based 3D single object tracking (SOT) methods
typically rely on point-based representation network. Despite demonstrated
success, such networks suffer from some fundamental problems: 1) It contains
pooling operation to cope with inherently disordered point clouds, hindering
the capture of 3D spatial information that is useful for tracking, a regression
task. 2) The adopted set abstraction operation hardly handles
density-inconsistent point clouds, also preventing 3D spatial information from
being modeled. To solve these problems, we introduce a novel tracking
framework, termed VoxelTrack. By voxelizing inherently disordered point clouds
into 3D voxels and extracting their features via sparse convolution blocks,
VoxelTrack effectively models precise and robust 3D spatial information,
thereby guiding accurate position prediction for tracked objects. Moreover,
VoxelTrack incorporates a dual-stream encoder with cross-iterative feature
fusion module to further explore fine-grained 3D spatial information for
tracking. Benefiting from accurate 3D spatial information being modeled, our
VoxelTrack simplifies tracking pipeline with a single regression loss.
Extensive experiments are conducted on three widely-adopted datasets including
KITTI, NuScenes and Waymo Open Dataset. The experimental results confirm that
VoxelTrack achieves state-of-the-art performance (88.3%, 71.4% and 63.6% mean
precision on the three datasets, respectively), and outperforms the existing
trackers with a real-time speed of 36 Fps on a single TITAN RTX GPU. The source
code and model will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongkee Lim, Yusung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of semantic segmentation in Unsupervised Domain Adaptation
(UDA) emerges not only from domain shifts between source and target images but
also from discrepancies in class taxonomies across domains. Traditional UDA
research assumes consistent taxonomy between the source and target domains,
thereby limiting their ability to recognize and adapt to the taxonomy of the
target domain. This paper introduces a novel approach, Cross-Domain Semantic
Segmentation on Inconsistent Taxonomy using Vision Language Models (CSI), which
effectively performs domain-adaptive semantic segmentation even in situations
of source-target class mismatches. CSI leverages the semantic generalization
potential of Visual Language Models (VLMs) to create synergy with previous UDA
methods. It leverages segment reasoning obtained through traditional UDA
methods, combined with the rich semantic knowledge embedded in VLMs, to relabel
new classes in the target domain. This approach allows for effective adaptation
to extended taxonomies without requiring any ground truth label for the target
domain. Our method has shown to be effective across various benchmarks in
situations of inconsistent taxonomy settings (coarse-to-fine taxonomy and open
taxonomy) and demonstrates consistent synergy effects when integrated with
previous state-of-the-art UDA methods. The implementation is available at
http://github.com/jkee58/CSI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Clustering using Reversible Binary Cellular Automata for
  High-Dimensional Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baby C. J., Kamalika Bhattacharjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a hierarchical clustering algorithm for high-dimensional
datasets using the cyclic space of reversible finite cellular automata. In
cellular automaton (CA) based clustering, if two objects belong to the same
cycle, they are closely related and considered as part of the same cluster.
However, if a high-dimensional dataset is clustered using the cycles of one CA,
closely related objects may belong to different cycles. This paper identifies
the relationship between objects in two different cycles based on the median of
all elements in each cycle so that they can be grouped in the next stage.
Further, to minimize the number of intermediate clusters which in turn reduces
the computational cost, a rule selection strategy is taken to find the best
rules based on information propagation and cycle structure. After encoding the
dataset using frequency-based encoding such that the consecutive data elements
maintain a minimum hamming distance in encoded form, our proposed clustering
algorithm iterates over three stages to finally cluster the data elements into
the desired number of clusters given by user. This algorithm can be applied to
various fields, including healthcare, sports, chemical research, agriculture,
etc. When verified over standard benchmark datasets with various performance
metrics, our algorithm is at par with the existing algorithms with quadratic
time complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum learning based <span class="highlight-title">pre-train</span>ing using Multi-Modal Contrastive
  Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Abdullah Jamal, Omid Mohareri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new pre-training method for image understanding
tasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method
utilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques.
Recent approaches either use masked autoencoding (e.g., MultiMAE) or
contrastive learning(e.g., Pri3D, or combine them in a single contrastive
masked autoencoder architecture such as CMAE and CAV-MAE. However, none of the
single contrastive masked autoencoder is applicable to RGB-D datasets. To
improve the performance and efficacy of such methods, we propose a new
pre-training strategy based on CL. Specifically, in the first stage, we
pre-train the model using contrastive learning to learn cross-modal
representations. In the second stage, we initialize the modality-specific
encoders using the weights from the first stage and then pre-train the model
using masked autoencoding and denoising/noise prediction used in diffusion
models. Masked autoencoding focuses on reconstructing the missing patches in
the input modality using local spatial correlations, while denoising learns
high frequency components of the input data. Our approach is scalable, robust
and suitable for pre-training with limited RGB-D datasets. Extensive
experiments on multiple datasets such as ScanNet, NYUv2 and SUN RGB-D show the
efficacy and superior performance of our approach. Specifically, we show an
improvement of +1.0% mIoU against Mask3D on ScanNet semantic segmentation. We
further demonstrate the effectiveness of our approach in low-data regime by
evaluating it for semantic segmentation task against the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Vision-Language Models for Zero-Shot Detection,
  Classification, and Association of Motorcycles, Passengers, and Helmets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Choi, Ross Greer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motorcycle accidents pose significant risks, particularly when riders and
passengers do not wear helmets. This study evaluates the efficacy of an
advanced vision-language foundation model, OWLv2, in detecting and classifying
various helmet-wearing statuses of motorcycle occupants using video data. We
extend the dataset provided by the CVPR AI City Challenge and employ a cascaded
model approach for detection and classification tasks, integrating OWLv2 and
CNN models. The results highlight the potential of zero-shot learning to
address challenges arising from incomplete and biased training datasets,
demonstrating the usage of such models in detecting motorcycles, helmet usage,
and occupant positions under varied conditions. We have achieved an average
precision of 0.5324 for helmet detection and provided precision-recall curves
detailing the detection and classification performance. Despite limitations
such as low-resolution data and poor visibility, our research shows promising
advancements in automated vehicle safety and traffic safety enforcement
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language
  Models <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agneet Chatterjee, Yiran Luo, Tejas Gokhale, Yezhou Yang, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) and multimodal large language models (MLLMs) have been
adopted in solutions for several computer vision and multimodal learning tasks.
However, it has been found that such vision-language models lack the ability to
correctly reason over spatial relationships. To tackle this shortcoming, we
develop the REVISION framework which improves spatial fidelity in
vision-language models. REVISION is a 3D rendering based pipeline that
generates spatially accurate synthetic images, given a textual prompt. REVISION
is an extendable framework, which currently supports 100+ 3D assets, 11 spatial
relationships, all with diverse camera perspectives and backgrounds. Leveraging
images from REVISION as additional guidance in a training-free manner
consistently improves the spatial consistency of T2I models across all spatial
relationships, achieving competitive performance on the VISOR and T2I-CompBench
benchmarks. We also design RevQA, a question-answering benchmark to evaluate
the spatial reasoning abilities of MLLMs, and find that state-of-the-art models
are not robust to complex spatial reasoning under adversarial settings. Our
results and findings indicate that utilizing rendering-based frameworks is an
effective approach for developing spatially-aware generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024. Project Page :
  https://agneetchatterjee.com/revision/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProCreate, Dont Reproduce! Propulsive Energy Diffusion for Creative
  Generation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Lu, Ryan Teehan, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose ProCreate, a simple and easy-to-implement method to
improve sample diversity and creativity of diffusion-based image generative
models and to prevent training data reproduction. ProCreate operates on a set
of reference images and actively propels the generated image embedding away
from the reference embeddings during the generation process. We propose FSCG-8
(Few-Shot Creative Generation 8), a few-shot creative generation dataset on
eight different categories -- encompassing different concepts, styles, and
settings -- in which ProCreate achieves the highest sample diversity and
fidelity. Furthermore, we show that ProCreate is effective at preventing
replicating training data in a large-scale evaluation using training text
prompts. Code and FSCG-8 are available at
https://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The
project page is available at https://procreate-diffusion.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ECCV 2024. Project page:
  https://procreate-diffusion.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-modulated Attention <span class="highlight-title">Transformer</span> for RGBT Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Xiao, Jiacong Zhao, Andong Lu, Chenglong Li, Yin Lin, Bing Yin, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Transformer-based RGBT trackers achieve remarkable performance
benefits by leveraging self-attention to extract uni-modal features and
cross-attention to enhance multi-modal feature interaction and template-search
correlation computation. Nevertheless, the independent search-template
correlation calculations ignore the consistency between branches, which can
result in ambiguous and inappropriate correlation weights. It not only limits
the intra-modal feature representation, but also harms the robustness of
cross-attention for multi-modal feature interaction and search-template
correlation computation. To address these issues, we propose a novel approach
called Cross-modulated Attention Transformer (CAFormer), which performs
intra-modality self-correlation, inter-modality feature interaction, and
search-template correlation computation in a unified attention model, for RGBT
tracking. In particular, we first independently generate correlation maps for
each modality and feed them into the designed Correlation Modulated Enhancement
module, modulating inaccurate correlation weights by seeking the consensus
between modalities. Such kind of design unifies self-attention and
cross-attention schemes, which not only alleviates inaccurate attention weight
computation in self-attention but also eliminates redundant computation
introduced by extra cross-attention scheme. In addition, we propose a
collaborative token elimination strategy to further improve tracking inference
efficiency and accuracy. Extensive experiments on five public RGBT tracking
benchmarks show the outstanding performance of the proposed CAFormer against
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ More Than Positive and Negative: Communicating Fine Granularity in
  Medical Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Peng, Kai Wang, Jianfei Yang, Yingying Zhu, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advance of deep learning, much progress has been made in building
powerful artificial intelligence (AI) systems for automatic Chest X-ray (CXR)
analysis. Most existing AI models are trained to be a binary classifier with
the aim of distinguishing positive and negative cases. However, a large gap
exists between the simple binary setting and complicated real-world medical
scenarios. In this work, we reinvestigate the problem of automatic radiology
diagnosis. We first observe that there is considerable diversity among cases
within the positive class, which means simply classifying them as positive
loses many important details. This motivates us to build AI models that can
communicate fine-grained knowledge from medical images like human experts. To
this end, we first propose a new benchmark on fine granularity learning from
medical images. Specifically, we devise a division rule based on medical
knowledge to divide positive cases into two subcategories, namely atypical
positive and typical positive. Then, we propose a new metric termed
AUC$^\text{FG}$ on the two subcategories for evaluation of the ability to
separate them apart. With the proposed benchmark, we encourage the community to
develop AI diagnosis systems that could better learn fine granularity from
medical images. Last, we propose a simple risk modulation approach to this
problem by only using coarse labels in training. Empirical results show that
despite its simplicity, the proposed method achieves superior performance and
thus serves as a strong baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExoViP: Step-by-step Verification and Exploration with Exoskeleton
  Modules for Compositional Visual Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Alan Yuille, Zhuowan Li, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional visual reasoning methods, which translate a complex query into
a structured composition of feasible visual tasks, have exhibited a strong
potential in complicated multi-modal tasks. Empowered by recent advances in
large language models (LLMs), this multi-modal challenge has been brought to a
new stage by treating LLMs as few-shot/zero-shot planners, i.e.,
vision-language (VL) programming. Such methods, despite their numerous merits,
suffer from challenges due to LLM planning mistakes or inaccuracy of visual
execution modules, lagging behind the non-compositional models. In this work,
we devise a "plug-and-play" method, ExoViP, to correct errors in both the
planning and execution stages through introspective verification. We employ
verification modules as "exoskeletons" to enhance current VL programming
schemes. Specifically, our proposed verification module utilizes a mixture of
three sub-verifiers to validate predictions after each reasoning step,
subsequently calibrating the visual module predictions and refining the
reasoning trace planned by LLMs. Experimental results on two representative VL
programming methods showcase consistent improvements on five compositional
reasoning tasks on standard benchmarks. In light of this, we believe that
ExoViP can foster better performance and generalization on open-domain
multi-modal challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Appear at COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Domain Adaption Harnessing Vision-Language <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlve Zhou, Zhiheng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses two vital challenges in Unsupervised Domain Adaptation
(UDA) with a focus on harnessing the power of Vision-Language Pre-training
(VLP) models. Firstly, UDA has primarily relied on ImageNet pre-trained models.
However, the potential of VLP models in UDA remains largely unexplored. The
rich representation of VLP models holds significant promise for enhancing UDA
tasks. To address this, we propose a novel method called Cross-Modal Knowledge
Distillation (CMKD), leveraging VLP models as teacher models to guide the
learning process in the target domain, resulting in state-of-the-art
performance. Secondly, current UDA paradigms involve training separate models
for each task, leading to significant storage overhead and impractical model
deployment as the number of transfer tasks grows. To overcome this challenge,
we introduce Residual Sparse Training (RST) exploiting the benefits conferred
by VLP's extensive pre-training, a technique that requires minimal adjustment
(approximately 0.1\%$\sim$0.5\%) of VLP model parameters to achieve performance
comparable to fine-tuning. Combining CMKD and RST, we present a comprehensive
solution that effectively leverages VLP models for UDA tasks while reducing
storage overhead for model deployment. Furthermore, CMKD can serve as a
baseline in conjunction with other methods like FixMatch, enhancing the
performance of UDA. Our proposed method outperforms existing techniques on
standard benchmarks. Our code will be available at:
https://github.com/Wenlve-Zhou/VLP-UDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Feature Interaction Network for Image Inpainting Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Yao, Tingfeng Han, Shan Jia, Siwei Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image inpainting, which is the task of filling in missing areas in an image,
is a common image editing technique. Inpainting can be used to conceal or alter
image contents in malicious manipulation of images, driving the need for
research in image inpainting detection. Existing methods mostly rely on a basic
encoder-decoder structure, which often results in a high number of false
positives or misses the inpainted regions, especially when dealing with targets
of varying semantics and scales. Additionally, the absence of an effective
approach to capture boundary artifacts leads to less accurate edge
localization. In this paper, we describe a new method for inpainting detection
based on a Dense Feature Interaction Network (DeFI-Net). DeFI-Net uses a novel
feature pyramid architecture to capture and amplify multi-scale representations
across various stages, thereby improving the detection of image inpainting by
better revealing feature-level interactions. Additionally, the network can
adaptively direct the lower-level features, which carry edge and shape
information, to refine the localization of manipulated regions while
integrating the higher-level semantic features. Using DeFI-Net, we develop a
method combining complementary representations to accurately identify inpainted
areas. Evaluation on five image inpainting datasets demonstrate the
effectiveness of our approach, which achieves state-of-the-art performance in
detecting inpainting across diverse models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing
  Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjith Prasad, Chathurangi Shyalika, Ramtin Zand, Fadi El Kalach, Revathy Venkataramanan, Ramy Harik, Amit Sheth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in manufacturing pipelines remains a critical challenge,
intensified by the complexity and variability of industrial environments. This
paper introduces AssemAI, an interpretable image-based anomaly detection system
tailored for smart manufacturing pipelines. Our primary contributions include
the creation of a tailored image dataset and the development of a custom object
detection model, YOLO-FF, designed explicitly for anomaly detection in
manufacturing assembly environments. Utilizing the preprocessed image dataset
derived from an industry-focused rocket assembly pipeline, we address the
challenge of imbalanced image data and demonstrate the importance of
image-based methods in anomaly detection. The proposed approach leverages
domain knowledge in data preparation, model development and reasoning. We
compare our method against several baselines, including simple CNN and custom
Visual Transformer (ViT) models, showcasing the effectiveness of our custom
data preparation and pretrained CNN integration. Additionally, we incorporate
explainability techniques at both user and model levels, utilizing ontology for
user-friendly explanations and SCORE-CAM for in-depth feature and model
analysis. Finally, the model was also deployed in a real-time setting. Our
results include ablation studies on the baselines, providing a comprehensive
evaluation of the proposed system. This work highlights the broader impact of
advanced image-based anomaly detection in enhancing the reliability and
efficiency of smart manufacturing processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 6 Figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisionUnite: A Vision-Language Foundation Model for Ophthalmology
  Enhanced with Clinical Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Li, Diping Song, Zefeng Yang, Deming Wang, Fei Li, Xiulan Zhang, Paul E. Kinahan, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need for improved diagnostic methods in ophthalmology is acute,
especially in the less developed regions with limited access to specialists and
advanced equipment. Therefore, we introduce VisionUnite, a novel
vision-language foundation model for ophthalmology enhanced with clinical
knowledge. VisionUnite has been pretrained on an extensive dataset comprising
1.24 million image-text pairs, and further refined using our proposed MMFundus
dataset, which includes 296,379 high-quality fundus image-text pairs and
889,137 simulated doctor-patient dialogue instances. Our experiments indicate
that VisionUnite outperforms existing generative foundation models such as
GPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable
to junior ophthalmologists. VisionUnite performs well in various clinical
scenarios including open-ended multi-disease diagnosis, clinical explanation,
and patient interaction, making it a highly versatile tool for initial
ophthalmic disease screening. VisionUnite can also serve as an educational aid
for junior ophthalmologists, accelerating their acquisition of knowledge
regarding both common and rare ophthalmic conditions. VisionUnite represents a
significant advancement in ophthalmology, with broad implications for
diagnostics, medical education, and understanding of disease mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multistain <span class="highlight-title">Pretrain</span>ing for Slide Representation Learning in Pathology <span class="chip">ECCV'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Jaume, Anurag Vaidya, Andrew Zhang, Andrew H. Song, Richard J. Chen, Sharifa Sahai, Dandan Mo, Emilio Madrigal, Long Phi Le, Faisal Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing self-supervised learning (SSL) models that can learn universal and
transferable representations of H&E gigapixel whole-slide images (WSIs) is
becoming increasingly valuable in computational pathology. These models hold
the potential to advance critical tasks such as few-shot classification, slide
retrieval, and patient stratification. Existing approaches for slide
representation learning extend the principles of SSL from small images (e.g.,
224 x 224 patches) to entire slides, usually by aligning two different
augmentations (or views) of the slide. Yet the resulting representation remains
constrained by the limited clinical and biological diversity of the views.
Instead, we postulate that slides stained with multiple markers, such as
immunohistochemistry, can be used as different views to form a rich
task-agnostic training signal. To this end, we introduce Madeleine, a
multimodal pretraining strategy for slide representation learning. Madeleine is
trained with a dual global-local cross-stain alignment objective on large
cohorts of breast cancer samples (N=4,211 WSIs across five stains) and kidney
transplant samples (N=12,070 WSIs across four stains). We demonstrate the
quality of slide representations learned by Madeleine on various downstream
evaluations, ranging from morphological and molecular classification to
prognostic prediction, comprising 21 tasks using 7,299 WSIs from multiple
medical centers. Code is available at https://github.com/mahmoodlab/MADELEINE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Data Efficiency and Performance of Machine Learning Algorithms
  for Assessing Low Back Pain Physical Rehabilitation Exercises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksa Marusic, Louis Annabi, Sao Msi Nguyen, Adriana Tapus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing human motion is an active research area, with various applications.
In this work, we focus on human motion analysis in the context of physical
rehabilitation using a robot coach system. Computer-aided assessment of
physical rehabilitation entails evaluation of patient performance in completing
prescribed rehabilitation exercises, based on processing movement data captured
with a sensory system, such as RGB and RGB-D cameras. As 2D and 3D human pose
estimation from RGB images had made impressive improvements, we aim to compare
the assessment of physical rehabilitation exercises using movement data
obtained from both RGB-D camera (Microsoft Kinect) and estimation from RGB
videos (OpenPose and BlazePose algorithms). A Gaussian Mixture Model (GMM) is
employed from position (and orientation) features, with performance metrics
defined based on the log-likelihood values from GMM. The evaluation is
performed on a medical database of clinical patients carrying out low back-pain
rehabilitation exercises, previously coached by robot Poppy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>European Conference on Mobile Robots (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAReT: Cross-view Video Geolocalization with Adapters and
  Auto-Regressive <span class="highlight-title">Transformer</span>s <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manu S Pillai, Mamshad Nayeem Rizve, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-view video geo-localization (CVGL) aims to derive GPS trajectories from
street-view videos by aligning them with aerial-view images. Despite their
promising performance, current CVGL methods face significant challenges. These
methods use camera and odometry data, typically absent in real-world scenarios.
They utilize multiple adjacent frames and various encoders for feature
extraction, resulting in high computational costs. Moreover, these approaches
independently predict each street-view frame's location, resulting in
temporally inconsistent GPS trajectories. To address these challenges, in this
work, we propose GAReT, a fully transformer-based method for CVGL that does not
require camera and odometry data. We introduce GeoAdapter, a
transformer-adapter module designed to efficiently aggregate image-level
representations and adapt them for video inputs. Specifically, we train a
transformer encoder on video frames and aerial images, then freeze the encoder
to optimize the GeoAdapter module to obtain video-level representation. To
address temporally inconsistent trajectories, we introduce TransRetriever, an
encoder-decoder transformer model that predicts GPS locations of street-view
frames by encoding top-k nearest neighbor predictions per frame and
auto-regressively decoding the best neighbor based on the previous frame's
predictions. Our method's effectiveness is validated through extensive
experiments, demonstrating state-of-the-art performance on benchmark datasets.
Our code is available at https://github.com/manupillai308/GAReT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DaCapo: a modular deep learning framework for scalable 3D image
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Patton, Jeff L. Rhoades, Marwan Zouinkhi, David G. Ackerman, Caroline Malin-Mayor, Diane Adjavon, Larissa Heinrich, Davis Bennett, Yurii Zubov, CellMap Project Team, Aubrey V. Weigel, Jan Funke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DaCapo is a specialized deep learning library tailored to expedite the
training and application of existing machine learning approaches on large,
near-isotropic image data. In this correspondence, we introduce DaCapo's unique
features optimized for this specific domain, highlighting its modular
structure, efficient experiment management tools, and scalable deployment
capabilities. We discuss its potential to improve access to large-scale,
isotropic image segmentation and invite the community to explore and contribute
to this open-source initiative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Malicious Attacks in Federated Learning via Confidence-aware
  Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilei Li, Ahmed M. Abdelmoniem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is an emerging distributed machine learning paradigm
that allows multiple clients to collaboratively train a global model without
sharing private local data. However, FL systems are vulnerable to attacks from
malicious clients, who can degrade the global model performance through data
poisoning and model poisoning. Existing defense methods typically focus on a
single type of attack, such as Byzantine attacks or backdoor attacks, and are
often ineffective against potential data poisoning attacks like label flipping
and label shuffling. Additionally, these methods often lack accuracy and
robustness in detecting and handling malicious updates. To address these
issues, we propose a novel method based on model confidence scores, which
evaluates the uncertainty of client model updates to detect and defend against
malicious clients. Our approach is comprehensively effective for both model
poisoning and data poisoning attacks and is capable of accurately identifying
and mitigating potential malicious updates from being aggregated. Experimental
results demonstrate that our method significantly improves the robustness of FL
systems against various types of attacks, also achieving higher model accuracy
and stability across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiCo: A Size-Controllable Virtual Try-On Approach for Informed
  Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherry X. Chen, Alex Christopher Lim, Yimeng Liu, Pradeep Sen, Misha Sra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual try-on (VTO) applications aim to improve the online shopping
experience by allowing users to preview garments, before making purchase
decisions. However, many VTO tools fail to consider the crucial relationship
between a garment's size and the user's body size, often employing a
one-size-fits-all approach when visualizing a clothing item. This results in
poor size recommendations and purchase decisions leading to increased return
rates. To address this limitation, we introduce SiCo, an online VTO system,
where users can upload images of themselves and visualize how different sizes
of clothing would look on their body to help make better-informed purchase
decisions. Our user study shows SiCo's superiority over baseline VTO. The
results indicate that our approach significantly enhances user ability to gauge
the appearance of outfits on their bodies and boosts their confidence in
selecting clothing sizes that match desired goals. Based on our evaluation, we
believe our VTO design has the potential to reduce return rates and enhance the
online clothes shopping experience. Our code is available at
https://github.com/SherryXTChen/SiCo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Mixture based Evidential Learning for Stereo Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weide Liu, Xingxing Wang, Lu Wang, Jun Cheng, Fayao Liu, Xulei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel Gaussian mixture based evidential
learning solution for robust stereo matching. Diverging from previous
evidential deep learning approaches that rely on a single Gaussian
distribution, our framework posits that individual image data adheres to a
mixture-of-Gaussian distribution in stereo matching. This assumption yields
more precise pixel-level predictions and more accurately mirrors the real-world
image distribution. By further employing the inverse-Gamma distribution as an
intermediary prior for each mixture component, our probabilistic model achieves
improved depth estimation compared to its counterpart with the single Gaussian
and effectively captures the model uncertainty, which enables a strong
cross-domain generation ability. We evaluated our method for stereo matching by
training the model using the Scene Flow dataset and testing it on KITTI 2015
and Middlebury 2014. The experiment results consistently show that our method
brings improvements over the baseline methods in a trustworthy manner. Notably,
our approach achieved new state-of-the-art results on both the in-domain
validated data and the cross-domain datasets, demonstrating its effectiveness
and robustness in stereo matching tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lesion Elevation Prediction from Skin Images Improves Diagnosis <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Abhishek, Ghassan Hamarneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning-based computer-aided diagnosis for skin lesion image
analysis is approaching dermatologists' performance levels, there are several
works showing that incorporating additional features such as shape priors,
texture, color constancy, and illumination further improves the lesion
diagnosis performance. In this work, we look at another clinically useful
feature, skin lesion elevation, and investigate the feasibility of predicting
and leveraging skin lesion elevation labels. Specifically, we use a deep
learning model to predict image-level lesion elevation labels from 2D skin
lesion images. We test the elevation prediction accuracy on the derm7pt
dataset, and use the elevation prediction model to estimate elevation labels
for images from five other datasets: ISIC 2016, 2017, and 2018 Challenge
datasets, MSK, and DermoFit. We evaluate cross-domain generalization by using
these estimated elevation labels as auxiliary inputs to diagnosis models, and
show that these improve the classification performance, with AUROC improvements
of up to 6.29% and 2.69% for dermoscopic and clinical images, respectively. The
code is publicly available at https://github.com/sfu-mial/LesionElevation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Medical Image Computing and Computer-Assisted Intervention (MICCAI)
  ISIC Skin Image Analysis Workshop (MICCAI ISIC) 2024; 12 pages, 2 tables, 4
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GazeXplain: Learning to Predict Natural Language Explanations of Visual
  Scanpaths <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianyu Chen, Ming Jiang, Qi Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While exploring visual scenes, humans' scanpaths are driven by their
underlying attention processes. Understanding visual scanpaths is essential for
various applications. Traditional scanpath models predict the where and when of
gaze shifts without providing explanations, creating a gap in understanding the
rationale behind fixations. To bridge this gap, we introduce GazeXplain, a
novel study of visual scanpath prediction and explanation. This involves
annotating natural-language explanations for fixations across eye-tracking
datasets and proposing a general model with an attention-language decoder that
jointly predicts scanpaths and generates explanations. It integrates a unique
semantic alignment mechanism to enhance the consistency between fixations and
explanations, alongside a cross-dataset co-training approach for
generalization. These novelties present a comprehensive and adaptable solution
for explainable human visual scanpath prediction. Extensive experiments on
diverse eye-tracking datasets demonstrate the effectiveness of GazeXplain in
both scanpath prediction and explanation, offering valuable insights into human
visual attention and cognitive processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segmentation Style Discovery: Application to Skin Lesion Images <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variability in medical image segmentation, arising from annotator
preferences, expertise, and their choice of tools, has been well documented.
While the majority of multi-annotator segmentation approaches focus on modeling
annotator-specific preferences, they require annotator-segmentation
correspondence. In this work, we introduce the problem of segmentation style
discovery, and propose StyleSeg, a segmentation method that learns plausible,
diverse, and semantically consistent segmentation styles from a corpus of
image-mask pairs without any knowledge of annotator correspondence. StyleSeg
consistently outperforms competing methods on four publicly available skin
lesion segmentation (SLS) datasets. We also curate ISIC-MultiAnnot, the largest
multi-annotator SLS dataset with annotator correspondence, and our results show
a strong alignment, using our newly proposed measure AS2, between the predicted
styles and annotator preferences. The code and the dataset are available at
https://github.com/sfu-mial/StyleSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Medical Image Computing and Computer-Assisted Intervention (MICCAI)
  ISIC Skin Image Analysis Workshop (MICCAI ISIC) 2024; 13 pages, 2 tables, 3
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LR-Net: A Lightweight and Robust Network for Infrared Small Target
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Yu, Yunpeng Liu, Jinmiao Zhao, Zelin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Limited by equipment limitations and the lack of target intrinsic features,
existing infrared small target detection methods have difficulty meeting actual
comprehensive performance requirements. Therefore, we propose an innovative
lightweight and robust network (LR-Net), which abandons the complex structure
and achieves an effective balance between detection accuracy and resource
consumption. Specifically, to ensure the lightweight and robustness, on the one
hand, we construct a lightweight feature extraction attention (LFEA) module,
which can fully extract target features and strengthen information interaction
across channels. On the other hand, we construct a simple refined feature
transfer (RFT) module. Compared with direct cross-layer connections, the RFT
module can improve the network's feature refinement extraction capability with
little resource consumption. Meanwhile, to solve the problem of small target
loss in high-level feature maps, on the one hand, we propose a low-level
feature distribution (LFD) strategy to use low-level features to supplement the
information of high-level features. On the other hand, we introduce an
efficient simplified bilinear interpolation attention module (SBAM) to promote
the guidance constraints of low-level features on high-level features and the
fusion of the two. In addition, We abandon the traditional resizing method and
adopt a new training and inference cropping strategy, which is more robust to
datasets with multi-scale samples. Extensive experimental results show that our
LR-Net achieves state-of-the-art (SOTA) performance. Notably, on the basis of
the proposed LR-Net, we achieve 3rd place in the "ICPR 2024 Resource-Limited
Infrared Small Target Detection Challenge Track 2: Lightweight Infrared Small
Target Detection".
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refined Infrared Small Target Detection Scheme with Single-Point
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinmiao Zhao, Zelin Shi, Chuang Yu, Yunpeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, infrared small target detection with single-point supervision has
attracted extensive attention. However, the detection accuracy of existing
methods has difficulty meeting actual needs. Therefore, we propose an
innovative refined infrared small target detection scheme with single-point
supervision, which has excellent segmentation accuracy and detection rate.
Specifically, we introduce label evolution with single point supervision
(LESPS) framework and explore the performance of various excellent infrared
small target detection networks based on this framework. Meanwhile, to improve
the comprehensive performance, we construct a complete post-processing
strategy. On the one hand, to improve the segmentation accuracy, we use a
combination of test-time augmentation (TTA) and conditional random field (CRF)
for post-processing. On the other hand, to improve the detection rate, we
introduce an adjustable sensitivity (AS) strategy for post-processing, which
fully considers the advantages of multiple detection results and reasonably
adds some areas with low confidence to the fine segmentation image in the form
of centroid points. In addition, to further improve the performance and explore
the characteristics of this task, on the one hand, we construct and find that a
multi-stage loss is helpful for fine-grained detection. On the other hand, we
find that a reasonable sliding window cropping strategy for test samples has
better performance for actual multi-size samples. Extensive experimental
results show that the proposed scheme achieves state-of-the-art (SOTA)
performance. Notably, the proposed scheme won the third place in the "ICPR 2024
Resource-Limited Infrared Small Target Detection Challenge Track 1: Weakly
Supervised Infrared Small Target Detection".
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Recognition to Prediction: Leveraging Sequence Reasoning for Action
  Anticipation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Liu, Chao Hao, Zitong Yu, Huanjing Yue, Jingyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The action anticipation task refers to predicting what action will happen
based on observed videos, which requires the model to have a strong ability to
summarize the present and then reason about the future. Experience and common
sense suggest that there is a significant correlation between different
actions, which provides valuable prior knowledge for the action anticipation
task. However, previous methods have not effectively modeled this underlying
statistical relationship. To address this issue, we propose a novel end-to-end
video modeling architecture that utilizes attention mechanisms, named
Anticipation via Recognition and Reasoning (ARR). ARR decomposes the action
anticipation task into action recognition and sequence reasoning tasks, and
effectively learns the statistical relationship between actions by next action
prediction (NAP). In comparison to existing temporal aggregation strategies,
ARR is able to extract more effective features from observable videos to make
more reasonable predictions. In addition, to address the challenge of
relationship modeling that requires extensive training data, we propose an
innovative approach for the unsupervised pre-training of the decoder, which
leverages the inherent temporal dynamics of video to enhance the reasoning
capabilities of the network. Extensive experiments on the Epic-kitchen-100,
EGTEA Gaze+, and 50salads datasets demonstrate the efficacy of the proposed
methods. The code is available at https://github.com/linuxsino/ARR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TOMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConDL: Detector-Free Dense Image Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monika Kwiatkowski, Simon Matern, Olaf Hellwich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce a deep-learning framework designed for estimating
dense image correspondences. Our fully convolutional model generates dense
feature maps for images, where each pixel is associated with a descriptor that
can be matched across multiple images. Unlike previous methods, our model is
trained on synthetic data that includes significant distortions, such as
perspective changes, illumination variations, shadows, and specular highlights.
Utilizing contrastive learning, our feature maps achieve greater invariance to
these distortions, enabling robust matching. Notably, our method eliminates the
need for a keypoint detector, setting it apart from many existing
image-matching techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dimensionality Reduction and Nearest Neighbors for Improving
  Out-of-Distribution Detection in Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        McKell Woodland, Nihil Patel, Austin Castelo, Mais Al Taie, Mohamed Eltaher, Joshua P. Yung, Tucker J. Netherton, Tiffany L. Calderone, Jessica I. Sanchez, Darrel W. Cleere, Ahmed Elsaiey, Nakul Gupta, David Victor, Laura Beretta, Ankit B. Patel Kristy K. Brock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinically deployed deep learning-based segmentation models are known to fail
on data outside of their training distributions. While clinicians review the
segmentations, these models tend to perform well in most instances, which could
exacerbate automation bias. Therefore, detecting out-of-distribution images at
inference is critical to warn the clinicians that the model likely failed. This
work applied the Mahalanobis distance (MD) post hoc to the bottleneck features
of four Swin UNETR and nnU-net models that segmented the liver on T1-weighted
magnetic resonance imaging and computed tomography. By reducing the dimensions
of the bottleneck features with either principal component analysis or uniform
manifold approximation and projection, images the models failed on were
detected with high performance and minimal computational load. In addition,
this work explored a non-parametric alternative to the MD, a k-th nearest
neighbors distance (KNN). KNN drastically improved scalability and performance
over MD when both were applied to raw and average-pooled bottleneck features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Expansion of "Dimensionality Reduction for Improving
  Out-of-Distribution Detection in Medical Image Segmentation" arXiv:2308.03723
  . Submitted to the Journal for Machine Learning in Biomedical Imaging. Code
  available at https://github.com/mckellwoodland/dimen_reduce_mahal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Safe Iris Presentation Attack Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Mitcheff, Patrick Tinsley, Adam Czajka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a framework for a privacy-safe iris presentation attack
detection (PAD) method, designed solely with synthetically-generated,
identity-leakage-free iris images. Once trained, the method is evaluated in a
classical way using state-of-the-art iris PAD benchmarks. We designed two
generative models for the synthesis of ISO/IEC 19794-6-compliant iris images.
The first model synthesizes bona fide-looking samples. To avoid ``identity
leakage,'' the generated samples that accidentally matched those used in the
model's training were excluded. The second model synthesizes images of irises
with textured contact lenses and is conditioned by a given contact lens brand
to have better control over textured contact lens appearance when forming the
training set. Our experiments demonstrate that models trained solely on
synthetic data achieve a lower but still reasonable performance when compared
to solutions trained with iris images collected from human subjects. This is
the first-of-its-kind attempt to use solely synthetic data to train a
fully-functional iris PAD solution, and despite the performance gap between
regular and the proposed methods, this study demonstrates that with the
increasing fidelity of generative models, creating such privacy-safe iris PAD
methods may be possible. The source codes and generative models trained for
this work are offered along with the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMIU: Multimodal Multi-image Understanding for Evaluating Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, Kaipeng Zhang, Wenqi Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability to process multiple images is crucial for Large
Vision-Language Models (LVLMs) to develop a more thorough and nuanced
understanding of a scene. Recent multi-image LVLMs have begun to address this
need. However, their evaluation has not kept pace with their development. To
fill this gap, we introduce the Multimodal Multi-image Understanding (MMIU)
benchmark, a comprehensive evaluation suite designed to assess LVLMs across a
wide range of multi-image tasks. MMIU encompasses 7 types of multi-image
relationships, 52 tasks, 77K images, and 11K meticulously curated
multiple-choice questions, making it the most extensive benchmark of its kind.
Our evaluation of 24 popular LVLMs, including both open-source and proprietary
models, reveals significant challenges in multi-image comprehension,
particularly in tasks involving spatial understanding. Even the most advanced
models, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Through
multi-faceted analytical experiments, we identify key performance gaps and
limitations, providing valuable insights for future model and data
improvements. We aim for MMIU to advance the frontier of LVLM research and
development, moving us toward achieving sophisticated multimodal multi-image
user interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mmiu-bench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RCDM: Enabling Robustness for Conditional Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weifeng Xu, Xiang Zhu, Xiaoyong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conditional diffusion model (CDM) enhances the standard diffusion model
by providing more control, improving the quality and relevance of the outputs,
and making the model adaptable to a wider range of complex tasks. However,
inaccurate conditional inputs in the inverse process of CDM can easily lead to
generating fixed errors in the neural network, which diminishes the
adaptability of a well-trained model. The existing methods like data
augmentation, adversarial training, robust optimization can improve the
robustness, while they often face challenges such as high computational
complexity, limited applicability to unknown perturbations, and increased
training difficulty. In this paper, we propose a lightweight solution, the
Robust Conditional Diffusion Model (RCDM), based on control theory to
dynamically reduce the impact of noise and significantly enhance the model's
robustness. RCDM leverages the collaborative interaction between two neural
networks, along with optimal control strategies derived from control theory, to
optimize the weights of two networks during the sampling process. Unlike
conventional techniques, RCDM establishes a mathematical relationship between
fixed errors and the weights of the two neural networks without incurring
additional computational overhead. Extensive experiments were conducted on
MNIST and CIFAR-10 datasets, and the results demonstrate the effectiveness and
adaptability of our proposed model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scribble-Based Interactive Segmentation of Medical Hyperspectral Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonghao Wang, Junwen Wang, Charlie Budd, Oscar MacCormac, Jonathan Shapey, Tom Vercauteren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral imaging (HSI) is an advanced medical imaging modality that
captures optical data across a broad spectral range, providing novel insights
into the biochemical composition of tissues. HSI may enable precise
differentiation between various tissue types and pathologies, making it
particularly valuable for tumour detection, tissue classification, and disease
diagnosis.
  Deep learning-based segmentation methods have shown considerable
advancements, offering automated and accurate results. However, these methods
face challenges with HSI datasets due to limited annotated data and
discrepancies from hardware and acquisition
techniques~\cite{clancy2020surgical,studier2023heiporspectral}. Variability in
clinical protocols also leads to different definitions of structure boundaries.
Interactive segmentation methods, utilizing user knowledge and clinical
insights, can overcome these issues and achieve precise segmentation results
\cite{zhao2013overview}.
  This work introduces a scribble-based interactive segmentation framework for
medical hyperspectral images. The proposed method utilizes deep learning for
feature extraction and a geodesic distance map generated from user-provided
scribbles to obtain the segmentation results. The experiment results show that
utilising the geodesic distance maps based on deep learning-extracted features
achieved better segmentation results than geodesic distance maps directly
generated from hyperspectral images, reconstructed RGB images, or Euclidean
distance maps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models
  within Perturbed Inputs <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen, Jiajun Chen, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have demonstrated remarkable
performance on various visual-language understanding and generation tasks.
However, MLLMs occasionally generate content inconsistent with the given
images, which is known as "hallucination". Prior works primarily center on
evaluating hallucination using standard, unperturbed benchmarks, which overlook
the prevalent occurrence of perturbed inputs in real-world scenarios-such as
image cropping or blurring-that are critical for a comprehensive assessment of
MLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,
the first benchmark designed to evaluate Hallucination in MLLMs within
Perturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,
containing 1,260 perturbed images from 11 object types. Each image is
accompanied by detailed annotations, which include fine-grained hallucination
types, such as existence, attribute, and relation. We equip these annotations
with a rich set of questions, making Hallu-PI suitable for both discriminative
and generative tasks. Extensive experiments on 12 mainstream MLLMs, such as
GPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant
hallucinations on Hallu-PI, which is not observed in unperturbed scenarios.
Furthermore, our research reveals a severe bias in MLLMs' ability to handle
different types of hallucinations. We also design two baselines specifically
for perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope
that our study will bring researchers' attention to the limitations of MLLMs
when dealing with perturbed inputs, and spur further investigations to address
this issue. Our code and datasets are publicly available at
https://github.com/NJUNLP/Hallu-PI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acccepted by ACM MM 2024, 14 pages, 11 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting
  Idiopathic Pulmonary Fibrosis Progression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00938v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00938v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caiwen Jiang, Xiaodan Xing, Zaixin Ou, Mianxin Liu, Walsh Simon, Guang Yang, Dinggang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly
correlates with higher patient mortality rates. Early detection of IPF
progression is critical for initiating timely treatment, which can effectively
slow down the advancement of the disease. However, the current clinical
criteria define disease progression requiring two CT scans with a one-year
interval, presenting a dilemma: a disease progression is identified only after
the disease has already progressed. To this end, in this paper, we develop a
novel diffusion model to accurately predict the progression of IPF by
generating patient's follow-up CT scan from the initial CT scan. Specifically,
from the clinical prior knowledge, we tailor improvements to the traditional
diffusion model and propose a Clinically-Informed Residual Diffusion model,
called CIResDiff. The key innovations of CIResDiff include 1) performing the
target region pre-registration to align the lung regions of two CT scans at
different time points for reducing the generation difficulty, 2) adopting the
residual diffusion instead of traditional diffusion to enable the model focus
more on differences (i.e., lesions) between the two CT scans rather than the
largely identical anatomical content, and 3) designing the clinically-informed
process based on CLIP technology to integrate lung function information which
is highly relevant to diagnosis into the reverse process for assisting
generation. Extensive experiments on clinical data demonstrate that our
approach can outperform state-of-the-art methods and effectively predict the
progression of IPF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Whole Slide Pathology Foundation Models through Stain
  Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juseung Yun, Yi Hu, Jinhyung Kim, Jongseong Jang, Soonyoung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in digital pathology have led to the development of
numerous foundational models that utilize self-supervised learning on patches
extracted from gigapixel whole slide images (WSIs). While this approach
leverages vast amounts of unlabeled data, we have discovered a significant
issue: features extracted from these self-supervised models tend to cluster by
individual WSIs, a phenomenon we term WSI-specific feature collapse. This
problem can potentially limit the model's generalization ability and
performance on various downstream tasks. To address this issue, we introduce
Stain Normalized Pathology Foundational Model, a novel foundational model
trained on patches that have undergone stain normalization. Stain normalization
helps reduce color variability arising from different laboratories and
scanners, enabling the model to learn more consistent features. Stain
Normalized Pathology Foundational Model is trained using 285,153,903 patches
extracted from a total of 34,795 WSIs, combining data from The Cancer Genome
Atlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments
demonstrate that Stain Normalized Pathology Foundational Model significantly
mitigates the feature collapse problem, indicating that the model has learned
more generalized features rather than overfitting to individual WSI
characteristics. We compared Stain Normalized Pathology Foundational Model with
state-of-the-art models across six downstream task datasets, and our results
show that Stain Normalized Pathology Foundational Model achieves excellent
performance relative to the number of WSIs used and the model's parameter
count. This suggests that the application of stain normalization has
substantially improved the model's efficiency and generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRACTAL: An Ultra-Large-Scale Aerial Lidar <span class="highlight-title">Dataset</span> for 3D Semantic
  Segmentation of Diverse Landscapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04634v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04634v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Gaydon, Michel Daab, Floryne Roche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a
new tool to monitor territory and support public policies. Processing ALS data
at scale requires efficient point classification methods that perform well over
highly diverse territories. To evaluate them, researchers need large annotated
Lidar datasets, however, current Lidar benchmark datasets have restricted scope
and often cover a single urban area. To bridge this data gap, we present the
FRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an
ultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with
high-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is
built upon France's nationwide open Lidar data. It achieves spatial and
semantic diversity via a sampling scheme that explicitly concentrates rare
classes and challenging landscapes from five French regions. It should support
the development of 3D deep learning approaches for large-scale land monitoring.
We describe the nature of the source data, the sampling workflow, the content
of the resulting dataset, and provide an initial evaluation of segmentation
performance using a performant 3D neural architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages | 9 figures | 8 tables | Dataset is available at
  https://huggingface.co/datasets/IGNF/FRACTAL | Trained model is available at
  https://huggingface.co/IGNF/FRACTAL-LidarHD_7cl_randlanet | Deep learning
  code repository is on Gihtub at https://github.com/IGNF/myria3d | Data
  engineering code repository is on Github at https://github.com/IGNF/pacasam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantised Global Autoencoder: A Holistic Approach to Representing Visual
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Elsner, Paula Usinger, Victor Czech, Gregor Kobsik, Yanjiang He, Isaak Lim, Leif Kobbelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In quantised autoencoders, images are usually split into local patches, each
encoded by one token. This representation is redundant in the sense that the
same number of tokens is spend per region, regardless of the visual information
content in that region. Adaptive discretisation schemes like quadtrees are
applied to allocate tokens for patches with varying sizes, but this just varies
the region of influence for a token which nevertheless remains a local
descriptor. Modern architectures add an attention mechanism to the autoencoder
which infuses some degree of global information into the local tokens. Despite
the global context, tokens are still associated with a local image region. In
contrast, our method is inspired by spectral decompositions which transform an
input signal into a superposition of global frequencies. Taking the data-driven
perspective, we learn custom basis functions corresponding to the codebook
entries in our VQ-VAE setup. Furthermore, a decoder combines these basis
functions in a non-linear fashion, going beyond the simple linear superposition
of spectral decompositions. We can achieve this global description with an
efficient transpose operation between features and channels and demonstrate our
performance on compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Change Detection for Space Habitats Using 3D Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02396v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02396v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Santos, Holly Dinkel, Julia Di, Paulo V. K. Borges, Marina Moreira, Oleg Alexandrov, Brian Coltin, Trey Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an algorithm for scene change detection from point clouds
to enable autonomous robotic caretaking in future space habitats. Autonomous
robotic systems will help maintain future deep-space habitats, such as the
Gateway space station, which will be uncrewed for extended periods. Existing
scene analysis software used on the International Space Station (ISS) relies on
manually-labeled images for detecting changes. In contrast, the algorithm
presented in this work uses raw, unlabeled point clouds as inputs. The
algorithm first applies modified Expectation-Maximization Gaussian Mixture
Model (GMM) clustering to two input point clouds. It then performs change
detection by comparing the GMMs using the Earth Mover's Distance. The algorithm
is validated quantitatively and qualitatively using a test dataset collected by
an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth
images taken directly by Astrobee and full-scene reconstructed maps built with
RGB-D and pose data from Astrobee. The runtimes of the approach are also
analyzed in depth. The source code is publicly released to promote further
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, Manuscript was presented at the AIAA SciTech
  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:
  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:
  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption
  of Monocular Depth Estimation in Autonomous Navigation Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Bassem Ouni, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation (MDE) has advanced significantly, primarily
through the integration of convolutional neural networks (CNNs) and more
recently, Transformers. However, concerns about their susceptibility to
adversarial attacks have emerged, especially in safety-critical domains like
autonomous driving and robotic navigation. Existing approaches for assessing
CNN-based depth prediction methods have fallen short in inducing comprehensive
disruptions to the vision system, often limited to specific local areas. In
this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel
approach designed to comprehensively disrupt monocular depth estimation (MDE)
in autonomous navigation applications. Our patch is crafted to selectively
undermine MDE in two distinct ways: by distorting estimated distances or by
creating the illusion of an object disappearing from the system's perspective.
Notably, our patch is shape-sensitive, meaning it considers the specific shape
and scale of the target object, thereby extending its influence beyond
immediate proximity. Furthermore, our patch is trained to effectively address
different scales and distances from the camera. Experimental results
demonstrate that our approach induces a mean depth estimation error surpassing
0.5, impacting up to 99% of the targeted region for CNN-based MDE models.
Additionally, we investigate the vulnerability of Transformer-based MDE models
to patch-based attacks, revealing that SSAP yields a significant error of 0.59
and exerts substantial influence over 99% of the target region on these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2303.01351</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth
  Estimation for Autonomous Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01351v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01351v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, monocular depth estimation (MDE) has experienced significant
advancements in performance, largely attributed to the integration of
innovative architectures, i.e., convolutional neural networks (CNNs) and
Transformers. Nevertheless, the susceptibility of these models to adversarial
attacks has emerged as a noteworthy concern, especially in domains where safety
and security are paramount. This concern holds particular weight for MDE due to
its critical role in applications like autonomous driving and robotic
navigation, where accurate scene understanding is pivotal. To assess the
vulnerability of CNN-based depth prediction methods, recent work tries to
design adversarial patches against MDE. However, the existing approaches fall
short of inducing a comprehensive and substantially disruptive impact on the
vision system. Instead, their influence is partial and confined to specific
local areas. These methods lead to erroneous depth predictions only within the
overlapping region with the input image, without considering the
characteristics of the target object, such as its size, shape, and position. In
this paper, we introduce a novel adversarial patch named APARATE. This patch
possesses the ability to selectively undermine MDE in two distinct ways: by
distorting the estimated distances or by creating the illusion of an object
disappearing from the perspective of the autonomous system. Notably, APARATE is
designed to be sensitive to the shape and scale of the target object, and its
influence extends beyond immediate proximity. APARATE, results in a mean depth
estimation error surpassing $0.5$, significantly impacting as much as $99\%$ of
the targeted region when applied to CNN-based MDE models. Furthermore, it
yields a significant error of $0.34$ and exerts substantial influence over
$94\%$ of the target region in the context of Transformer-based MDE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Class-Incremental Learning with <span class="highlight-title">Pre-Train</span>ed Models:
  Generalizability and Adaptivity are All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da-Wei Zhou, Zi-Wen Cai, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) aims to adapt to emerging new classes
without forgetting old ones. Traditional CIL models are trained from scratch to
continually acquire knowledge as data evolves. Recently, pre-training has
achieved substantial progress, making vast pre-trained models (PTMs) accessible
for CIL. Contrary to traditional methods, PTMs possess generalizable
embeddings, which can be easily transferred for CIL. In this work, we revisit
CIL with PTMs and argue that the core factors in CIL are adaptivity for model
updating and generalizability for knowledge transferring. 1) We first reveal
that frozen PTM can already provide generalizable embeddings for CIL.
Surprisingly, a simple baseline (SimpleCIL) which continually sets the
classifiers of PTM to prototype features can beat state-of-the-art even without
training on the downstream task. 2) Due to the distribution gap between
pre-trained and downstream datasets, PTM can be further cultivated with
adaptivity via model adaptation. We propose AdaPt and mERge (APER), which
aggregates the embeddings of PTM and adapted models for classifier
construction. APER is a general framework that can be orthogonally combined
with any parameter-efficient tuning method, which holds the advantages of PTM's
generalizability and adapted model's adaptivity. 3) Additionally, considering
previous ImageNet-based benchmarks are unsuitable in the era of PTM due to data
overlapping, we propose four new benchmarks for assessment, namely ImageNet-A,
ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the
effectiveness of APER with a unified and concise framework. Code is available
at https://github.com/zhoudw-zdw/RevisitingCIL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCV. Code is available at:
  https://github.com/zhoudw-zdw/RevisitingCIL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Learners Meet Web Image-Text Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07088v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07088v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Zhao, Quan Cui, Hao Wu, Osamu Yoshie, Cheng Yang, Oisin Mac Aodha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many self-supervised learning methods are pre-trained on the well-curated
ImageNet-1K dataset. In this work, given the excellent scalability of web data,
we consider self-supervised pre-training on noisy web sourced image-text paired
data. First, we conduct a benchmark study of representative self-supervised
pre-training methods on large-scale web data in a like-for-like setting. We
compare a range of methods, including single-modal ones that use masked
training objectives and multi-modal ones that use image-text constrastive
training. We observe that existing multi-modal methods do not outperform their
single-modal counterparts on vision transfer learning tasks. We derive an
information-theoretical view to explain these benchmark results, which provides
insight into how to design a novel vision learner. Inspired by this insight, we
present a new visual representation pre-training method, MUlti-modal
Generator~(MUG), that learns from scalable web sourced image-text data. MUG
achieves state-of-the-art transfer performance on a variety of tasks and
demonstrates promising scaling properties. Pre-trained models and code will be
made public upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://bzhao.me/MUG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation Cost-Efficient Active Learning for Deep Metric Learning
  Driven Remote Sensing Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genc Hoxha, Gencer Sumbul, Julia Henkel, Lars Möllenbrok, Begüm Demir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep metric learning (DML) has shown to be effective for content-based image
retrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on a
high number of annotated images to accurately learn model parameters of deep
neural networks (DNNs). However, gathering such data is time-consuming and
costly. To address this, we propose an annotation cost-efficient active
learning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims to
create a small but informative training set made up of similar and dissimilar
image pairs to be utilized for accurately learning a metric space. The
informativeness of image pairs is evaluated by combining uncertainty and
diversity criteria. To assess the uncertainty of image pairs, we introduce two
algorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binary
classifier guided uncertainty estimation (BCGUE). MGUE algorithm automatically
estimates a threshold value that acts as a boundary between similar and
dissimilar image pairs based on the distances in the metric space. The closer
the similarity between image pairs is to the estimated threshold value the
higher their uncertainty. BCGUE algorithm estimates the uncertainty of the
image pairs based on the confidence of the classifier in assigning correct
similarity labels. The diversity criterion is assessed through a
clustering-based strategy. ANNEAL combines either MGUE or BCGUE algorithm with
the clustering-based strategy to select the most informative image pairs, which
are then labelled by expert annotators as similar or dissimilar. This way of
annotating images significantly reduces the annotation cost compared to
annotating images with land-use land-cover class labels. Experimental results
on two RS benchmark datasets demonstrate the effectiveness of our method. The
code of this work is publicly available at
\url{https://git.tu-berlin.de/rsim/anneal_tgrs}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the IEEE Transactions on Geoscience and
  Remote Sensing (TGRS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Conceptual Understanding in Multimodal Contrastive Learning
  through Hard Negative Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp J. Rösch, Norbert Oswald, Michaela Geierhos, Jindřich Libovický
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current multimodal models leveraging contrastive learning often face
limitations in developing fine-grained conceptual understanding. This is due to
random negative samples during pretraining, causing almost exclusively very
dissimilar concepts to be compared in the loss function. Consequently, the
models struggle with fine-grained semantic differences. To address this
problem, we introduce a novel pretraining method incorporating synthetic hard
negative text examples. The hard negatives permute terms corresponding to
visual concepts, leading to a more fine-grained visual and textual concept
alignment. Further, we introduce InpaintCOCO, a new challenging dataset for
assessing the fine-grained alignment of colors, objects, and sizes in
vision-language models. We created the dataset using generative inpainting from
COCO images by changing the visual concepts so that the images no longer match
their original captions. Our results show significant improvements in
fine-grained concept understanding across a wide range of vision-language
datasets, including our InpaintCOCO dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ You Only Acquire Sparse-channel (YOAS): A Unified Framework for
  Dense-channel EEG Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Chen, Weiming Zeng, Luhui Cai, Lei Wang, Jia Lu, Yueyang Li, Hongjie Yan, Wai Ting Siok, Nizhuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-precision acquisition of dense-channel electroencephalogram (EEG)
signals is often impeded by the costliness and lack of portability of
equipment. In contrast, generating dense-channel EEG signals effectively from
sparse channels shows promise and economic viability. However, sparse-channel
EEG poses challenges such as reduced spatial resolution, information loss,
signal mixing, and heightened susceptibility to noise and interference. To
address these challenges, we first theoretically formulate the dense-channel
EEG generation problem as by optimizing a set of cross-channel EEG signal
generation problems. Then, we propose the YOAS framework for generating
dense-channel data from sparse-channel EEG signals. The YOAS totally consists
of four sequential stages: Data Preparation, Data Preprocessing, Biased-EEG
Generation, and Synthetic EEG Generation. Data Preparation and Preprocessing
carefully consider the distribution of EEG electrodes and low signal-to-noise
ratio problem of EEG signals. Biased-EEG Generation includes sub-modules of
BiasEEGGanFormer and BiasEEGDiffFormer, which facilitate long-term feature
extraction with attention and generate signals by combining electrode position
alignment with diffusion model, respectively. Synthetic EEG Generation
synthesizes the final signals, employing a deduction paradigm for multi-channel
EEG generation. Extensive experiments confirmed YOAS's feasibility, efficiency,
and theoretical validity, even remarkably enhancing data discernibility. This
breakthrough in dense-channel EEG signal generation from sparse-channel data
opens new avenues for exploration in EEG signal processing and application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ De-fine: Decomposing and Refining Visual Programs with Auto-Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12890v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12890v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghe Gao, Juncheng Li, Hao Fei, Liang Pang, Wei Ji, Guoming Wang, Zheqi Lv, Wenqiao Zhang, Siliang Tang, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual programming, a modular and generalizable paradigm, integrates
different modules and Python operators to solve various vision-language tasks.
Unlike end-to-end models that need task-specific data, it advances in
performing visual processing and reasoning in an unsupervised manner. Current
visual programming methods generate programs in a single pass for each task
where the ability to evaluate and optimize based on feedback, unfortunately, is
lacking, which consequentially limits their effectiveness for complex,
multi-step problems. Drawing inspiration from benders decomposition, we
introduce De-fine, a training-free framework that automatically decomposes
complex tasks into simpler subtasks and refines programs through auto-feedback.
This model-agnostic approach can improve logical reasoning performance by
integrating the strengths of multiple models. Our experiments across various
visual tasks show that De-fine creates more robust programs. Moreover, viewing
each feedback module as an independent agent will yield fresh prospects for the
field of agent research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenBias: Open-set Bias Detection in Text-to-Image Generative Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moreno D'Incà, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image generative models are becoming increasingly popular and
accessible to the general public. As these models see large-scale deployments,
it is necessary to deeply investigate their safety and fairness to not
disseminate and perpetuate any kind of biases. However, existing works focus on
detecting closed sets of biases defined a priori, limiting the studies to
well-known concepts. In this paper, we tackle the challenge of open-set bias
detection in text-to-image generative models presenting OpenBias, a new
pipeline that identifies and quantifies the severity of biases agnostically,
without access to any precompiled set. OpenBias has three stages. In the first
phase, we leverage a Large Language Model (LLM) to propose biases given a set
of captions. Secondly, the target generative model produces images using the
same set of captions. Lastly, a Vision Question Answering model recognizes the
presence and extent of the previously proposed biases. We study the behavior of
Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated
before. Via quantitative experiments, we demonstrate that OpenBias agrees with
current closed-set bias detection methods and human judgement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 Highlight - Code:
  https://github.com/Picsart-AI-Research/OpenBias</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning Friendly Vision-Language Model for Minecraft <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobin Jiang, Junpeng Yue, Hao Luo, Ziluo Ding, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the essential missions in the AI research community is to build an
autonomous embodied agent that can achieve high-level performance across a wide
spectrum of tasks. However, acquiring or manually designing rewards for all
open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal
contrastive learning framework architecture, CLIP4MC, aiming to learn a
reinforcement learning (RL) friendly vision-language model (VLM) that serves as
an intrinsic reward function for open-ended tasks. Simply utilizing the
similarity between the video snippet and the language prompt is not RL-friendly
since standard VLMs may only capture the similarity at a coarse level. To
achieve RL-friendliness, we incorporate the task completion degree into the VLM
training objective, as this information can assist agents in distinguishing the
importance between different states. Moreover, we provide neat YouTube datasets
based on the large-scale YouTube database provided by MineDojo. Specifically,
two rounds of filtering operations guarantee that the dataset covers enough
essential information and that the video-text pair is highly correlated.
Empirically, we demonstrate that the proposed method achieves better
performance on RL tasks compared with baselines. The code and datasets are
available at https://github.com/PKU-RL/CLIP4MC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghe Gao, Shuang Chen, Liang Pang, Yuan Yao, Jisheng Dang, Wenqiao Zhang, Juncheng Li, Siliang Tang, Yueting Zhuang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance of Multimodal Large Language Models (MLLMs) has
unequivocally demonstrated their proficient understanding capabilities in
handling a wide array of visual tasks. Nevertheless, the opaque nature of their
black-box reasoning processes persists as an enigma, rendering them
uninterpretable and struggling with hallucination. Their ability to execute
intricate compositional reasoning tasks is also constrained, culminating in a
stagnation of learning progression for these models. In this work, we introduce
Fact, a novel paradigm designed to generate multimodal rationales that are
faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes
verifiable visual programming to generate executable code guaranteeing
faithfulness and precision. Subsequently, through a series of operations
including pruning, merging, and bridging, the rationale enhances its
conciseness. Furthermore, we filter rationales that can be transferred to
end-to-end paradigms from programming paradigms to guarantee transferability.
Empirical evidence from experiments demonstrates the superiority of our method
across models of varying parameter sizes, significantly enhancing their
compositional reasoning and generalization ability. Our approach also reduces
hallucinations owing to its high correlation between images and text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revolutionizing Urban Safety Perception Assessments: Integrating
  Multimodal Large Language Models with Street View Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Zhang, Yunqin Li, Tomohiro Fukuda, Bowen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring urban safety perception is an important and complex task that
traditionally relies heavily on human resources. This process often involves
extensive field surveys, manual data collection, and subjective assessments,
which can be time-consuming, costly, and sometimes inconsistent. Street View
Images (SVIs), along with deep learning methods, provide a way to realize
large-scale urban safety detection. However, achieving this goal often requires
extensive human annotation to train safety ranking models, and the
architectural differences between cities hinder the transferability of these
models. Thus, a fully automated method for conducting safety evaluations is
essential. Recent advances in multimodal large language models (MLLMs) have
demonstrated powerful reasoning and analytical capabilities. Cutting-edge
models, e.g., GPT-4 have shown surprising performance in many tasks. We
employed these models for urban safety ranking on a human-annotated anchor set
and validated that the results from MLLMs align closely with human perceptions.
Additionally, we proposed a method based on the pre-trained Contrastive
Language-Image Pre-training (CLIP) feature and K-Nearest Neighbors (K-NN)
retrieval to quickly assess the safety index of the entire city. Experimental
results show that our method outperforms existing training needed deep learning
approaches, achieving efficient and accurate urban safety evaluations. The
proposed automation for urban safety perception assessment is a valuable tool
for city planners, policymakers, and researchers aiming to improve urban
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero shot VLMs for hate meme detection: Are we there yet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia content on social media is rapidly evolving, with memes gaining
prominence as a distinctive form. Unfortunately, some malicious users exploit
memes to target individuals or vulnerable communities, making it imperative to
identify and address such instances of hateful memes. Extensive research has
been conducted to address this issue by developing hate meme detection models.
However, a notable limitation of traditional machine/deep learning models is
the requirement for labeled datasets for accurate classification. Recently, the
research community has witnessed the emergence of several visual language
models that have exhibited outstanding performance across various tasks. In
this study, we aim to investigate the efficacy of these visual language models
in handling intricate tasks such as hate meme detection. We use various prompt
settings to focus on zero-shot classification of hateful/harmful memes. Through
our analysis, we observe that large VLMs are still vulnerable for zero-shot
hate meme detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Token Gradient Conflict in Mixture-of-Experts for Large
  Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longrong Yang, Dong Shen, Chaoxiang Cai, Fan Yang, Size Li, Di Zhang, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Mixture-of-Experts (MoE) has gained increasing attention in studying
Large Vision-Language Models (LVLMs). It uses a sparse model to replace the
dense model, achieving comparable performance while activating fewer parameters
during inference, thus significantly reducing the inference cost. Existing MoE
methods in LVLMs encourage different experts to handle different tokens, and
they usually employ a router to predict the routing of each token. However, the
predictions are based solely on sample features and do not truly reveal the
optimization directions of tokens. This may lead to severe optimization
interference between different tokens assigned to an expert. To address this
problem, this paper proposes a novel method based on token-level gradient
analysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first
use token-level gradients to identify conflicting tokens in experts. After
that, we add a specialized loss tailored to eliminate conflicts among tokens
within each expert. Our method can serve as a plug-in for diverse Large
Vision-Language Models, and extensive experimental results demonstrate its
effectiveness. The code will be publicly available at
https://github.com/longrongyang/STGC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open Sesame! Universal Black Box Jailbreaking of Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01446v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01446v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raz Lapid, Ron Langberg, Moshe Sipper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), designed to provide helpful and safe responses,
often rely on alignment techniques to align with user intent and social
guidelines. Unfortunately, this alignment can be exploited by malicious actors
seeking to manipulate an LLM's outputs for unintended purposes. In this paper
we introduce a novel approach that employs a genetic algorithm (GA) to
manipulate LLMs when model architecture and parameters are inaccessible. The GA
attack works by optimizing a universal adversarial prompt that -- when combined
with a user's query -- disrupts the attacked model's alignment, resulting in
unintended and potentially harmful outputs. Our novel approach systematically
reveals a model's limitations and vulnerabilities by uncovering instances where
its responses deviate from expected behavior. Through extensive experiments we
demonstrate the efficacy of our technique, thus contributing to the ongoing
discussion on responsible AI development by providing a diagnostic tool for
evaluating and enhancing alignment of LLMs with human intent. To our knowledge
this is the first automated universal black box jailbreak attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SeT-LLM @ ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Synergy between Data and Multi-Modal Large Language Models: A <span class="highlight-title">Survey</span>
  from Co-Development Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Qin, Daoyuan Chen, Wenhao Zhang, Liuyi Yao, Yilun Huang, Bolin Ding, Yaliang Li, Shuiguang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has been witnessed in
recent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the
modality from text to a broader spectrum of domains, attracting widespread
attention due to the broader range of application scenarios. As LLMs and MLLMs
rely on vast amounts of model parameters and data to achieve emergent
capabilities, the importance of data is receiving increasingly widespread
attention and recognition. Tracing and analyzing recent data-oriented works for
MLLMs, we find that the development of models and data is not two separate
paths but rather interconnected. On the one hand, vaster and higher-quality
data contribute to better performance of MLLMs; on the other hand, MLLMs can
facilitate the development of data. The co-development of multi-modal data and
MLLMs requires a clear view of 1) at which development stages of MLLMs specific
data-centric approaches can be employed to enhance certain MLLM capabilities,
and 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal
data in specific roles. To promote the data-model co-development for MLLM
community, we systematically review existing works related to MLLMs from the
data-model co-development perspective. A regularly maintained project
associated with this survey is accessible at
https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work. 21 pages. Related materials are continually maintained
  and available at
  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reduced storage direct tensor ring decomposition for convolutional
  neural networks compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Gabor, Rafał Zdunek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are among the most widely used machine
learning models for computer vision tasks, such as image classification. To
improve the efficiency of CNNs, many CNNs compressing approaches have been
developed. Low-rank methods approximate the original convolutional kernel with
a sequence of smaller convolutional kernels, which leads to reduced storage and
time complexities. In this study, we propose a novel low-rank CNNs compression
method that is based on reduced storage direct tensor ring decomposition
(RSDTR). The proposed method offers a higher circular mode permutation
flexibility, and it is characterized by large parameter and FLOPS compression
rates, while preserving a good classification accuracy of the compressed
network. The experiments, performed on the CIFAR-10 and ImageNet datasets,
clearly demonstrate the efficiency of RSDTR in comparison to other
state-of-the-art CNNs compression approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatiotemporal Graph Guided Multi-modal Network for Livestreaming
  Product Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16248v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16248v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowan Hu, Yiyi Chen, Yan Li, Minquan Wang, Haoqian Wang, Quan Chen, Han Li, Peng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid expansion of e-commerce, more consumers have become accustomed
to making purchases via livestreaming. Accurately identifying the products
being sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a
fundamental and daunting challenge. The LPR task encompasses three primary
dilemmas in real-world scenarios: 1) the recognition of intended products from
distractor products present in the background; 2) the video-image heterogeneity
that the appearance of products showcased in live streams often deviates
substantially from standardized product images in stores; 3) there are numerous
confusing products with subtle visual nuances in the shop. To tackle these
challenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).
First, we employ a text-guided attention mechanism that leverages the spoken
content of salespeople to guide the model to focus toward intended products,
emphasizing their salience over cluttered background products. Second, a
long-range spatiotemporal graph network is further designed to achieve both
instance-level interaction and frame-level matching, solving the misalignment
caused by video-image heterogeneity. Third, we propose a multi-modal hard
example mining, assisting the model in distinguishing highly similar products
with fine-grained features across the video-image-text domain. Through
extensive quantitative and qualitative experiments, we demonstrate the superior
performance of our proposed SGMN model, surpassing the state-of-the-art methods
by a substantial margin. The code is available at
https://github.com/Huxiaowan/SGMN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infusion: internal diffusion for inpainting of dynamic textures and
  complex motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Cherel, Andrés Almansa, Yann Gousseau, Alasdair Newson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video inpainting is the task of filling a region in a video in a visually
convincing manner. It is very challenging due to the high dimensionality of the
data and the temporal consistency required for obtaining convincing results.
Recently, diffusion models have shown impressive results in modeling complex
data distributions, including images and videos. Such models remain nonetheless
very expensive to train and to perform inference with, which strongly reduce
their applicability to videos, and yields unreasonable computational loads. We
show that in the case of video inpainting, thanks to the highly auto-similar
nature of videos, the training data of a diffusion model can be restricted to
the input video and still produce very satisfying results. This leads us to
adopt an internal learning approach, which also allows us to greatly reduce the
neural network size by about three orders of magnitude less than current
diffusion models used for image inpainting. We also introduce a new method for
efficient training and inference of diffusion models in the context of internal
learning, by splitting the diffusion process into different learning intervals
corresponding to different noise levels of the diffusion process. To the best
of our knowledge, this is the first video inpainting method based purely on
diffusion. Other methods require additional components such as optical flow
estimation, which limits their performance in the case of dynamic textures and
complex motions. We show qualitative and quantitative results, demonstrating
that our method reaches state of the art performance in the case of dynamic
textures and complex dynamic backgrounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxue Xu, Yihui Wang, Fengtao Zhou, Jiabo Ma, Shu Yang, Huangjing Lin, Xin Wang, Jiguang Wang, Li Liang, Anjia Han, Ronald Cheong Kin Chan, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remarkable strides in computational pathology have been made in the
task-agnostic foundation model that advances the performance of a wide array of
downstream clinical tasks. Despite the promising performance, there are still
several challenges. First, prior works have resorted to either vision-only or
vision-captions data, disregarding invaluable pathology reports and gene
expression profiles which respectively offer distinct knowledge for versatile
clinical applications. Second, the current progress in pathology FMs
predominantly concentrates on the patch level, where the restricted context of
patch-level pretraining fails to capture whole-slide patterns. Here we curated
the largest multimodal dataset consisting of H\&E diagnostic whole slide images
and their associated pathology reports and RNA-Seq data, resulting in 26,169
slide-level modality pairs from 10,275 patients across 32 cancer types. To
leverage these data for CPath, we propose a novel whole-slide pretraining
paradigm which injects multimodal knowledge at the whole-slide context into the
pathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed
paradigm revolutionizes the workflow of pretraining for CPath, which enables
the pathology FM to acquire the whole-slide context. To our knowledge, this is
the first attempt to incorporate multimodal knowledge at the slide level for
enhancing pathology FMs, expanding the modelling context from unimodal to
multimodal knowledge and from patch-level to slide-level. To systematically
evaluate the capabilities of mSTAR, extensive experiments including slide-level
unimodal and multimodal applications, are conducted across 7 diverse types of
tasks on 43 subtasks, resulting in the largest spectrum of downstream tasks.
The average performance in various slide-level applications consistently
demonstrates significant performance enhancements for mSTAR compared to SOTA
FMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Scale-Variant Attention for Segmenting Small Medical Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07720v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07720v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Dai, Rui Liu, Zixuan Wu, Tianyi Wu, Min Wang, Junxian Zhou, Yixuan Yuan, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection and accurate diagnosis can predict the risk of malignant
disease transformation, thereby increasing the probability of effective
treatment. Identifying mild syndrome with small pathological regions serves as
an ominous warning and is fundamental in the early diagnosis of diseases. While
deep learning algorithms, particularly convolutional neural networks (CNNs),
have shown promise in segmenting medical objects, analyzing small areas in
medical images remains challenging. This difficulty arises due to information
losses and compression defects from convolution and pooling operations in CNNs,
which become more pronounced as the network deepens, especially for small
medical objects. To address these challenges, we propose a novel scale-variant
attention-based network (SvANet) for accurately segmenting small-scale objects
in medical images. The SvANet consists of scale-variant attention, cross-scale
guidance, Monte Carlo attention, and vision transformer, which incorporates
cross-scale features and alleviates compression artifacts for enhancing the
discrimination of small medical objects. Quantitative experimental results
demonstrate the superior performance of SvANet, achieving 96.12%, 96.11%,
89.79%, 84.15%, 80.25%, 73.05%, and 72.58% in mean Dice coefficient for
segmenting kidney tumors, skin lesions, hepatic tumors, polyps, surgical
excision cells, retinal vasculatures, and sperms, which occupy less than 1% of
the image areas in KiTS23, ISIC 2018, ATLAS, PolypGen, TissueNet, FIVES, and
SpermHealth datasets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Cross-Domain Point Classification via Distilling Relational
  Priors from 2D <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longkun Zou, Wanru Zhu, Ke Chen, Lihua Guo, Kailing Guo, Kui Jia, Yaowei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic pattern of an object point cloud is determined by its topological
configuration of local geometries. Learning discriminative representations can
be challenging due to large shape variations of point sets in local regions and
incomplete surface in a global perspective, which can be made even more severe
in the context of unsupervised domain adaptation (UDA). In specific,
traditional 3D networks mainly focus on local geometric details and ignore the
topological structure between local geometries, which greatly limits their
cross-domain generalization. Recently, the transformer-based models have
achieved impressive performance gain in a range of image-based tasks,
benefiting from its strong generalization capability and scalability stemming
from capturing long range correlation across local patches. Inspired by such
successes of visual transformers, we propose a novel Relational Priors
Distillation (RPD) method to extract relational priors from the well-trained
transformers on massive images, which can significantly empower cross-domain
representations with consistent topological priors of objects. To this end, we
establish a parameter-frozen pre-trained transformer module shared between 2D
teacher and 3D student models, complemented by an online knowledge distillation
strategy for semantically regularizing the 3D student model. Furthermore, we
introduce a novel self-supervised task centered on reconstructing masked point
cloud patches using corresponding masked multi-view image features, thereby
empowering the model with incorporating 3D geometric information. Experiments
on the PointDA-10 and the Sim-to-Real datasets verify that the proposed method
consistently achieves the state-of-the-art performance of UDA for point cloud
classification. The source code of this work is available at
https://github.com/zou-longkun/RPD.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARINE: A Computer Vision Model for Detecting Rare Predator-Prey
  Interactions in Animal Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zsófia Katona, Seyed Sahand Mohammadi Ziabari, Fatemeh Karimi Nejadasl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encounters between predator and prey play an essential role in ecosystems,
but their rarity makes them difficult to detect in video recordings. Although
advances in action recognition (AR) and temporal action detection (AD),
especially transformer-based models and vision foundation models, have achieved
high performance on human action datasets, animal videos remain relatively
under-researched. This thesis addresses this gap by proposing the model MARINE,
which utilizes motion-based frame selection designed for fast animal actions
and DINOv2 feature extraction with a trainable classification head for action
recognition. MARINE outperforms VideoMAE in identifying predator attacks in
videos of fish, both on a small and specific coral reef dataset (81.53\%
against 52.64\% accuracy), and on a subset of the more extensive Animal Kingdom
dataset (94.86\% against 83.14\% accuracy). In a multi-label setting on a
representative sample of Animal Kingdom, MARINE achieves 23.79\% mAP,
positioning it mid-field among existing benchmarks. Furthermore, in an AD task
on the coral reef dataset, MARINE achieves 80.78\% AP (against VideoMAE's
34.89\%) although at a lowered t-IoU threshold of 25\%. Therefore, despite room
for improvement, MARINE offers an effective starter framework to apply to AR
and AD tasks on animal recordings and thus contribute to the study of natural
ecosystems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an MSc thesis by Zsofia Katona, supervised by the two other
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Foundation Models via Knowledge Distillation in Multi-Object
  Tracking: Distilling DINOv2 Features to FairMOT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niels G. Faber, Seyed Sahand Mohammadi Ziabari, Fatemeh Karimi Nejadasl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple Object Tracking (MOT) is a computer vision task that has been
employed in a variety of sectors. Some common limitations in MOT are varying
object appearances, occlusions, or crowded scenes. To address these challenges,
machine learning methods have been extensively deployed, leveraging large
datasets, sophisticated models, and substantial computational resources. Due to
practical limitations, access to the above is not always an option. However,
with the recent release of foundation models by prominent AI companies,
pretrained models have been trained on vast datasets and resources using
state-of-the-art methods. This work tries to leverage one such foundation
model, called DINOv2, through using knowledge distillation. The proposed method
uses a teacher-student architecture, where DINOv2 is the teacher and the
FairMOT backbone HRNetv2 W18 is the student. The results imply that although
the proposed method shows improvements in certain scenarios, it does not
consistently outperform the original FairMOT model. These findings highlight
the potential and limitations of applying foundation models in knowledge
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an MSc thesis by Niels Faber, supervised by the two other
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GTNet: Graph <span class="highlight-title">Transformer</span> Network for 3D Point Cloud Classification and
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15213v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15213v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhou, Qian Wang, Weiwei Jin, Xinzhe Shi, Ying He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, graph-based and Transformer-based deep learning networks have
demonstrated excellent performances on various point cloud tasks. Most of the
existing graph methods are based on static graph, which take a fixed input to
establish graph relations. Moreover, many graph methods apply maximization and
averaging to aggregate neighboring features, so that only a single neighboring
point affects the feature of centroid or different neighboring points have the
same influence on the centroid's feature, which ignoring the correlation and
difference between points. Most Transformer-based methods extract point cloud
features based on global attention and lack the feature learning on local
neighbors. To solve the problems of these two types of models, we propose a new
feature extraction block named Graph Transformer and construct a 3D point point
cloud learning network called GTNet to learn features of point clouds on local
and global patterns. Graph Transformer integrates the advantages of graph-based
and Transformer-based methods, and consists of Local Transformer and Global
Transformer modules. Local Transformer uses a dynamic graph to calculate all
neighboring point weights by intra-domain cross-attention with dynamically
updated graph relations, so that every neighboring point could affect the
features of centroid with different weights; Global Transformer enlarges the
receptive field of Local Transformer by a global self-attention. In addition,
to avoid the disappearance of the gradient caused by the increasing depth of
network, we conduct residual connection for centroid features in GTNet; we also
adopt the features of centroid and neighbors to generate the local geometric
descriptors in Local Transformer to strengthen the local information learning
capability of the model. Finally, we use GTNet for shape classification, part
segmentation and semantic segmentation tasks in this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HAIFIT: Fashion Image Translation for Human-to-AI Style Learning and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08651v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08651v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Jiang, Xinglin Li, Weiren Yu, Di Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of fashion design, sketches serve as the canvas for expressing
an artist's distinctive drawing style and creative vision, capturing intricate
details like stroke variations and texture nuances. The advent of
sketch-to-image cross-modal translation technology has notably aided designers.
However, existing methods often compromise these sketch details during image
generation, resulting in images that deviate from the designer's intended
concept. This limitation hampers the ability to offer designers a precise
preview of the final output. To overcome this challenge, we introduce HAIFIT, a
novel approach that transforms sketches into high-fidelity, lifelike clothing
images by integrating multi-scale features and capturing extensive feature map
dependencies from diverse perspectives. Through extensive qualitative and
quantitative evaluations conducted on our self-collected dataset, our method
demonstrates superior performance compared to existing methods in generating
photorealistic clothing images. Our method excels in preserving the distinctive
style and intricate details essential for fashion design applications. In
addition, our method also has obvious advantages in model training and
inference speed, contributing to reducing designers' time costs and improving
design efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group Multi-View <span class="highlight-title">Transformer</span> for 3D Shape Analysis with Spatial Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16477v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16477v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixiang Xu, Qingzhe Cui, Richang Hong, Wei Xu, Enhong Chen, Xin Yuan, Chenglong Li, Yuanyan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the results of view-based 3D shape recognition methods have
saturated, and models with excellent performance cannot be deployed on
memory-limited devices due to their huge size of parameters. To address this
problem, we introduce a compression method based on knowledge distillation for
this field, which largely reduces the number of parameters while preserving
model performance as much as possible. Specifically, to enhance the
capabilities of smaller models, we design a high-performing large model called
Group Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first
establishes relationships between view-level features. Additionally, to capture
deeper features, we employ the grouping module to enhance view-level features
into group-level features. Finally, the group-level ViT aggregates group-level
features into complete, well-formed 3D shape descriptors. Notably, in both
ViTs, we introduce spatial encoding of camera coordinates as innovative
position embeddings. Furthermore, we propose two compressed versions based on
GMViT, namely GMViT-simple and GMViT-mini. To enhance the training
effectiveness of the small models, we introduce a knowledge distillation method
throughout the GMViT process, where the key outputs of each GMViT component
serve as distillation targets. Extensive experiments demonstrate the efficacy
of the proposed method. The large model GMViT achieves excellent 3D
classification and retrieval results on the benchmark datasets ModelNet,
ShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,
reduce the parameter size by 8 and 17.6 times, respectively, and improve shape
recognition speed by 1.5 times on average, while preserving at least 90% of the
classification and retrieval performance. The code is available at
https://github.com/bigdata-graph/GMViT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13pages, 8 figuers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based
  sky-segmentation in urban canyon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingrong Wang, Bo Xu, Ronghe Jin, Shoujian Zhang, Kefu Gao, Jingnan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate, continuous, and reliable positioning is a critical component of
achieving autonomous driving. However, in complex urban canyon environments,
the vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused
by high buildings, trees, and elevated structures seriously affect positioning
results. To address these challenges, a sky-view images segmentation algorithm
based on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection.
Building upon this, a novel NLOS detection and mitigation algorithm (named
S-NDM) is extended to the tightly coupled Global Navigation Satellite Systems
(GNSS), Inertial Measurement Units (IMU), and visual feature system which is
called Sky-GVIO, with the aim of achieving continuous and accurate positioning
in urban canyon environments. Furthermore, the system harmonizes Single Point
Positioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its
operational versatility and resilience. In urban canyon environments, the
positioning performance of S-NDM algorithm proposed in this paper is evaluated
under different tightly coupled SPP-related and RTK-related models. The results
exhibit that Sky-GVIO system achieves meter-level accuracy under SPP mode and
sub-decimeter precision with RTK, surpassing the performance of GNSS/INS/Vision
frameworks devoid of S-NDM. Additionally, the sky-view image dataset, inclusive
of training and evaluation subsets, has been made publicly accessible for
scholarly exploration at https://github.com/whuwangjr/sky-view-images .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17521v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17521v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Yang, Dingkang Liang, Dingyuan Zhang, Zhe Liu, Zhikang Zou, Xingyu Jiang, Yingying Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in point cloud learning have enabled intelligent
vehicles and robots to comprehend 3D environments better. However, processing
large-scale 3D scenes remains a challenging problem, such that efficient
downsampling methods play a crucial role in point cloud learning. Existing
downsampling methods either require a huge computational burden or sacrifice
fine-grained geometric information. For such purpose, this paper presents an
advanced sampler that achieves both high accuracy and efficiency. The proposed
method utilizes voxel centroid sampling as a foundation but effectively
addresses the challenges regarding voxel size determination and the
preservation of critical geometric cues. Specifically, we propose a Voxel
Adaptation Module that adaptively adjusts voxel sizes with the reference of
point-based downsampling ratio. This ensures that the sampling results exhibit
a favorable distribution for comprehending various 3D objects or scenes.
Meanwhile, we introduce a network compatible with arbitrary voxel sizes for
sampling and feature extraction while maintaining high efficiency. The proposed
approach is demonstrated with 3D object detection and 3D semantic segmentation.
Compared to existing state-of-the-art methods, our approach achieves better
accuracy on outdoor and indoor large-scale datasets, e.g. Waymo and ScanNet,
with promising efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03713v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03713v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekta Prashnani, Koki Nagano, Shalini De Mello, David Luebke, Orazio Gallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern avatar generators allow anyone to synthesize photorealistic real-time
talking avatars, ushering in a new era of avatar-based human communication,
such as with immersive AR/VR interactions or videoconferencing with limited
bandwidths. Their safe adoption, however, requires a mechanism to verify if the
rendered avatar is trustworthy: does it use the appearance of an individual
without their consent? We term this task avatar fingerprinting. To tackle it,
we first introduce a large-scale dataset of real and synthetic videos of people
interacting on a video call, where the synthetic videos are generated using the
facial appearance of one person and the expressions of another. We verify the
identity driving the expressions in a synthetic video, by learning motion
signatures that are independent of the facial appearance shown. Our solution,
the first in this space, achieves an average AUC of 0.85. Critical to its
practical use, it also generalizes to new generators never seen in training
(average AUC of 0.83). The proposed dataset and other resources can be found
at: https://research.nvidia.com/labs/nxp/avatar-fingerprinting/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Realistic Human Motion Generation with Cross-Diffusion Models <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10993v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10993v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeping Ren, Shaoli Huang, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Cross Human Motion Diffusion Model (CrossDiff), a novel
approach for generating high-quality human motion based on textual
descriptions. Our method integrates 3D and 2D information using a shared
transformer network within the training of the diffusion model, unifying motion
noise into a single feature space. This enables cross-decoding of features into
both 3D and 2D motion representations, regardless of their original dimension.
The primary advantage of CrossDiff is its cross-diffusion mechanism, which
allows the model to reverse either 2D or 3D noise into clean motion during
training. This capability leverages the complementary information in both
motion representations, capturing intricate human movement details often missed
by models relying solely on 3D information. Consequently, CrossDiff effectively
combines the strengths of both representations to generate more realistic
motion sequences. In our experiments, our model demonstrates competitive
state-of-the-art performance on text-to-motion benchmarks. Moreover, our method
consistently provides enhanced motion generation quality, capturing complex
full-body movement intricacies. Additionally, with a pretrained model,our
approach accommodates using in the wild 2D motion data without 3D motion ground
truth during training to generate 3D motion, highlighting its potential for
broader applications and efficient use of available data resources. Project
page: https://wonderno.github.io/CrossDiff-webpage/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from
  bi-planar X-ray images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixing Tan, Shuang Song, Yaofeng He, Kangneng Zhou, Tong Lu, Ruoxiu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray images ease the diagnosis and treatment process due to their rapid
imaging speed and high resolution. However, due to the projection process of
X-ray imaging, much spatial information has been lost. To accurately provide
efficient spinal morphological and structural information, reconstructing the
3-D structures of the spine from the 2-D X-ray images is essential. It is
challenging for current reconstruction methods to preserve the edge information
and local shapes of the asymmetrical vertebrae structures. In this study, we
propose a new Edge-Aware Reconstruction network (EAR) to focus on the
performance improvement of the edge information and vertebrae shapes. In our
network, by using the auto-encoder architecture as the backbone, the edge
attention module and frequency enhancement module are proposed to strengthen
the perception of the edge reconstruction. Meanwhile, we also combine four loss
terms, including reconstruction loss, edge loss, frequency loss and projection
loss. The proposed method is evaluated using three publicly accessible datasets
and compared with four state-of-the-art models. The proposed method is superior
to other methods and achieves 25.32%, 15.32%, 86.44%, 80.13%, 23.7612 and
0.3014 with regard to MSE, MAE, Dice, SSIM, PSNR and frequency distance. Due to
the end-to-end and accurate reconstruction process, EAR can provide sufficient
3-D spatial information and precise preoperative surgical planning guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Region-Growing Network for Object Segmentation in
  Atmospheric Turbulence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehao Qin, Ripon Saha, Suren Jayasuriya, Jinwei Ye, Nianyi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moving object segmentation in the presence of atmospheric turbulence is
highly challenging due to turbulence-induced irregular and time-varying
distortions. In this paper, we present an unsupervised approach for segmenting
moving objects in videos downgraded by atmospheric turbulence. Our key approach
is a detect-then-grow scheme: we first identify a small set of moving object
pixels with high confidence, then gradually grow a foreground mask from those
seeds to segment all moving objects. This method leverages rigid geometric
consistency among video frames to disentangle different types of motions, and
then uses the Sampson distance to initialize the seedling pixels. After growing
per-frame foreground masks, we use spatial grouping loss and temporal
consistency loss to further refine the masks in order to ensure their
spatio-temporal consistency. Our method is unsupervised and does not require
training on labeled data. For validation, we collect and release the first
real-captured long-range turbulent video dataset with ground truth masks for
moving objects. Results show that our method achieves good accuracy in
segmenting moving objects and is robust for long-range videos with various
turbulence strengths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Lin, Jiajun Chen, Kunyu Peng, Xuan He, Zhiyong Li, Rainer Stiefelhagen, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the task of Auditory Referring Multi-Object Tracking
(AR-MOT), which dynamically tracks specific objects in a video sequence based
on audio expressions and appears as a challenging problem in autonomous
driving. Due to the lack of semantic modeling capacity in audio and video,
existing works have mainly focused on text-based multi-object tracking, which
often comes at the cost of tracking quality, interaction efficiency, and even
the safety of assistance systems, limiting the application of such methods in
autonomous driving. In this paper, we delve into the problem of AR-MOT from the
perspective of audio-video fusion and audio-video tracking. We put forward
EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers.
The dual streams are intertwined with our Bidirectional Frequency-domain
Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and
video features from both frequency- and spatiotemporal domains. Moreover, we
propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract
homogeneous semantic features between expressions and visual objects by
learning homogeneous features between different audio and video objects
effectively. Aside from the architectural design, we establish the first set of
large-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD.
Extensive experiments on the established benchmarks demonstrate the
effectiveness of the proposed EchoTrack and its components. The source code and
datasets are available at https://github.com/lab206/EchoTrack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Intelligent Transportation Systems
  (T-ITS). The source code and datasets are available at
  https://github.com/lab206/EchoTrack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularized Multi-Decoder Ensemble for an Error-Aware Scene
  Representation Network <span class="chip">IEEE VIS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Xiong, Skylar W. Wurster, Hanqi Guo, Tom Peterka, Han-Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature grid Scene Representation Networks (SRNs) have been applied to
scientific data as compact functional surrogates for analysis and
visualization. As SRNs are black-box lossy data representations, assessing the
prediction quality is critical for scientific visualization applications to
ensure that scientists can trust the information being visualized. Currently,
existing architectures do not support inference time reconstruction quality
assessment, as coordinate-level errors cannot be evaluated in the absence of
ground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN)
ensemble architecture consisting of a shared feature grid with multiple
lightweight multi-layer perceptron decoders. MDSRN can generate a set of
plausible predictions for a given input coordinate to compute the mean as the
prediction of the multi-decoder ensemble and the variance as a confidence
score. The coordinate-level variance can be rendered along with the data to
inform the reconstruction quality, or be integrated into uncertainty-aware
volume visualization algorithms. To prevent the misalignment between the
quantified variance and the prediction quality, we propose a novel variance
regularization loss for ensemble learning that promotes the Regularized
multi-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates
closely to the true model error. We comprehensively evaluate the quality of
variance quantification and data reconstruction of Monte Carlo Dropout, Mean
Field Variational Inference, Deep Ensemble, and Predicting Variance compared to
the proposed MDSRN and RMDSRN across diverse scalar field datasets. We
demonstrate that RMDSRN attains the most accurate data reconstruction and
competitive variance-error correlation among uncertain SRNs under the same
neural network parameter budgets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proc. IEEE VIS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relational Representation Learning Network for Cross-Spectral Image
  Patch Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Yu, Yunpeng Liu, Jinmiao Zhao, Dou Quan, Zelin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, feature relation learning has drawn widespread attention in
cross-spectral image patch matching. However, existing related research focuses
on extracting diverse relations between image patch features and ignores
sufficient intrinsic feature representations of individual image patches.
Therefore, we propose an innovative relational representation learning idea
that simultaneously focuses on sufficiently mining the intrinsic features of
individual image patches and the relations between image patch features. Based
on this, we construct a Relational Representation Learning Network (RRL-Net).
Specifically, we innovatively construct an autoencoder to fully characterize
the individual intrinsic features, and introduce a feature interaction learning
(FIL) module to extract deep-level feature relations. To further fully mine
individual intrinsic features, a lightweight multi-dimensional global-to-local
attention (MGLA) module is constructed to enhance the global feature extraction
of individual image patches and capture local dependencies within global
features. By combining the MGLA module, we further explore the feature
extraction network and construct an attention-based lightweight feature
extraction (ALFE) network. In addition, we propose a multi-loss post-pruning
(MLPP) optimization strategy, which greatly promotes network optimization while
avoiding increases in parameters and inference time. Extensive experiments
demonstrate that our RRL-Net achieves state-of-the-art (SOTA) performance on
multiple public datasets. Our code will be made public later.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PathoDuet: Foundation Models for Pathological Slide Analysis of H&E and
  IHC Stains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyi Hua, Fang Yan, Tianle Shen, Lei Ma, Xiaofan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large amounts of digitized histopathological data display a promising future
for developing pathological foundation models via self-supervised learning
methods. Foundation models pretrained with these methods serve as a good basis
for downstream tasks. However, the gap between natural and histopathological
images hinders the direct application of existing methods. In this work, we
present PathoDuet, a series of pretrained models on histopathological images,
and a new self-supervised learning framework in histopathology. The framework
is featured by a newly-introduced pretext token and later task raisers to
explicitly utilize certain relations between images, like multiple
magnifications and multiple stains. Based on this, two pretext tasks,
cross-scale positioning and cross-stain transferring, are designed to pretrain
the model on Hematoxylin and Eosin (H&E) images and transfer the model to
immunohistochemistry (IHC) images, respectively. To validate the efficacy of
our models, we evaluate the performance over a wide variety of downstream
tasks, including patch-level colorectal cancer subtyping and whole slide image
(WSI)-level classification in H&E field, together with expression level
prediction of IHC marker, tumor identification and slide-level qualitative
analysis in IHC field. The experimental results show the superiority of our
models over most tasks and the efficacy of proposed pretext tasks. The codes
and models are available at https://github.com/openmedlab/PathoDuet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Semantic Segmentation with Query Points Supervision on Aerial
  Images <span class="chip">ICIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Rivier, Carlos Hinojosa, Silvio Giancola, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation is crucial in remote sensing, where high-resolution
satellite images are segmented into meaningful regions. Recent advancements in
deep learning have significantly improved satellite image segmentation.
However, most of these methods are typically trained in fully supervised
settings that require high-quality pixel-level annotations, which are expensive
and time-consuming to obtain. In this work, we present a weakly supervised
learning algorithm to train semantic segmentation algorithms that only rely on
query point annotations instead of full mask labels. Our proposed approach
performs accurate semantic segmentation and improves efficiency by
significantly reducing the cost and time required for manual annotation.
Specifically, we generate superpixels and extend the query point labels into
those superpixels that group similar meaningful semantics. Then, we train
semantic segmentation models supervised with images partially labeled with the
superpixel pseudo-labels. We benchmark our weakly supervised training approach
on an aerial image dataset and different semantic segmentation architectures,
showing that we can reach competitive performance compared to fully supervised
training while reducing the annotation effort. The code of our proposed
approach is publicly available at: https://github.com/santiago2205/LSSQPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper Accepted at ICIP 2024 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Unlearning: Fast and Efficient Gradient-free Approach to Class
  Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00761v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00761v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangamesh Kodge, Gobinda Saha, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning is a prominent and challenging field, driven by regulatory
demands for user data deletion and heightened privacy awareness. Existing
approaches involve retraining model or multiple finetuning steps for each
deletion request, often constrained by computational limits and restricted data
access. In this work, we introduce a novel class unlearning algorithm designed
to strategically eliminate specific classes from the learned model. Our
algorithm first estimates the Retain and the Forget Spaces using Singular Value
Decomposition on the layerwise activations for a small subset of samples from
the retain and unlearn classes, respectively. We then compute the shared
information between these spaces and remove it from the forget space to isolate
class-discriminatory feature space. Finally, we obtain the unlearned model by
updating the weights to suppress the class discriminatory features from the
activation spaces. We demonstrate our algorithm's efficacy on ImageNet using a
Vision Transformer with only $\sim 1.5\%$ drop in retain accuracy compared to
the original model while maintaining under $1\%$ accuracy on the unlearned
class samples. Furthermore, our algorithm exhibits competitive unlearning
performance and resilience against Membership Inference Attacks (MIA). Compared
to baselines, it achieves an average accuracy improvement of $1.38\%$ on the
ImageNet dataset while requiring up to $10 \times$ fewer samples for
unlearning. Additionally, under stronger MIA attacks on the CIFAR-100 dataset
using a ResNet18 architecture, our approach outperforms the best baseline by
$1.8\%$. Our code is available at
https://github.com/sangamesh-kodge/class_forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Intervention Efficacy via Concept Realignment in Concept
  Bottleneck Models <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishad Singhi, Jae Myung Kim, Karsten Roth, Zeynep Akata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept Bottleneck Models (CBMs) ground image classification on
human-understandable concepts to allow for interpretable model decisions.
Crucially, the CBM design inherently allows for human interventions, in which
expert users are given the ability to modify potentially misaligned concept
choices to influence the decision behavior of the model in an interpretable
fashion. However, existing approaches often require numerous human
interventions per image to achieve strong performances, posing practical
challenges in scenarios where obtaining human feedback is expensive. In this
paper, we find that this is noticeably driven by an independent treatment of
concepts during intervention, wherein a change of one concept does not
influence the use of other ones in the model's final decision. To address this
issue, we introduce a trainable concept intervention realignment module, which
leverages concept relations to realign concept assignments post-intervention.
Across standard, real-world benchmarks, we find that concept realignment can
significantly improve intervention efficacy; significantly reducing the number
of interventions needed to reach a target classification performance or concept
prediction accuracy. In addition, it easily integrates into existing
concept-based architectures without requiring changes to the models themselves.
This reduced cost of human-model collaboration is crucial to enhancing the
feasibility of CBMs in resource-constrained environments. Our code is available
at: https://github.com/ExplainableML/concept_realignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retargeting Visual Data with Deformation Fields <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Elsner, Julia Berger, Tong Wu, Victor Czech, Lin Gao, Leif Kobbelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seam carving is an image editing method that enable content-aware resizing,
including operations like removing objects. However, the seam-finding strategy
based on dynamic programming or graph-cut limits its applications to broader
visual data formats and degrees of freedom for editing. Our observation is that
describing the editing and retargeting of images more generally by a
displacement field yields a generalisation of content-aware deformations. We
propose to learn a deformation with a neural network that keeps the output
plausible while trying to deform it only in places with low information
content. This technique applies to different kinds of visual data, including
images, 3D scenes given as neural radiance fields, or even polygon meshes.
Experiments conducted on different visual data show that our method achieves
better content-aware retargeting compared to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">125</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Using Quasirandom Sequences in Machine Learning for Model Weight
  Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andriy Miranskyy, Adam Sorrenti, Viral Thakar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of training neural networks directly impacts computational
costs, resource allocation, and model development timelines in machine learning
applications. An optimizer's ability to train the model adequately (in terms of
trained model performance) depends on the model's initial weights. Model weight
initialization schemes use pseudorandom number generators (PRNGs) as a source
of randomness.
  We investigate whether substituting PRNGs for low-discrepancy quasirandom
number generators (QRNGs) -- namely Sobol' sequences -- as a source of
randomness for initializers can improve model performance. We examine
Multi-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long
Short-Term Memory (LSTM), and Transformer architectures trained on MNIST,
CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses
ten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);
Orthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with
weights set using PRNG- and QRNG-based initializers are compared pairwise for
each combination of dataset, architecture, optimizer, and initialization
scheme.
  Our findings indicate that QRNG-based neural network initializers either
reach a higher accuracy or achieve the same accuracy more quickly than
PRNG-based initializers in 60% of the 120 experiments conducted. Thus, using
QRNG-based initializers instead of PRNG-based initializers can speed up and
improve model training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of Compromised Functions in a Serverless Cloud Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danielle Lavi, Oleg Brodt, Dudu Mimran, Yuval Elovici, Asaf Shabtai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing is an emerging cloud paradigm with serverless functions
at its core. While serverless environments enable software developers to focus
on developing applications without the need to actively manage the underlying
runtime infrastructure, they open the door to a wide variety of security
threats that can be challenging to mitigate with existing methods. Existing
security solutions do not apply to all serverless architectures, since they
require significant modifications to the serverless infrastructure or rely on
third-party services for the collection of more detailed data. In this paper,
we present an extendable serverless security threat detection model that
leverages cloud providers' native monitoring tools to detect anomalous behavior
in serverless applications. Our model aims to detect compromised serverless
functions by identifying post-exploitation abnormal behavior related to
different types of attacks on serverless functions, and therefore, it is a last
line of defense. Our approach is not tied to any specific serverless
application, is agnostic to the type of threats, and is adaptable through model
adjustments. To evaluate our model's performance, we developed a serverless
cybersecurity testbed in an AWS cloud environment, which includes two different
serverless applications and simulates a variety of attack scenarios that cover
the main security threats faced by serverless functions. Our evaluation
demonstrates our model's ability to detect all implemented attacks while
maintaining a negligible false alarm rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Command-line Obfuscation Detection using Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtech Outrata, Michael Adam Polak, Martin Kopp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To avoid detection, adversaries often use command-line obfuscation. There are
numerous techniques of the command-line obfuscation, all designed to alter the
command-line syntax without affecting its original functionality. This
variability forces most security solutions to create an exhaustive enumeration
of signatures for even a single pattern. In contrast to using signatures, we
have implemented a scalable NLP-based detection method that leverages a
custom-trained, small transformer language model that can be applied to any
source of execution logs. The evaluation on top of real-world telemetry
demonstrates that our approach yields high-precision detections even on
high-volume telemetry from a diverse set of environments spanning from
universities and businesses to healthcare or finance. The practical value is
demonstrated in a case study of real-world samples detected by our model. We
show the model's superiority to signatures on established malware known to
employ obfuscation and showcase previously unseen obfuscated samples detected
by our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning rheological parameters of non-Newtonian fluids from velocimetry
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandros Kontogiannis, Richard Hodgkinson, Emily L. Manchester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates
velocimetry data in order to jointly reconstruct the flow field and learn the
unknown N-S parameters. By incorporating a Carreau shear-thinning viscosity
model into the N-S problem, we devise an algorithm that learns the most likely
Carreau parameters of a shear-thinning fluid, and estimates their
uncertainties, from velocimetry data alone. We then conduct a flow-MRI
experiment to obtain velocimetry data of an axisymmetric laminar jet through an
idealised medical device (FDA nozzle) for a blood analogue fluid. We show that
the algorithm can successfully reconstruct the flow field by learning the most
likely Carreau parameters, and that the learned parameters are in very good
agreement with rheometry measurements. The algorithm accepts any algebraic
effective viscosity model, as long as the model is differentiable, and it can
be extended to more complicated non-Newtonian fluids (e.g. Oldroyd-B fluid) if
a viscoelastic model is incorporated into the N-S problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Driven Strategies for Reducing Student Withdrawal -- A Study of EMU
  Student Stopout 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhao, Amy Otteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Not everyone who enrolls in college will leave with a certificate or degree,
but the number of people who drop out or take a break is much higher than
experts previously believed. In December 2013, there were 29 million people
with some college education but no degree. That number jumped to 36 million by
December of 2018, according to a new report from the National Student
Clearinghouse Research Center[1]. It is imperative to understand the underlying
factors contributing to student withdrawal and to assist decision-makers to
identify effective strategies to prevent it. By analyzing the characteristics
and educational pathways of the stopout student population, our aim is to
provide actionable insights that can benefit institutions facing similar
challenges. Eastern Michigan University (EMU) faces significant challenges in
student retention, with approximately 55% of its undergraduate students not
completing their degrees within six years. As an institution committed to
student success, EMU conducted a comprehensive study of student withdrawals to
understand the influencing factors. And the paper revealed a high correlation
between certain factors and withdrawals, even in the early stages of university
attendance. Based on these findings, we developed a predictive model that
employs artificial intelligence techniques to assess the potential risk that
students abandon their studies. These models enable universities to implement
early intervention strategies, support at-risk students, and improve overall
higher education success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operational range bounding of spectroscopy models with anomaly detection <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luís F. Simões, Pierluigi Casale, Marília Felismino, Kai Hou Yip, Ingo P. Waldmann, Giovanna Tinetti, Theresa Lueftinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe operation of machine learning models requires architectures that
explicitly delimit their operational ranges. We evaluate the ability of anomaly
detection algorithms to provide indicators correlated with degraded model
performance. By placing acceptance thresholds over such indicators, hard
boundaries are formed that define the model's coverage. As a use case, we
consider the extraction of exoplanetary spectra from transit light curves,
specifically within the context of ESA's upcoming Ariel mission. Isolation
Forests are shown to effectively identify contexts where prediction models are
likely to fail. Coverage/error trade-offs are evaluated under conditions of
data and concept drift. The best performance is seen when Isolation Forests
model projections of the prediction model's explainability SHAP values.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in "Proceedings of SPAICE 2024: 1st ESA/IAA conference on
  AI in and for Space". Conference page at https://spaice.esa.int/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence for Public Health Surveillance in Africa:
  Applications and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Marie Tshimula, Mitterrand Kalengayi, Dieumerci Makenga, Dorcas Lilonge, Marius Asumani, Déborah Madiya, Élie Nkuba Kalonji, Hugues Kanda, René Manassé Galekwa, Josias Kumbu, Hardy Mikese, Grace Tshimula, Jean Tshibangu Muabila, Christian N. Mayemba, D'Jeff K. Nkashama, Kalonji Kalala, Steve Ataky, Tighana Wenge Basele, Mbuyi Mukendi Didier, Selain K. Kasereka, Maximilien V. Dialufuma, Godwill Ilunga Wa Kumwita, Lionel Muyuku, Jean-Paul Kimpesa, Dominique Muteba, Aaron Aruna Abedi, Lambert Mukendi Ntobo, Gloria M. Bundutidi, Désiré Kulimba Mashinda, Emmanuel Kabengele Mpinga, Nathanaël M. Kasoro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) is revolutionizing various fields, including
public health surveillance. In Africa, where health systems frequently
encounter challenges such as limited resources, inadequate infrastructure,
failed health information systems and a shortage of skilled health
professionals, AI offers a transformative opportunity. This paper investigates
the applications of AI in public health surveillance across the continent,
presenting successful case studies and examining the benefits, opportunities,
and challenges of implementing AI technologies in African healthcare settings.
Our paper highlights AI's potential to enhance disease monitoring and health
outcomes, and support effective public health interventions. The findings
presented in the paper demonstrate that AI can significantly improve the
accuracy and timeliness of disease detection and prediction, optimize resource
allocation, and facilitate targeted public health strategies. Additionally, our
paper identified key barriers to the widespread adoption of AI in African
public health systems and proposed actionable recommendations to overcome these
challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modality Clustering-based Self-Labeling for Multimodal Data
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paweł Zyblewski, Leandro L. Minku
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technological advances facilitate the ability to acquire multimodal data,
posing a challenge for recognition systems while also providing an opportunity
to use the heterogeneous nature of the information to increase the
generalization capability of models. An often overlooked issue is the cost of
the labeling process, which is typically high due to the need for a significant
investment in time and money associated with human experts. Existing
semi-supervised learning methods often focus on operating in the feature space
created by the fusion of available modalities, neglecting the potential for
cross-utilizing complementary information available in each modality. To
address this problem, we propose Cross-Modality Clustering-based Self-Labeling
(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances
belonging to each modality in the deep feature space and then propagates known
labels within the resulting clusters. Next, information about the instances'
class membership in each modality is exchanged based on the Euclidean distance
to ensure more accurate labeling. Experimental evaluation conducted on 20
datasets derived from the MM-IMDb dataset indicates that cross-propagation of
labels between modalities -- especially when the number of pre-labeled
instances is small -- can allow for more reliable labeling and thus increase
the classification performance in each modality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process-constrained batch Bayesian approaches for yield optimization in
  multi-reactor systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Grimm, Sébastien Paul, Pierre Chainais
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimization of yields in multi-reactor systems, which are advanced tools
in heterogeneous catalysis research, presents a significant challenge due to
hierarchical technical constraints. To this respect, this work introduces a
novel approach called process-constrained batch Bayesian optimization via
Thompson sampling (pc-BO-TS) and its generalized hierarchical extension
(hpc-BO-TS). This method, tailored for the efficiency demands in multi-reactor
systems, integrates experimental constraints and balances between exploration
and exploitation in a sequential batch optimization strategy. It offers an
improvement over other Bayesian optimization methods. The performance of
pc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in a
realistic scenario based on data obtained from high-throughput experiments done
on a multi-reactor system available in the REALCAT platform. The proposed
methods often outperform other sequential Bayesian optimizations and existing
process-constrained batch Bayesian optimization methods. This work proposes a
novel approach to optimize the yield of a reaction in a multi-reactor system,
marking a significant step forward in digital catalysis and generally in
optimization methods for chemical engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Functional Muscle Networks in Improving Hand Gesture
  Perception for Human-Machine Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Costanza Armanini, Tuka Alhanai, Farah E. Shamout, S. Farokh Atashzar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing accurate hand gesture perception models is critical for various
robotic applications, enabling effective communication between humans and
machines and directly impacting neurorobotics and interactive robots. Recently,
surface electromyography (sEMG) has been explored for its rich informational
context and accessibility when combined with advanced machine learning
approaches and wearable systems. The literature presents numerous approaches to
boost performance while ensuring robustness for neurorobots using sEMG, often
resulting in models requiring high processing power, large datasets, and less
scalable solutions. This paper addresses this challenge by proposing the
decoding of muscle synchronization rather than individual muscle activation. We
study coherence-based functional muscle networks as the core of our perception
model, proposing that functional synchronization between muscles and the
graph-based network of muscle connectivity encode contextual information about
intended hand gestures. This can be decoded using shallow machine learning
approaches without the need for deep temporal networks. Our technique could
impact myoelectric control of neurorobots by reducing computational burdens and
enhancing efficiency. The approach is benchmarked on the Ninapro database,
which contains 12 EMG signals from 40 subjects performing 17 hand gestures. It
achieves an accuracy of 85.1%, demonstrating improved performance compared to
existing methods while requiring much less computational power. The results
support the hypothesis that a coherence-based functional muscle network encodes
critical information related to gesture execution, significantly enhancing hand
gesture perception with potential applications for neurorobotic systems and
interactive machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat, Peter Izsak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing Retrieval-Augmented Generation (RAG) systems is inherently
complex, requiring deep understanding of data, use cases, and intricate design
decisions. Additionally, evaluating these systems presents significant
challenges, necessitating assessment of both retrieval accuracy and generative
quality through a multi-faceted approach. We introduce RAG Foundry, an
open-source framework for augmenting large language models for RAG use cases.
RAG Foundry integrates data creation, training, inference and evaluation into a
single workflow, facilitating the creation of data-augmented datasets for
training and evaluating large language models in RAG settings. This integration
enables rapid prototyping and experimentation with various RAG techniques,
allowing users to easily generate datasets and train RAG models using internal
or specialized knowledge sources. We demonstrate the framework effectiveness by
augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG
configurations, showcasing consistent improvements across three
knowledge-intensive datasets. Code is released as open-source in
https://github.com/IntelLabs/RAGFoundry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMEMs for post-hoc analysis of HPO Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Geburek, Neeratyoy Mallik, Danny Stoll, Xavier Bouthillier, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of tuning hyperparameters in Machine Learning (ML) and Deep
Learning (DL) is established through empirical research and applications,
evident from the increase in new hyperparameter optimization (HPO) algorithms
and benchmarks steadily added by the community. However, current benchmarking
practices using averaged performance across many datasets may obscure key
differences between HPO methods, especially for pairwise comparisons. In this
work, we apply Linear Mixed-Effect Models-based (LMEMs) significance testing
for post-hoc analysis of HPO benchmarking runs. LMEMs allow flexible and
expressive modeling on the entire experiment data, including information such
as benchmark meta-features, offering deeper insights than current analysis
practices. We demonstrate this through a case study on the PriorBand paper's
experiment data to find insights not reported in the original work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-tap Latency Reduction with Single- or Double- tap Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoto Nishida, Kaori Ikematsu, Junichi Sato, Shota Yamanaka, Kota Tsubouchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops
(touchpad), and single and double taps are the most basic and common operations
on them. The detection of single or double taps causes the single-tap latency
problem, which creates a bottleneck in terms of the sensitivity of touch
inputs. To reduce the single-tap latency, we propose a novel
machine-learning-based tap prediction method called PredicTaps. Our method
predicts whether a detected tap is a single tap or the first contact of a
double tap without having to wait for the hundreds of milliseconds
conventionally required. We present three evaluations and one user evaluation
that demonstrate its broad applicability and usability for various tap
situations on two form factors (touchpad and smartphone). The results showed
PredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops
and to 17.6 ms on smartphones without reducing usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem
  Compatibility Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alain Riou, Stefan Lattner, Gaëtan Hadjeres, Michael Anslow, Geoffroy Peeters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the automated process of determining stem compatibility
by identifying audio recordings of single instruments that blend well with a
given musical context. To tackle this challenge, we present Stem-JEPA, a novel
Joint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset
using a self-supervised learning approach.
  Our model comprises two networks: an encoder and a predictor, which are
jointly trained to predict the embeddings of compatible stems from the
embeddings of a given context, typically a mix of several instruments. Training
a model in this manner allows its use in estimating stem compatibility -
retrieving, aligning, or generating a stem to match a given mix - or for
downstream tasks such as genre or key estimation, as the training paradigm
requires the model to learn information related to timbre, harmony, and rhythm.
  We evaluate our model's performance on a retrieval task on the MUSDB18
dataset, testing its ability to find the missing stem from a mix and through a
subjective user study. We also show that the learned embeddings capture
temporal alignment information and, finally, evaluate the representations
learned by our model on several downstream tasks, highlighting that they
effectively capture meaningful musical features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 25th International Society for Music Information
  Retrieval Conference, ISMIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Attacks against Black-box Code Completion Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Slobodan Jenko, Jingxuan He, Niels Mündler, Mark Vero, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern code completion engines, powered by large language models, have
demonstrated impressive capabilities to generate functionally correct code
based on surrounding context. As these tools are extensively used by millions
of developers, it is crucial to investigate their security implications. In
this work, we present INSEC, a novel attack that directs code completion
engines towards generating vulnerable code. In line with most commercial
completion engines, such as GitHub Copilot, INSEC assumes only black-box query
access to the targeted engine, without requiring any knowledge of the engine's
internals. Our attack works by inserting a malicious attack string as a short
comment in the completion input. To derive the attack string, we design a
series of specialized initialization schemes and an optimization procedure for
further refinement. We demonstrate the strength of INSEC not only on
state-of-the-art open-source models but also on black-box commercial services
such as the OpenAI API and GitHub Copilot. On a comprehensive set of
security-critical test cases covering 16 CWEs across 5 programming languages,
INSEC significantly increases the likelihood of the considered completion
engines in generating unsafe code by >50% in absolute, while maintaining the
ability in producing functionally correct code. At the same time, our attack
has low resource requirements, and can be developed for a cost of well under
ten USD on commodity hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic rating of incomplete hippocampal inversions evaluated across
  multiple cohorts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Hemforth, Baptiste Couvy-Duchesne, Kevin De Matos, Camille Brianceau, Matthieu Joulot, Tobias Banaschewski, Arun L. W. Bokde, Sylvane Desrivières, Herta Flor, Antoine Grigis, Hugh Garavan, Penny Gowland, Andreas Heinz, Rüdiger Brühl, Jean-Luc Martinot, Marie-Laure Paillère Martinot, Eric Artiges, Dimitri Papadopoulos, Herve Lemaitre, Tomas Paus, Luise Poustka, Sarah Hohmann, Nathalie Holz, Juliane H. Fröhner, Michael N. Smolka, Nilakshi Vaidya, Henrik Walter, Robert Whelan, Gunter Schumann, Christian Büchel, JB Poline, Bernd Itterman, Vincent Frouin, Alexandre Martin, IMAGEN study group, Claire Cury, Olivier Colliot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal
malrotation, is an atypical anatomical pattern of the hippocampus found in
about 20% of the general population. IHI can be visually assessed on coronal
slices of T1 weighted MR images, using a composite score that combines four
anatomical criteria. IHI has been associated with several brain disorders
(epilepsy, schizophrenia). However, these studies were based on small samples.
Furthermore, the factors (genetic or environmental) that contribute to the
genesis of IHI are largely unknown. Large-scale studies are thus needed to
further understand IHI and their potential relationships to neurological and
psychiatric disorders. However, visual evaluation is long and tedious,
justifying the need for an automatic method. In this paper, we propose, for the
first time, to automatically rate IHI. We proceed by predicting four anatomical
criteria, which are then summed up to form the IHI score, providing the
advantage of an interpretable score. We provided an extensive experimental
investigation of different machine learning methods and training strategies. We
performed automatic rating using a variety of deep learning models (conv5-FC3,
ResNet and SECNN) as well as a ridge regression. We studied the generalization
of our models using different cohorts and performed multi-cohort learning. We
relied on a large population of 2,008 participants from the IMAGEN study, 993
and 403 participants from the QTIM/QTAB studies as well as 985 subjects from
the UKBiobank. We showed that deep learning models outperformed a ridge
regression. We demonstrated that the performances of the conv5-FC3 network were
at least as good as more complex networks while maintaining a low complexity
and computation time. We showed that training on a single cohort may lack in
variability while training on several cohorts improves generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:016</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A First Look at License Compliance Capability of LLMs in Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Xu, Kai Gao, Hao He, Minghui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have revolutionized code
generation, leading to widespread adoption of AI coding tools by developers.
However, LLMs can generate license-protected code without providing the
necessary license information, leading to potential intellectual property
violations during software production. This paper addresses the critical, yet
underexplored, issue of license compliance in LLM-generated code by
establishing a benchmark to evaluate the ability of LLMs to provide accurate
license information for their generated code. To establish this benchmark, we
conduct an empirical study to identify a reasonable standard for "striking
similarity" that excludes the possibility of independent creation, indicating a
copy relationship between the LLM output and certain open-source code. Based on
this standard, we propose an evaluation benchmark LiCoEval, to evaluate the
license compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popular
LLMs, finding that even top-performing LLMs produce a non-negligible proportion
(0.88% to 2.01%) of code strikingly similar to existing open-source
implementations. Notably, most LLMs fail to provide accurate license
information, particularly for code under copyleft licenses. These findings
underscore the urgent need to enhance LLM compliance capabilities in code
generation tasks. Our study provides a foundation for future research and
development to improve license compliance in AI-assisted software development,
contributing to both the protection of open-source software copyrights and the
mitigation of legal risks for LLM users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture
  and Automated Deployment Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Wiese, Gamze İslamoğlu, Moritz Scherer, Luka Macan, Victor J. B. Jung, Alessio Burrello, Francesco Conti, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the challenges for Tiny Machine Learning (tinyML) is keeping up with
the evolution of Machine Learning models from Convolutional Neural Networks to
Transformers. We address this by leveraging a heterogeneous architectural
template coupling RISC-V processors with hardwired accelerators supported by an
automated deployment flow. We demonstrate an Attention-based model in a tinyML
power envelope with an octa-core cluster coupled with an accelerator for
quantized Attention. Our deployment flow enables an end-to-end 8-bit
MobileBERT, achieving leading-edge energy efficiency and throughput of 2960
GOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOI
technology).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print manuscript submitted for review to the IEEE Design and Test
  Special Issue on tinyML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Heterogeneous Knowledge Graph Completion with a Novel
  GAT-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanxu Wei, Yitong Song, Bin Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs (KGs) play a vital role in enhancing search results and
recommendation systems. With the rapid increase in the size of the KGs, they
are becoming inaccuracy and incomplete. This problem can be solved by the
knowledge graph completion methods, of which graph attention network
(GAT)-based methods stand out since their superior performance. However,
existing GAT-based knowledge graph completion methods often suffer from
overfitting issues when dealing with heterogeneous knowledge graphs, primarily
due to the unbalanced number of samples. Additionally, these methods
demonstrate poor performance in predicting the tail (head) entity that shares
the same relation and head (tail) entity with others. To solve these problems,
we propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH
incorporates two separate attention network modules that work synergistically
to predict the missing entities. We also introduce novel encoding and feature
transformation approaches, enabling the robust performance of GATH in scenarios
with imbalanced samples. Comprehensive experiments are conducted to evaluate
the GATH's performance. Compared with the existing SOTA GAT-based model on
Hits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the
FB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Probabilistic Embeddings in Optimal Dimension Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Murray, Adam Pickarski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimension reduction algorithms are a crucial part of many data science
pipelines, including data exploration, feature creation and selection, and
denoising. Despite their wide utilization, many non-linear dimension reduction
algorithms are poorly understood from a theoretical perspective. In this work
we consider a generalized version of multidimensional scaling, which is posed
as an optimization problem in which a mapping from a high-dimensional feature
space to a lower-dimensional embedding space seeks to preserve either inner
products or norms of the distribution in feature space, and which encompasses
many commonly used dimension reduction algorithms. We analytically investigate
the variational properties of this problem, leading to the following insights:
1) Solutions found using standard particle descent methods may lead to
non-deterministic embeddings, 2) A relaxed or probabilistic formulation of the
problem admits solutions with easily interpretable necessary conditions, 3) The
globally optimal solutions to the relaxed problem actually must give a
deterministic embedding. This progression of results mirrors the classical
development of optimal transportation, and in a case relating to the
Gromov-Wasserstein distance actually gives explicit insight into the structure
of the optimal embeddings, which are parametrically determined and
discontinuous. Finally, we illustrate that a standard computational
implementation of this task does not learn deterministic embeddings, which
means that it learns sub-optimal mappings, and that the embeddings learned in
that context have highly misleading clustering structure, underscoring the
delicate nature of solving this problem computationally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attenuation-adjusted deep learning of pore defects in 2D radiographs of
  additive manufacturing powders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Bjerregaard, David Schumacher, Jon Sporring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The presence of gas pores in metal feedstock powder for additive
manufacturing greatly affects the final AM product. Since current porosity
analysis often involves lengthy X-ray computed tomography (XCT) scans with a
full rotation around the sample, motivation exists to explore methods that
allow for high throughput -- possibly enabling in-line porosity analysis during
manufacturing. Through labelling pore pixels on single 2D radiographs of
powders, this work seeks to simulate such future efficient setups. High
segmentation accuracy is achieved by combining a model of X-ray attenuation
through particles with a variant of the widely applied UNet architecture;
notably, F1-score increases by $11.4\%$ compared to the baseline UNet. The
proposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)
making tight particle cutouts, and 3) subtracting an ideal particle without
pores generated from a distance map inspired by Lambert-Beers law. This paper
explores four image processing methods, where the fastest (yet still
unoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,
and the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalable
nature, these strategies can be involved in making high throughput porosity
analysis of metal feedstock powder for additive manufacturing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Implementation on https://github.com/yhsure/porosity</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PENDRAM: Enabling High-Performance and Energy-Efficient Processing of
  Deep Neural Networks through a Generalized DRAM Data Mapping Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachmad Vidya Wicaksana Putra, Muhammad Abdullah Hanif, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs), a prominent type of Deep Neural
Networks (DNNs), have emerged as a state-of-the-art solution for solving
machine learning tasks. To improve the performance and energy efficiency of CNN
inference, the employment of specialized hardware accelerators is prevalent.
However, CNN accelerators still face performance- and energy-efficiency
challenges due to high off-chip memory (DRAM) access latency and energy, which
are especially crucial for latency- and energy-constrained embedded
applications. Moreover, different DRAM architectures have different profiles of
access latency and energy, thus making it challenging to optimize them for high
performance and energy-efficient CNN accelerators. To address this, we present
PENDRAM, a novel design space exploration methodology that enables
high-performance and energy-efficient CNN acceleration through a generalized
DRAM data mapping policy. Specifically, it explores the impact of different
DRAM data mapping policies and DRAM architectures across different CNN
partitioning and scheduling schemes on the DRAM access latency and energy, then
identifies the pareto-optimal design choices. The experimental results show
that our DRAM data mapping policy improves the energy-delay-product of DRAM
accesses in the CNN accelerator over other mapping policies by up to 96%. In
this manner, our PENDRAM methodology offers high-performance and
energy-efficient CNN acceleration under any given DRAM architectures for
diverse embedded AI applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 15 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:2004.10341</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Terracorder: Sense Long and Prosper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Millar, Sarab Sethi, Hamed Haddadi, Anil Madhavapeddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-situ sensing devices need to be deployed in remote environments for long
periods of time; minimizing their power consumption is vital for maximising
both their operational lifetime and coverage. We introduce Terracorder -- a
versatile multi-sensor device -- and showcase its exceptionally low power
consumption using an on-device reinforcement learning scheduler. We prototype a
unique device setup for biodiversity monitoring and compare its battery life
using our scheduler against a number of fixed schedules; the scheduler captures
more than 80% of events at less than 50% of the number of activations of the
best-performing fixed schedule. We then explore how a collaborative scheduler
can maximise the useful operation of a network of devices, improving overall
network power consumption and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strategic Federated Learning: Application to Smart Meter Data Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Mohamad, Chao Zhang, Samson Lasaulce, Vineeth S Varma, Mérouane Debbah, Mounir Ghogho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) involves several clients that share with a fusion
center (FC), the model each client has trained with its own data. Conventional
FL, which can be interpreted as an estimation or distortion-based approach,
ignores the final use of model information (MI) by the FC and the other
clients. In this paper, we introduce a novel FL framework in which the FC uses
an aggregate version of the MI to make decisions that affect the client's
utility functions. Clients cannot choose the decisions and can only use the MI
reported to the FC to maximize their utility. Depending on the alignment
between the client and FC utilities, the client may have an individual interest
in adding strategic noise to the model. This general framework is stated and
specialized to the case of clustering, in which noisy cluster representative
information is reported. This is applied to the problem of power consumption
scheduling. In this context, utility non-alignment occurs, for instance, when
the client wants to consume when the price of electricity is low, whereas the
FC wants the consumption to occur when the total power is the lowest. This is
illustrated with aggregated real data from Ausgrid \cite{ausgrid}. Our
numerical analysis clearly shows that the client can increase his utility by
adding noise to the model reported to the FC. Corresponding results and source
codes can be downloaded from \cite{source-code}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StoDIP: Efficient 3D MRF image reconstruction with deep image priors and
  stochastic iterations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Perla Mayo, Matteo Cencini, Carolin M. Pirkl, Marion I. Menzel, Michela Tosetti, Bjoern H. Menze, Mohammad Golbabaee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to
quantitative MRI for multiparametric tissue mapping. The reconstruction of
quantitative maps requires tailored algorithms for removing aliasing artefacts
from the compressed sampled MRF acquisitions. Within approaches found in the
literature, many focus solely on two-dimensional (2D) image reconstruction,
neglecting the extension to volumetric (3D) scans despite their higher
relevance and clinical value. A reason for this is that transitioning to 3D
imaging without appropriate mitigations presents significant challenges,
including increased computational cost and storage requirements, and the need
for large amount of ground-truth (artefact-free) data for training. To address
these issues, we introduce StoDIP, a new algorithm that extends the
ground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging.
StoDIP employs memory-efficient stochastic updates across the multicoil MRF
data, a carefully selected neural network architecture, as well as faster
nonuniform FFT (NUFFT) transformations. This enables a faster convergence
compared against a conventional DIP implementation without these features.
Tested on a dataset of whole-brain scans from healthy volunteers, StoDIP
demonstrated superior performance over the ground-truth-free reconstruction
baselines, both quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 1 table, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought
  Decoding <span class="chip">SIGDIAL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renato Vukovic, David Arps, Carel van Niekerk, Benjamin Matthias Ruppik, Hsien-Chin Lin, Michael Heck, Milica Gašić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art task-oriented dialogue systems typically rely on
task-specific ontologies for fulfilling user queries. The majority of
task-oriented dialogue data, such as customer service recordings, comes without
ontology and annotation. Such ontologies are normally built manually, limiting
the application of specialised systems. Dialogue ontology construction is an
approach for automating that process and typically consists of two steps: term
extraction and relation extraction. In this work, we focus on relation
extraction in a transfer learning set-up. To improve the generalisation, we
propose an extension to the decoding mechanism of large language models. We
adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning
problems, to generative relation extraction. Here, we generate multiple
branches in the decoding space and select the relations based on a confidence
threshold. By constraining the decoding to ontology terms and relations, we aim
to decrease the risk of hallucination. We conduct extensive experimentation on
two widely used datasets and find improvements in performance on target
ontology for source fine-tuned and one-shot prompted large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear at SIGDIAL 2024. 9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the consistent reasoning paradox of intelligence and optimal trust in
  AI: The power of 'I don't know' 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Bastounis, Paolo Campodonico, Mihaela van der Schaar, Ben Adcock, Anders C. Hansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning,
which lies at the core of human intelligence, is the ability to handle tasks
that are equivalent, yet described by different sentences ('Tell me the time!'
and 'What is the time?'). The CRP asserts that consistent reasoning implies
fallibility -- in particular, human-like intelligence in AI necessarily comes
with human-like fallibility. Specifically, it states that there are problems,
e.g. in basic arithmetic, where any AI that always answers and strives to mimic
human intelligence by reasoning consistently will hallucinate (produce wrong,
yet plausible answers) infinitely often. The paradox is that there exists a
non-consistently reasoning AI (which therefore cannot be on the level of human
intelligence) that will be correct on the same set of problems. The CRP also
shows that detecting these hallucinations, even in a probabilistic sense, is
strictly harder than solving the original problems, and that there are problems
that an AI may answer correctly, but it cannot provide a correct logical
explanation for how it arrived at the answer. Therefore, the CRP implies that
any trustworthy AI (i.e., an AI that never answers incorrectly) that also
reasons consistently must be able to say 'I don't know'. Moreover, this can
only be done by implicitly computing a new concept that we introduce, termed
the 'I don't know' function -- something currently lacking in modern AI. In
view of these insights, the CRP also provides a glimpse into the behaviour of
Artificial General Intelligence (AGI). An AGI cannot be 'almost sure', nor can
it always explain itself, and therefore to be trustworthy it must be able to
say 'I don't know'.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages and 50 pages of supplementary material, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantile Regression using Random Forest Proximities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingshu Li, Bhaskarjit Sarmah, Dhruv Desai, Joshua Rosaler, Snigdha Bhagat, Philip Sommer, Dhagash Mehta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the dynamic nature of financial markets, maintaining models that
produce precise predictions over time is difficult. Often the goal isn't just
point prediction but determining uncertainty. Quantifying uncertainty,
especially the aleatoric uncertainty due to the unpredictable nature of market
drivers, helps investors understand varying risk levels. Recently, quantile
regression forests (QRF) have emerged as a promising solution: Unlike most
basic quantile regression methods that need separate models for each quantile,
quantile regression forests estimate the entire conditional distribution of the
target variable with a single model, while retaining all the salient features
of a typical random forest. We introduce a novel approach to compute quantile
regressions from random forests that leverages the proximity (i.e., distance
metric) learned by the model and infers the conditional distribution of the
target variable. We evaluate the proposed methodology using publicly available
datasets and then apply it towards the problem of forecasting the average daily
volume of corporate bonds. We show that using quantile regression using Random
Forest proximities demonstrates superior performance in approximating
conditional target distributions and prediction intervals to the original
version of QRF. We also demonstrate that the proposed framework is
significantly more computationally efficient than traditional approaches to
quantile regressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Sensing of Knee Osteoarthritis Progression with Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khanh Nguyen, Huy Hoang Nguyen, Egor Panfilov, Aleksei Tiulpin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Osteoarthritis (OA) is the most common musculoskeletal disease, which has no
cure. Knee OA (KOA) is one of the highest causes of disability worldwide, and
it costs billions of United States dollars to the global community. Prediction
of KOA progression has been of high interest to the community for years, as it
can advance treatment development through more efficient clinical trials and
improve patient outcomes through more efficient healthcare utilization.
Existing approaches for predicting KOA, however, are predominantly static, i.e.
consider data from a single time point to predict progression many years into
the future, and knee level, i.e. consider progression in a single joint only.
Due to these and related reasons, these methods fail to deliver the level of
predictive performance, which is sufficient to result in cost savings and
better patient outcomes. Collecting extensive data from all patients on a
regular basis could address the issue, but it is limited by the high cost at a
population level. In this work, we propose to go beyond static prediction
models in OA, and bring a novel Active Sensing (AS) approach, designed to
dynamically follow up patients with the objective of maximizing the number of
informative data acquisitions, while minimizing their total cost over a period
of time. Our approach is based on Reinforcement Learning (RL), and it leverages
a novel reward function designed specifically for AS of disease progression in
more than one part of a human body. Our method is end-to-end, relies on
multi-modal Deep Learning, and requires no human input at inference time.
Throughout an exhaustive experimental evaluation, we show that using RL can
provide a higher monetary benefit when compared to state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Hankel-Toeplitz Structures for Fast Computation of Kernel
  Precision Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frida Viset, Anton Kullberg, Frederiek Wesel, Arno Solin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Hilbert-space Gaussian Process (HGP) approach offers a
hyperparameter-independent basis function approximation for speeding up
Gaussian Process (GP) inference by projecting the GP onto M basis functions.
These properties result in a favorable data-independent $\mathcal{O}(M^3)$
computational complexity during hyperparameter optimization but require a
dominating one-time precomputation of the precision matrix costing
$\mathcal{O}(NM^2)$ operations. In this paper, we lower this dominating
computational complexity to $\mathcal{O}(NM)$ with no additional
approximations. We can do this because we realize that the precision matrix can
be split into a sum of Hankel-Toeplitz matrices, each having $\mathcal{O}(M)$
unique entries. Based on this realization we propose computing only these
unique entries at $\mathcal{O}(NM)$ costs. Further, we develop two theorems
that prescribe sufficient conditions for the complexity reduction to hold
generally for a wide range of other approximate GP models, such as the
Variational Fourier Feature (VFF) approach. The two theorems do this with no
assumptions on the data and no additional approximations of the GP models
themselves. Thus, our contribution provides a pure speed-up of several
existing, widely used, GP approximations, without further approximations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning (TMLR) July 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Applications in Medical Prognostics: A Comprehensive
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Fascia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) has revolutionized medical prognostics by integrating
advanced algorithms with clinical data to enhance disease prediction, risk
assessment, and patient outcome forecasting. This comprehensive review
critically examines the application of various ML techniques in medical
prognostics, focusing on their efficacy, challenges, and future directions. The
methodologies discussed include Random Forest (RF) for sepsis prediction,
logistic regression for cardiovascular risk assessment, Convolutional Neural
Networks (CNNs) for cancer detection, and Long Short-Term Memory (LSTM)
networks for predicting clinical deterioration. RF models demonstrate robust
performance in handling high-dimensional data and capturing non-linear
relationships, making them particularly effective for sepsis prediction.
Logistic regression remains valuable for its interpretability and ease of use
in cardiovascular risk assessment. CNNs have shown exceptional accuracy in
cancer detection, leveraging their ability to learn complex visual patterns
from medical imaging. LSTM networks excel in analyzing temporal data, providing
accurate predictions of clinical deterioration. The review highlights the
strengths and limitations of each technique, the importance of model
interpretability, and the challenges of data quality and privacy. Future
research directions include the integration of multi-modal data sources, the
application of transfer learning, and the development of continuous learning
systems. These advancements aim to enhance the predictive power and clinical
applicability of ML models, ultimately improving patient outcomes in healthcare
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR
  <span class="highlight-title">Dataset</span> Construction <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Sawczyn, Katsiaryna Viarenich, Konrad Wojtasik, Aleksandra Domogała, Marcin Oleksy, Maciej Piasecki, Tomasz Kajdanowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in AI and natural language processing have revolutionized
machine-human language interactions, with question answering (QA) systems
playing a pivotal role. The knowledge base question answering (KBQA) task,
utilizing structured knowledge graphs (KG), allows for handling extensive
knowledge-intensive questions. However, a significant gap exists in KBQA
datasets, especially for low-resource languages. Many existing construction
pipelines for these datasets are outdated and inefficient in human labor, and
modern assisting tools like Large Language Models (LLM) are not utilized to
reduce the workload. To address this, we have designed and implemented a
modern, semi-automated approach for creating datasets, encompassing tasks such
as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),
tailored explicitly for low-resource environments. We executed this pipeline
and introduced the PUGG dataset, the first Polish KBQA dataset, and novel
datasets for MRC and IR. Additionally, we provide a comprehensive
implementation, insightful findings, detailed statistics, and evaluation of
baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ACL 2024 (findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Sharp Convergence Theory for The Probability Flow ODEs of Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gen Li, Yuting Wei, Yuejie Chi, Yuxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, which convert noise into new data instances by learning to
reverse a diffusion process, have become a cornerstone in contemporary
generative modeling. In this work, we develop non-asymptotic convergence theory
for a popular diffusion-based sampler (i.e., the probability flow ODE sampler)
in discrete time, assuming access to $\ell_2$-accurate estimates of the (Stein)
score functions. For distributions in $\mathbb{R}^d$, we prove that
$d/\varepsilon$ iterations -- modulo some logarithmic and lower-order terms --
are sufficient to approximate the target distribution to within $\varepsilon$
total-variation distance. This is the first result establishing nearly linear
dimension-dependency (in $d$) for the probability flow ODE sampler. Imposing
only minimal assumptions on the target data distribution (e.g., no smoothness
assumption is imposed), our results also characterize how $\ell_2$ score
estimation errors affect the quality of the data generation processes. In
contrast to prior works, our theory is developed based on an elementary yet
versatile non-asymptotic approach without the need of resorting to SDE and ODE
toolboxes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript presents improved theory for probability flow ODEs
  compared to its earlier version arXiv:2306.09251</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lean <span class="highlight-title">Transformer</span> Model for Dynamic Malware Analysis and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Quertier, Benjamin Marais, Grégoire Barrué, Stéphane Morucci, Sévan Azé, Sébastien Salladin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malware is a fast-growing threat to the modern computing world and existing
lines of defense are not efficient enough to address this issue. This is mainly
due to the fact that many prevention solutions rely on signature-based
detection methods that can easily be circumvented by hackers. Therefore, there
is a recurrent need for behavior-based analysis where a suspicious file is ran
in a secured environment and its traces are collected to reports for analysis.
Previous works have shown some success leveraging Neural Networks and API calls
sequences extracted from these execution reports.
  Recently, Large Language Models and Generative AI have demonstrated
impressive capabilities mainly in Natural Language Processing tasks and
promising applications in the cybersecurity field for both attackers and
defenders.
  In this paper, we design an Encoder-Only model, based on the Transformers
architecture, to detect malicious files, digesting their API call sequences
collected by an execution emulation solution. We are also limiting the size of
the model architecture and the number of its parameters since it is often
considered that Large Language Models may be overkill for specific tasks such
as the one we are dealing with hereafter. In addition to achieving decent
detection results, this approach has the advantage of reducing our carbon
footprint by limiting training and inference times and facilitating technical
operations with less hardware requirements.
  We also carry out some analysis of our results and highlight the limits and
possible improvements when using Transformers to analyze malicious files.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization of Iterative Blind Detection based on Expectation
  Maximization and Belief Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Schmid, Tomer Raviv, Nir Shlezinger, Laurent Schmalen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study iterative blind symbol detection for block-fading linear
inter-symbol interference channels. Based on the factor graph framework, we
design a joint channel estimation and detection scheme that combines the
expectation maximization (EM) algorithm and the ubiquitous belief propagation
(BP) algorithm. Interweaving the iterations of both schemes significantly
reduces the EM algorithm's computational burden while retaining its excellent
performance. To this end, we apply simple yet effective model-based learning
methods to find a suitable parameter update schedule by introducing momentum in
both the EM parameter updates as well as in the BP message passing. Numerical
simulations verify that the proposed method can learn efficient schedules that
generalize well and even outperform coherent BP detection in high
signal-to-noise scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at Asilomar Conference on Signals, Systems,
  and Computers 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Robustness of Malware Detectors to Adversarial Samples <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Salman, Benjamin Zi Hao Zhao, Hassan Jameel Asghar, Muhammad Ikram, Sidharth Kaushik, Mohamed Ali Kaafar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples add imperceptible alterations to inputs with the
objective to induce misclassification in machine learning models. They have
been demonstrated to pose significant challenges in domains like image
classification, with results showing that an adversarially perturbed image to
evade detection against one classifier is most likely transferable to other
classifiers. Adversarial examples have also been studied in malware analysis.
Unlike images, program binaries cannot be arbitrarily perturbed without
rendering them non-functional. Due to the difficulty of crafting adversarial
program binaries, there is no consensus on the transferability of adversarially
perturbed programs to different detectors. In this work, we explore the
robustness of malware detectors against adversarially perturbed malware. We
investigate the transferability of adversarial attacks developed against one
detector, against other machine learning-based malware detectors, and code
similarity techniques, specifically, locality sensitive hashing-based
detectors. Our analysis reveals that adversarial program binaries crafted for
one detector are generally less effective against others. We also evaluate an
ensemble of detectors and show that they can potentially mitigate the impact of
adversarial program binaries. Finally, we demonstrate that substantial program
changes made to evade detection may result in the transformation technique
being identified, implying that the adversary must make minimal changes to the
program binary.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the full version of the paper with the same title to appear
  in the proceedings of the 2024 Workshop on Security and Artificial
  Intelligence (SECAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network Fission Ensembles for Low-Cost Self-Ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojung Lee, Jong-Seok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent ensemble learning methods for image classification have been shown to
improve classification accuracy with low extra cost. However, they still
require multiple trained models for ensemble inference, which eventually
becomes a significant burden when the model size increases. In this paper, we
propose a low-cost ensemble learning and inference, called Network Fission
Ensembles (NFE), by converting a conventional network itself into a multi-exit
structure. Starting from a given initial network, we first prune some of the
weights to reduce the training burden. We then group the remaining weights into
several sets and create multiple auxiliary paths using each set to construct
multi-exits. We call this process Network Fission. Through this, multiple
outputs can be obtained from a single network, which enables ensemble learning.
Since this process simply changes the existing network structure to multi-exits
without using additional networks, there is no extra computational burden for
ensemble learning and inference. Moreover, by learning from multiple losses of
all exits, the multi-exits improve performance via regularization, and high
performance can be achieved even with increased network sparsity. With our
simple yet effective method, we achieve significant improvement compared to
existing ensemble methods. The code is available at
https://github.com/hjdw2/NFE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backward Compatibility in Attributive Explanation and Enhanced Model
  Training Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryuta Matsuno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model update is a crucial process in the operation of ML/AI systems. While
updating a model generally enhances the average prediction performance, it also
significantly impacts the explanations of predictions. In real-world
applications, even minor changes in explanations can have detrimental
consequences. To tackle this issue, this paper introduces BCX, a quantitative
metric that evaluates the backward compatibility of feature attribution
explanations between pre- and post-update models. BCX utilizes practical
agreement metrics to calculate the average agreement between the explanations
of pre- and post-update models, specifically among samples on which both models
accurately predict. In addition, we propose BCXR, a BCX-aware model training
method by designing surrogate losses which theoretically lower bounds agreement
scores. Furthermore, we present a universal variant of BCXR that improves all
agreement metrics, utilizing L2 distance among the explanations of the models.
To validate our approach, we conducted experiments on eight real-world
datasets, demonstrating that BCXR achieves superior trade-offs between
predictive performances and BCX scores, showcasing the effectiveness of our
BCXR methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heart Rate and its Variability from Short-term ECG Recordings as
  Biomarkers for Detecting Mild Cognitive Impairment in Indian Population 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjo Xavier, Sneha Noble, Justin Joseph, Thomas Gregor Issac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alterations in Heart Rate (HR) and Heart Rate Variability (HRV) can reflect
autonomic dysfunction associated with neurodegeneration. We investigate the
influence of Mild Cognitive Impairment (MCI) on HR and its variability measures
in the Indian population by designing a complete signal processing pipeline to
detect the R-wave peaks and compute HR and HRV features from ECG recordings of
10 seconds, for point-of-care applications. The study cohort involves 297 urban
participants, among which 48.48% are male and 51.51% are female. From the
Addenbrooke's Cognitive Examination-III (ACE-III), MCI is detected in 19.19% of
participants and the rest, 80.8% of them are cognitively healthy. Statistical
features like central tendency (mean and root mean square (RMS) of the
Normal-to-Normal (NN) intervals) and dispersion (standard deviation (SD) of all
NN intervals (SDNN) and root mean square of successive differences of NN
intervals (RMSSD)) of beat-to-beat intervals are computed. The Wilcoxon rank
sum test reveals that mean of NN intervals (p = 0.0021), the RMS of NN
intervals (p = 0.0014), the SDNN (p = 0.0192) and the RMSSD (p = 0.0206) values
differ significantly between MCI and non-MCI classes, for a level of
significance, 0.05. Machine learning classifiers like, Support Vector Machine
(SVM), Discriminant Analysis (DA) and Naive Bayes (NB) driven by mean NN
intervals, RMS, SDNN and RMSSD, show a high accuracy of 80.80% on each
individual feature input. Individuals with MCI are observed to have
comparatively higher HR than healthy subjects. HR and its variability can be
considered as potential biomarkers for detecting MCI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Nil</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Gaussian Temporal Difference Error For Uncertainty-aware
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyeon Kim, Joonhun Lee, Namhoon Cho, Sungjun Han, Seungeon Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional uncertainty-aware temporal difference (TD) learning methods
often rely on simplistic assumptions, typically including a zero-mean Gaussian
distribution for TD errors. Such oversimplification can lead to inaccurate
error representations and compromised uncertainty estimation. In this paper, we
introduce a novel framework for generalized Gaussian error modeling in deep
reinforcement learning, applicable to both discrete and continuous control
settings. Our framework enhances the flexibility of error distribution modeling
by incorporating higher-order moments, particularly kurtosis, thereby improving
the estimation and mitigation of data-dependent noise, i.e., aleatoric
uncertainty. We examine the influence of the shape parameter of the generalized
Gaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form
expression that demonstrates an inverse relationship between uncertainty and
the shape parameter. Additionally, we propose a theoretically grounded
weighting scheme to fully leverage the GGD. To address epistemic uncertainty,
we enhance the batch inverse variance weighting by incorporating bias reduction
and kurtosis considerations, resulting in improved robustness. Extensive
experimental evaluations using policy gradient algorithms demonstrate the
consistent efficacy of our method, showcasing significant performance
improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardware Aware Ensemble Selection for Balancing Predictive Accuracy and
  Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannis Maier, Felix Möller, Lennart Purucker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated Machine Learning (AutoML) significantly simplifies the deployment
of machine learning models by automating tasks from data preprocessing to model
selection to ensembling. AutoML systems for tabular data often employ post hoc
ensembling, where multiple models are combined to improve predictive accuracy.
This typically results in longer inference times, a major limitation in
practical deployments. Addressing this, we introduce a hardware-aware ensemble
selection approach that integrates inference time into post hoc ensembling. By
leveraging an existing framework for ensemble selection with quality diversity
optimization, our method evaluates ensemble candidates for their predictive
accuracy and hardware efficiency. This dual focus allows for a balanced
consideration of accuracy and operational efficiency. Thus, our approach
enables practitioners to choose from a Pareto front of accurate and efficient
ensembles. Our evaluation using 83 classification datasets shows that our
approach sustains competitive accuracy and can significantly improve ensembles'
operational efficiency. The results of this study provide a foundation for
extending these principles to additional hardware constraints, setting the
stage for the development of more resource-efficient AutoML systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Third International Conference on Automated Machine
  Learning (AutoML 2024), Workshop Track; for code, see
  https://github.com/Atraxus/HA-ES</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRFormer: Multi-Scale <span class="highlight-title">Transformer</span> Utilizing Diverse Receptive Fields for
  Long Time-Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixin Ding, Yuqi Chen, Yu-Ting Lan, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term time series forecasting (LTSF) has been widely applied in finance,
traffic prediction, and other domains. Recently, patch-based transformers have
emerged as a promising approach, segmenting data into sub-level patches that
serve as input tokens. However, existing methods mostly rely on predetermined
patch lengths, necessitating expert knowledge and posing challenges in
capturing diverse characteristics across various scales. Moreover, time series
data exhibit diverse variations and fluctuations across different temporal
scales, which traditional approaches struggle to model effectively. In this
paper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm
to capture diverse receptive fields and sparse patterns of time series data. In
order to build hierarchical receptive fields, we develop a multi-scale
Transformer model, coupled with multi-scale sequence extraction, capable of
capturing multi-resolution features. Additionally, we introduce a group-aware
rotary position encoding technique to enhance intra- and inter-group position
awareness among representations across different temporal scales. Our proposed
model, named DRFormer, is evaluated on various real-world datasets, and
experimental results demonstrate its superiority compared to existing methods.
Our code is available at: https://github.com/ruixindingECNU/DRFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Collaborative Data Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rayne Holland, Chandra Thapa, Sarah Ali Siddiqui, Wei Shao, Seyit Camtepe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large machine-learning training datasets can be distilled into small
collections of informative synthetic data samples. These synthetic sets support
efficient model learning and reduce the communication cost of data sharing.
Thus, high-fidelity distilled data can support the efficient deployment of
machine learning applications in distributed network environments. A naive way
to construct a synthetic set in a distributed environment is to allow each
client to perform local data distillation and to merge local distillations at a
central server. However, the quality of the resulting set is impaired by
heterogeneity in the distributions of the local data held by clients. To
overcome this challenge, we introduce the first collaborative data distillation
technique, called CollabDM, which captures the global distribution of the data
and requires only a single round of communication between client and server.
Our method outperforms the state-of-the-art one-shot learning method on skewed
data in distributed learning environments. We also show the promising practical
benefits of our method when applied to attack detection in 5G networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning and Abstract Concepts: The Case of Natural Numbers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel N. Nissani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Learning (CL) has been successfully applied to classification and
other downstream tasks related to concrete concepts, such as objects contained
in the ImageNet dataset. No attempts seem to have been made so far in applying
this promising scheme to more abstract entities. A prominent example of these
could be the concept of (discrete) Quantity. CL can be frequently interpreted
as a self-supervised scheme guided by some profound and ubiquitous conservation
principle (e.g. conservation of identity in object classification tasks). In
this introductory work we apply a suitable conservation principle to the
semi-abstract concept of natural numbers by which discrete quantities can be
estimated or predicted. We experimentally show, by means of a toy problem, that
contrastive learning can be trained to count at a glance with high accuracy
both at human as well as at super-human ranges.. We compare this with the
results of a trained-to-count at a glance supervised learning (SL) neural
network scheme of similar architecture. We show that both schemes exhibit
similar good performance on baseline experiments, where the distributions of
the training and testing stages are equal. Importantly, we demonstrate that in
some generalization scenarios, where training and testing distributions differ,
CL boasts more robust and much better error performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methods to improve run time of hydrologic models: opportunities and
  challenges in the machine learning era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Supath Dhital
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Machine Learning (ML) to hydrologic modeling is fledgling.
Its applicability to capture the dependencies on watersheds to forecast better
within a short period is fascinating. One of the key reasons to adopt ML
algorithms over physics-based models is its computational efficiency advantage
and flexibility to work with various data sets. The diverse applications,
particularly in emergency response and expanding over a large scale, demand the
hydrological model in a short time and make researchers adopt data-driven
modeling approaches unhesitatingly. In this work, in the era of ML and deep
learning (DL), how it can help to improve the overall run time of physics-based
model and potential constraints that should be addressed while modeling. This
paper covers the opportunities and challenges of adopting ML for hydrological
modeling and subsequently how it can help to improve the simulation time of
physics-based models and future works that should be addressed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model Aided QoS Prediction for Service Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiying Liu, Zekun Zhang, Qilin Wu, Yiwen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have seen rapid improvement in the recent years,
and are used in a wider range of applications. After being trained on large
text corpus, LLMs obtain the capability of extracting rich features from
textual data. Such capability is potentially useful for the web service
recommendation task, where the web users and services have intrinsic attributes
that can be described using natural language sentences and are useful for
recommendation. In this paper, we explore the possibility and practicality of
using LLMs for web service recommendation. We propose the large language model
aided QoS prediction (llmQoS) model, which use LLMs to extract useful
information from attributes of web users and services via descriptive
sentences. This information is then used in combination with the QoS values of
historical interactions of users and services, to predict QoS values for any
given user-service pair. Our proposed model is shown to overcome the data
sparsity issue for QoS prediction. We show that on the WSDream dataset, llmQoS
outperforms comparable baseline models consistently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Climate-Driven Doubling of Maize Loss Probability in U.S. Crop
  Insurance: Spatiotemporal Prediction and Possible Policy Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A Samuel Pottinger, Lawson Connor, Brookie Guzder-Williams, Maya Weltman-Fahs, Timothy Bowles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change not only threatens agricultural producers but also strains
financial institutions. These important food system actors include government
entities tasked with both insuring grower livelihoods and supporting response
to continued global warming. We use an artificial neural network to predict
future maize yields in the U.S. Corn Belt, finding alarming changes to
institutional risk exposure within the Federal Crop Insurance Program.
Specifically, our machine learning method anticipates more frequent and more
severe yield losses that would result in the annual probability of Yield
Protection (YP) claims to more than double at mid-century relative to
simulations without continued climate change. Furthermore, our dual finding of
relatively unchanged average yields paired with decreasing yield stability
reveals targeted opportunities to adjust coverage formulas to include
variability. This important structural shift may help regulators support grower
adaptation to continued climate change by recognizing the value of
risk-reducing strategies such as regenerative agriculture. Altogether, paired
with open source interactive tools for deeper investigation, our risk profile
simulations fill an actionable gap in current understanding, bridging granular
historic yield estimation and climate-informed prediction of future
insurer-relevant loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-level Traffic-Responsive Tilt Camera Surveillance through
  Predictive Correlated Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Li, Zilin Bian, Haozhe Lei, Fan Zuo, Ya-Ting Yang, Quanyan Zhu, Zhenning Li, Kaan Ozbay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In urban traffic management, the primary challenge of dynamically and
efficiently monitoring traffic conditions is compounded by the insufficient
utilization of thousands of surveillance cameras along the intelligent
transportation system. This paper introduces the multi-level Traffic-responsive
Tilt Camera surveillance system (TTC-X), a novel framework designed for dynamic
and efficient monitoring and management of traffic in urban networks. By
leveraging widely deployed pan-tilt-cameras (PTCs), TTC-X overcomes the
limitations of a fixed field of view in traditional surveillance systems by
providing mobilized and 360-degree coverage. The innovation of TTC-X lies in
the integration of advanced machine learning modules, including a
detector-predictor-controller structure, with a novel Predictive Correlated
Online Learning (PiCOL) methodology and the Spatial-Temporal Graph Predictor
(STGP) for real-time traffic estimation and PTC control. The TTC-X is tested
and evaluated under three experimental scenarios (e.g., maximum traffic flow
capture, dynamic route planning, traffic state estimation) based on a
simulation environment calibrated using real-world traffic data in Brooklyn,
New York. The experimental results showed that TTC-X captured over 60\% total
number of vehicles at the network level, dynamically adjusted its route
recommendation in reaction to unexpected full-lane closure events, and
reconstructed link-level traffic states with best MAE less than 1.25
vehicle/hour. Demonstrating scalability, cost-efficiency, and adaptability,
TTC-X emerges as a powerful solution for urban traffic management in both
cyber-physical and real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transportation Research Part C special issue: Modelling,
  Learning, and Control of Conventional, Cooperative and Automated Motorway and
  Urban Traffic Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Performance of Large Language Models for SDG Mapping
  (Technical Report) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Yin, Amir Aryani, Nakul Nambiar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of large language models (LLMs) is expanding rapidly, and open-source
versions are becoming available, offering users safer and more adaptable
options. These models enable users to protect data privacy by eliminating the
need to provide data to third parties and can be customized for specific tasks.
In this study, we compare the performance of various language models on the
Sustainable Development Goal (SDG) mapping task, using the output of GPT-4o as
the baseline. The selected open-source models for comparison include Mixtral,
LLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more
specialized version of GPT-4o, was included to extend the comparison. Given the
multi-label nature of the SDG mapping task, we employed metrics such as F1
score, precision, and recall with micro-averaging to evaluate different aspects
of the models' performance. These metrics are derived from the confusion matrix
to ensure a comprehensive evaluation. We provide a clear observation and
analysis of each model's performance by plotting curves based on F1 score,
precision, and recall at different thresholds. According to the results of this
experiment, LLaMA 2 and Gemma still have significant room for improvement. The
other four models do not exhibit particularly large differences in performance.
The outputs from all seven models are available on Zenodo:
https://doi.org/10.5281/zenodo.12789375.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synergistic Learning with Multi-Task DeepONet for Efficient PDE Problem
  Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Kumar, Somdatta Goswami, Katiana Kontolati, Michael D. Shields, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) is an inductive transfer mechanism designed to
leverage useful information from multiple tasks to improve generalization
performance compared to single-task learning. It has been extensively explored
in traditional machine learning to address issues such as data sparsity and
overfitting in neural networks. In this work, we apply MTL to problems in
science and engineering governed by partial differential equations (PDEs).
However, implementing MTL in this context is complex, as it requires
task-specific modifications to accommodate various scenarios representing
different physical processes. To this end, we present a multi-task deep
operator network (MT-DeepONet) to learn solutions across various functional
forms of source terms in a PDE and multiple geometries in a single concurrent
training session. We introduce modifications in the branch network of the
vanilla DeepONet to account for various functional forms of a parameterized
coefficient in a PDE. Additionally, we handle parameterized geometries by
introducing a binary mask in the branch network and incorporating it into the
loss term to improve convergence and generalization to new geometry tasks. Our
approach is demonstrated on three benchmark problems: (1) learning different
functional forms of the source term in the Fisher equation; (2) learning
multiple geometries in a 2D Darcy Flow problem and showcasing better transfer
learning capabilities to new geometries; and (3) learning 3D parameterized
geometries for a heat transfer problem and demonstrate the ability to predict
on new but similar geometries. Our MT-DeepONet framework offers a novel
approach to solving PDE problems in engineering and science under a unified
umbrella based on synergistic learning that reduces the overall training cost
for neural operators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Lv, Xuan Xia, Sheng-Jun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown great potential in code-related
tasks, yet open-source models lag behind their closed-source counterparts. To
bridge this performance gap, existing methods generate vast amounts of
synthetic data for fine-tuning, leading to inefficiencies in training.
Motivated by the need for more effective and efficient training, we propose the
Code Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces
the Complexity and Diversity Aware Sampling (CDAS) method to select
high-quality training data based on complexity and diversity, and the Dynamic
Pack padding strategy to reduce computational resource usage by minimizing
padding tokens during training. Experimental results demonstrate that
CodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,
achieves an 8.6% performance increase on HumanEval, reduces training time by
78%, and decreases peak GPU memory usage by 27%. These findings underscore
CodeACT's ability to enhance the performance and efficiency of open-source
models. By optimizing both the data selection and training processes, CodeACT
offers a comprehensive approach to improving the capabilities of open-source
LLMs while significantly reducing computational requirements, addressing the
dual challenges of data quality and training efficiency, and paving the way for
more resource-efficient and performant models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Back-Projection Diffusion: Solving the Wideband Inverse Scattering
  Problem with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borong Zhang, Martín Guerra, Qin Li, Leonardo Zepeda-Núñez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present \textit{Wideband back-projection diffusion}, an end-to-end
probabilistic framework for approximating the posterior distribution induced by
the inverse scattering map from wideband scattering data. This framework
leverages conditional diffusion models coupled with the underlying physics of
wave-propagation and symmetries in the problem, to produce highly accurate
reconstructions. The framework introduces a factorization of the score function
into a physics-based latent representation inspired by the filtered
back-propagation formula and a conditional score function conditioned on this
latent representation. These two steps are also constrained to obey symmetries
in the formulation while being amenable to compression by imposing the rank
structure found in the filtered back-projection formula. As a result,
empirically, our framework is able to provide sharp reconstructions
effortlessly, even recovering sub-Nyquist features in the multiple-scattering
regime. It has low-sample and computational complexity, its number of
parameters scales sub-linearly with the target resolution, and it has stable
training dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Fine-Tuning LLMs using Heterogeneous Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Aponte, Ryan A. Rossi, Shunan Guo, Franck Dernoncourt, Tong Yu, Xiang Chen, Subrata Mitra, Nedim Lipka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been applied to a wide range of tasks,
including text summarization, web navigation, and chatbots. They have
benefitted from supervised fine-tuning (SFT) and reinforcement learning from
human feedback (RLHF) following an unsupervised pretraining. These datasets can
be difficult to collect, limited in scope, and vary in sample quality.
Additionally, datasets can vary extensively in supervision format, from
numerical to binary as well as multi-dimensional with many different values. We
present a framework for fine-tuning LLMs using heterogeneous feedback, which
has two main components. First, we combine the heterogeneous feedback data into
a single supervision format, compatible with methods like SFT and RLHF. Next,
given this unified feedback dataset, we extract a high-quality and diverse
subset to obtain performance increases potentially exceeding the full dataset.
We conduct extensive experiments to understand the effectiveness of these
techniques for incorporating heterogeneous feedback, and demonstrate
improvements from using a high-quality and diverse subset of the data. We find
that our framework is able to improve models in multiple areas simultaneously,
such as in instruction following and bias reduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Learning for WBAN-based Health Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cho-Chun Chiu, Tuan Nguyen, Ting He, Shiqiang Wang, Beom-Su Kim, Ki-Il Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a novel active learning problem motivated by the need of learning
machine learning models for health monitoring in wireless body area network
(WBAN). Due to the limited resources at body sensors, collecting each unlabeled
sample in WBAN incurs a nontrivial cost. Moreover, training health monitoring
models typically requires labels indicating the patient's health state that
need to be generated by healthcare professionals, which cannot be obtained at
the same pace as data collection. These challenges make our problem
fundamentally different from classical active learning, where unlabeled samples
are free and labels can be queried in real time. To handle these challenges, we
propose a two-phased active learning method, consisting of an online phase
where a coreset construction algorithm is proposed to select a subset of
unlabeled samples based on their noisy predictions, and an offline phase where
the selected samples are labeled to train the target model. The samples
selected by our algorithm are proved to yield a guaranteed error in
approximating the full dataset in evaluating the loss function. Our evaluation
based on real health monitoring data and our own experimentation demonstrates
that our solution can drastically save the data curation cost without
sacrificing the quality of the target model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous graph attention network improves cancer multiomics
  integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Tabakhi, Charlotte Vandermeulen, Ian Sudbery, Haiping Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increase in high-dimensional multiomics data demands advanced integration
models to capture the complexity of human diseases. Graph-based deep learning
integration models, despite their promise, struggle with small patient cohorts
and high-dimensional features, often applying independent feature selection
without modeling relationships among omics. Furthermore, conventional
graph-based omics models focus on homogeneous graphs, lacking multiple types of
nodes and edges to capture diverse structures. We introduce a Heterogeneous
Graph ATtention network for omics integration (HeteroGATomics) to improve
cancer diagnosis. HeteroGATomics performs joint feature selection through a
multi-agent system, creating dedicated networks of feature and patient
similarity for each omic modality. These networks are then combined into one
heterogeneous graph for learning holistic omic-specific representations and
integrating predictions across modalities. Experiments on three cancer
multiomics datasets demonstrate HeteroGATomics' superior performance in cancer
diagnosis. Moreover, HeteroGATomics enhances interpretability by identifying
important biomarkers contributing to the diagnosis outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Posterior Probabilities: Decision Theory, Proper Scoring
  Rules, and Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luciana Ferrer, Daniel Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most machine learning classifiers are designed to output posterior
probabilities for the classes given the input sample. These probabilities may
be used to make the categorical decision on the class of the sample; provided
as input to a downstream system; or provided to a human for interpretation.
Evaluating the quality of the posteriors generated by these system is an
essential problem which was addressed decades ago with the invention of proper
scoring rules (PSRs). Unfortunately, much of the recent machine learning
literature uses calibration metrics -- most commonly, the expected calibration
error (ECE) -- as a proxy to assess posterior performance. The problem with
this approach is that calibration metrics reflect only one aspect of the
quality of the posteriors, ignoring the discrimination performance. For this
reason, we argue that calibration metrics should play no role in the assessment
of posterior quality. Expected PSRs should instead be used for this job,
preferably normalized for ease of interpretation. In this work, we first give a
brief review of PSRs from a practical perspective, motivating their definition
using Bayes decision theory. We discuss why expected PSRs provide a principled
measure of the quality of a system's posteriors and why calibration metrics are
not the right tool for this job. We argue that calibration metrics, while not
useful for performance assessment, may be used as diagnostic tools during
system development. With this purpose in mind, we discuss a simple and
practical calibration metric, called calibration loss, derived from a
decomposition of expected PSRs. We compare this metric with the ECE and with
the expected score divergence calibration metric from the PSR literature and
argue, using theoretical and empirical evidence, that calibration loss is
superior to these two metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Cox Models with Stochastic Gradient Descent: Theoretical
  Foundations and Practical Guidances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lang Zeng, Weijing Tang, Zhao Ren, Ying Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing Cox regression and its neural network variants poses substantial
computational challenges in large-scale studies. Stochastic gradient descent
(SGD), known for its scalability in model optimization, has recently been
adapted to optimize Cox models. Unlike its conventional application, which
typically targets a sum of independent individual loss, SGD for Cox models
updates parameters based on the partial likelihood of a subset of data. Despite
its empirical success, the theoretical foundation for optimizing Cox partial
likelihood with SGD is largely underexplored. In this work, we demonstrate that
the SGD estimator targets an objective function that is batch-size-dependent.
We establish that the SGD estimator for the Cox neural network (Cox-NN) is
consistent and achieves the optimal minimax convergence rate up to a
polylogarithmic factor. For Cox regression, we further prove the
$\sqrt{n}$-consistency and asymptotic normality of the SGD estimator, with
variance depending on the batch size. Furthermore, we quantify the impact of
batch size on Cox-NN training and its effect on the SGD estimator's asymptotic
efficiency in Cox regression. These findings are validated by extensive
numerical experiments and provide guidance for selecting batch sizes in SGD
applications. Finally, we demonstrate the effectiveness of SGD in a real-world
application where GD is unfeasible due to the large scale of data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretation of the Intent Detection Problem as Dynamics in a
  Low-dimensional Space <span class="chip">ECAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Sanchez-Karhunen, Jose F. Quesada-Moreno, Miguel A. Gutiérrez-Naranjo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intent detection is a text classification task whose aim is to recognize and
label the semantics behind a users query. It plays a critical role in various
business applications. The output of the intent detection module strongly
conditions the behavior of the whole system. This sequence analysis task is
mainly tackled using deep learning techniques. Despite the widespread use of
these techniques, the internal mechanisms used by networks to solve the problem
are poorly understood. Recent lines of work have analyzed the computational
mechanisms learned by RNNs from a dynamical systems perspective. In this work,
we investigate how different RNN architectures solve the SNIPS intent detection
problem. Sentences injected into trained networks can be interpreted as
trajectories traversing a hidden state space. This space is constrained to a
low-dimensional manifold whose dimensionality is related to the embedding and
hidden layer sizes. To generate predictions, RNN steers the trajectories
towards concrete regions, spatially aligned with the output layer matrix rows
directions. Underlying the system dynamics, an unexpected fixed point topology
has been identified with a limited number of attractors. Our results provide
new insights into the inner workings of networks that solve the intent
detection task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-Ready version. Accepted paper at 27th European Conference on
  Artificial Intelligence (ECAI-2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DaCapo: a modular deep learning framework for scalable 3D image
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Patton, Jeff L. Rhoades, Marwan Zouinkhi, David G. Ackerman, Caroline Malin-Mayor, Diane Adjavon, Larissa Heinrich, Davis Bennett, Yurii Zubov, CellMap Project Team, Aubrey V. Weigel, Jan Funke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DaCapo is a specialized deep learning library tailored to expedite the
training and application of existing machine learning approaches on large,
near-isotropic image data. In this correspondence, we introduce DaCapo's unique
features optimized for this specific domain, highlighting its modular
structure, efficient experiment management tools, and scalable deployment
capabilities. We discuss its potential to improve access to large-scale,
isotropic image segmentation and invite the community to explore and contribute
to this open-source initiative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Learning for Quantum Linear Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Costantino Carugno, Maurizio Ferrari Dacrema, Paolo Cremonesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent availability of quantum annealers as cloud-based services has
enabled new ways to handle machine learning problems, and several relevant
algorithms have been adapted to run on these devices. In a recent work, linear
regression was formulated as a quadratic binary optimization problem that can
be solved via quantum annealing. Although this approach promises a
computational time advantage for large datasets, the quality of the solution is
limited by the necessary use of a precision vector, used to approximate the
real-numbered regression coefficients in the quantum formulation. In this work,
we focus on the practical challenge of improving the precision vector encoding:
instead of setting an array of generic values equal for all coefficients, we
allow each one to be expressed by its specific precision, which is tuned with a
simple adaptive algorithm. This approach is evaluated on synthetic datasets of
increasing size, and linear regression is solved using the D-Wave Advantage
quantum annealer, as well as classical solvers. To the best of our knowledge,
this is the largest dataset ever evaluated for linear regression on a quantum
annealer. The results show that our formulation is able to deliver improved
solution quality in all instances, and could better exploit the potential of
current quantum devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Setting the duration of online A/B experiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harrison H. Li, Chaoyu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In designing an online A/B experiment, it is crucial to select a sample size
and duration that ensure the resulting confidence interval (CI) for the
treatment effect is the right width to detect an effect of meaningful magnitude
with sufficient statistical power without wasting resources. While the
relationship between sample size and CI width is well understood, the effect of
experiment duration on CI width remains less clear. This paper provides an
analytical formula for the width of a CI based on a ratio treatment effect
estimator as a function of both sample size (N) and duration (T). The formula
is derived from a mixed effects model with two variance components. One
component, referred to as the temporal variance, persists over time for
experiments where the same users are kept in the same experiment arm across
different days. The remaining error variance component, by contrast, decays to
zero as T gets large. The formula we derive introduces a key parameter that we
call the user-specific temporal correlation (UTC), which quantifies the
relative sizes of the two variance components and can be estimated from
historical experiments. Higher UTC indicates a slower decay in CI width over
time. On the other hand, when the UTC is 0 -- as for experiments where users
shuffle in and out of the experiment across days -- the CI width decays at the
standard parametric 1/T rate. We also study how access to pre-period data for
the users in the experiment affects the CI width decay. We show our formula
closely explains CI widths on real A/B experiments at YouTube.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wave-RVFL: A Randomized Neural Network Based on Wave Loss Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Sajid, A. Quadir, M. Tanveer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The random vector functional link (RVFL) network is well-regarded for its
strong generalization capabilities in the field of machine learning. However,
its inherent dependencies on the square loss function make it susceptible to
noise and outliers. Furthermore, the calculation of RVFL's unknown parameters
necessitates matrix inversion of the entire training sample, which constrains
its scalability. To address these challenges, we propose the Wave-RVFL, an RVFL
model incorporating the wave loss function. We formulate and solve the proposed
optimization problem of the Wave-RVFL using the adaptive moment estimation
(Adam) algorithm in a way that successfully eliminates the requirement for
matrix inversion and significantly enhances scalability. The Wave-RVFL exhibits
robustness against noise and outliers by preventing over-penalization of
deviations, thereby maintaining a balanced approach to managing noise and
outliers. The proposed Wave-RVFL model is evaluated on multiple UCI datasets,
both with and without the addition of noise and outliers, across various
domains and sizes. Empirical results affirm the superior performance and
robustness of the Wave-RVFL compared to baseline models, establishing it as a
highly effective and scalable classification solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Monitoring via Repeated Significance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Bax, Arundhyoti Sarkar, Alex Shtoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Requiring statistical significance at multiple interim analyses to declare a
statistically significant result for an AB test allows less stringent
requirements for significance at each interim analysis. Repeated repeated
significance competes well with methods built on assumptions about the test --
assumptions that may be impossible to evaluate a priori and may require extra
data to evaluate empirically.
  Instead, requiring repeated significance allows the data itself to prove
directly that the required results are not due to chance alone. We explain how
to apply tests with repeated significance to continuously monitor unbounded
tests -- tests that do not have an a priori bound on running time or number of
observations. We show that it is impossible to maintain a constant requirement
for significance for unbounded tests, but that we can come arbitrarily close to
that goal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ed Encoder Inference: Revealing Upstream Encoders In Downstream
  Machine Learning Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaopeng Fu, Xuexue Sun, Ke Qing, Tianhang Zheng, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though pre-trained encoders can be easily accessed online to build downstream
machine learning (ML) services quickly, various attacks have been designed to
compromise the security and privacy of these encoders. While most attacks
target encoders on the upstream side, it remains unknown how an encoder could
be threatened when deployed in a downstream ML service. This paper unveils a
new vulnerability: the Pre-trained Encoder Inference (PEI) attack, which posts
privacy threats toward encoders hidden behind downstream ML services. By only
providing API accesses to a targeted downstream service and a set of candidate
encoders, the PEI attack can infer which encoder is secretly used by the
targeted service based on candidate ones. We evaluate the attack performance of
PEI against real-world encoders on three downstream tasks: image
classification, text classification, and text-to-image generation. Experiments
show that the PEI attack succeeds in revealing the hidden encoder in most cases
and seldom makes mistakes even when the hidden encoder is not in the candidate
set. We also conducted a case study on one of the most recent vision-language
models, LLaVA, to illustrate that the PEI attack is useful in assisting other
ML attacks such as adversarial attacks. The code is available at
https://github.com/fshp971/encoder-inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Malicious Attacks in Federated Learning via Confidence-aware
  Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilei Li, Ahmed M. Abdelmoniem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is an emerging distributed machine learning paradigm
that allows multiple clients to collaboratively train a global model without
sharing private local data. However, FL systems are vulnerable to attacks from
malicious clients, who can degrade the global model performance through data
poisoning and model poisoning. Existing defense methods typically focus on a
single type of attack, such as Byzantine attacks or backdoor attacks, and are
often ineffective against potential data poisoning attacks like label flipping
and label shuffling. Additionally, these methods often lack accuracy and
robustness in detecting and handling malicious updates. To address these
issues, we propose a novel method based on model confidence scores, which
evaluates the uncertainty of client model updates to detect and defend against
malicious clients. Our approach is comprehensively effective for both model
poisoning and data poisoning attacks and is capable of accurately identifying
and mitigating potential malicious updates from being aggregated. Experimental
results demonstrate that our method significantly improves the robustness of FL
systems against various types of attacks, also achieving higher model accuracy
and stability across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering Air Travel Disruptions: A Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aravinda Jatavallabha, Jacob Gerlach, Aadithya Naresh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research investigates flight delay trends by examining factors such as
departure time, airline, and airport. It employs regression machine learning
methods to predict the contributions of various sources to delays. Time-series
models, including LSTM, Hybrid LSTM, and Bi-LSTM, are compared with baseline
regression models such as Multiple Regression, Decision Tree Regression, Random
Forest Regression, and Neural Network. Despite considerable errors in the
baseline models, the study aims to identify influential features in delay
prediction, potentially informing flight planning strategies. Unlike previous
work, this research focuses on regression tasks and explores the use of
time-series models for predicting flight delays. It offers insights into
aviation operations by independently analyzing each delay component (e.g.,
security, weather).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Deep Learning Models with the $\ell_1$ Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixin Shen, Rui Wang, Yuesheng Xu, Mingsong Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse neural networks are highly desirable in deep learning in reducing its
complexity. The goal of this paper is to study how choices of regularization
parameters influence the sparsity level of learned neural networks. We first
derive the $\ell_1$-norm sparsity-promoting deep learning models including
single and multiple regularization parameters models, from a statistical
viewpoint. We then characterize the sparsity level of a regularized neural
network in terms of the choice of the regularization parameters. Based on the
characterizations, we develop iterative algorithms for selecting regularization
parameters so that the weight parameters of the resulting deep neural network
enjoy prescribed sparsity levels. Numerical experiments are presented to
demonstrate the effectiveness of the proposed algorithms in choosing desirable
regularization parameters and obtaining corresponding neural networks having
both of predetermined sparsity levels and satisfactory approximation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining Gender and Power on Wikipedia Through Face and Politeness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adil Soubki, Shyne Choi, Owen Rambow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a framework for analyzing discourse by combining two
interdependent concepts from sociolinguistic theory: face acts and politeness.
While politeness has robust existing tools and data, face acts are less
resourced. We introduce a new corpus created by annotating Wikipedia talk pages
with face acts and we use this to train a face act tagger. We then employ our
framework to study how face and politeness interact with gender and power in
discussions between Wikipedia editors. Among other findings, we observe that
female Wikipedians are not only more polite, which is consistent with prior
studies, but that this difference corresponds with significantly more language
directed at humbling aspects of their own face. Interestingly, the distinction
nearly vanishes once limiting to editors with administrative power.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithm-Informed Graph Neural Networks for Leakage Detection and
  Localization in Water Distribution Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zepeng Zhang, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and localizing leakages is a significant challenge for the
efficient and sustainable management of water distribution networks (WDN).
Leveraging the inherent graph structure of WDNs, recent approaches have used
graph-based data-driven methods. However, these methods often learn shortcuts
that work well with in-distribution data but fail to generalize to
out-of-distribution data. To address this limitation and inspired by the
perfect generalization ability of classical algorithms, we propose an
algorithm-informed graph neural network (AIGNN). Recognizing that WDNs function
as flow networks, incorporating max-flow information can be beneficial for
inferring pressures. In the proposed framework, we first train AIGNN to emulate
the Ford-Fulkerson algorithm for solving max-flow problems. This algorithmic
knowledge is then transferred to address the pressure estimation problem in
WDNs. Two AIGNNs are deployed, one to reconstruct pressure based on the current
measurements, and another to predict pressure based on previous measurements.
Leakages are detected and localized by comparing the outputs of the
reconstructor and the predictor. By pretraining AIGNNs to reason like
algorithms, they are expected to extract more task-relevant and generalizable
features. Experimental results demonstrate that the proposed algorithm-informed
approach achieves superior results with better generalization ability compared
to GNNs that do not incorporate algorithmic knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 4D-Var using Hessian approximation and backpropagation applied to
  automatically-differentiable numerical and machine learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kylen Solvik, Stephen G. Penny, Stephan Hoyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constraining a numerical weather prediction (NWP) model with observations via
4D variational (4D-Var) data assimilation is often difficult to implement in
practice due to the need to develop and maintain a software-based tangent
linear model and adjoint model. One of the most common 4D-Var algorithms uses
an incremental update procedure, which has been shown to be an approximation of
the Gauss-Newton method. Here we demonstrate that when using a forecast model
that supports automatic differentiation, an efficient and in some cases more
accurate alternative approximation of the Gauss-Newton method can be applied by
combining backpropagation of errors with Hessian approximation. This approach
can be used with either a conventional numerical model implemented within a
software framework that supports automatic differentiation, or a machine
learning (ML) based surrogate model. We test the new approach on a variety of
Lorenz-96 and quasi-geostrophic models. The results indicate potential for a
deeper integration of modeling, data assimilation, and new technologies in a
next-generation of operational forecast systems that leverage weather models
designed to support automatic differentiation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConDL: Detector-Free Dense Image Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Monika Kwiatkowski, Simon Matern, Olaf Hellwich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce a deep-learning framework designed for estimating
dense image correspondences. Our fully convolutional model generates dense
feature maps for images, where each pixel is associated with a descriptor that
can be matched across multiple images. Unlike previous methods, our model is
trained on synthetic data that includes significant distortions, such as
perspective changes, illumination variations, shadows, and specular highlights.
Utilizing contrastive learning, our feature maps achieve greater invariance to
these distortions, enabling robust matching. Notably, our method eliminates the
need for a keypoint detector, setting it apart from many existing
image-matching techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dimensionality Reduction and Nearest Neighbors for Improving
  Out-of-Distribution Detection in Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        McKell Woodland, Nihil Patel, Austin Castelo, Mais Al Taie, Mohamed Eltaher, Joshua P. Yung, Tucker J. Netherton, Tiffany L. Calderone, Jessica I. Sanchez, Darrel W. Cleere, Ahmed Elsaiey, Nakul Gupta, David Victor, Laura Beretta, Ankit B. Patel Kristy K. Brock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinically deployed deep learning-based segmentation models are known to fail
on data outside of their training distributions. While clinicians review the
segmentations, these models tend to perform well in most instances, which could
exacerbate automation bias. Therefore, detecting out-of-distribution images at
inference is critical to warn the clinicians that the model likely failed. This
work applied the Mahalanobis distance (MD) post hoc to the bottleneck features
of four Swin UNETR and nnU-net models that segmented the liver on T1-weighted
magnetic resonance imaging and computed tomography. By reducing the dimensions
of the bottleneck features with either principal component analysis or uniform
manifold approximation and projection, images the models failed on were
detected with high performance and minimal computational load. In addition,
this work explored a non-parametric alternative to the MD, a k-th nearest
neighbors distance (KNN). KNN drastically improved scalability and performance
over MD when both were applied to raw and average-pooled bottleneck features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Expansion of "Dimensionality Reduction for Improving
  Out-of-Distribution Detection in Medical Image Segmentation" arXiv:2308.03723
  . Submitted to the Journal for Machine Learning in Biomedical Imaging. Code
  available at https://github.com/mckellwoodland/dimen_reduce_mahal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification of Raw MEG/EEG Data with Detach-Rocket Ensemble: An
  Improved ROCKET Algorithm for Multivariate Time Series Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrià Solana, Erik Fransén, Gonzalo Uribarri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate Time Series Classification (MTSC) is a ubiquitous problem in
science and engineering, particularly in neuroscience, where most data
acquisition modalities involve the simultaneous time-dependent recording of
brain activity in multiple brain regions. In recent years, Random Convolutional
Kernel models such as ROCKET and MiniRocket have emerged as highly effective
time series classification algorithms, capable of achieving state-of-the-art
accuracy results with low computational load. Despite their success, these
types of models face two major challenges when employed in neuroscience: 1)
they struggle to deal with high-dimensional data such as EEG and MEG, and 2)
they are difficult to interpret. In this work, we present a novel ROCKET-based
algorithm, named Detach-Rocket Ensemble, that is specifically designed to
address these two problems in MTSC. Our algorithm leverages pruning to provide
an integrated estimation of channel importance, and ensembles to achieve better
accuracy and provide a label probability. Using a synthetic multivariate time
series classification dataset in which we control the amount of information
carried by each of the channels, we first show that our algorithm is able to
correctly recover the channel importance for classification. Then, using two
real-world datasets, a MEG dataset and an EEG dataset, we show that
Detach-Rocket Ensemble is able to provide both interpretable channel relevance
and competitive classification accuracy, even when applied directly to the raw
brain data, without the need for feature engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in European Conference on Machine Learning and Data
  Mining 2024, 20 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Hybrid Approach for Tornado Prediction in the United States:
  Kalman-Convolutional BiLSTM with Multi-Head Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tornadoes are among the most intense atmospheric vortex phenomena and pose
significant challenges for detection and forecasting. Conventional methods,
which heavily depend on ground-based observations and radar data, are limited
by issues such as decreased accuracy over greater distances and a high rate of
false positives. To address these challenges, this study utilizes the Seamless
Hybrid Scan Reflectivity (SHSR) dataset from the Multi-Radar Multi-Sensor
(MRMS) system, which integrates data from multiple radar sources to enhance
accuracy. A novel hybrid model, the Kalman-Convolutional BiLSTM with Multi-Head
Attention, is introduced to improve dynamic state estimation and capture both
spatial and temporal dependencies within the data. This model demonstrates
superior performance in precision, recall, F1-Score, and accuracy compared to
methods such as K-Nearest Neighbors (KNN) and LightGBM. The results highlight
the considerable potential of advanced machine learning techniques to improve
tornado prediction and reduce false alarm rates. Future research will focus on
expanding datasets, exploring innovative model architectures, and incorporating
large language models (LLMs) to provide deeper insights. This research
introduces a novel model for tornado prediction, offering a robust framework
for enhancing forecasting accuracy and public safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KAN we improve on HEP classification tasks? Kolmogorov-Arnold Networks
  applied to an LHC physics example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Erdmann, Florian Mausolf, Jan Lukas Späh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an
alternative to multilayer perceptrons, suggesting advantages in performance and
interpretability. We study a typical binary event classification task in
high-energy physics including high-level features and comment on the
performance and interpretability of KANs in this context. We find that the
learned activation functions of a one-layer KAN resemble the log-likelihood
ratio of the input features. In deeper KANs, the activations in the first KAN
layer differ from those in the one-layer KAN, which indicates that the deeper
KANs learn more complex representations of the data. We study KANs with
different depths and widths and we compare them to multilayer perceptrons in
terms of performance and number of trainable parameters. For the chosen
classification task, we do not find that KANs are more parameter efficient.
However, small KANs may offer advantages in terms of interpretability that come
at the cost of only a moderate loss in performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Combining Data and Knowledge: <span class="highlight-title">GPT</span>-4o is an Effective
  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of
  Lung Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17900v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17900v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danqing Hu, Bing Liu, Xiaofeng Zhu, Nan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lymph node metastasis (LNM) is a crucial factor in determining the initial
treatment for patients with lung cancer, yet accurate preoperative diagnosis of
LNM remains challenging. Recently, large language models (LLMs) have garnered
significant attention due to their remarkable text generation capabilities.
Leveraging the extensive medical knowledge learned from vast corpora, LLMs can
estimate probabilities for clinical problems, though their performance has
historically been inferior to data-driven machine learning models. In this
paper, we propose a novel ensemble method that combines the medical knowledge
acquired by LLMs with the latent patterns identified by machine learning models
to enhance LNM prediction performance. Initially, we developed machine learning
models using patient data. We then designed a prompt template to integrate the
patient data with the predicted probability from the machine learning model.
Subsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,
to estimate the likelihood of LNM based on patient data and then adjust the
estimate using the machine learning output. Finally, we collected three outputs
from the GPT-4o using the same prompt and ensembled these results as the final
prediction. Using the proposed method, our models achieved an AUC value of
0.778 and an AP value of 0.426 for LNM prediction, significantly improving
predictive performance compared to baseline machine learning models. The
experimental results indicate that GPT-4o can effectively leverage its medical
knowledge and the probabilities predicted by machine learning models to achieve
more accurate LNM predictions. These findings demonstrate that LLMs can perform
well in clinical risk prediction tasks, offering a new paradigm for integrating
medical knowledge and patient data in clinical predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness in Large Language Models in Three Hour 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00992v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00992v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thang Doan Viet, Zichong Wang, Minh Nhat Nguyen, Wenbin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable success across
various domains but often lack fairness considerations, potentially leading to
discriminatory outcomes against marginalized populations. Unlike fairness in
traditional machine learning, fairness in LLMs involves unique backgrounds,
taxonomies, and fulfillment techniques. This tutorial provides a systematic
overview of recent advances in the literature concerning fair LLMs, beginning
with real-world case studies to introduce LLMs, followed by an analysis of bias
causes therein. The concept of fairness in LLMs is then explored, summarizing
the strategies for evaluating bias and the algorithms designed to promote
fairness. Additionally, resources for assessing bias in LLMs, including
toolkits and datasets, are compiled, and current research challenges and open
questions in the field are discussed. The repository is available at
\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparks of Quantum Advantage and Rapid Retraining in Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16020v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16020v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Troy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of quantum computing holds the potential to revolutionize various
fields by solving complex problems more efficiently than classical computers.
Despite this promise, practical quantum advantage is hindered by current
hardware limitations, notably the small number of qubits and high noise levels.
In this study, we leverage adiabatic quantum computers to optimize
Kolmogorov-Arnold Networks, a powerful neural network architecture for
representing complex functions with minimal parameters. By modifying the
network to use Bezier curves as the basis functions and formulating the
optimization problem into a Quadratic Unconstrained Binary Optimization
problem, we create a fixed-sized solution space, independent of the number of
training samples. This strategy allows for the optimization of an entire neural
network in a single training iteration in which, due to order of operations, a
majority of the processing is done using a collapsed version of the training
dataset. This inherently creates extremely fast training speeds, which are
validated experimentally, compared to classical optimizers including Adam,
Stochastic Gradient Descent, Adaptive Gradient, and simulated annealing.
Additionally, we introduce a novel rapid retraining capability, enabling the
network to be retrained with new data without reprocessing old samples, thus
enhancing learning efficiency in dynamic environments. Experiments on
retraining demonstrate a hundred times speed up using adiabatic quantum
computing based optimization compared to that of the gradient descent based
optimizers, with theoretical models allowing this speed up to be much larger!
Our findings suggest that with further advancements in quantum hardware and
algorithm optimization, quantum-optimized machine learning models could have
broad applications across various domains, with initial focus on rapid
retraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Major updates to the paper for timings and explanations of
  optimization strategies used. Further optimized the code and updated the
  figures to reflect the faster timings for v3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRACTAL: An Ultra-Large-Scale Aerial Lidar <span class="highlight-title">Dataset</span> for 3D Semantic
  Segmentation of Diverse Landscapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04634v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04634v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Gaydon, Michel Daab, Floryne Roche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a
new tool to monitor territory and support public policies. Processing ALS data
at scale requires efficient point classification methods that perform well over
highly diverse territories. To evaluate them, researchers need large annotated
Lidar datasets, however, current Lidar benchmark datasets have restricted scope
and often cover a single urban area. To bridge this data gap, we present the
FRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an
ultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with
high-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is
built upon France's nationwide open Lidar data. It achieves spatial and
semantic diversity via a sampling scheme that explicitly concentrates rare
classes and challenging landscapes from five French regions. It should support
the development of 3D deep learning approaches for large-scale land monitoring.
We describe the nature of the source data, the sampling workflow, the content
of the resulting dataset, and provide an initial evaluation of segmentation
performance using a performant 3D neural architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages | 9 figures | 8 tables | Dataset is available at
  https://huggingface.co/datasets/IGNF/FRACTAL | Trained model is available at
  https://huggingface.co/IGNF/FRACTAL-LidarHD_7cl_randlanet | Deep learning
  code repository is on Gihtub at https://github.com/IGNF/myria3d | Data
  engineering code repository is on Github at https://github.com/IGNF/pacasam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantised Global Autoencoder: A Holistic Approach to Representing Visual
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Elsner, Paula Usinger, Victor Czech, Gregor Kobsik, Yanjiang He, Isaak Lim, Leif Kobbelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In quantised autoencoders, images are usually split into local patches, each
encoded by one token. This representation is redundant in the sense that the
same number of tokens is spend per region, regardless of the visual information
content in that region. Adaptive discretisation schemes like quadtrees are
applied to allocate tokens for patches with varying sizes, but this just varies
the region of influence for a token which nevertheless remains a local
descriptor. Modern architectures add an attention mechanism to the autoencoder
which infuses some degree of global information into the local tokens. Despite
the global context, tokens are still associated with a local image region. In
contrast, our method is inspired by spectral decompositions which transform an
input signal into a superposition of global frequencies. Taking the data-driven
perspective, we learn custom basis functions corresponding to the codebook
entries in our VQ-VAE setup. Furthermore, a decoder combines these basis
functions in a non-linear fashion, going beyond the simple linear superposition
of spectral decompositions. We can achieve this global description with an
efficient transpose operation between features and channels and demonstrate our
performance on compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Change Detection for Space Habitats Using 3D Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02396v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02396v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Santos, Holly Dinkel, Julia Di, Paulo V. K. Borges, Marina Moreira, Oleg Alexandrov, Brian Coltin, Trey Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an algorithm for scene change detection from point clouds
to enable autonomous robotic caretaking in future space habitats. Autonomous
robotic systems will help maintain future deep-space habitats, such as the
Gateway space station, which will be uncrewed for extended periods. Existing
scene analysis software used on the International Space Station (ISS) relies on
manually-labeled images for detecting changes. In contrast, the algorithm
presented in this work uses raw, unlabeled point clouds as inputs. The
algorithm first applies modified Expectation-Maximization Gaussian Mixture
Model (GMM) clustering to two input point clouds. It then performs change
detection by comparing the GMMs using the Earth Mover's Distance. The algorithm
is validated quantitatively and qualitatively using a test dataset collected by
an Astrobee robot in the NASA Ames Granite Lab comprising single frame depth
images taken directly by Astrobee and full-scene reconstructed maps built with
RGB-D and pose data from Astrobee. The runtimes of the approach are also
analyzed in depth. The source code is publicly released to promote further
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, Manuscript was presented at the AIAA SciTech
  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:
  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:
  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Class-Incremental Learning with <span class="highlight-title">Pre-Train</span>ed Models:
  Generalizability and Adaptivity are All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da-Wei Zhou, Zi-Wen Cai, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) aims to adapt to emerging new classes
without forgetting old ones. Traditional CIL models are trained from scratch to
continually acquire knowledge as data evolves. Recently, pre-training has
achieved substantial progress, making vast pre-trained models (PTMs) accessible
for CIL. Contrary to traditional methods, PTMs possess generalizable
embeddings, which can be easily transferred for CIL. In this work, we revisit
CIL with PTMs and argue that the core factors in CIL are adaptivity for model
updating and generalizability for knowledge transferring. 1) We first reveal
that frozen PTM can already provide generalizable embeddings for CIL.
Surprisingly, a simple baseline (SimpleCIL) which continually sets the
classifiers of PTM to prototype features can beat state-of-the-art even without
training on the downstream task. 2) Due to the distribution gap between
pre-trained and downstream datasets, PTM can be further cultivated with
adaptivity via model adaptation. We propose AdaPt and mERge (APER), which
aggregates the embeddings of PTM and adapted models for classifier
construction. APER is a general framework that can be orthogonally combined
with any parameter-efficient tuning method, which holds the advantages of PTM's
generalizability and adapted model's adaptivity. 3) Additionally, considering
previous ImageNet-based benchmarks are unsuitable in the era of PTM due to data
overlapping, we propose four new benchmarks for assessment, namely ImageNet-A,
ObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the
effectiveness of APER with a unified and concise framework. Code is available
at https://github.com/zhoudw-zdw/RevisitingCIL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCV. Code is available at:
  https://github.com/zhoudw-zdw/RevisitingCIL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Artificial Viscosity Models for Discontinuous Galerkin
  Approximation of Conservation Laws using Physics-Informed Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Caldana, Paola F. Antonietti, Luca Dede'
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finite element-based high-order solvers of conservation laws offer large
accuracy but face challenges near discontinuities due to the Gibbs phenomenon.
Artificial viscosity is a popular and effective solution to this problem based
on physical insight. In this work, we present a physics-informed machine
learning algorithm to automate the discovery of artificial viscosity models in
a non-supervised paradigm. The algorithm is inspired by reinforcement learning
and trains a neural network acting cell-by-cell (the viscosity model) by
minimizing a loss defined as the difference with respect to a reference
solution thanks to automatic differentiation. This enables a dataset-free
training procedure. We prove that the algorithm is effective by integrating it
into a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase
several numerical tests on scalar and vectorial problems, such as Burgers' and
Euler's equations in one and two dimensions. Results demonstrate that the
proposed approach trains a model that is able to outperform classical viscosity
models. Moreover, we show that the learnt artificial viscosity model is able to
generalize across different problems and parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human
  Annotation: A Case Study Using Schedule-of-Event Table Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawesh Kumar, Jonathan Amar, Eric Yang, Nan Li, Yugang Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated their efficacy across a broad
spectrum of tasks in healthcare applications. However, often LLMs need to be
fine-tuned on task-specific expert annotated data to achieve optimal
performance, which can be expensive and time consuming. In this study, we
fine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels
obtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)
tables, which specify care plan in clinical trial protocols. We introduce a
filtering mechanism to select high-confidence labels for this table
classification task, thereby reducing the noise in the auto-generated labels.
We show that fine-tuned PaLM-2 with those labels achieves performance that
exceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is
close to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our
results show that leveraging LLM-generated labels through powerful models like
gemini-pro can potentially serve as a viable strategy for improving LLM
performance through fine-tuning in specialized tasks, particularly in domains
where expert annotations are scarce, expensive, or time-consuming to obtain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages. Published in MLHC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Smoothness and Approximation: Theoretical Insights into
  Over-Smoothing in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangrui Yang, Jianfei Li, Ming Li, Han Feng, Ding-Xuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the approximation theory of functions defined on
graphs. Our study builds upon the approximation results derived from the
$K$-functional. We establish a theoretical framework to assess the lower bounds
of approximation for target functions using Graph Convolutional Networks (GCNs)
and examine the over-smoothing phenomenon commonly observed in these networks.
Initially, we introduce the concept of a $K$-functional on graphs, establishing
its equivalence to the modulus of smoothness. We then analyze a typical type of
GCN to demonstrate how the high-frequency energy of the output decays, an
indicator of over-smoothing. This analysis provides theoretical insights into
the nature of over-smoothing within GCNs. Furthermore, we establish a lower
bound for the approximation of target functions by GCNs, which is governed by
the modulus of smoothness of these functions. This finding offers a new
perspective on the approximation capabilities of GCNs. In our numerical
experiments, we analyze several widely applied GCNs and observe the phenomenon
of energy decay. These observations corroborate our theoretical results on
exponential decay order.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Learners Meet Web Image-Text Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07088v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07088v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingchen Zhao, Quan Cui, Hao Wu, Osamu Yoshie, Cheng Yang, Oisin Mac Aodha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many self-supervised learning methods are pre-trained on the well-curated
ImageNet-1K dataset. In this work, given the excellent scalability of web data,
we consider self-supervised pre-training on noisy web sourced image-text paired
data. First, we conduct a benchmark study of representative self-supervised
pre-training methods on large-scale web data in a like-for-like setting. We
compare a range of methods, including single-modal ones that use masked
training objectives and multi-modal ones that use image-text constrastive
training. We observe that existing multi-modal methods do not outperform their
single-modal counterparts on vision transfer learning tasks. We derive an
information-theoretical view to explain these benchmark results, which provides
insight into how to design a novel vision learner. Inspired by this insight, we
present a new visual representation pre-training method, MUlti-modal
Generator~(MUG), that learns from scalable web sourced image-text data. MUG
achieves state-of-the-art transfer performance on a variety of tasks and
demonstrates promising scaling properties. Pre-trained models and code will be
made public upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://bzhao.me/MUG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Do Language Models Learn in Context? The Structured Task Hypothesis <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04216v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04216v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaoda Li, Yifan Hou, Mrinmaya Sachan, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit an intriguing ability to learn a novel
task from in-context examples presented in a demonstration, termed in-context
learning (ICL). Understandably, a swath of research has been dedicated to
uncovering the theories underpinning ICL. One popular hypothesis explains ICL
by task selection. LLMs identify the task based on the demonstration and
generalize it to the prompt. Another popular hypothesis is that ICL is a form
of meta-learning, i.e., the models learn a learning algorithm at pre-training
time and apply it to the demonstration. Finally, a third hypothesis argues that
LLMs use the demonstration to select a composition of tasks learned during
pre-training to perform ICL. In this paper, we empirically explore these three
hypotheses that explain LLMs' ability to learn in context with a suite of
experiments derived from common text classification tasks. We invalidate the
first two hypotheses with counterexamples and provide evidence in support of
the last hypothesis. Our results suggest an LLM could learn a novel task in
context via composing tasks learned during pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is published in ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Series Classification in Smart Manufacturing Systems: An
  Experimental Evaluation of State-of-the-Art Machine Learning Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba A. Farahani, M. R. McCormick, Ramy Harik, Thorsten Wuest
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manufacturing is gathering extensive amounts of diverse data, thanks to the
growing number of sensors and rapid advances in sensing technologies. Among the
various data types available in SMS settings, time-series data plays a pivotal
role. Hence, TSC emerges is crucial in this domain. The objective of this study
is to fill this gap by providing a rigorous experimental evaluation of the SoTA
ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We
first explored and compiled a comprehensive list of more than 92 SoTA
algorithms from both TSC and manufacturing literature. Following, we selected
the 36 most representative algorithms from this list. To evaluate their
performance across various manufacturing classification tasks, we curated a set
of 22 manufacturing datasets, representative of different characteristics that
cover diverse manufacturing problems. Subsequently, we implemented and
evaluated the algorithms on the manufacturing benchmark datasets, and analyzed
the results for each dataset. Based on the results, ResNet, DrCIF,
InceptionTime, and ARSENAL are the top-performing algorithms, boasting an
average accuracy of over 96.6% across all 22 manufacturing TSC datasets. These
findings underscore the robustness, efficiency, scalability, and effectiveness
of convolutional kernels in capturing temporal features in time-series data, as
three out of the top four performing algorithms leverage these kernels for
feature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve
recognition for their effectiveness in capturing features within time-series
data using RNN-based structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Robotics and Computer-Integrated Manufacturing journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-arity PAC learning via exchangeability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo N. Coregliano, Maryanthe Malliaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a theory of high-arity PAC learning, which is statistical learning
in the presence of "structured correlation". In this theory, hypotheses are
either graphs, hypergraphs or, more generally, structures in finite relational
languages, and i.i.d. sampling is replaced by sampling an induced substructure,
producing an exchangeable distribution. Our main theorems establish a
high-arity (agnostic) version of the fundamental theorem of statistical
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>150 pages, 1 figure. (This version makes expository changes to
  Sections 1 and 2 and adds Appendix B on Bayes predictors.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tell me why: Training preferences-based RL with human preferences and
  step-level explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Karalus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-in-the-loop reinforcement learning allows the training of agents
through various interfaces, even for non-expert humans. Recently,
preference-based methods (PbRL), where the human has to give his preference
over two trajectories, increased in popularity since they allow training in
domains where more direct feedback is hard to formulate. However, the current
PBRL methods have limitations and do not provide humans with an expressive
interface for giving feedback. With this work, we propose a new
preference-based learning method that provides humans with a more expressive
interface to provide their preference over trajectories and a factual
explanation (or annotation of why they have this preference). These
explanations allow the human to explain what parts of the trajectory are most
relevant for the preference. We allow the expression of the explanations over
individual trajectory steps. We evaluate our method in various simulations
using a simulated human oracle (with realistic restrictions), and our results
show that our extended feedback can improve the speed of learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Reinforcement Learning Beyond Rewards @ Reinforcement
  Learning Conference (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vertical Federated Learning: Challenges, Methodologies and Experiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Wei, Jun Li, Chuan Ma, Ming Ding, Sha Wei, Fan Wu, Guihai Chen, Thilina Ranbaduge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, federated learning (FL) has emerged as a promising distributed
machine learning (ML) technology, owing to the advancing computational and
sensing capacities of end-user devices, however with the increasing concerns on
users' privacy. As a special architecture in FL, vertical FL (VFL) is capable
of constructing a hyper ML model by embracing sub-models from different
clients. These sub-models are trained locally by vertically partitioned data
with distinct attributes. Therefore, the design of VFL is fundamentally
different from that of conventional FL, raising new and unique research issues.
In this paper, we aim to discuss key challenges in VFL with effective
solutions, and conduct experiments on real-life datasets to shed light on these
issues. Specifically, we first propose a general framework on VFL, and
highlight the key differences between VFL and conventional FL. Then, we discuss
research challenges rooted in VFL systems under four aspects, i.e., security
and privacy risks, expensive computation and communication costs, possible
structural damage caused by model splitting, and system heterogeneity.
Afterwards, we develop solutions to addressing the aforementioned challenges,
and conduct extensive experiments to showcase the effectiveness of our proposed
solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning Friendly Vision-Language Model for Minecraft <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobin Jiang, Junpeng Yue, Hao Luo, Ziluo Ding, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the essential missions in the AI research community is to build an
autonomous embodied agent that can achieve high-level performance across a wide
spectrum of tasks. However, acquiring or manually designing rewards for all
open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal
contrastive learning framework architecture, CLIP4MC, aiming to learn a
reinforcement learning (RL) friendly vision-language model (VLM) that serves as
an intrinsic reward function for open-ended tasks. Simply utilizing the
similarity between the video snippet and the language prompt is not RL-friendly
since standard VLMs may only capture the similarity at a coarse level. To
achieve RL-friendliness, we incorporate the task completion degree into the VLM
training objective, as this information can assist agents in distinguishing the
importance between different states. Moreover, we provide neat YouTube datasets
based on the large-scale YouTube database provided by MineDojo. Specifically,
two rounds of filtering operations guarantee that the dataset covers enough
essential information and that the video-text pair is highly correlated.
Empirically, we demonstrate that the proposed method achieves better
performance on RL tasks compared with baselines. The code and datasets are
available at https://github.com/PKU-RL/CLIP4MC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Powered Energy Algorithmic Trading: Integrating Hidden Markov Models
  with Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago Monteiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In quantitative finance, machine learning methods are essential for alpha
generation. This study introduces a new approach that combines Hidden Markov
Models (HMM) and neural networks, integrated with Black-Litterman portfolio
optimization. During the COVID period (2019-2022), this dual-model approach
achieved a 97% return with a Sharpe ratio of 0.992. It incorporates two risk
models to enhance risk management, showing efficiency during volatile periods.
The methodology was implemented on the QuantConnect platform, which was chosen
for its robust framework and experimental reproducibility. The system, which
predicts future price movements, includes a three-year warm-up to ensure proper
algorithm function. It targets highly liquid, large-cap energy stocks to ensure
stable and predictable performance while also considering broker payments. The
dual-model alpha system utilizes log returns to select the optimal state based
on the historical performance. It combines state predictions with neural
network outputs, which are based on historical data, to generate trading
signals. This study examined the architecture of the trading system, data
pre-processing, training, and performance. The full code and backtesting data
are available under the MIT license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero shot VLMs for hate meme detection: Are we there yet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12198v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12198v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia content on social media is rapidly evolving, with memes gaining
prominence as a distinctive form. Unfortunately, some malicious users exploit
memes to target individuals or vulnerable communities, making it imperative to
identify and address such instances of hateful memes. Extensive research has
been conducted to address this issue by developing hate meme detection models.
However, a notable limitation of traditional machine/deep learning models is
the requirement for labeled datasets for accurate classification. Recently, the
research community has witnessed the emergence of several visual language
models that have exhibited outstanding performance across various tasks. In
this study, we aim to investigate the efficacy of these visual language models
in handling intricate tasks such as hate meme detection. We use various prompt
settings to focus on zero-shot classification of hateful/harmful memes. Through
our analysis, we observe that large VLMs are still vulnerable for zero-shot
hate meme detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dissecting Deep RL with High Update Ratios: Combatting Value Divergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05996v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05996v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Hussing, Claas Voelcker, Igor Gilitschenski, Amir-massoud Farahmand, Eric Eaton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that deep reinforcement learning algorithms can retain their ability
to learn without resetting network parameters in settings where the number of
gradient updates greatly exceeds the number of environment samples by
combatting value function divergence. Under large update-to-data ratios, a
recent study by Nikishin et al. (2022) suggested the emergence of a primacy
bias, in which agents overfit early interactions and downplay later experience,
impairing their ability to learn. In this work, we investigate the phenomena
leading to the primacy bias. We inspect the early stages of training that were
conjectured to cause the failure to learn and find that one fundamental
challenge is a long-standing acquaintance: value function divergence.
Overinflated Q-values are found not only on out-of-distribution but also
in-distribution data and can be linked to overestimation on unseen action
prediction propelled by optimizer momentum. We employ a simple unit-ball
normalization that enables learning under large update ratios, show its
efficacy on the widely used dm_control suite, and obtain strong performance on
the challenging dog tasks, competitive with model-based approaches. Our results
question, in parts, the prior explanation for sub-optimal learning due to
overfitting early data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at the First Reinforcement Learning
  Conference (RLC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural networks for bifurcation and linear stability analysis of steady
  states in partial differential equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19707v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19707v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Luthfi Shahab, Hadi Susanto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces an extended application of neural networks for
solving nonlinear partial differential equations (PDEs). A neural network,
combined with a pseudo-arclength continuation, is proposed to construct
bifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neural
network approach is also presented for solving eigenvalue problems to analyze
solution linear stability, focusing on identifying the largest eigenvalue. The
effectiveness of the proposed neural network is examined through experiments on
the Bratu equation and the Burgers equation. Results from a finite difference
method are also presented as comparison. Varying numbers of grid points are
employed in each case to assess the behavior and accuracy of both the neural
network and the finite difference method. The experimental results demonstrate
that the proposed neural network produces better solutions, generates more
accurate bifurcation diagrams, has reasonable computational times, and proves
effective for linear stability analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Applied Mathematics and Computation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cluster Exploration using Informative Manifold Projections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stavros Gerolymatos, Xenophon Evangelopoulos, Vladimir Gusev, John Y. Goulermas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction (DR) is one of the key tools for the visual
exploration of high-dimensional data and uncovering its cluster structure in
two- or three-dimensional spaces. The vast majority of DR methods in the
literature do not take into account any prior knowledge a practitioner may have
regarding the dataset under consideration. We propose a novel method to
generate informative embeddings which not only factor out the structure
associated with different kinds of prior knowledge but also aim to reveal any
remaining underlying structure. To achieve this, we employ a linear combination
of two objectives: firstly, contrastive PCA that discounts the structure
associated with the prior information, and secondly, kurtosis projection
pursuit which ensures meaningful data separation in the obtained embeddings. We
formulate this task as a manifold optimization problem and validate it
empirically across a variety of datasets considering three distinct types of
prior knowledge. Lastly, we provide an automated framework to perform iterative
visual exploration of high-dimensional data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning Large Language Models for Stock Return Prediction Using
  Newsflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Guo, Emmanuel Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) and their fine-tuning techniques have
demonstrated superior performance in various language understanding and
generation tasks. This paper explores fine-tuning LLMs for stock return
forecasting with financial newsflow. In quantitative investing, return
forecasting is fundamental for subsequent tasks like stock picking, portfolio
optimization, etc. We formulate the model to include text representation and
forecasting modules. We propose to compare the encoder-only and decoder-only
LLMs, considering they generate text representations in distinct ways. The
impact of these different representations on forecasting performance remains an
open question. Meanwhile, we compare two simple methods of integrating LLMs'
token-level representations into the forecasting module. The experiments on
real news and investment universes reveal that: (1) aggregated representations
from LLMs' token-level embeddings generally produce return predictions that
enhance the performance of long-only and long-short portfolios; (2) in the
relatively large investment universe, the decoder LLMs-based prediction model
leads to stronger portfolios, whereas in the small universes, there are no
consistent winners. Among the three LLMs studied (DeBERTa, Mistral, Llama),
Mistral performs more robustly across different universes; (3) return
predictions derived from LLMs' text representations are a strong signal for
portfolio construction, outperforming conventional sentiment scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Synergy between Data and Multi-Modal Large Language Models: A <span class="highlight-title">Survey</span>
  from Co-Development Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Qin, Daoyuan Chen, Wenhao Zhang, Liuyi Yao, Yilun Huang, Bolin Ding, Yaliang Li, Shuiguang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has been witnessed in
recent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the
modality from text to a broader spectrum of domains, attracting widespread
attention due to the broader range of application scenarios. As LLMs and MLLMs
rely on vast amounts of model parameters and data to achieve emergent
capabilities, the importance of data is receiving increasingly widespread
attention and recognition. Tracing and analyzing recent data-oriented works for
MLLMs, we find that the development of models and data is not two separate
paths but rather interconnected. On the one hand, vaster and higher-quality
data contribute to better performance of MLLMs; on the other hand, MLLMs can
facilitate the development of data. The co-development of multi-modal data and
MLLMs requires a clear view of 1) at which development stages of MLLMs specific
data-centric approaches can be employed to enhance certain MLLM capabilities,
and 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal
data in specific roles. To promote the data-model co-development for MLLM
community, we systematically review existing works related to MLLMs from the
data-model co-development perspective. A regularly maintained project
associated with this survey is accessible at
https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work. 21 pages. Related materials are continually maintained
  and available at
  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reduced storage direct tensor ring decomposition for convolutional
  neural networks compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Gabor, Rafał Zdunek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are among the most widely used machine
learning models for computer vision tasks, such as image classification. To
improve the efficiency of CNNs, many CNNs compressing approaches have been
developed. Low-rank methods approximate the original convolutional kernel with
a sequence of smaller convolutional kernels, which leads to reduced storage and
time complexities. In this study, we propose a novel low-rank CNNs compression
method that is based on reduced storage direct tensor ring decomposition
(RSDTR). The proposed method offers a higher circular mode permutation
flexibility, and it is characterized by large parameter and FLOPS compression
rates, while preserving a good classification accuracy of the compressed
network. The experiments, performed on the CIFAR-10 and ImageNet datasets,
clearly demonstrate the efficiency of RSDTR in comparison to other
state-of-the-art CNNs compression approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Partition-Based Cross-Validation With Centering and Scaling for
  $\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ole-Christian Galbo Engstrøm, Martin Holm Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present algorithms that substantially accelerate partition-based
cross-validation for machine learning models that require matrix products
$\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$. Our
algorithms have applications in model selection for, e.g., principal component
analysis (PCA), principal component regression (PCR), ridge regression (RR),
ordinary least squares (OLS), and partial least squares (PLS). Our algorithms
support all combinations of column-wise centering and scaling of $\mathbf{X}$
and $\mathbf{Y}$, and we demonstrate in our accompanying implementation that
this adds only a manageable, practical constant over efficient variants without
preprocessing. We prove the correctness of our algorithms under a fold-based
partitioning scheme and show that the running time is independent of the number
of folds; that is, they have the same time complexity as that of computing
$\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$ and
space complexity equivalent to storing $\mathbf{X}$, $\mathbf{Y}$,
$\mathbf{X}^\mathbf{T}\mathbf{X}$, and $\mathbf{X}^\mathbf{T}\mathbf{Y}$.
Importantly, unlike alternatives found in the literature, we avoid data leakage
due to preprocessing. We achieve these results by eliminating redundant
computations in the overlap between training partitions. Concretely, we show
how to manipulate $\mathbf{X}^\mathbf{T}\mathbf{X}$ and
$\mathbf{X}^\mathbf{T}\mathbf{Y}$ using only samples from the validation
partition to obtain the preprocessed training partition-wise
$\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$. To our
knowledge, we are the first to derive correct and efficient cross-validation
algorithms for any of the $16$ combinations of column-wise centering and
scaling, for which we also prove only $12$ give distinct matrix products.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 tables, 1 figure, 7 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving PDEs on Spheres with Physics-Informed Convolutional Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09605v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09605v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanhang Lei, Zhen Lei, Lei Shi, Chenyu Zeng, Ding-Xuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) have been demonstrated to be
efficient in solving partial differential equations (PDEs) from a variety of
experimental perspectives. Some recent studies have also proposed PINN
algorithms for PDEs on surfaces, including spheres. However, theoretical
understanding of the numerical performance of PINNs, especially PINNs on
surfaces or manifolds, is still lacking. In this paper, we establish rigorous
analysis of the physics-informed convolutional neural network (PICNN) for
solving PDEs on the sphere. By using and improving the latest approximation
results of deep convolutional neural networks and spherical harmonic analysis,
we prove an upper bound for the approximation error with respect to the Sobolev
norm. Subsequently, we integrate this with innovative localization complexity
analysis to establish fast convergence rates for PICNN. Our theoretical results
are also confirmed and supplemented by our experiments. In light of these
findings, we explore potential strategies for circumventing the curse of
dimensionality that arises when solving high-dimensional PDEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the resilience of the quadratic Littlewood-Offord problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elad Aigner-Horev, Daniel Rosenberg, Roi Weiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the statistical resilience of the anti-concentration properties of
Rademacher polynomials in face of adversarial deterministic noise taking the
form of sign-flips. Given a multilinear polynomial $f:\mathbb{R}^n \to
\mathbb{R}$ and a Rademacher vector $\boldsymbol{\xi} \in \{\pm 1\}^n$ (with
independent entries), our results provide probabilistic lower bound estimations
on the number of sign-flips that $\boldsymbol{\xi}$ can sustain without
``inflating" the atom probability $\sup_{x \in \mathbb{R} }
\mathbb{P}\{f(\boldsymbol{\xi}) = x\}$ otherwise resulting in an adversarially
biased distribution. Special emphasis is put on bilinear and quadratic forms,
for which strengthened estimates are attained. From a computational
perspective, our results in this venue are instance-bound in such a way that
allows for an efficient computation of the statistical resilience guarantees
from the quadratic polynomial itself directly. All of our probabilistic lower
bound resilience guarantees are asymptotically tight.
  On route, we provide a short proof for a new small-ball probability estimate
fitting Rademacher multilinear polynomials $f: \mathbb{R}^n \to \mathbb{R}$
removeing a polylog-factor from the classical Meka-Nguyen-Vu bound provided the
coefficients are independent of $n$ (dimension-free, hereafter). This removal
was conjectured to be possible by Meka-Nguyen-Vu regardless of our assumption.
Bilinear Rademacher forms with dimension-free coefficients arise naturally in
Combinatorics and specifically in the dense case of the edge-statistics
conjecture posed by Alon, Hefetz, Krivelevich, and Tyomkyn. This case of the
conjecture was resolved by Kwan and Sauermann. Replacing the appeal to the
Meka-Nguyen-Vu classical bound in the work of Kwan, Sudakov, and Tran with our
shortly proved result attains an additional proof of the dense case of the
edge-statistics conjecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Numerous changes from the last version: 1. An oversight in the proof
  fixed. 2. Added treatment of high degree polynomials 3. New results added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Credit Risk Meets Large Language Models: Building a Risk Indicator from
  Loan Descriptions in P2P Lending 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Sanz-Guerrero, Javier Arroyo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,
linking borrowers with lenders through online platforms. However, P2P lending
faces the challenge of information asymmetry, as lenders often lack sufficient
data to assess the creditworthiness of borrowers. This paper proposes a novel
approach to address this issue by leveraging the textual descriptions provided
by borrowers during the loan application process. Our methodology involves
processing these textual descriptions using a Large Language Model (LLM), a
powerful tool capable of discerning patterns and semantics within the text.
Transfer learning is applied to adapt the LLM to the specific task at hand.
  Our results derived from the analysis of the Lending Club dataset show that
the risk score generated by BERT, a widely used LLM, significantly improves the
performance of credit risk classifiers. However, the inherent opacity of
LLM-based systems, coupled with uncertainties about potential biases,
underscores critical considerations for regulatory frameworks and engenders
trust-related concerns among end-users, opening new avenues for future research
in the dynamic landscape of P2P lending and artificial intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyCL: An Efficient Hardware Architecture for Continual Learning on
  Autonomous Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugenio Ressa, Alberto Marchisio, Maurizio Martina, Guido Masera, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Continuous Learning (CL) paradigm consists of continuously evolving the
parameters of the Deep Neural Network (DNN) model to progressively learn to
perform new tasks without reducing the performance on previous tasks, i.e.,
avoiding the so-called catastrophic forgetting. However, the DNN parameter
update in CL-based autonomous systems is extremely resource-hungry. The
existing DNN accelerators cannot be directly employed in CL because they only
support the execution of the forward propagation. Only a few prior
architectures execute the backpropagation and weight update, but they lack the
control and management for CL. Towards this, we design a hardware architecture,
TinyCL, to perform CL on resource-constrained autonomous systems. It consists
of a processing unit that executes both forward and backward propagation, and a
control unit that manages memory-based CL workload. To minimize the memory
accesses, the sliding window of the convolutional layer moves in a snake-like
fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at
runtime to execute different operations. As per our knowledge, our proposed
TinyCL represents the first hardware accelerator that executes CL on autonomous
systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS
technology node with the conventional ASIC design flow. It executes 1 epoch of
training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while
1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,
thus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02963v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02963v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Liu, Jiawei Chen, Defang Chen, Zhehui Zhou, Yan Feng, Can Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graph Embedding (KGE), which projects entities and relations into
continuous vector spaces, has garnered significant attention. Although
high-dimensional KGE methods offer better performance, they come at the expense
of significant computation and memory overheads. Decreasing embedding
dimensions significantly deteriorates model performance. While several recent
efforts utilize knowledge distillation or non-Euclidean representation learning
to augment the effectiveness of low-dimensional KGE, they either necessitate a
pre-trained high-dimensional teacher model or involve complex non-Euclidean
operations, thereby incurring considerable additional computational costs. To
address this, this work proposes Confidence-aware Self-Knowledge Distillation
(CSD) that learns from the model itself to enhance KGE in a low-dimensional
space. Specifically, CSD extracts knowledge from embeddings in previous
iterations, which would be utilized to supervise the learning of the model in
the next iterations. Moreover, a specific semantic module is developed to
filter reliable knowledge by estimating the confidence of previously learned
embeddings. This straightforward strategy bypasses the need for time-consuming
pre-training of teacher models and can be integrated into various KGE methods
to improve their performance. Our comprehensive experiments on six KGE
backbones and four datasets underscore the effectiveness of the proposed CSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeqLink: A Robust Neural-ODE Architecture for Modelling Partially
  Observed Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03560v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03560v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Futoon M. Abushaqra, Hao Xue, Yongli Ren, Flora D. Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ordinary Differential Equations (ODE) based models have become popular as
foundation models for solving many time series problems. Combining neural ODEs
with traditional RNN models has provided the best representation for irregular
time series. However, ODE-based models typically require the trajectory of
hidden states to be defined based on either the initial observed value or the
most recent observation, raising questions about their effectiveness when
dealing with longer sequences and extended time intervals. In this article, we
explore the behaviour of the ODE models in the context of time series data with
varying degrees of sparsity. We introduce SeqLink, an innovative neural
architecture designed to enhance the robustness of sequence representation.
Unlike traditional approaches that solely rely on the hidden state generated
from the last observed value, SeqLink leverages ODE latent representations
derived from multiple data samples, enabling it to generate robust data
representations regardless of sequence length or data sparsity level. The core
concept behind our model is the definition of hidden states for the unobserved
values based on the relationships between samples (links between sequences).
Through extensive experiments on partially observed synthetic and real-world
datasets, we demonstrate that SeqLink improves the modelling of intermittent
time series, consistently outperforming state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Structure Estimation from Bandit Feedback using Nonvanishing
  Exponential Sums 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motoya Ohnishi, Isao Ishikawa, Yuko Kuroki, Masahiro Ikeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work tackles the dynamic structure estimation problems for periodically
behaved discrete dynamical system in the Euclidean space. We assume the
observations become sequentially available in a form of bandit feedback
contaminated by a sub-Gaussian noise. Under such fairly general assumptions on
the noise distribution, we carefully identify a set of recoverable information
of periodic structures. Our main results are the (computation and sample)
efficient algorithms that exploit asymptotic behaviors of exponential sums to
effectively average out the noise effect while preventing the information to be
estimated from vanishing. In particular, the novel use of the Weyl sum, a
variant of exponential sums, allows us to extract spectrum information for
linear systems. We provide sample complexity bounds for our algorithms, and we
experimentally validate our theoretical claims on simulations of toy examples,
including Cellular Automata.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Dense Embeddings with Sparse Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles O'Neill, Christine Ye, Kartheik Iyer, John F. Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) have shown promise in extracting interpretable
features from complex neural networks. We present one of the first applications
of SAEs to dense text embeddings from large language models, demonstrating
their effectiveness in disentangling semantic concepts. By training SAEs on
embeddings of over 420,000 scientific paper abstracts from computer science and
astronomy, we show that the resulting sparse representations maintain semantic
fidelity while offering interpretability. We analyse these learned features,
exploring their behaviour across different model capacities and introducing a
novel method for identifying ``feature families'' that represent related
concepts at varying levels of abstraction. To demonstrate the practical utility
of our approach, we show how these interpretable features can be used to
precisely steer semantic search, allowing for fine-grained control over query
semantics. This work bridges the gap between the semantic richness of dense
embeddings and the interpretability of sparse representations. We open source
our embeddings, trained sparse autoencoders, and interpreted features, as well
as a web app for exploring them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FroSSL: Frobenius Norm Minimization for Efficient Multiview
  <span class="highlight-title">Self-Supervised</span> Learning <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02903v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02903v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Skean, Aayush Dhakal, Nathan Jacobs, Luis Gonzalo Sanchez Giraldo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) is a popular paradigm for representation
learning. Recent multiview methods can be classified as sample-contrastive,
dimension-contrastive, or asymmetric network-based, with each family having its
own approach to avoiding informational collapse. While these families converge
to solutions of similar quality, it can be empirically shown that some methods
are epoch-inefficient and require longer training to reach a target
performance. Two main approaches to improving efficiency are covariance
eigenvalue regularization and using more views. However, these two approaches
are difficult to combine due to the computational complexity of computing
eigenvalues. We present the objective function FroSSL which reconciles both
approaches while avoiding eigendecomposition entirely. FroSSL works by
minimizing covariance Frobenius norms to avoid collapse and minimizing
mean-squared error to achieve augmentation invariance. We show that FroSSL
reaches competitive accuracies more quickly than any other SSL method and
provide theoretical and empirical support that this faster convergence is due
to how FroSSL affects the eigenvalues of the embedding covariance matrices. We
also show that FroSSL learns competitive representations on linear probe
evaluation when used to train a ResNet-18 on several datasets, including
STL-10, Tiny ImageNet, and ImageNet-100.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Whole Slide Pathology Foundation Models through Stain
  Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juseung Yun, Yi Hu, Jinhyung Kim, Jongseong Jang, Soonyoung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in digital pathology have led to the development of
numerous foundational models that utilize self-supervised learning on patches
extracted from gigapixel whole slide images (WSIs). While this approach
leverages vast amounts of unlabeled data, we have discovered a significant
issue: features extracted from these self-supervised models tend to cluster by
individual WSIs, a phenomenon we term WSI-specific feature collapse. This
problem can potentially limit the model's generalization ability and
performance on various downstream tasks. To address this issue, we introduce
Stain Normalized Pathology Foundational Model, a novel foundational model
trained on patches that have undergone stain normalization. Stain normalization
helps reduce color variability arising from different laboratories and
scanners, enabling the model to learn more consistent features. Stain
Normalized Pathology Foundational Model is trained using 285,153,903 patches
extracted from a total of 34,795 WSIs, combining data from The Cancer Genome
Atlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments
demonstrate that Stain Normalized Pathology Foundational Model significantly
mitigates the feature collapse problem, indicating that the model has learned
more generalized features rather than overfitting to individual WSI
characteristics. We compared Stain Normalized Pathology Foundational Model with
state-of-the-art models across six downstream task datasets, and our results
show that Stain Normalized Pathology Foundational Model achieves excellent
performance relative to the number of WSIs used and the model's parameter
count. This suggests that the application of stain normalization has
substantially improved the model's efficiency and generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Flow Models: Flowing in Your Style 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02977v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02977v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kien Do, Duc Kieu, Toan Nguyen, Dang Nguyen, Hung Le, Dung Nguyen, Thin Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a systematic training-free method to transform the probability
flow of a "linear" stochastic process characterized by the equation
X_{t}=a_{t}X_{0}+\sigma_{t}X_{1} into a straight constant-speed (SC) flow,
reminiscent of Rectified Flow. This transformation facilitates fast sampling
along the original probability flow via the Euler method without training a new
model of the SC flow. The flexibility of our approach allows us to extend our
transformation to inter-convert two posterior flows of two distinct linear
stochastic processes. Moreover, we can easily integrate high-order numerical
solvers into the transformed SC flow, further enhancing the sampling accuracy
and efficiency. Rigorous theoretical analysis and extensive experimental
results substantiate the advantages of our framework. Our code is available at
this [https://github.com/clarken92/VFM||link].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at: https://github.com/clarken92/VFM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithm Design for Online Meta-Learning with Task Boundary Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daouda Sow, Sen Lin, Yingbin Liang, Junshan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online meta-learning has recently emerged as a marriage between batch
meta-learning and online learning, for achieving the capability of quick
adaptation on new tasks in a lifelong manner. However, most existing approaches
focus on the restrictive setting where the distribution of the online tasks
remains fixed with known task boundaries. In this work, we relax these
assumptions and propose a novel algorithm for task-agnostic online
meta-learning in non-stationary environments. More specifically, we first
propose two simple but effective detection mechanisms of task switches and
distribution shift based on empirical observations, which serve as a key
building block for more elegant online model updates in our algorithm: the task
switch detection mechanism allows reusing of the best model available for the
current task at hand, and the distribution shift detection mechanism
differentiates the meta model update in order to preserve the knowledge for
in-distribution tasks and quickly learn the new knowledge for
out-of-distribution tasks. In particular, our online meta model updates are
based only on the current data, which eliminates the need of storing previous
data as required in most existing methods. We further show that a sublinear
task-averaged regret can be achieved for our algorithm under mild conditions.
Empirical studies on three different benchmarks clearly demonstrate the
significant advantage of our algorithm over related baseline approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CPAL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Black-box Robustness with In-Context Rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08225v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08225v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle O'Brien, Nathan Ng, Isha Puri, Jorge Mendez, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi, Thomas Hartvigsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models for text classification often excel on
in-distribution (ID) data but struggle with unseen out-of-distribution (OOD)
inputs. Most techniques for improving OOD robustness are not applicable to
settings where the model is effectively a black box, such as when the weights
are frozen, retraining is costly, or the model is leveraged via an API.
Test-time augmentation (TTA) is a simple post-hoc technique for improving
robustness that sidesteps black-box constraints by aggregating predictions
across multiple augmentations of the test input. TTA has seen limited use in
NLP due to the challenge of generating effective natural language
augmentations. In this work, we propose LLM-TTA, which uses LLM-generated
augmentations as TTA's augmentation function. LLM-TTA outperforms conventional
augmentation functions across sentiment, toxicity, and news classification
tasks for BERT and T5 models, with BERT's OOD robustness improving by an
average of 4.48 percentage points without regressing average ID performance. We
explore selectively augmenting inputs based on prediction entropy to reduce the
rate of expensive LLM augmentations, allowing us to maintain performance gains
while reducing the average number of generated augmentations by 57.74\%.
LLM-TTA is agnostic to the task model architecture, does not require OOD
labels, and is effective across low and high-resource settings. We share our
data, models, and code for reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Forecasting with Coherent Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kin G. Olivares, Geoffrey Négiar, Ruijun Ma, O. Nangba Meetei, Mengfei Cao, Michael W. Mahoney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining accurate probabilistic forecasts is an important operational
challenge in many applications, perhaps most obviously in energy management,
climate forecasting, supply chain planning, and resource allocation. In many of
these applications, there is a natural hierarchical structure over the
forecasted quantities; and forecasting systems that adhere to this hierarchical
structure are said to be coherent. Furthermore, operational planning benefits
from accuracy at all levels of the aggregation hierarchy. Building accurate and
coherent forecasting systems, however, is challenging: classic multivariate
time series tools and neural network methods are still being adapted for this
purpose. In this paper, we augment an MQForecaster neural network architecture
with a novel deep Gaussian factor forecasting model that achieves coherence by
construction, yielding a method we call the Deep Coherent Factor Model Neural
Network (DeepCoFactor) model. DeepCoFactor generates samples that can be
differentiated with respect to model parameters, allowing optimization on
various sample-based learning objectives that align with the forecasting
system's goals, including quantile loss and the scaled Continuous Ranked
Probability Score (CRPS). In a comparison to state-of-the-art coherent
forecasting methods, DeepCoFactor achieves significant improvements in scaled
CRPS forecast accuracy, with gains between 4.16 and 54.40%, as measured on
three publicly available hierarchical forecasting datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages of main text. Updated method and results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hessian-Aware Stochastic Differential Equation for Modelling SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Zebang Shen, Liang Zhang, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous-time approximation of Stochastic Gradient Descent (SGD) is a
crucial tool to study its escaping behaviors from stationary points. However,
existing stochastic differential equation (SDE) models fail to fully capture
these behaviors, even for simple quadratic objectives. Built on a novel
stochastic backward error analysis framework, we derive the Hessian-Aware
Stochastic Modified Equation (HA-SME), an SDE that incorporates Hessian
information of the objective function into both its drift and diffusion terms.
Our analysis shows that HA-SME matches the order-best approximation error
guarantee among existing SDE models in the literature, while achieving a
significantly reduced dependence on the smoothness parameter of the objective.
Further, for quadratic objectives, under mild conditions, HA-SME is proved to
be the first SDE model that recovers exactly the SGD dynamics in the
distributional sense. Consequently, when the local landscape near a stationary
point can be approximated by quadratics, HA-SME is expected to accurately
predict the local escaping behaviors of SGD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PBSCR: The Piano Bootleg Score Composer Recognition <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16803v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16803v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arhan Jain, Alec Bunn, Austin Pham, TJ Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article motivates, describes, and presents the PBSCR dataset for
studying composer recognition of classical piano music. Our goal was to design
a dataset that facilitates large-scale research on composer recognition that is
suitable for modern architectures and training practices. To achieve this goal,
we utilize the abundance of sheet music images and rich metadata on IMSLP, use
a previously proposed feature representation called a bootleg score to encode
the location of noteheads relative to staff lines, and present the data in an
extremely simple format (2D binary images) to encourage rapid exploration and
iteration. The dataset itself contains 40,000 62x64 bootleg score images for a
9-class recognition task, 100,000 62x64 bootleg score images for a 100-class
recognition task, and 29,310 unlabeled variable-length bootleg score images for
pretraining. The labeled data is presented in a form that mirrors MNIST images,
in order to make it extremely easy to visualize, manipulate, and train models
in an efficient manner. We include relevant information to connect each bootleg
score image with its underlying raw sheet music image, and we scrape, organize,
and compile metadata from IMSLP on all piano works to facilitate multimodal
research and allow for convenient linking to other datasets. We release
baseline results in a supervised and low-shot setting for future works to
compare against, and we discuss open research questions that the PBSCR data is
especially well suited to facilitate research on.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, to be published in Transactions of the
  International Society for Music Information Retrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Differential Privacy in Graph Neural Networks: a Reconstruction
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karuna Bhaila, Wen Huang, Yongkai Wu, Xintao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks have achieved tremendous success in modeling complex
graph data in a variety of applications. However, there are limited studies
investigating privacy protection in GNNs. In this work, we propose a learning
framework that can provide node privacy at the user level, while incurring low
utility loss. We focus on a decentralized notion of Differential Privacy,
namely Local Differential Privacy, and apply randomization mechanisms to
perturb both feature and label data at the node level before the data is
collected by a central server for model training. Specifically, we investigate
the application of randomization mechanisms in high-dimensional feature
settings and propose an LDP protocol with strict privacy guarantees. Based on
frequency estimation in statistical analysis of randomized data, we develop
reconstruction methods to approximate features and labels from perturbed data.
We also formulate this learning framework to utilize frequency estimates of
graph clusters to supervise the training procedure at a sub-graph level.
Extensive experiments on real-world and semi-synthetic datasets demonstrate the
validity of our proposed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 SIAM International Conference on Data Mining</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft <span class="highlight-title">Prompt</span>ing for Unlearning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karuna Bhaila, Minh-Hao Van, Xintao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread popularity of Large Language Models (LLMs), partly due to
their unique ability to perform in-context learning, has also brought to light
the importance of ethical and safety considerations when deploying these
pre-trained models. In this work, we focus on investigating machine unlearning
for LLMs motivated by data protection regulations. In contrast to the growing
literature on fine-tuning methods to achieve unlearning, we focus on a
comparatively lightweight alternative called soft prompting to realize the
unlearning of a subset of training data. With losses designed to enforce
forgetting as well as utility preservation, our framework \textbf{S}oft
\textbf{P}rompting for \textbf{U}n\textbf{l}earning (SPUL) learns prompt tokens
that can be appended to an arbitrary query to induce unlearning of specific
examples at inference time without updating LLM parameters. We conduct a
rigorous evaluation of the proposed method and our results indicate that SPUL
can significantly improve the trade-off between utility and forgetting in the
context of text classification and question answering with LLMs. We further
validate our method using multiple LLMs to highlight the scalability of our
framework and provide detailed insights into the choice of hyperparameters and
the influence of the size of unlearning data. Our implementation is available
at \url{https://github.com/karuna-bhaila/llm_unlearning}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularized Multi-Decoder Ensemble for an Error-Aware Scene
  Representation Network <span class="chip">IEEE VIS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Xiong, Skylar W. Wurster, Hanqi Guo, Tom Peterka, Han-Wei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature grid Scene Representation Networks (SRNs) have been applied to
scientific data as compact functional surrogates for analysis and
visualization. As SRNs are black-box lossy data representations, assessing the
prediction quality is critical for scientific visualization applications to
ensure that scientists can trust the information being visualized. Currently,
existing architectures do not support inference time reconstruction quality
assessment, as coordinate-level errors cannot be evaluated in the absence of
ground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN)
ensemble architecture consisting of a shared feature grid with multiple
lightweight multi-layer perceptron decoders. MDSRN can generate a set of
plausible predictions for a given input coordinate to compute the mean as the
prediction of the multi-decoder ensemble and the variance as a confidence
score. The coordinate-level variance can be rendered along with the data to
inform the reconstruction quality, or be integrated into uncertainty-aware
volume visualization algorithms. To prevent the misalignment between the
quantified variance and the prediction quality, we propose a novel variance
regularization loss for ensemble learning that promotes the Regularized
multi-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates
closely to the true model error. We comprehensively evaluate the quality of
variance quantification and data reconstruction of Monte Carlo Dropout, Mean
Field Variational Inference, Deep Ensemble, and Predicting Variance compared to
the proposed MDSRN and RMDSRN across diverse scalar field datasets. We
demonstrate that RMDSRN attains the most accurate data reconstruction and
competitive variance-error correlation among uncertain SRNs under the same
neural network parameter budgets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proc. IEEE VIS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Semantic Segmentation with Query Points Supervision on Aerial
  Images <span class="chip">ICIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Rivier, Carlos Hinojosa, Silvio Giancola, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation is crucial in remote sensing, where high-resolution
satellite images are segmented into meaningful regions. Recent advancements in
deep learning have significantly improved satellite image segmentation.
However, most of these methods are typically trained in fully supervised
settings that require high-quality pixel-level annotations, which are expensive
and time-consuming to obtain. In this work, we present a weakly supervised
learning algorithm to train semantic segmentation algorithms that only rely on
query point annotations instead of full mask labels. Our proposed approach
performs accurate semantic segmentation and improves efficiency by
significantly reducing the cost and time required for manual annotation.
Specifically, we generate superpixels and extend the query point labels into
those superpixels that group similar meaningful semantics. Then, we train
semantic segmentation models supervised with images partially labeled with the
superpixel pseudo-labels. We benchmark our weakly supervised training approach
on an aerial image dataset and different semantic segmentation architectures,
showing that we can reach competitive performance compared to fully supervised
training while reducing the annotation effort. The code of our proposed
approach is publicly available at: https://github.com/santiago2205/LSSQPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper Accepted at ICIP 2024 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Unlearning: Fast and Efficient Gradient-free Approach to Class
  Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00761v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00761v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangamesh Kodge, Gobinda Saha, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning is a prominent and challenging field, driven by regulatory
demands for user data deletion and heightened privacy awareness. Existing
approaches involve retraining model or multiple finetuning steps for each
deletion request, often constrained by computational limits and restricted data
access. In this work, we introduce a novel class unlearning algorithm designed
to strategically eliminate specific classes from the learned model. Our
algorithm first estimates the Retain and the Forget Spaces using Singular Value
Decomposition on the layerwise activations for a small subset of samples from
the retain and unlearn classes, respectively. We then compute the shared
information between these spaces and remove it from the forget space to isolate
class-discriminatory feature space. Finally, we obtain the unlearned model by
updating the weights to suppress the class discriminatory features from the
activation spaces. We demonstrate our algorithm's efficacy on ImageNet using a
Vision Transformer with only $\sim 1.5\%$ drop in retain accuracy compared to
the original model while maintaining under $1\%$ accuracy on the unlearned
class samples. Furthermore, our algorithm exhibits competitive unlearning
performance and resilience against Membership Inference Attacks (MIA). Compared
to baselines, it achieves an average accuracy improvement of $1.38\%$ on the
ImageNet dataset while requiring up to $10 \times$ fewer samples for
unlearning. Additionally, under stronger MIA attacks on the CIFAR-100 dataset
using a ResNet18 architecture, our approach outperforms the best baseline by
$1.8\%$. Our code is available at
https://github.com/sangamesh-kodge/class_forgetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Intervention Efficacy via Concept Realignment in Concept
  Bottleneck Models <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishad Singhi, Jae Myung Kim, Karsten Roth, Zeynep Akata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept Bottleneck Models (CBMs) ground image classification on
human-understandable concepts to allow for interpretable model decisions.
Crucially, the CBM design inherently allows for human interventions, in which
expert users are given the ability to modify potentially misaligned concept
choices to influence the decision behavior of the model in an interpretable
fashion. However, existing approaches often require numerous human
interventions per image to achieve strong performances, posing practical
challenges in scenarios where obtaining human feedback is expensive. In this
paper, we find that this is noticeably driven by an independent treatment of
concepts during intervention, wherein a change of one concept does not
influence the use of other ones in the model's final decision. To address this
issue, we introduce a trainable concept intervention realignment module, which
leverages concept relations to realign concept assignments post-intervention.
Across standard, real-world benchmarks, we find that concept realignment can
significantly improve intervention efficacy; significantly reducing the number
of interventions needed to reach a target classification performance or concept
prediction accuracy. In addition, it easily integrates into existing
concept-based architectures without requiring changes to the models themselves.
This reduced cost of human-model collaboration is crucial to enhancing the
feasibility of CBMs in resource-constrained environments. Our code is available
at: https://github.com/ExplainableML/concept_realignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Growth of Mistakes in Differentially Private Online Learning: A
  Lower Bound Perspective <span class="chip">COLT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Dmitriev, Kristóf Szabó, Amartya Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we provide lower bounds for Differentially Private (DP) Online
Learning algorithms. Our result shows that, for a broad class of
$(\varepsilon,\delta)$-DP online algorithms, for number of rounds $T$ such that
$\log T\leq O(1 / \delta)$, the expected number of mistakes incurred by the
algorithm grows as $\Omega(\log \frac{T}{\delta})$. This matches the upper
bound obtained by Golowich and Livni (2021) and is in contrast to non-private
online learning where the number of mistakes is independent of $T$. To the best
of our knowledge, our work is the first result towards settling lower bounds
for DP-Online learning and partially addresses the open question in Sanyal and
Ramponi (2022).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Conference on Learning Theory (COLT) 2024, Edmonton,
  Canada</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Expressiveness through Preprocessed Graph Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11714v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11714v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danial Saber, Amirali Salehi-Abari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as the predominant method for
analyzing graph-structured data. However, canonical GNNs have limited
expressive power and generalization capability, thus triggering the development
of more expressive yet computationally intensive methods. One such approach is
to create a series of perturbed versions of input graphs and then repeatedly
conduct multiple message-passing operations on all variations during training.
Despite their expressive power, this approach does not scale well on larger
graphs. To address this scalability issue, we introduce Scalable Expressiveness
through Preprocessed Graph Perturbation (SE2P). This model offers a flexible,
configurable balance between scalability and generalizability with four
distinct configuration classes. At one extreme, the configuration prioritizes
scalability through minimal learnable feature extraction and extensive
preprocessing; at the other extreme, it enhances generalizability with more
learnable feature extractions, though this increases scalability costs. We
conduct extensive experiments on real-world datasets to evaluate the
generalizability and scalability of SE2P variants compared to various
state-of-the-art benchmarks. Our results indicate that, depending on the chosen
SE2P configuration, the model can enhance generalizability compared to
benchmarks while achieving significant speed improvements of up to 8-fold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-04T00:00:00Z">2024-08-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">64</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Affect Analysis: A Protocol for Ensuring Fairness and
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyu Hu, Dimitrios Kollias, Eleni Papadopoulou, Paraskevi Tzouveli, Jie Wei, Xinyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating affect analysis methods presents challenges due to inconsistencies
in database partitioning and evaluation protocols, leading to unfair and biased
results. Previous studies claim continuous performance improvements, but our
findings challenge such assertions. Using these insights, we propose a unified
protocol for database partitioning that ensures fairness and comparability. We
provide detailed demographic annotations (in terms of race, gender and age),
evaluation metrics, and a common framework for expression recognition, action
unit detection and valence-arousal estimation. We also rerun the methods with
the new protocol and introduce a new leaderboards to encourage future research
in affect recognition with a fairer comparison. Our annotations, code, and
pre-trained models are available on
\hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.06841</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PanoFree: Tuning-Free Holistic Multi-view Image Generation with
  Cross-view Self-Guidance <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoming Liu, Zhong Li, Zhang Chen, Nannan Li, Yi Xu, Bryan A. Plummer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Immersive scene generation, notably panorama creation, benefits significantly
from the adaptation of large pre-trained text-to-image (T2I) models for
multi-view image generation. Due to the high cost of acquiring multi-view
images, tuning-free generation is preferred. However, existing methods are
either limited to simple correspondences or require extensive fine-tuning to
capture complex ones. We present PanoFree, a novel method for tuning-free
multi-view image generation that supports an extensive array of
correspondences. PanoFree sequentially generates multi-view images using
iterative warping and inpainting, addressing the key issues of inconsistency
and artifacts from error accumulation without the need for fine-tuning. It
improves error accumulation by enhancing cross-view awareness and refines the
warping and inpainting processes via cross-view guidance, risky area estimation
and erasing, and symmetric bidirectional guided generation for loop closure,
alongside guidance-based semantic and density control for scene structure
preservation. In experiments on Planar, 360{\deg}, and Full Spherical
Panoramas, PanoFree demonstrates significant error reduction, improves global
consistency, and boosts image quality without extra fine-tuning. Compared to
existing methods, PanoFree is up to 5x more efficient in time and 3x more
efficient in GPU memory usage, and maintains superior diversity of results (2x
better in our user study). PanoFree offers a viable alternative to costly
fine-tuning or the use of additional pre-trained models. Project website at
https://panofree.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-based Pedestrian and Vehicle Traffic Analysis During Football
  Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacques P. Fleischer, Ryan Pallack, Ahan Mishra, Gustavo Riente de Andrade, Subhadipto Poddar, Emmanuel Posadas, Robert Schenck, Tania Banerjee, Anand Rangarajan, Sanjay Ranka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper utilizes video analytics to study pedestrian and vehicle traffic
behavior, focusing on analyzing traffic patterns during football gamedays. The
University of Florida (UF) hosts six to seven home football games on Saturdays
during the college football season, attracting significant pedestrian activity.
Through video analytics, this study provides valuable insights into the impact
of these events on traffic volumes and safety at intersections. Comparing
pedestrian and vehicle activities on gamedays versus non-gamedays reveals
differing patterns. For example, pedestrian volume substantially increases
during gamedays, which is positively correlated with the probability of the
away team winning. This correlation is likely because fans of the home team
enjoy watching difficult games. Win probabilities as an early predictor of
pedestrian volumes at intersections can be a tool to help traffic professionals
anticipate traffic management needs. Pedestrian-to-vehicle (P2V) conflicts
notably increase on gamedays, particularly a few hours before games start.
Addressing this, a "Barnes Dance" movement phase within the intersection is
recommended. Law enforcement presence during high-activity gamedays can help
ensure pedestrian compliance and enhance safety. In contrast, we identified
that vehicle-to-vehicle (V2V) conflicts generally do not increase on gamedays
and may even decrease due to heightened driver caution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VidModEx: Interpretable and Efficient Black Box Model Extraction for
  High-Dimensional Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somnath Sendhil Kumar, Yuvaraj Govindarajulu, Pavan Kulkarni, Manojkumar Parmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of black-box model extraction, conventional methods reliant on
soft labels or surrogate datasets struggle with scaling to high-dimensional
input spaces and managing the complexity of an extensive array of interrelated
classes. In this work, we present a novel approach that utilizes SHAP (SHapley
Additive exPlanations) to enhance synthetic data generation. SHAP quantifies
the individual contributions of each input feature towards the victim model's
output, facilitating the optimization of an energy-based GAN towards a
desirable output. This method significantly boosts performance, achieving a
16.45% increase in the accuracy of image classification models and extending to
video classification models with an average improvement of 26.11% and a maximum
of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics
600, and Something-Something V2. We further demonstrate the effectiveness and
practical utility of our method under various scenarios, including the
availability of top-k prediction probabilities, top-k prediction labels, and
top-1 labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RICA^2: Rubric-Informed, Calibrated Assessment of Actions <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abrar Majeedi, Viswanatha Reddy Gajjala, Satya Sai Srinath Namburi GNVV, Yin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to quantify how well an action is carried out, also known as
action quality assessment (AQA), has attracted recent interest in the vision
community. Unfortunately, prior methods often ignore the score rubric used by
human experts and fall short of quantifying the uncertainty of the model
prediction. To bridge the gap, we present RICA^2 - a deep probabilistic model
that integrates score rubric and accounts for prediction uncertainty for AQA.
Central to our method lies in stochastic embeddings of action steps, defined on
a graph structure that encodes the score rubric. The embeddings spread
probabilistic density in the latent space and allow our method to represent
model uncertainty. The graph encodes the scoring criteria, based on which the
quality scores can be decoded. We demonstrate that our method establishes new
state of the art on public benchmarks, including FineDiving, MTL-AQA, and
JIGSAWS, with superior performance in score prediction and uncertainty
calibration. Our code is available at https://abrarmajeedi.github.io/rica2_aqa/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at European Conference on Computer Vision (ECCV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A First Look at Chebyshev-Sobolev Series for Digital Ink 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Singh Kalhan, Stephen M. Watt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering digital ink as plane curves provides a valuable framework for
various applications, including signature verification, note-taking, and
mathematical handwriting recognition. These plane curves can be obtained as
parameterized pairs of approximating truncated series (x(s), y(s)) determined
by sampled points. Earlier work has found that representing these truncated
series (polynomials) in a Legendre or Legendre-Sobolev basis has a number of
desirable properties. These include compact data representation, meaningful
clustering of like symbols in the vector space of polynomial coefficients,
linear separability of classes in this space, and highly efficient calculation
of variation between curves. In this work, we take a first step at examining
the use of Chebyshev-Sobolev series for symbol recognition. The early
indication is that this representation may be superior to Legendre-Sobolev
representation for some purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MathUI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FovEx: Human-inspired Explanations for Vision <span class="highlight-title">Transformer</span>s and
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahadev Prasad Panda, Matteo Tiezzi, Martina Vilas, Gemma Roig, Bjoern M. Eskofier, Dario Zanca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability in artificial intelligence (XAI) remains a crucial aspect for
fostering trust and understanding in machine learning models. Current visual
explanation techniques, such as gradient-based or class-activation-based
methods, often exhibit a strong dependence on specific model architectures.
Conversely, perturbation-based methods, despite being model-agnostic, are
computationally expensive as they require evaluating models on a large number
of forward passes. In this work, we introduce Foveation-based Explanations
(FovEx), a novel XAI method inspired by human vision. FovEx seamlessly
integrates biologically inspired perturbations by iteratively creating foveated
renderings of the image and combines them with gradient-based visual
explorations to determine locations of interest efficiently. These locations
are selected to maximize the performance of the model to be explained with
respect to the downstream task and then combined to generate an attribution
map. We provide a thorough evaluation with qualitative and quantitative
assessments on established benchmarks. Our method achieves state-of-the-art
performance on both transformers (on 4 out of 5 metrics) and convolutional
models (on 3 out of 5 metrics), demonstrating its versatility among various
architectures. Furthermore, we show the alignment between the explanation map
produced by FovEx and human gaze patterns (+14\% in NSS compared to RISE,
+203\% in NSS compared to GradCAM). This comparison enhances our confidence in
FovEx's ability to close the interpretation gap between humans and machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction
  from Sparse Multi-view Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feichi Lu, Zijian Dong, Jie Song, Otmar Hilliges
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite progress in human motion capture, existing multi-view methods often
face challenges in estimating the 3D pose and shape of multiple closely
interacting people. This difficulty arises from reliance on accurate 2D joint
estimations, which are hard to obtain due to occlusions and body contact when
people are in close interaction. To address this, we propose a novel method
leveraging the personalized implicit neural avatar of each individual as a
prior, which significantly improves the robustness and precision of this
challenging pose estimation task. Concretely, the avatars are efficiently
reconstructed via layered volume rendering from sparse multi-view videos. The
reconstructed avatar prior allows for the direct optimization of 3D poses based
on color and silhouette rendering loss, bypassing the issues associated with
noisy 2D detections. To handle interpenetration, we propose a collision loss on
the overlapping shape regions of avatars to add penetration constraints.
Moreover, both 3D poses and avatars are optimized in an alternating manner. Our
experimental results demonstrate state-of-the-art performance on several public
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://feichilu.github.io/AvatarPose/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ View-consistent Object Removal in Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiren Lu, Jing Ma, Yu Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiance Fields (RFs) have emerged as a crucial technology for 3D scene
representation, enabling the synthesis of novel views with remarkable realism.
However, as RFs become more widely used, the need for effective editing
techniques that maintain coherence across different perspectives becomes
evident. Current methods primarily depend on per-frame 2D image inpainting,
which often fails to maintain consistency across views, thus compromising the
realism of edited RF scenes. In this work, we introduce a novel RF editing
pipeline that significantly enhances consistency by requiring the inpainting of
only a single reference image. This image is then projected across multiple
views using a depth-based approach, effectively reducing the inconsistencies
observed with per-frame inpainting. However, projections typically assume
photometric consistency across views, which is often impractical in real-world
settings. To accommodate realistic variations in lighting and viewpoint, our
pipeline adjusts the appearance of the projected views by generating multiple
directional variants of the inpainted image, thereby adapting to different
photometric conditions. Additionally, we present an effective and robust
multi-view object segmentation approach as a valuable byproduct of our
pipeline. Extensive experiments demonstrate that our method significantly
surpasses existing frameworks in maintaining content consistency across views
and enhancing visual quality. More results are available at
https://vulab-ai.github.io/View-consistent_Object_Removal_in_Radiance_Fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia (MM) 2024. Project website is accessible
  at
  https://vulab-ai.github.io/View-consistent_Object_Removal_in_Radiance_Fields</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Past Movements-Guided Motion Representation Learning for Human Motion
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Shi, Baoxuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion prediction based on 3D skeleton is a significant challenge in
computer vision, primarily focusing on the effective representation of motion.
In this paper, we propose a self-supervised learning framework designed to
enhance motion representation. This framework consists of two stages: first,
the network is pretrained through the self-reconstruction of past sequences,
and the guided reconstruction of future sequences based on past movements. We
design a velocity-based mask strategy to focus on the joints with large-scale
moving. Subsequently, the pretrained network undergoes finetuning for specific
tasks. Self-reconstruction, guided by patterns of past motion, substantially
improves the model's ability to represent the spatiotemporal relationships
among joints but also captures the latent relationships between past and future
sequences. This capability is crucial for motion prediction tasks that solely
depend on historical motion data. By employing this straightforward yet
effective training paradigm, our method outperforms existing
\textit{state-of-the-art} methods, reducing the average prediction errors by
8.8\% across Human3.6M, 3DPW, and AMASS datasets. The code is available at
https://github.com/JunyuShi02/PMG-MRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KAN-RCBEVDepth: A multi-modal fusion algorithm in object detection for
  autonomous driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Lai, Chuanhao Liu, Shihui Sheng, Zhiqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D object detection in autonomous driving is critical yet
challenging due to occlusions, varying object scales, and complex urban
environments. This paper introduces the RCBEV-KAN algorithm, a pioneering
method designed to enhance 3D object detection by fusing multimodal sensor data
from cameras, LiDAR, and millimeter-wave radar. Our innovative Bird's Eye View
(BEV)-based approach, utilizing a Transformer architecture, significantly
boosts detection precision and efficiency by seamlessly integrating diverse
data sources, improving spatial relationship handling, and optimizing
computational processes. Experimental results show that the RCBEV-KAN model
demonstrates superior performance across most detection categories, achieving
higher Mean Distance AP (0.389 vs. 0.316, a 23% improvement), better ND Score
(0.484 vs. 0.415, a 17% improvement), and faster Evaluation Time (71.28s, 8%
faster). These results indicate that RCBEV-KAN is more accurate, reliable, and
efficient, making it ideal for dynamic and challenging autonomous driving
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Neural Surface Reconstruction with Feature Priors from
  Multi-View Image <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlin Ren, Chenjie Cao, Yanwei Fu, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Neural Surface Reconstruction (NSR) have significantly
improved multi-view reconstruction when coupled with volume rendering. However,
relying solely on photometric consistency in image space falls short of
addressing complexities posed by real-world data, including occlusions and
non-Lambertian surfaces. To tackle these challenges, we propose an
investigation into feature-level consistent loss, aiming to harness valuable
feature priors from diverse pretext visual tasks and overcome current
limitations. It is crucial to note the existing gap in determining the most
effective pretext visual task for enhancing NSR. In this study, we
comprehensively explore multi-view feature priors from seven pretext visual
tasks, comprising thirteen methods. Our main goal is to strengthen NSR training
by considering a wide range of possibilities. Additionally, we examine the
impact of varying feature resolutions and evaluate both pixel-wise and
patch-wise consistent losses, providing insights into effective strategies for
improving NSR performance. By incorporating pre-trained representations from
MVSFormer and QuadTree, our approach can generate variations of MVS-NeuS and
Match-NeuS, respectively. Our results, analyzed on DTU and EPFL datasets,
reveal that feature priors from image matching and multi-view stereo outperform
other pretext tasks. Moreover, we discover that extending patch-wise
photometric consistency to the feature level surpasses the performance of
pixel-wise approaches. These findings underscore the effectiveness of these
techniques in enhancing NSR outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDFaceNet: Latent Diffusion-based Network for High-Fidelity Deepfake
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dwij Mehta, Aditya Mehta, Pratik Narang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, there has been tremendous progress in the domain of
synthetic media generation. This is mainly due to the powerful methods based on
generative adversarial networks (GANs). Very recently, diffusion probabilistic
models, which are inspired by non-equilibrium thermodynamics, have taken the
spotlight. In the realm of image generation, diffusion models (DMs) have
exhibited remarkable proficiency in producing both realistic and heterogeneous
imagery through their stochastic sampling procedure. This paper proposes a
novel facial swapping module, termed as LDFaceNet (Latent Diffusion based Face
Swapping Network), which is based on a guided latent diffusion model that
utilizes facial segmentation and facial recognition modules for a conditioned
denoising process. The model employs a unique loss function to offer
directional guidance to the diffusion process. Notably, LDFaceNet can
incorporate supplementary facial guidance for desired outcomes without any
retraining. To the best of our knowledge, this represents the first application
of the latent diffusion model in the face-swapping task without prior training.
The results of this study demonstrate that the proposed method can generate
extremely realistic and coherent images by leveraging the potential of the
diffusion model for facial swapping, thereby yielding superior visual outcomes
and greater diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ParkingE2E: Camera-based End-to-end Parking Network, from Images to
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changze Li, Ziheng Ji, Zhe Chen, Tong Qin, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous parking is a crucial task in the intelligent driving field.
Traditional parking algorithms are usually implemented using rule-based
schemes. However, these methods are less effective in complex parking scenarios
due to the intricate design of the algorithms. In contrast,
neural-network-based methods tend to be more intuitive and versatile than the
rule-based methods. By collecting a large number of expert parking trajectory
data and emulating human strategy via learning-based methods, the parking task
can be effectively addressed. In this paper, we employ imitation learning to
perform end-to-end planning from RGB images to path planning by imitating human
driving trajectories. The proposed end-to-end approach utilizes a target query
encoder to fuse images and target features, and a transformer-based decoder to
autoregressively predict future waypoints. We conducted extensive experiments
in real-world scenarios, and the results demonstrate that the proposed method
achieved an average parking success rate of 87.8% across four different
real-world garages. Real-vehicle experiments further validate the feasibility
and effectiveness of the method proposed in this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Step Saver: Predicting Minimum Denoising Steps for Diffusion Model Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Yu, Haim Barad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce an innovative NLP model specifically fine-tuned
to determine the minimal number of denoising steps required for any given text
prompt. This advanced model serves as a real-time tool that recommends the
ideal denoise steps for generating high-quality images efficiently. It is
designed to work seamlessly with the Diffusion model, ensuring that images are
produced with superior quality in the shortest possible time. Although our
explanation focuses on the DDIM scheduler, the methodology is adaptable and can
be applied to various other schedulers like Euler, Euler Ancestral, Heun, DPM2
Karras, UniPC, and more. This model allows our customers to conserve costly
computing resources by executing the fewest necessary denoising steps to
achieve optimal quality in the produced images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PanicleNeRF: low-cost, high-precision in-field phenotypingof rice
  panicles with smartphone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yang, Xuqi Lu, Pengyao Xie, Ziyue Guo, Hui Fang, Haowei Fu, Xiaochun Hu, Zhenbiao Sun, Haiyan Cen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rice panicle traits significantly influence grain yield, making them a
primary target for rice phenotyping studies. However, most existing techniques
are limited to controlled indoor environments and difficult to capture the rice
panicle traits under natural growth conditions. Here, we developed PanicleNeRF,
a novel method that enables high-precision and low-cost reconstruction of rice
panicle three-dimensional (3D) models in the field using smartphone. The
proposed method combined the large model Segment Anything Model (SAM) and the
small model You Only Look Once version 8 (YOLOv8) to achieve high-precision
segmentation of rice panicle images. The NeRF technique was then employed for
3D reconstruction using the images with 2D segmentation. Finally, the resulting
point clouds are processed to successfully extract panicle traits. The results
show that PanicleNeRF effectively addressed the 2D image segmentation task,
achieving a mean F1 Score of 86.9% and a mean Intersection over Union (IoU) of
79.8%, with nearly double the boundary overlap (BO) performance compared to
YOLOv8. As for point cloud quality, PanicleNeRF significantly outperformed
traditional SfM-MVS (structure-from-motion and multi-view stereo) methods, such
as COLMAP and Metashape. The panicle length was then accurately extracted with
the rRMSE of 2.94% for indica and 1.75% for japonica rice. The panicle volume
estimated from 3D point clouds strongly correlated with the grain number (R2 =
0.85 for indica and 0.82 for japonica) and grain mass (0.80 for indica and 0.76
for japonica). This method provides a low-cost solution for high-throughput
in-field phenotyping of rice panicles, accelerating the efficiency of rice
breeding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EOL: Transductive Few-Shot Open-Set Recognition by Enhancing Outlier
  Logits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Ochal, Massimiliano Patacchiola, Malik Boudiaf, Sen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Few-Shot Learning (FSL), models are trained to recognise unseen objects
from a query set, given a few labelled examples from a support set. In standard
FSL, models are evaluated on query instances sampled from the same class
distribution of the support set. In this work, we explore the more nuanced and
practical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard
FSL, OSFSL incorporates unknown classes into the query set, thereby requiring
the model not only to classify known classes but also to identify outliers.
Building on the groundwork laid by previous studies, we define a novel
transductive inference technique that leverages the InfoMax principle to
exploit the unlabelled query set. We called our approach the Enhanced Outlier
Logit (EOL) method. EOL refines class prototype representations through model
calibration, effectively balancing the inlier-outlier ratio. This calibration
enhances pseudo-label accuracy for the query set and improves the optimisation
objective within the transductive inference process. We provide a comprehensive
empirical evaluation demonstrating that EOL consistently surpasses traditional
methods, recording performance improvements ranging from approximately $+1.3%$
to $+6.3%$ across a variety of classification and outlier detection metrics and
benchmarks, even in the presence of inlier-outlier imbalance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Single-object Tracking in Point Clouds with High Temporal Variation <span class="chip">ECCV24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Wu, Kun Sun, Pei An, Mathieu Salzmann, Yanning Zhang, Jiaqi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high temporal variation of the point clouds is the key challenge of 3D
single-object tracking (3D SOT). Existing approaches rely on the assumption
that the shape variation of the point clouds and the motion of the objects
across neighboring frames are smooth, failing to cope with high temporal
variation data. In this paper, we present a novel framework for 3D SOT in point
clouds with high temporal variation, called HVTrack. HVTrack proposes three
novel components to tackle the challenges in the high temporal variation
scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud
shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal
with similar object distractions in expanded search areas; 3) a Contextual
Point Guided Self-Attention module for suppressing heavy background noise. We
construct a dataset with high temporal variation (KITTI-HV) by setting
different frame intervals for sampling in the KITTI dataset. On the KITTI-HV
with 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker
CXTracker by 11.3%/15.7% in Success/Precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Spectral Methods for Unsupervised Ultrasound Image Interpretation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandra Tmenova, Yordanka Velikova, Mahdi Saleh, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging is challenging to interpret due to non-uniform
intensities, low contrast, and inherent artifacts, necessitating extensive
training for non-specialists. Advanced representation with clear tissue
structure separation could greatly assist clinicians in mapping underlying
anatomy and distinguishing between tissue layers. Decomposing an image into
semantically meaningful segments is mainly achieved using supervised
segmentation algorithms. Unsupervised methods are beneficial, as acquiring
large labeled datasets is difficult and costly, but despite their advantages,
they still need to be explored in ultrasound. This paper proposes a novel
unsupervised deep learning strategy tailored to ultrasound to obtain easily
interpretable tissue separations. We integrate key concepts from unsupervised
deep spectral methods, which combine spectral graph theory with deep learning
methods. We utilize self-supervised transformer features for spectral
clustering to generate meaningful segments based on ultrasound-specific metrics
and shape and positional priors, ensuring semantic consistency across the
dataset. We evaluate our unsupervised deep learning strategy on three
ultrasound datasets, showcasing qualitative results across anatomical contexts
without label requirements. We also conduct a comparative analysis against
other clustering algorithms to demonstrate superior segmentation performance,
boundary preservation, and label consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Medical Image Computing and
  Computer Assisted Intervention, MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly
  Supervised Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Du, Zehua Fu, Qingjie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent attention has been devoted to the pursuit of learning semantic
segmentation models exclusively from image tags, a paradigm known as
image-level Weakly Supervised Semantic Segmentation (WSSS). Existing attempts
adopt the Class Activation Maps (CAMs) as priors to mine object regions yet
observe the imbalanced activation issue, where only the most discriminative
object parts are located. In this paper, we argue that the distribution
discrepancy between the discriminative and the non-discriminative parts of
objects prevents the model from producing complete and precise pseudo masks as
ground truths. For this purpose, we propose a Pixel-Level Domain Adaptation
(PLDA) method to encourage the model in learning pixel-wise domain-invariant
features. Specifically, a multi-head domain classifier trained adversarially
with the feature extraction is introduced to promote the emergence of pixel
features that are invariant with respect to the shift between the source (i.e.,
the discriminative object parts) and the target (\textit{i.e.}, the
non-discriminative object parts) domains. In addition, we come up with a
Confident Pseudo-Supervision strategy to guarantee the discriminative ability
of each pixel for the segmentation task, which serves as a complement to the
intra-image domain adversarial training. Our method is conceptually simple,
intuitive and can be easily integrated into existing WSSS methods. Taking
several strong baseline models as instances, we experimentally demonstrate the
effectiveness of our approach under a wide range of settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEGO: <span class="highlight-title">Self-Supervised</span> Representation Learning for Scene Text Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Ren, Jiaxin Zhang, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, significant progress has been made in scene text recognition
by data-driven methods. However, due to the scarcity of annotated real-world
data, the training of these methods predominantly relies on synthetic data. The
distribution gap between synthetic and real data constrains the further
performance improvement of these methods in real-world applications. To tackle
this problem, a highly promising approach is to utilize massive amounts of
unlabeled real data for self-supervised training, which has been widely proven
effective in many NLP and CV tasks. Nevertheless, generic self-supervised
methods are unsuitable for scene text images due to their sequential nature. To
address this issue, we propose a Local Explicit and Global Order-aware
self-supervised representation learning method (LEGO) that accounts for the
characteristics of scene text images. Inspired by the human cognitive process
of learning words, which involves spelling, reading, and writing, we propose
three novel pre-text tasks for LEGO to model sequential, semantic, and
structural features, respectively. The entire pre-training process is optimized
by using a consistent Text Knowledge Codebook. Extensive experiments validate
that LEGO outperforms previous scene text self-supervised methods. The
recognizer incorporated with our pre-trained model achieves superior or
comparable performance compared to state-of-the-art scene text recognition
methods on six benchmarks. Furthermore, we demonstrate that LEGO can achieve
superior performance in other text-related tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Monkey: Alleviate the Sawtooth Effect by Multi-Scale Adaptive
  Cropping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been significant interest in enhancing the capability of
multimodal large language models (MLLMs) to process high-resolution images.
Most existing methods focus on adopting a cropping strategy to improve the
ability of multimodal large language models to understand image details.
However, this cropping operation inevitably causes the segmentation of objects
and connected areas, which impairs the MLLM's ability to recognize small or
irregularly shaped objects or text. This issue is particularly evident in
lightweight MLLMs. Addressing this issue, we propose Mini-Monkey, a lightweight
MLLM that incorporates a plug-and-play method called multi-scale adaptive crop
strategy (MSAC). Mini-Monkey adaptively generates multi-scale representations,
allowing it to select non-segmented objects from various scales. To mitigate
the computational overhead introduced by MSAC, we propose a Scale Compression
Mechanism (SCM), which effectively compresses image tokens. Mini-Monkey
achieves state-of-the-art performance among 2B-parameter MLLMs. It not only
demonstrates leading performance on a variety of general multimodal
understanding tasks but also shows consistent improvements in document
understanding capabilities. On the OCRBench, Mini-Monkey achieves a score of
802, outperforming 8B-parameter state-of-the-art model InternVL2-8B. Besides,
our model and training strategy are very efficient, which can be trained with
only eight RTX 3090. The code is available at
https://github.com/Yuliang-Liu/Monkey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Human Action Recognition and Violence Detection Through Deep
  Learning Audiovisual Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooya Janani, Amirabolfazl Suratgar, Afshin Taghvaeipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a hybrid fusion-based deep learning approach based on two
different modalities, audio and video, to improve human activity recognition
and violence detection in public places. To take advantage of audiovisual
fusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning
(HFBDL) are used and compared. Since the objective is to detect and recognize
human violence in public places, Real-life violence situation (RLVS) dataset is
expanded and used. Simulating results of HFBDL show 96.67\% accuracy on
validation data, which is more accurate than the other state-of-the-art methods
on this dataset. To showcase our model's ability in real-world scenarios,
another dataset of 54 sounded videos of both violent and non-violent situations
was recorded. The model could successfully detect 52 out of 54 videos
correctly. The proposed method shows a promising performance on real scenarios.
Thus, it can be used for human action recognition and violence detection in
public places for security purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication, 10
  pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Introspective Decoding: Alleviating Hallucinations for Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, Peilin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Vision-Language Models (LVLMs) have rapidly advanced in recent
years, the prevalent issue known as the `hallucination' problem has emerged as
a significant bottleneck, hindering their real-world deployments. Existing
methods mitigate this issue mainly from two perspectives: One approach
leverages extra knowledge like robust instruction tuning LVLMs with curated
datasets or employing auxiliary analysis networks, which inevitable incur
additional costs. Another approach, known as contrastive decoding, induces
hallucinations by manually disturbing the vision or instruction raw inputs and
mitigates them by contrasting the outputs of the disturbed and original LVLMs.
However, these approaches rely on empirical holistic input disturbances and
double the inference cost. To avoid these issues, we propose a simple yet
effective method named Self-Introspective Decoding (SID). Our empirical
investigation reveals that pretrained LVLMs can introspectively assess the
importance of vision tokens based on preceding vision and text (both
instruction and generated) tokens. We develop the Context and Text-aware Token
Selection (CT2S) strategy, which preserves only unimportant vision tokens after
early layers of LVLMs to adaptively amplify text-informed hallucination during
the auto-regressive decoding. This approach ensures that multimodal knowledge
absorbed in the early layers induces multimodal contextual rather than aimless
hallucinations. Subsequently, the original token logits subtract the amplified
vision-and-text association hallucinations, guiding LVLMs decoding faithfully.
Extensive experiments illustrate SID generates less-hallucination and
higher-quality texts across various metrics, without extra knowledge and much
additional computation burdens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Diffusion Action Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaibing Wang, Shunli Wang, Mingcheng Li, Dingkang Yang, Haopeng Kuang, Ziyun Qian, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Action Segmentation (TAS) is an essential task in video analysis,
aiming to segment and classify continuous frames into distinct action segments.
However, the ambiguous boundaries between actions pose a significant challenge
for high-precision segmentation. Recent advances in diffusion models have
demonstrated substantial success in TAS tasks due to their stable training
process and high-quality generation capabilities. However, the heavy sampling
steps required by diffusion models pose a substantial computational burden,
limiting their practicality in real-time applications. Additionally, most
related works utilize Transformer-based encoder architectures. Although these
architectures excel at capturing long-range dependencies, they incur high
computational costs and face feature-smoothing issues when processing long
video sequences. To address these challenges, we propose EffiDiffAct, an
efficient and high-performance TAS algorithm. Specifically, we develop a
lightweight temporal feature encoder that reduces computational overhead and
mitigates the rank collapse phenomenon associated with traditional
self-attention mechanisms. Furthermore, we introduce an adaptive skip strategy
that allows for dynamic adjustment of timestep lengths based on computed
similarity metrics during inference, thereby further enhancing computational
efficiency. Comprehensive experiments on the 50Salads, Breakfast, and GTEA
datasets demonstrated the effectiveness of the proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Individualized multi-horizon MRI trajectory prediction for Alzheimer's
  Disease <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rosemary He, Gabriella Ang, Daniel Tward
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurodegeneration as measured through magnetic resonance imaging (MRI) is
recognized as a potential biomarker for diagnosing Alzheimer's disease (AD),
but is generally considered less specific than amyloid or tau based biomarkers.
Due to a large amount of variability in brain anatomy between different
individuals, we hypothesize that leveraging MRI time series can help improve
specificity, by treating each patient as their own baseline. Here we turn to
conditional variational autoencoders to generate individualized MRI predictions
given the subject's age, disease status and one previous scan. Using serial
imaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a
novel architecture to build a latent space distribution which can be sampled
from to generate future predictions of changing anatomy. This enables us to
extrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated
the model on a held-out set from ADNI and an independent dataset (from Open
Access Series of Imaging Studies). By comparing to several alternatives, we
show that our model produces more individualized images with higher resolution.
Further, if an individual already has a follow-up MRI, we demonstrate a usage
of our model to compute a likelihood ratio classifier for disease status. In
practice, the model may be able to assist in early diagnosis of AD and provide
a counterfactual baseline trajectory for treatment effect estimation.
Furthermore, it generates a synthetic dataset that can potentially be used for
downstream tasks such as anomaly detection and classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024 LDTM workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Representation Learning by Balanced Self Attention Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Shalam, Simon Korman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many leading self-supervised methods for unsupervised representation
learning, in particular those for embedding image features, are built on
variants of the instance discrimination task, whose optimization is known to be
prone to instabilities that can lead to feature collapse. Different techniques
have been devised to circumvent this issue, including the use of negative pairs
with different contrastive losses, the use of external memory banks, and
breaking of symmetry by using separate encoding networks with possibly
different structures. Our method, termed BAM, rather than directly matching
features of different views (augmentations) of input images, is based on
matching their self-attention vectors, which are the distributions of
similarities to the entire set of augmented images of a batch. We obtain rich
representations and avoid feature collapse by minimizing a loss that matches
these distributions to their globally balanced and entropy regularized version,
which is obtained through a simple self-optimal-transport computation. We
ablate and verify our method through a wide set of experiments that show
competitive performance with leading methods on both semi-supervised and
transfer-learning benchmarks. Our implementation and pre-trained models are
available at github.com/DanielShalam/BAM .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision Support System to triage of liver trauma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Jamali, Azadeh Nazemi, Ashkan Sami, Rosemina Bahrololoom, Shahram Paydar, Alireza Shakibafar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trauma significantly impacts global health, accounting for over 5 million
deaths annually, which is comparable to mortality rates from diseases such as
tuberculosis, AIDS, and malaria. In Iran, the financial repercussions of road
traffic accidents represent approximately 2% of the nation's Gross National
Product each year. Bleeding is the leading cause of mortality in trauma
patients within the first 24 hours following an injury, making rapid diagnosis
and assessment of severity crucial. Trauma patients require comprehensive scans
of all organs, generating a large volume of data. Evaluating CT images for the
entire body is time-consuming and requires significant expertise, underscoring
the need for efficient time management in diagnosis. Efficient diagnostic
processes can significantly reduce treatment costs and decrease the likelihood
of secondary complications. In this context, the development of a reliable
Decision Support System (DSS) for trauma triage, particularly focused on the
abdominal area, is vital. This paper presents a novel method for detecting
liver bleeding and lacerations using CT scans, utilising the GAN Pix2Pix
translation model. The effectiveness of the method is quantified by Dice score
metrics, with the model achieving an accuracy of 97% for liver bleeding and 93%
for liver laceration detection. These results represent a notable improvement
over current state-of-the-art technologies. The system's design integrates
seamlessly with existing medical imaging technologies, making it a practical
addition to emergency medical services. This research underscores the potential
of advanced image translation models like GAN Pix2Pix in improving the
precision and speed of medical diagnostics in critical care scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and
  Accurate Diagnosis <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Townim F. Chowdhury, Vu Minh Hieu Phan, Kewen Liao, Minh-Son To, Yutong Xie, Anton van den Hengel, Johan W. Verjans, Zhibin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of vision-language models such as CLIP and Concept Bottleneck
Models (CBMs) offers a promising approach to explaining deep neural network
(DNN) decisions using concepts understandable by humans, addressing the
black-box concern of DNNs. While CLIP provides both explainability and
zero-shot classification capability, its pre-training on generic image and text
data may limit its classification accuracy and applicability to medical image
diagnostic tasks, creating a transfer learning problem. To maintain
explainability and address transfer learning needs, CBM methods commonly design
post-processing modules after the bottleneck module. However, this way has been
ineffective. This paper takes an unconventional approach by re-examining the
CBM framework through the lens of its geometrical representation as a simple
linear classification system. The analysis uncovers that post-CBM fine-tuning
modules merely rescale and shift the classification outcome of the system,
failing to fully leverage the system's learning potential. We introduce an
adaptive module strategically positioned between CLIP and CBM to bridge the gap
between source and downstream domains. This simple yet effective approach
enhances classification performance while preserving the explainability
afforded by the framework. Our work offers a comprehensive solution that
encompasses the entire process, from concept discovery to model training,
providing a holistic recipe for leveraging the strengths of GPT, CLIP, and CBM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2024, the 27th International Conference on Medical
  Image Computing and Computer Assisted Intervention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Happens Without Background? Constructing Foreground-Only Data for
  Fine-Grained Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuetian Wang, Wenjin Hou, Qinmu Peng, Xinge You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained recognition, a pivotal task in visual signal processing, aims to
distinguish between similar subclasses based on discriminative information
present in samples. However, prevailing methods often erroneously focus on
background areas, neglecting the capture of genuinely effective discriminative
information from the subject, thus impeding practical application. To
facilitate research into the impact of background noise on models and enhance
their ability to concentrate on the subject's discriminative features, we
propose an engineered pipeline that leverages the capabilities of SAM and Detic
to create fine-grained datasets with only foreground subjects, devoid of
background. Extensive cross-experiments validate this approach as a
preprocessing step prior to training, enhancing algorithmic performance and
holding potential for further modal expansion of the data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeMansia: Mamba Never Forgets Any Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricky Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines the mathematical foundations of transformer
architectures, highlighting their limitations particularly in handling long
sequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM),
and LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia
integrates state space models with token labeling techniques to enhance
performance in image classification tasks, efficiently addressing the
computational challenges posed by traditional transformers. The architecture,
benchmark, and comparisons with contemporary models demonstrate DeMansia's
effectiveness. The implementation of this paper is available on GitHub at
https://github.com/catalpaaa/DeMansia
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdvQDet: Detecting Query-Based Adversarial Attacks with Adversarial
  Contrastive <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Kai Chen, Xingjun Ma, Zhineng Chen, Jingjing Chen, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks
even under a black-box setting where the adversary can only query the model.
Particularly, query-based black-box adversarial attacks estimate adversarial
gradients based on the returned probability vectors of the target model for a
sequence of queries. During this process, the queries made to the target model
are intermediate adversarial examples crafted at the previous attack step,
which share high similarities in the pixel space. Motivated by this
observation, stateful detection methods have been proposed to detect and reject
query-based attacks. While demonstrating promising results, these methods
either have been evaded by more advanced attacks or suffer from low efficiency
in terms of the number of shots (queries) required to detect different attacks.
Arguably, the key challenge here is to assign high similarity scores for any
two intermediate adversarial examples perturbed from the same clean image. To
address this challenge, we propose a novel Adversarial Contrastive Prompt
Tuning (ACPT) method to robustly fine-tune the CLIP image encoder to extract
similar embeddings for any two intermediate adversarial queries. With ACPT, we
further introduce a detection framework AdvQDet that can detect 7
state-of-the-art query-based attacks with $>99\%$ detection rate within 5
shots. We also show that ACPT is robust to 3 types of adaptive attacks. Code is
available at https://github.com/xinwong/AdvQDet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label Augmentation for Neural Networks Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Amerehi, Patrick Healy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution generalization can be categorized into two types: common
perturbations arising from natural variations in the real world and adversarial
perturbations that are intentionally crafted to deceive neural networks. While
deep neural networks excel in accuracy under the assumption of identical
distributions between training and test data, they often encounter
out-of-distribution scenarios resulting in a significant decline in accuracy.
Data augmentation methods can effectively enhance robustness against common
corruptions, but they typically fall short in improving robustness against
adversarial perturbations. In this study, we develop Label Augmentation (LA),
which enhances robustness against both common and intentional perturbations and
improves uncertainty estimation. Our findings indicate a Clean error rate
improvement of up to 23.29% when employing LA in comparisons to the baseline.
Additionally, it enhances robustness under common corruptions benchmark by up
to 24.23%. When tested against FGSM and PGD attacks, improvements in
adversarial robustness are noticeable, with enhancements of up to 53.18% for
FGSM and 24.46% for PGD attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures, Published at 3rd Conference on Lifelong Learning
  Agents (CoLLAs), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Point Supervised High-Resolution Dynamic Network for Infrared
  Small Target Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wu, Rixiang Ni, Feng Huang, Zhaobing Qiu, Liqiong Chen, Changhai Luo, Yunxiang Li, Youli Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target detection (IRSTD) tasks are extremely challenging for
two main reasons: 1) it is difficult to obtain accurate labelling information
that is critical to existing methods, and 2) infrared (IR) small target
information is easily lost in deep networks. To address these issues, we
propose a single-point supervised high-resolution dynamic network (SSHD-Net).
In contrast to existing methods, we achieve state-of-the-art (SOTA) detection
performance using only single-point supervision. Specifically, we first design
a high-resolution cross-feature extraction module (HCEM), that achieves
bi-directional feature interaction through stepped feature cascade channels
(SFCC). It balances network depth and feature resolution to maintain deep IR
small-target information. Secondly, the effective integration of global and
local features is achieved through the dynamic coordinate fusion module (DCFM),
which enhances the anti-interference ability in complex backgrounds. In
addition, we introduce the high-resolution multilevel residual module (HMRM) to
enhance the semantic information extraction capability. Finally, we design the
adaptive target localization detection head (ATLDH) to improve detection
accuracy. Experiments on the publicly available datasets NUDT-SIRST and
IRSTD-1k demonstrate the effectiveness of our method. Compared to other SOTA
methods, our method can achieve better detection performance with only a single
point of supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SR-CIS: Self-Reflective Incremental System with Decoupled Memory and
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biqing Qi, Junqi Gao, Xinquan Chen, Dong Li, Weinan Zhang, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of humans to rapidly learn new knowledge while retaining old
memories poses a significant challenge for current deep learning models. To
handle this challenge, we draw inspiration from human memory and learning
mechanisms and propose the Self-Reflective Complementary Incremental System
(SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and
Complementary Memory Module (CMM), SR-CIS features a small model for fast
inference and a large model for slow deliberation in CIM, enabled by the
Confidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient
collaboration. CMM consists of task-specific Short-Term Memory (STM) region and
a universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank
Adaptive (LoRA) and corresponding prototype weights and biases, it instantiates
external storage for parameter and representation memory, thus deconstructing
the memory module from the inference module. By storing textual descriptions of
images during training and combining them with the Scenario Replay Module (SRM)
post-training for memory combination, along with periodic short-to-long-term
memory restructuring, SR-CIS achieves stable incremental memory with limited
storage requirements. Balancing model plasticity and memory stability under
constraints of limited storage and low data resources, SR-CIS surpasses
existing competitive baselines on multiple standard and few-shot incremental
learning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnomalySD: Few-Shot Multi-Class Anomaly Detection with Stable Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Yan, Qingqing Fang, Wenxi Lv, Qinliang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is a critical task in industrial manufacturing, aiming to
identify defective parts of products. Most industrial anomaly detection methods
assume the availability of sufficient normal data for training. This assumption
may not hold true due to the cost of labeling or data privacy policies.
Additionally, mainstream methods require training bespoke models for different
objects, which incurs heavy costs and lacks flexibility in practice. To address
these issues, we seek help from Stable Diffusion (SD) model due to its
capability of zero/few-shot inpainting, which can be leveraged to inpaint
anomalous regions as normal. In this paper, a few-shot multi-class anomaly
detection framework that adopts Stable Diffusion model is proposed, named
AnomalySD. To adapt SD to anomaly detection task, we design different
hierarchical text descriptions and the foreground mask mechanism for
fine-tuning SD. In the inference stage, to accurately mask anomalous regions
for inpainting, we propose multi-scale mask strategy and prototype-guided mask
strategy to handle diverse anomalous regions. Hierarchical text prompts are
also utilized to guide the process of inpainting in the inference stage. The
anomaly score is estimated based on inpainting result of all masks. Extensive
experiments on the MVTec-AD and VisA datasets demonstrate the superiority of
our approach. We achieved anomaly classification and segmentation results of
93.6%/94.8% AUROC on the MVTec-AD dataset and 86.1%/96.5% AUROC on the VisA
dataset under multi-class and one-shot settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> Scale and Societal Consistency Mediate Facial Impression Bias in
  Vision-Language AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Wolfe, Aayushi Dangol, Alexis Hiniker, Bill Howe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal AI models capable of associating images and text hold promise for
numerous domains, ranging from automated image captioning to accessibility
applications for blind and low-vision users. However, uncertainty about bias
has in some cases limited their adoption and availability. In the present work,
we study 43 CLIP vision-language models to determine whether they learn
human-like facial impression biases, and we find evidence that such biases are
reflected across three distinct CLIP model families. We show for the first time
that the the degree to which a bias is shared across a society predicts the
degree to which it is reflected in a CLIP model. Human-like impressions of
visually unobservable attributes, like trustworthiness and sexuality, emerge
only in models trained on the largest dataset, indicating that a better fit to
uncurated cultural data results in the reproduction of increasingly subtle
social biases. Moreover, we use a hierarchical clustering approach to show that
dataset size predicts the extent to which the underlying structure of facial
impression bias resembles that of facial impression bias in humans. Finally, we
show that Stable Diffusion models employing CLIP as a text encoder learn facial
impression biases, and that these biases intersect with racial biases in Stable
Diffusion XL-Turbo. While pretrained CLIP models may prove useful for
scientific studies of bias, they will also require significant dataset curation
when intended for use as general-purpose models in a zero-shot setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Artificial Intelligence, Ethics, and Society 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Chen, Chenrui Tie, Ruihai Wu, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans perceive and interact with the world with the awareness of
equivariance, facilitating us in manipulating different objects in diverse
poses. For robotic manipulation, such equivariance also exists in many
scenarios. For example, no matter what the pose of a drawer is (translation,
rotation and tilt), the manipulation strategy is consistent (grasp the handle
and pull in a line). While traditional models usually do not have the awareness
of equivariance for robotic manipulation, which might result in more data for
training and poor performance in novel object poses, we propose our EqvAfford
framework, with novel designs to guarantee the equivariance in point-level
affordance learning for downstream robotic manipulation, with great performance
and generalization ability on representative tasks on objects in diverse poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CACE-Net: Co-guidance Attention and Contrastive Enhancement for
  Effective Audio-Visual Event Localization <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang He, Xiangxi Liu, Yang Li, Dongcheng Zhao, Guobin Shen, Qingqun Kong, Xin Yang, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The audio-visual event localization task requires identifying concurrent
visual and auditory events from unconstrained videos within a network model,
locating them, and classifying their category. The efficient extraction and
integration of audio and visual modal information have always been challenging
in this field. In this paper, we introduce CACE-Net, which differs from most
existing methods that solely use audio signals to guide visual information. We
propose an audio-visual co-guidance attention mechanism that allows for
adaptive bi-directional cross-modal attentional guidance between audio and
visual information, thus reducing inconsistencies between modalities. Moreover,
we have observed that existing methods have difficulty distinguishing between
similar background and event and lack the fine-grained features for event
classification. Consequently, we employ background-event contrast enhancement
to increase the discrimination of fused feature and fine-tuned pre-trained
model to extract more refined and discernible features from complex multimodal
inputs. Specifically, we have enhanced the model's ability to discern subtle
differences between event and background and improved the accuracy of event
classification in our model. Experiments on the AVE dataset demonstrate that
CACE-Net sets a new benchmark in the audio-visual event localization task,
proving the effectiveness of our proposed methods in handling complex
multimodal learning and event localization in unconstrained videos. Code is
available at https://github.com/Brain-Cog-Lab/CACE-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024. Code is available at this
  https://github.com/Brain-Cog-Lab/CACE-Net</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Angle-Aware Autoencoder for Remote Sensing Images <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Li, Biao Hou, Siteng Ma, Zitong Wu, Xianpeng Guo, Bo Ren, Licheng Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To overcome the inherent domain gap between remote sensing (RS) images and
natural images, some self-supervised representation learning methods have made
promising progress. However, they have overlooked the diverse angles present in
RS objects. This paper proposes the Masked Angle-Aware Autoencoder (MA3E) to
perceive and learn angles during pre-training. We design a \textit{scaling
center crop} operation to create the rotated crop with random orientation on
each original image, introducing the explicit angle variation. MA3E inputs this
composite image while reconstruct the original image, aiming to effectively
learn rotation-invariant representations by restoring the angle variation
introduced on the rotated crop. To avoid biases caused by directly
reconstructing the rotated crop, we propose an Optimal Transport (OT) loss that
automatically assigns similar original image patches to each rotated crop patch
for reconstruction. MA3E demonstrates more competitive performance than
existing pre-training methods on seven different RS image datasets in three
downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Maximum Likelihood Estimation for Perspective-n-Point
  Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Zhan, Chunfeng Xu, Cheng Zhang, Ke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Perspective-n-Point (PnP) problem has been widely studied in the
literature and applied in various vision-based pose estimation scenarios.
However, existing methods ignore the anisotropy uncertainty of observations, as
demonstrated in several real-world datasets in this paper. This oversight may
lead to suboptimal and inaccurate estimation, particularly in the presence of
noisy observations. To this end, we propose a generalized maximum likelihood
PnP solver, named GMLPnP, that minimizes the determinant criterion by iterating
the GLS procedure to estimate the pose and uncertainty simultaneously. Further,
the proposed method is decoupled from the camera model. Results of synthetic
and real experiments show that our method achieves better accuracy in common
pose estimation scenarios, GMLPnP improves rotation/translation accuracy by
4.7%/2.0% on TUM-RGBD and 18.6%/18.4% on KITTI-360 dataset compared to the best
baseline. It is more accurate under very noisy observations in a vision-based
UAV localization task, outperforming the best baseline by 34.4% in translation
estimation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RobNODDI: Robust NODDI Parameter Estimation with Adaptive Sampling under
  Continuous Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taohui Xiao, Jian Cheng, Wenxin Fan, Jing Yang, Cheng Li, Enqing Dong, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurite Orientation Dispersion and Density Imaging (NODDI) is an important
imaging technology used to evaluate the microstructure of brain tissue, which
is of great significance for the discovery and treatment of various
neurological diseases. Current deep learning-based methods perform parameter
estimation through diffusion magnetic resonance imaging (dMRI) with a small
number of diffusion gradients. These methods speed up parameter estimation and
improve accuracy. However, the diffusion directions used by most existing deep
learning models during testing needs to be strictly consistent with the
diffusion directions during training. This results in poor generalization and
robustness of deep learning models in dMRI parameter estimation. In this work,
we verify for the first time that the parameter estimation performance of
current mainstream methods will significantly decrease when the testing
diffusion directions and the training diffusion directions are inconsistent. A
robust NODDI parameter estimation method with adaptive sampling under
continuous representation (RobNODDI) is proposed. Furthermore, long short-term
memory (LSTM) units and fully connected layers are selected to learn continuous
representation signals. To this end, we use a total of 100 subjects to conduct
experiments based on the Human Connectome Project (HCP) dataset, of which 60
are used for training, 20 are used for validation, and 20 are used for testing.
The test results indicate that RobNODDI improves the generalization performance
and robustness of the deep learning model, enhancing the stability and
flexibility of deep learning NODDI parameter estimatimation applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Grounding for Object-Level Generalization in Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobin Jiang, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization is a pivotal challenge for agents following natural language
instructions. To approach this goal, we leverage a vision-language model (VLM)
for visual grounding and transfer its vision-language knowledge into
reinforcement learning (RL) for object-centric tasks, which makes the agent
capable of zero-shot generalization to unseen objects and instructions. By
visual grounding, we obtain an object-grounded confidence map for the target
object indicated in the instruction. Based on this map, we introduce two routes
to transfer VLM knowledge into RL. Firstly, we propose an object-grounded
intrinsic reward function derived from the confidence map to more effectively
guide the agent towards the target object. Secondly, the confidence map offers
a more unified, accessible task representation for the agent's policy, compared
to language embeddings. This enables the agent to process unseen objects and
instructions through comprehensible visual confidence maps, facilitating
zero-shot object-level generalization. Single-task experiments prove that our
intrinsic reward significantly improves performance on challenging skill
learning. In multi-task experiments, through testing on tasks beyond the
training set, we show that the agent, when provided with the confidence map as
the task representation, possesses better generalization capabilities than
language-based conditioning. The code is available at
https://github.com/PKU-RL/COPL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 14 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing H&E-to-IHC Stain Translation in Breast Cancer: A
  Multi-Magnification and Attention-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Qu, Chengsheng Zhang, Guihui Li, Haiyong Zheng, Chen Peng, Wei He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer presents a significant healthcare challenge globally, demanding
precise diagnostics and effective treatment strategies, where histopathological
examination of Hematoxylin and Eosin (H&E) stained tissue sections plays a
central role. Despite its importance, evaluating specific biomarkers like Human
Epidermal Growth Factor Receptor 2 (HER2) for personalized treatment remains
constrained by the resource-intensive nature of Immunohistochemistry (IHC).
Recent strides in deep learning, particularly in image-to-image translation,
offer promise in synthesizing IHC-HER2 slides from H\&E stained slides.
However, existing methodologies encounter challenges, including managing
multiple magnifications in pathology images and insufficient focus on crucial
information during translation. To address these issues, we propose a novel
model integrating attention mechanisms and multi-magnification information
processing. Our model employs a multi-magnification processing strategy to
extract and utilize information from various magnifications within pathology
images, facilitating robust image translation. Additionally, an attention
module within the generative network prioritizes critical information for image
distribution translation while minimizing less pertinent details. Rigorous
testing on a publicly available breast cancer dataset demonstrates superior
performance compared to existing methods, establishing our model as a
state-of-the-art solution in advancing pathology image translation from H&E to
IHC staining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE CIS-RAM 2024 Invited Session Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pretrain</span>ed Models and Latent Feature Distribution
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuyu Zhu, Liheng Hu, Sijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the face of complex natural images, existing deep clustering algorithms
fall significantly short in terms of clustering accuracy when compared to
supervised classification methods, making them less practical. This paper
introduces an image clustering algorithm based on self-supervised pretrained
models and latent feature distribution optimization, substantially enhancing
clustering performance. It is found that: (1) For complex natural images, we
effectively enhance the discriminative power of latent features by leveraging
self-supervised pretrained models and their fine-tuning, resulting in improved
clustering performance. (2) In the latent feature space, by searching for
k-nearest neighbor images for each training sample and shortening the distance
between the training sample and its nearest neighbor, the discriminative power
of latent features can be further enhanced, and clustering performance can be
improved. (3) In the latent feature space, reducing the distance between sample
features and the nearest predefined cluster centroids can optimize the
distribution of latent features, therefore further improving clustering
performance. Through experiments on multiple datasets, our approach outperforms
the latest clustering algorithms and achieves state-of-the-art clustering
results. When the number of categories in the datasets is small, such as
CIFAR-10 and STL-10, and there are significant differences between categories,
our clustering algorithm has similar accuracy to supervised methods without
using pretrained models, slightly lower than supervised methods using
pre-trained models. The code linked algorithm is
https://github.com/LihengHu/ICBPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAF-YOLO: A Robust Framework for Multi-Scale Lesion Detection in
  Biomedical Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilin Chen, Shengnan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection is of paramount importance in biomedical image analysis,
particularly for lesion identification. While current methodologies are
proficient in identifying and pinpointing lesions, they often lack the
precision needed to detect minute biomedical entities (e.g., abnormal cells,
lung nodules smaller than 3 mm), which are critical in blood and lung
pathology. To address this challenge, we propose CAF-YOLO, based on the YOLOv8
architecture, a nimble yet robust method for medical object detection that
leverages the strengths of convolutional neural networks (CNNs) and
transformers. To overcome the limitation of convolutional kernels, which have a
constrained capacity to interact with distant information, we introduce an
attention and convolution fusion module (ACFM). This module enhances the
modeling of both global and local features, enabling the capture of long-term
feature dependencies and spatial autocorrelation. Additionally, to improve the
restricted single-scale feature aggregation inherent in feed-forward networks
(FFN) within transformer architectures, we design a multi-scale neural network
(MSNN). This network improves multi-scale information aggregation by extracting
features across diverse scales. Experimental evaluations on widely used
datasets, such as BCCD and LUNA16, validate the rationale and efficacy of
CAF-YOLO. This methodology excels in detecting and precisely locating diverse
and intricate micro-lesions within biomedical imagery. Our codes are available
at https://github.com/xiaochen925/CAF-YOLO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Trichromacy Reconstruction: Empowering the Color-Vision
  Deficient to Recognize Colors Using Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhu, Ethan Chen, Colin Hascup, Yukang Yan, Gaurav Charma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an assistive technology that helps individuals with Color Vision
Deficiencies (CVD) to recognize/name colors. A dichromat's color perception is
a reduced two-dimensional (2D) subset of a normal trichromat's three
dimensional color (3D) perception, leading to confusion when visual stimuli
that appear identical to the dichromat are referred to by different color
names. Using our proposed system, CVD individuals can interactively induce
distinct perceptual changes to originally confusing colors via a computational
color space transformation. By combining their original 2D precepts for colors
with the discriminative changes, a three dimensional color space is
reconstructed, where the dichromat can learn to resolve color name confusions
and accurately recognize colors. Our system is implemented as an Augmented
Reality (AR) interface on smartphones, where users interactively control the
rotation through swipe gestures and observe the induced color shifts in the
camera view or in a displayed image. Through psychophysical experiments and a
longitudinal user study, we demonstrate that such rotational color shifts have
discriminative power (initially confusing colors become distinct under
rotation) and exhibit structured perceptual shifts dichromats can learn with
modest training. The AR App is also evaluated in two real-world scenarios
(building with lego blocks and interpreting artistic works); users all report
positive experience in using the App to recognize object colors that they
otherwise could not.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViG-Bias: Visually Grounded Bias Discovery and Mitigation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01996v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01996v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badr-Eddine Marani, Mohamed Hanini, Nihitha Malayarukil, Stergios Christodoulidis, Maria Vakalopoulou, Enzo Ferrante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of machine learning models in critical decision making
processes has underscored the need for bias discovery and mitigation
strategies. Identifying the reasons behind a biased system is not
straightforward, since in many occasions they are associated with hidden
spurious correlations which are not easy to spot. Standard approaches rely on
bias audits performed by analyzing model performance in pre-defined subgroups
of data samples, usually characterized by common attributes like gender or
ethnicity when it comes to people, or other specific attributes defining
semantically coherent groups of images. However, it is not always possible to
know a-priori the specific attributes defining the failure modes of visual
recognition systems. Recent approaches propose to discover these groups by
leveraging large vision language models, which enable the extraction of
cross-modal embeddings and the generation of textual descriptions to
characterize the subgroups where a certain model is underperforming. In this
work, we argue that incorporating visual explanations (e.g. heatmaps generated
via GradCAM or other approaches) can boost the performance of such bias
discovery and mitigation frameworks. To this end, we introduce Visually
Grounded Bias Discovery and Mitigation (ViG-Bias), a simple yet effective
technique which can be integrated to a variety of existing frameworks to
improve both, discovery and mitigation performance. Our comprehensive
evaluation shows that incorporating visual explanations enhances existing
techniques like DOMINO, FACTS and Bias-to-Text, across several challenging
datasets, including CelebA, Waterbirds, and NICO++.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring
  Unconstrained Photo Collections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Dudai, Morris Alper, Hana Bezalel, Rana Hanocka, Itai Lang, Hadar Averbuch-Elor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet image collections containing photos captured by crowds of
photographers show promise for enabling digital exploration of large-scale
tourist landmarks. However, prior works focus primarily on geometric
reconstruction and visualization, neglecting the key role of language in
providing a semantic interface for navigation and fine-grained understanding.
In constrained 3D domains, recent methods have leveraged vision-and-language
models as a strong prior of 2D visual semantics. While these models display an
excellent understanding of broad visual semantics, they struggle with
unconstrained photo collections depicting such tourist landmarks, as they lack
expert knowledge of the architectural domain. In this work, we present a
localization system that connects neural representations of scenes depicting
large-scale landmarks with text describing a semantic region within the scene,
by harnessing the power of SOTA vision-and-language models with adaptations for
understanding landmark scene semantics. To bolster such models with
fine-grained knowledge, we leverage large-scale Internet data containing images
of similar landmarks along with weakly-related textual information. Our
approach is built upon the premise that images physically grounded in space can
provide a powerful supervision signal for localizing new concepts, whose
semantics may be unlocked from Internet textual metadata with large language
models. We use correspondences between views of scenes to bootstrap spatial
understanding of these semantics, providing guidance for 3D-compatible
segmentation that ultimately lifts to a volumetric scene representation. Our
results show that HaLo-NeRF can accurately localize a variety of semantic
concepts related to architectural landmarks, surpassing the results of other 3D
models as well as strong 2D segmentation baselines. Our project page is at
https://tau-vailab.github.io/HaLo-NeRF/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Eurographics 2024. Project page:
  https://tau-vailab.github.io/HaLo-NeRF/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Regional Information <span class="highlight-title">Transformer</span> for Single Image Deraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiang Li, Zhao Zhang, Huan Zheng, Xiaogang Xu, Yanyan Wei, Jingyi Zhang, Jicong Fan, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based Single Image Deraining (SID) methods have achieved
remarkable success, primarily attributed to their robust capability in
capturing long-range interactions. However, we've noticed that current methods
handle rain-affected and unaffected regions concurrently, overlooking the
disparities between these areas, resulting in confusion between rain streaks
and background parts, and inabilities to obtain effective interactions,
ultimately resulting in suboptimal deraining outcomes. To address the above
issue, we introduce the Region Transformer (Regformer), a novel SID method that
underlines the importance of independently processing rain-affected and
unaffected regions while considering their combined impact for high-quality
image reconstruction. The crux of our method is the innovative Region
Transformer Block (RTB), which integrates a Region Masked Attention (RMA)
mechanism and a Mixed Gate Forward Block (MGFB). Our RTB is used for attention
selection of rain-affected and unaffected regions and local modeling of mixed
scales. The RMA generates attention maps tailored to these two regions and
their interactions, enabling our model to capture comprehensive features
essential for rain removal. To better recover high-frequency textures and
capture more local details, we develop the MGFB as a compensation module to
complete local mixed scale modeling. Extensive experiments demonstrate that our
model reaches state-of-the-art performance, significantly improving the image
deraining quality. Our code and trained models are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SE3D: A Framework For Saliency Method Evaluation In 3D Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariusz Wiśniewski, Loris Giulivi, Giacomo Boracchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For more than a decade, deep learning models have been dominating in various
2D imaging tasks. Their application is now extending to 3D imaging, with 3D
Convolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and
CT scans, with significant implications for fields such as autonomous driving
and medical imaging. In these critical settings, explaining the model's
decisions is fundamental. Despite recent advances in Explainable Artificial
Intelligence, however, little effort has been devoted to explaining 3D CNNs,
and many works explain these models via inadequate extensions of 2D saliency
methods.
  A fundamental limitation to the development of 3D saliency methods is the
lack of a benchmark to quantitatively assess these on 3D data. To address this
issue, we propose SE3D: a framework for Saliency method Evaluation in 3D
imaging. We propose modifications to ShapeNet, ScanNet, and BraTS datasets, and
evaluation metrics to assess saliency methods for 3D CNNs. We evaluate both
state-of-the-art saliency methods designed for 3D data and extensions of
popular 2D saliency methods to 3D. Our experiments show that 3D saliency
methods do not provide explanations of sufficient quality, and that there is
margin for future improvements and safer applications of 3D CNNs in critical
fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Twin Deformable Point Convolutions for Point Cloud Semantic Segmentation
  in Remote Sensing Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong-Qiang Mao, Hanbo Bi, Xuexue Li, Kaiqiang Chen, Zhirui Wang, Xian Sun, Kun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to the application of deep learning technology in point cloud
processing of the remote sensing field, point cloud segmentation has become a
research hotspot in recent years, which can be applied to real-world 3D, smart
cities, and other fields. Although existing solutions have made unprecedented
progress, they ignore the inherent characteristics of point clouds in remote
sensing fields that are strictly arranged according to latitude, longitude, and
altitude, which brings great convenience to the segmentation of point clouds in
remote sensing fields. To consider this property cleverly, we propose novel
convolution operators, termed Twin Deformable point Convolutions (TDConvs),
which aim to achieve adaptive feature learning by learning deformable sampling
points in the latitude-longitude plane and altitude direction, respectively.
First, to model the characteristics of the latitude-longitude plane, we propose
a Cylinder-wise Deformable point Convolution (CyDConv) operator, which
generates a two-dimensional cylinder map by constructing a cylinder-like grid
in the latitude-longitude direction. Furthermore, to better integrate the
features of the latitude-longitude plane and the spatial geometric features, we
perform a multi-scale fusion of the extracted latitude-longitude features and
spatial geometric features, and realize it through the aggregation of adjacent
point features of different scales. In addition, a Sphere-wise Deformable point
Convolution (SpDConv) operator is introduced to adaptively offset the sampling
points in three-dimensional space by constructing a sphere grid structure,
aiming at modeling the characteristics in the altitude direction. Experiments
on existing popular benchmarks conclude that our TDConvs achieve the best
segmentation performance, surpassing the existing state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LawLuo: A Chinese Law Firm Co-run by LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyun Sun, Chengxiao Dai, Zhongze Luo, Yangbo Chang, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate substantial potential in delivering
legal consultation services to users without a legal background, attributed to
their superior text comprehension and generation capabilities. Nonetheless,
existing Chinese legal LLMs limit interaction to a single model-user dialogue,
unlike the collaborative consultations typical of law firms, where multiple
staff members contribute to a single consultation. This limitation prevents an
authentic consultation experience. Additionally, extant Chinese legal LLMs
suffer from critical limitations: (1) insufficient control over the quality of
instruction fine-tuning data; (2) increased model hallucination resulting from
users' ambiguous queries; and (3) a reduction in the model's ability to follow
instructions over multiple dialogue turns. In response to these challenges, we
propose a novel legal dialogue framework that leverages the collaborative
capabilities of multiple LLM agents, termed LawLuo. This framework encompasses
four agents: a receptionist, a lawyer, a secretary, and a boss, each
responsible for different functionalities, collaboratively providing a
comprehensive legal consultation to users. Additionally, we constructed two
high-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned
ChatGLM-3-6b using these datasets. We propose a legal query clarification
algorithm called ToLC. Experimental results demonstrate that LawLuo outperforms
baseline LLMs, including GPT-4, across three dimensions: lawyer-like language
style, the usefulness of legal advice, and the accuracy of legal knowledge. Our
code and datasets are available at https://github.com/NEFUJing/LawLuo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 13 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Classification and Rejection: A One-versus-All Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifying patterns of known classes and rejecting ambiguous and novel (also
called as out-of-distribution (OOD)) inputs are involved in open world pattern
recognition. Deep neural network models usually excel in closed-set
classification while performs poorly in rejecting OOD inputs. To tackle this
problem, numerous methods have been designed to perform open set recognition
(OSR) or OOD rejection/detection tasks. Previous methods mostly take
post-training score transformation or hybrid models to ensure low scores on OOD
inputs while separating known classes. In this paper, we attempt to build a
unified framework for building open set classifiers for both classification and
OOD rejection. We formulate the open set recognition of $ K $-known-class as a
$ (K+1) $-class classification problem with model trained on known-class
samples only. By decomposing the $ K $-class problem into $ K $ one-versus-all
(OVA) binary classification tasks and binding some parameters, we show that
combining the scores of OVA classifiers can give $ (K+1) $-class posterior
probabilities, which enables classification and OOD rejection in a unified
framework. To maintain the closed-set classification accuracy of the OVA
trained classifier, we propose a hybrid training strategy combining OVA loss
and multi-class cross-entropy loss. We implement the OVA framework and hybrid
training strategy on the recently proposed convolutional prototype network and
prototype classifier on vision transformer (ViT) backbone. Experiments on
popular OSR and OOD detection datasets demonstrate that the proposed framework,
using a single multi-class classifier, yields competitive performance in
closed-set classification, OOD detection, and misclassification detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Machine Intelligence Research
  (https://link.springer.com/article/10.1007/s11633-024-1514-4)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Visual <span class="highlight-title">Prompt</span> Learning with Contrastive Feature Re-formation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08386v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08386v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Yuhan Zhu, Haocheng Shen, Boheng Chen, Yixuan Liao, Xiaoxin Chen, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning has been designed as an alternative to fine-tuning for
adapting Vision-language (V-L) models to the downstream tasks. Previous works
mainly focus on text prompt while visual prompt works are limited for V-L
models. The existing visual prompt methods endure either mediocre performance
or unstable training process, indicating the difficulty of visual prompt
learning. In this paper, we propose a new Progressive Visual Prompt (ProVP)
structure to strengthen the interactions among prompts of different layers.
More importantly, our ProVP could effectively propagate the image embeddings to
deep layers and behave partially similar to an instance adaptive prompt method.
To alleviate generalization deterioration, we further propose a new contrastive
feature re-formation, which prevents the serious deviation of the prompted
visual feature from the fixed CLIP visual feature distribution. Combining both,
our method (ProVP-Ref) is evaluated on 11 image benchmark datasets and achieves
7/11 state-of-theart results on both few-shot and base-to-novel settings. To
the best of our knowledge, we are the first to demonstrate the superior
performance of visual prompts in V-L models to previous prompt-based methods in
downstream tasks. Meanwhile, it implies that our ProVP-Ref shows the best
capability to adapt and to generalize.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCV 2024 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exposure Completing for Temporally Consistent Neural High Dynamic Range
  Video Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Cui, Wei Jiang, Zhan Peng, Zhiyu Pan, Zhiguo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High dynamic range (HDR) video rendering from low dynamic range (LDR) videos
where frames are of alternate exposure encounters significant challenges, due
to the exposure change and absence at each time stamp. The exposure change and
absence make existing methods generate flickering HDR results. In this paper,
we propose a novel paradigm to render HDR frames via completing the absent
exposure information, hence the exposure information is complete and
consistent. Our approach involves interpolating neighbor LDR frames in the time
dimension to reconstruct LDR frames for the absent exposures. Combining the
interpolated and given LDR frames, the complete set of exposure information is
available at each time stamp. This benefits the fusing process for HDR results,
reducing noise and ghosting artifacts therefore improving temporal consistency.
Extensive experimental evaluations on standard benchmarks demonstrate that our
method achieves state-of-the-art performance, highlighting the importance of
absent exposure completing in HDR video rendering. The code is available at
https://github.com/cuijiahao666/NECHDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, accepted by ACM-MM 2024 (poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Aesthetics: Cultural Competence in Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06863v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06863v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nithish Kannen, Arif Ahmad, Marco Andreetto, Vinodkumar Prabhakaran, Utsav Prabhu, Adji Bousso Dieng, Pushpak Bhattacharyya, Shachi Dave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) models are being increasingly adopted in diverse global
communities where they create visual representations of their unique cultures.
Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism
of generated images, overlooking the critical dimension of cultural competence.
In this work, we introduce a framework to evaluate cultural competence of T2I
models along two crucial dimensions: cultural awareness and cultural diversity,
and present a scalable approach using a combination of structured knowledge
bases and large language models to build a large dataset of cultural artifacts
to enable this evaluation. In particular, we apply this approach to build CUBE
(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to
evaluate cultural competence of T2I models. CUBE covers cultural artifacts
associated with 8 countries across different geo-cultural regions and along 3
concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of
high-quality prompts that enable the evaluation of cultural awareness, and 2)
CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to
evaluate cultural diversity. We also introduce cultural diversity as a novel
T2I evaluation component, leveraging quality-weighted Vendi score. Our
evaluations reveal significant gaps in the cultural awareness of existing
models across countries and provide valuable insights into the cultural
diversity of T2I outputs for under-specified prompts. Our methodology is
extendable to other cultural regions and concepts, and can facilitate the
development of T2I models that better cater to the global population.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapt PointFormer: 3D Point Cloud Analysis via Adapting 2D Visual
  <span class="highlight-title">Transformer</span>s <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengke Li, Da Li, Guoqing Yang, Yiu-ming Cheung, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained large-scale models have exhibited remarkable efficacy in computer
vision, particularly for 2D image analysis. However, when it comes to 3D point
clouds, the constrained accessibility of data, in contrast to the vast
repositories of images, poses a challenge for the development of 3D pre-trained
models. This paper therefore attempts to directly leverage pre-trained models
with 2D prior knowledge to accomplish the tasks for 3D point cloud analysis.
Accordingly, we propose the Adaptive PointFormer (APF), which fine-tunes
pre-trained 2D models with only a modest number of parameters to directly
process point clouds, obviating the need for mapping to images. Specifically,
we convert raw point clouds into point embeddings for aligning dimensions with
image tokens. Given the inherent disorder in point clouds, in contrast to the
structured nature of images, we then sequence the point embeddings to optimize
the utilization of 2D attention priors. To calibrate attention across 3D and 2D
domains and reduce computational overhead, a trainable PointFormer with a
limited number of parameters is subsequently concatenated to a frozen
pre-trained image model. Extensive experiments on various benchmarks
demonstrate the effectiveness of the proposed APF. The source code and more
details are available at https://vcc.tech/research/2024/PointFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECAI 2024 main conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for
  Multi-modal Sarcasm Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16464v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16464v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chen, Hang Yu, Weidong Liu, Subin Huang, Sanmin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalence of sarcasm in social media, conveyed through text-image
combinations, presents significant challenges for sentiment analysis and
intention mining. Existing multi-modal sarcasm detection methods have been
proven to overestimate performance, as they struggle to effectively capture the
intricate sarcastic cues that arise from the interaction between an image and
text. To address these issues, we propose InterCLIP-MEP, a novel framework for
multi-modal sarcasm detection. Specifically, we introduce an Interactive CLIP
(InterCLIP) as the backbone to extract text-image representations, enhancing
them by embedding cross-modality information directly within each encoder,
thereby improving the representations to capture text-image interactions
better. Furthermore, an efficient training strategy is designed to adapt
InterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic,
fixed-length dual-channel memory to store historical knowledge of valuable test
samples during inference. It then leverages this memory as a non-parametric
classifier to derive the final prediction, offering a more robust recognition
of multi-modal sarcasm. Experiments demonstrate that InterCLIP-MEP achieves
state-of-the-art performance on the MMSD2.0 benchmark, with an accuracy
improvement of 1.08% and an F1 score improvement of 1.51% over the previous
best method. Code and data are available at
https://github.com/CoderChen01/InterCLIP-MEP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fun with Flags: Robust Principal Directions via Flag Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04071v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04071v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Mankovich, Gustau Camps-Valls, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal component analysis (PCA), along with its extensions to manifolds
and outlier contaminated data, have been indispensable in computer vision and
machine learning. In this work, we present a unifying formalism for PCA and its
variants, and introduce a framework based on the flags of linear subspaces, ie
a hierarchy of nested linear subspaces of increasing dimension, which not only
allows for a common implementation but also yields novel variants, not explored
previously. We begin by generalizing traditional PCA methods that either
maximize variance or minimize reconstruction error. We expand these
interpretations to develop a wide array of new dimensionality reduction
algorithms by accounting for outliers and the data manifold. To devise a common
computational approach, we recast robust and dual forms of PCA as optimization
problems on flag manifolds. We then integrate tangent space approximations of
principal geodesic analysis (tangent-PCA) into this flag-based framework,
creating novel robust and dual geodesic PCA variations. The remarkable
flexibility offered by the 'flagification' introduced here enables even more
algorithmic variants identified by specific flag types. Last but not least, we
propose an effective convergent solver for these flag-formulations employing
the Stiefel manifold. Our empirical results on both real-world and synthetic
scenarios, demonstrate the superiority of our novel algorithms, especially in
terms of robustness to outliers on manifolds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>-Based Visual Segmentation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09854v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09854v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangtai Li, Henghui Ding, Haobo Yuan, Wenwei Zhang, Jiangmiao Pang, Guangliang Cheng, Kai Chen, Ziwei Liu, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual segmentation seeks to partition images, video frames, or point clouds
into multiple segments or groups. This technique has numerous real-world
applications, such as autonomous driving, image editing, robot sensing, and
medical analysis. Over the past decade, deep learning-based methods have made
remarkable strides in this area. Recently, transformers, a type of neural
network based on self-attention originally designed for natural language
processing, have considerably surpassed previous convolutional or recurrent
approaches in various vision processing tasks. Specifically, vision
transformers offer robust, unified, and even simpler solutions for various
segmentation tasks. This survey provides a thorough overview of
transformer-based visual segmentation, summarizing recent advancements. We
first review the background, encompassing problem definitions, datasets, and
prior convolutional methods. Next, we summarize a meta-architecture that
unifies all recent transformer-based approaches. Based on this
meta-architecture, we examine various method designs, including modifications
to the meta-architecture and associated applications. We also present several
closely related settings, including 3D point cloud segmentation, foundation
model tuning, domain-aware segmentation, efficient segmentation, and medical
segmentation. Additionally, we compile and re-evaluate the reviewed methods on
several well-established datasets. Finally, we identify open challenges in this
field and propose directions for future research. The project page can be found
at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also
continually monitor developments in this rapidly evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE T-PAMI. Project page:
  https://github.com/lxtGH/Awesome-Segmentation-With-Transformer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VSSD: Vision Mamba with Non-Causal State Space Duality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Shi, Minjing Dong, Mingjia Li, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers have significantly advanced the field of computer vision,
offering robust modeling capabilities and global receptive field. However,
their high computational demands limit their applicability in processing long
sequences. To tackle this issue, State Space Models (SSMs) have gained
prominence in vision tasks as they offer linear computational complexity.
Recently, State Space Duality (SSD), an improved variant of SSMs, was
introduced in Mamba2 to enhance model performance and efficiency. However, the
inherent causal nature of SSD/SSMs restricts their applications in non-causal
vision tasks. To address this limitation, we introduce Visual State Space
Duality (VSSD) model, which has a non-causal format of SSD. Specifically, we
propose to discard the magnitude of interactions between the hidden state and
tokens while preserving their relative weights, which relieves the dependencies
of token contribution on previous tokens. Together with the involvement of
multi-scan strategies, we show that the scanning results can be integrated to
achieve non-causality, which not only improves the performance of SSD in vision
tasks but also enhances its efficiency. We conduct extensive experiments on
various benchmarks including image classification, detection, and segmentation,
where VSSD surpasses existing state-of-the-art SSM-based models. Code and
weights are available at \url{https://github.com/YuHengsss/VSSD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adding Multimodal Controls to Whole-body Human Motion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Bian, Ailing Zeng, Xuan Ju, Xian Liu, Zhaoyang Zhang, Wei Liu, Qiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole-body multimodal motion generation, controlled by text, speech, or
music, has numerous applications including video generation and character
animation. However, employing a unified model to accomplish various generation
tasks with different condition modalities presents two main challenges: motion
distribution drifts across different generation scenarios and the complex
optimization of mixed conditions with varying granularity. Furthermore,
inconsistent motion formats in existing datasets further hinder effective
multimodal motion generation. In this paper, we propose ControlMM, a unified
framework to Control whole-body Multimodal Motion generation in a plug-and-play
manner. To effectively learn and transfer motion knowledge across different
motion distributions, we propose ControlMM-Attn, for parallel modeling of
static and dynamic human topology graphs. To handle conditions with varying
granularity, ControlMM employs a coarse-to-fine training strategy, including
stage-1 text-to-motion pre-training for semantic generation and stage-2
multimodal control adaptation for conditions of varying low-level granularity.
To address existing benchmarks' varying motion format limitations, we introduce
ControlMM-Bench, the first publicly available multimodal whole-body human
motion generation benchmark based on the unified whole-body SMPL-X format.
Extensive experiments show that ControlMM achieves state-of-the-art performance
across various standard motion generation tasks. Our website is at
https://yxbian23.github.io/ControlMM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activator: GLU Activation Function as the Core Component of a Vision
  <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Nazhat Abdullah, Tarkan Aydin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer architecture currently represents the main driver behind many
successes in a variety of tasks addressed by deep learning, especially the
recent advances in natural language processing (NLP) culminating with large
language models (LLM). In addition, transformer architecture has found a wide
spread of interest from computer vision (CV) researchers and practitioners,
allowing for many advancements in vision-related tasks and opening the door for
multi-task and multi-modal deep learning architectures that share the same
principle of operation. One drawback to these architectures is their reliance
on the scaled dot product attention mechanism with the softmax activation
function, which is computationally expensive and requires large compute
capabilities both for training and inference. This paper investigates
substituting the attention mechanism usually adopted for transformer
architecture with an architecture incorporating gated linear unit (GLU)
activation within a multi-layer perceptron (MLP) structure in conjunction with
the default MLP incorporated in the traditional transformer design. Another
step forward taken by this paper is to eliminate the second non-gated MLP to
further reduce the computational cost. Experimental assessments conducted by
this research show that both proposed modifications and reductions offer
competitive performance in relation to baseline architectures, in support of
the aims of this work in establishing a more efficient yet capable alternative
to the traditional attention mechanism as the core component in designing
transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2403.02411</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">55</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfBC: Self Behavior Cloning for Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirong Liu, Chenjia Bai, Zixian Guo, Hao Zhang, Gaurav Sharma, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy constraint methods in offline reinforcement learning employ additional
regularization techniques to constrain the discrepancy between the learned
policy and the offline dataset. However, these methods tend to result in overly
conservative policies that resemble the behavior policy, thus limiting their
performance. We investigate this limitation and attribute it to the static
nature of traditional constraints. In this paper, we propose a novel dynamic
policy constraint that restricts the learned policy on the samples generated by
the exponential moving average of previously learned policies. By integrating
this self-constraint mechanism into off-policy methods, our method facilitates
the learning of non-conservative policies while avoiding policy collapse in the
offline setting. Theoretical results show that our approach results in a nearly
monotonically improved reference policy. Extensive experiments on the D4RL
MuJoCo domain demonstrate that our proposed method achieves state-of-the-art
performance among the policy constraint methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Machine Learning's Added Value: Pareto Fronts in Atmospheric
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Beucler, Arthur Grundner, Sara Shamekh, Peter Ukkonen, Matthew Chantry, Ryan Lagerquist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the added value of machine learning (ML) for weather and climate
applications is measurable, explaining it remains challenging, especially for
large deep learning models. Inspired by climate model hierarchies, we propose
that a full hierarchy of Pareto-optimal models, defined within an appropriately
determined error-complexity plane, can guide model development and help
understand the models' added value. We demonstrate the use of Pareto fronts in
atmospheric physics through three sample applications, with hierarchies ranging
from semi-empirical models with minimal tunable parameters (simplest) to deep
learning algorithms (most complex). First, in cloud cover parameterization, we
find that neural networks identify nonlinear relationships between cloud cover
and its thermodynamic environment, and assimilate previously neglected features
such as vertical gradients in relative humidity that improve the representation
of low cloud cover. This added value is condensed into a ten-parameter equation
that rivals the performance of deep learning models. Second, we establish a ML
model hierarchy for emulating shortwave radiative transfer, distilling the
importance of bidirectional vertical connectivity for accurately representing
absorption and scattering, especially for multiple cloud layers. Third, we
emphasize the importance of convective organization information when modeling
the relationship between tropical precipitation and its surrounding
environment. We discuss the added value of temporal memory when high-resolution
spatial information is unavailable, with implications for precipitation
parameterization. Therefore, by comparing data-driven models directly with
existing schemes using Pareto optimality, we promote process understanding by
hierarchically unveiling system complexity, with the hope of improving the
trustworthiness of ML models in atmospheric applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures, submitted to AMS Artificial Intelligence for the
  Earth Systems (AIES)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPINEX-TimeSeries: Similarity-based Predictions with Explainable
  Neighbors Exploration for Time Series and Forecasting Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Z Naser, MZ Naser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new addition to the SPINEX (Similarity-based
Predictions with Explainable Neighbors Exploration) family, tailored
specifically for time series and forecasting analysis. This new algorithm
leverages the concept of similarity and higher-order temporal interactions
across multiple time scales to enhance predictive accuracy and interpretability
in forecasting. To evaluate the effectiveness of SPINEX, we present
comprehensive benchmarking experiments comparing it against 18 algorithms and
across 49 synthetic and real datasets characterized by varying trends,
seasonality, and noise levels. Our performance assessment focused on
forecasting accuracy and computational efficiency. Our findings reveal that
SPINEX consistently ranks among the top 5 performers in forecasting precision
and has a superior ability to handle complex temporal dynamics compared to
commonly adopted algorithms. Moreover, the algorithm's explainability features,
Pareto efficiency, and medium complexity (on the order of O(log n)) are
demonstrated through detailed visualizations to enhance the prediction and
decision-making process. We note that integrating similarity-based concepts
opens new avenues for research in predictive analytics, promising more accurate
and transparent decision making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ARVO: Atlas of Reproducible Vulnerabilities for Open Source Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Mei, Pulkit Singh Singaria, Jordi Del Castillo, Haoran Xi,  Abdelouahab,  Benchikh, Tiffany Bao, Ruoyu Wang, Yan Shoshitaishvili, Adam Doupé, Hammond Pearce, Brendan Dolan-Gavitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality datasets of real-world vulnerabilities are enormously valuable
for downstream research in software security, but existing datasets are
typically small, require extensive manual effort to update, and are missing
crucial features that such research needs. In this paper, we introduce ARVO: an
Atlas of Reproducible Vulnerabilities in Open-source software. By sourcing
vulnerabilities from C/C++ projects that Google's OSS-Fuzz discovered and
implementing a reliable re-compilation system, we successfully reproduce more
than 5,000 memory vulnerabilities across over 250 projects, each with a
triggering input, the canonical developer-written patch for fixing the
vulnerability, and the ability to automatically rebuild the project from source
and run it at its vulnerable and patched revisions. Moreover, our dataset can
be automatically updated as OSS-Fuzz finds new vulnerabilities, allowing it to
grow over time. We provide a thorough characterization of the ARVO dataset,
show that it can locate fixes more accurately than Google's own OSV
reproduction effort, and demonstrate its value for future research through two
case studies: firstly evaluating real-world LLM-based vulnerability repair, and
secondly identifying over 300 falsely patched (still-active) zero-day
vulnerabilities from projects improperly labeled by OSS-Fuzz.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Retrieval with Few-shot Indexing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arian Askari, Chuan Meng, Mohammad Aliannejadi, Zhaochun Ren, Evangelos Kanoulas, Suzan Verberne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing generative retrieval (GR) approaches rely on training-based
indexing, i.e., fine-tuning a model to memorise the associations between a
query and the document identifier (docid) of a relevant document.
Training-based indexing has three limitations: high training overhead,
under-utilization of the pre-trained knowledge of large language models (LLMs),
and challenges in adapting to a dynamic document corpus. To address the above
issues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).
It has a novel few-shot indexing process, where we prompt an LLM to generate
docids for all documents in a corpus, ultimately creating a docid bank for the
entire corpus. During retrieval, we feed a query to the same LLM and constrain
it to generate a docid within the docid bank created during indexing, and then
map the generated docid back to its corresponding document. Few-Shot GR relies
solely on prompting an LLM without requiring any training, making it more
efficient. Moreover, we devise few-shot indexing with one-to-many mapping to
further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves
superior performance to state-of-the-art GR methods that require heavy
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VidModEx: Interpretable and Efficient Black Box Model Extraction for
  High-Dimensional Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somnath Sendhil Kumar, Yuvaraj Govindarajulu, Pavan Kulkarni, Manojkumar Parmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of black-box model extraction, conventional methods reliant on
soft labels or surrogate datasets struggle with scaling to high-dimensional
input spaces and managing the complexity of an extensive array of interrelated
classes. In this work, we present a novel approach that utilizes SHAP (SHapley
Additive exPlanations) to enhance synthetic data generation. SHAP quantifies
the individual contributions of each input feature towards the victim model's
output, facilitating the optimization of an energy-based GAN towards a
desirable output. This method significantly boosts performance, achieving a
16.45% increase in the accuracy of image classification models and extending to
video classification models with an average improvement of 26.11% and a maximum
of 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics
600, and Something-Something V2. We further demonstrate the effectiveness and
practical utility of our method under various scenarios, including the
availability of top-k prediction probabilities, top-k prediction labels, and
top-1 labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Hijacking Attack in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Li, Siyuan Wu, Ruichuan Chen, Paarijaat Aditya, Istemi Ekin Akkus, Manohar Vanga, Min Zhang, Hao Li, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML), driven by prominent paradigms such as centralized and
federated learning, has made significant progress in various critical
applications ranging from autonomous driving to face recognition. However, its
remarkable success has been accompanied by various attacks. Recently, the model
hijacking attack has shown that ML models can be hijacked to execute tasks
different from their original tasks, which increases both accountability and
parasitic computational risks. Nevertheless, thus far, this attack has only
focused on centralized learning. In this work, we broaden the scope of this
attack to the federated learning domain, where multiple clients collaboratively
train a global model without sharing their data. Specifically, we present
HijackFL, the first-of-its-kind hijacking attack against the global model in
federated learning. The adversary aims to force the global model to perform a
different task (called hijacking task) from its original task without the
server or benign client noticing. To accomplish this, unlike existing methods
that use data poisoning to modify the target model's parameters, HijackFL
searches for pixel-level perturbations based on their local model (without
modifications) to align hijacking samples with the original ones in the feature
space. When performing the hijacking task, the adversary applies these cloaks
to the hijacking samples, compelling the global model to identify them as
original samples and predict them accordingly. We conduct extensive experiments
on four benchmark datasets and three popular models. Empirical results
demonstrate that its attack performance outperforms baselines. We further
investigate the factors that affect its performance and discuss possible
defenses to mitigate its impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FovEx: Human-inspired Explanations for Vision <span class="highlight-title">Transformer</span>s and
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahadev Prasad Panda, Matteo Tiezzi, Martina Vilas, Gemma Roig, Bjoern M. Eskofier, Dario Zanca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability in artificial intelligence (XAI) remains a crucial aspect for
fostering trust and understanding in machine learning models. Current visual
explanation techniques, such as gradient-based or class-activation-based
methods, often exhibit a strong dependence on specific model architectures.
Conversely, perturbation-based methods, despite being model-agnostic, are
computationally expensive as they require evaluating models on a large number
of forward passes. In this work, we introduce Foveation-based Explanations
(FovEx), a novel XAI method inspired by human vision. FovEx seamlessly
integrates biologically inspired perturbations by iteratively creating foveated
renderings of the image and combines them with gradient-based visual
explorations to determine locations of interest efficiently. These locations
are selected to maximize the performance of the model to be explained with
respect to the downstream task and then combined to generate an attribution
map. We provide a thorough evaluation with qualitative and quantitative
assessments on established benchmarks. Our method achieves state-of-the-art
performance on both transformers (on 4 out of 5 metrics) and convolutional
models (on 3 out of 5 metrics), demonstrating its versatility among various
architectures. Furthermore, we show the alignment between the explanation map
produced by FovEx and human gaze patterns (+14\% in NSS compared to RISE,
+203\% in NSS compared to GradCAM). This comparison enhances our confidence in
FovEx's ability to close the interpretation gap between humans and machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Value-Based Rationales Improve Social Experience: A Multiagent
  Simulation Study <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sz-Ting Tzeng, Nirav Ajmeri, Munindar P. Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Exanna, a framework to realize agents that incorporate values in
decision making. An Exannaagent considers the values of itself and others when
providing rationales for its actions and evaluating the rationales provided by
others. Via multiagent simulation, we demonstrate that considering values in
decision making and producing rationales, especially for norm-deviating
actions, leads to (1) higher conflict resolution, (2) better social experience,
(3) higher privacy, and (4) higher flexibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figures, 13 tables (and supplementary material with
  reproducibility and additional results), accepted at ECAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Advances in Multi-Choice Machine Reading Comprehension: A <span class="highlight-title">Survey</span>
  on Methods and <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shima Foolad, Kourosh Kiani, Razieh Rastgoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a thorough examination of recent developments in the
field of multi-choice Machine Reading Comprehension (MRC). Focused on benchmark
datasets, methodologies, challenges, and future trajectories, our goal is to
offer researchers a comprehensive overview of the current landscape in
multi-choice MRC. The analysis delves into 30 existing cloze-style and
multiple-choice MRC benchmark datasets, employing a refined classification
method based on attributes such as corpus style, domain, complexity, context
style, question style, and answer style. This classification system enhances
our understanding of each dataset's diverse attributes and categorizes them
based on their complexity. Furthermore, the paper categorizes recent
methodologies into Fine-tuned and Prompt-tuned methods. Fine-tuned methods
involve adapting pre-trained language models (PLMs) to a specific task through
retraining on domain-specific datasets, while prompt-tuned methods use prompts
to guide PLM response generation, presenting potential applications in
zero-shot or few-shot learning scenarios. By contributing to ongoing
discussions, inspiring future research directions, and fostering innovations,
this paper aims to propel multi-choice MRC towards new frontiers of
achievement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Deep Learning via Notions of Rank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Razin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the extreme popularity of deep learning in science and industry, its
formal understanding is limited. This thesis puts forth notions of rank as key
for developing a theory of deep learning, focusing on the fundamental aspects
of generalization and expressiveness. In particular, we establish that
gradient-based training can induce an implicit regularization towards low rank
for several neural network architectures, and demonstrate empirically that this
phenomenon may facilitate an explanation of generalization over natural data
(e.g., audio, images, and text). Then, we characterize the ability of graph
neural networks to model interactions via a notion of rank, which is commonly
used for quantifying entanglement in quantum physics. A central tool underlying
these results is a connection between neural networks and tensor
factorizations. Practical implications of our theory for designing explicit
regularization schemes and data preprocessing algorithms are presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-class Ride-hailing Service Subsidy System Utilizing Deep Causal
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Yu, Chi Xia, Shaosheng Cao, Lin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ride-hailing industry, subsidies are predominantly employed to
incentivize consumers to place more orders, thereby fostering market growth.
Causal inference techniques are employed to estimate the consumer elasticity
with different subsidy levels. However, the presence of confounding effects
poses challenges in achieving an unbiased estimate of the uplift effect. We
introduce a consumer subsidizing system to capture relationships between
subsidy propensity and the treatment effect, which proves effective while
maintaining a lightweight online environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedSyn: LLM-based Synthetic Medical Text Generation Framework <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Kumichev, Pavel Blinov, Yulia Kuzkina, Vasily Goncharov, Galina Zubkova, Nikolai Zenovkin, Aleksei Goncharov, Andrey Savchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating synthetic text addresses the challenge of data availability in
privacy-sensitive domains such as healthcare. This study explores the
applicability of synthetic data in real-world medical settings. We introduce
MedSyn, a novel medical text generation framework that integrates large
language models with a Medical Knowledge Graph (MKG). We use MKG to sample
prior medical information for the prompt and generate synthetic clinical notes
with GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data
through application in the ICD code prediction task. Our research indicates
that synthetic data can increase the classification accuracy of vital and
challenging codes by up to 17.8% compared to settings without synthetic data.
Furthermore, to provide new data for further research in the healthcare domain,
we present the largest open-source synthetic dataset of clinical notes for the
Russian language, comprising over 41k samples covering 219 ICD-10 codes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, accepted to ECML PKDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EOL: Transductive Few-Shot Open-Set Recognition by Enhancing Outlier
  Logits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Ochal, Massimiliano Patacchiola, Malik Boudiaf, Sen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Few-Shot Learning (FSL), models are trained to recognise unseen objects
from a query set, given a few labelled examples from a support set. In standard
FSL, models are evaluated on query instances sampled from the same class
distribution of the support set. In this work, we explore the more nuanced and
practical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard
FSL, OSFSL incorporates unknown classes into the query set, thereby requiring
the model not only to classify known classes but also to identify outliers.
Building on the groundwork laid by previous studies, we define a novel
transductive inference technique that leverages the InfoMax principle to
exploit the unlabelled query set. We called our approach the Enhanced Outlier
Logit (EOL) method. EOL refines class prototype representations through model
calibration, effectively balancing the inlier-outlier ratio. This calibration
enhances pseudo-label accuracy for the query set and improves the optimisation
objective within the transductive inference process. We provide a comprehensive
empirical evaluation demonstrating that EOL consistently surpasses traditional
methods, recording performance improvements ranging from approximately $+1.3%$
to $+6.3%$ across a variety of classification and outlier detection metrics and
benchmarks, even in the presence of inlier-outlier imbalance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recovering the state and dynamics of autonomous system with partial
  states solution using neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Kag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we explore the performance of deep hidden physics model (M.
Raissi 2018) for autonomous system, this systems do not explicitly depend on
time. The dynamics of states are dependent on states itself. Such systems can
be found in nature and have applications
  in modeling chemical concentrations, population dynamics, n-body problems in
physics etc. In this work we are going to see how we can obtain dynamics of
states based on solution of limited partial states. The proposed method can
find the state and dynamics of which the data is provided in the training,
although we do not claim to accurately find the solution of states whose data
is not utilized while training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNA-SE: Towards Deep Neural-Nets Assisted Semiparametric Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinshuo Liu, Zixin Wang, Xi-An Li, Xinyao Ji, Lei Zhang, Lin Liu, Zhonghua Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semiparametric statistics play a pivotal role in a wide range of domains,
including but not limited to missing data, causal inference, and transfer
learning, to name a few. In many settings, semiparametric theory leads to
(nearly) statistically optimal procedures that yet involve numerically solving
Fredholm integral equations of the second kind. Traditional numerical methods,
such as polynomial or spline approximations, are difficult to scale to
multi-dimensional problems. Alternatively, statisticians may choose to
approximate the original integral equations by ones with closed-form solutions,
resulting in computationally more efficient, but statistically suboptimal or
even incorrect procedures. To bridge this gap, we propose a novel framework by
formulating the semiparametric estimation problem as a bi-level optimization
problem; and then we develop a scalable algorithm called Deep Neural-Nets
Assisted Semiparametric Estimation (DNA-SE) by leveraging the universal
approximation property of Deep Neural-Nets (DNN) to streamline semiparametric
procedures. Through extensive numerical experiments and a real data analysis,
we demonstrate the numerical and statistical advantages of $\dnase$ over
traditional methods. To the best of our knowledge, we are the first to bring
DNN into semiparametric statistics as a numerical solver of integral equations
in our proposed general framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>semiparametric statistics, missing data, causal inference, Fredholm
  integral equations of the second kind, bi-level optimization, deep learning,
  AI for science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Human Action Recognition and Violence Detection Through Deep
  Learning Audiovisual Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooya Janani, Amirabolfazl Suratgar, Afshin Taghvaeipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a hybrid fusion-based deep learning approach based on two
different modalities, audio and video, to improve human activity recognition
and violence detection in public places. To take advantage of audiovisual
fusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning
(HFBDL) are used and compared. Since the objective is to detect and recognize
human violence in public places, Real-life violence situation (RLVS) dataset is
expanded and used. Simulating results of HFBDL show 96.67\% accuracy on
validation data, which is more accurate than the other state-of-the-art methods
on this dataset. To showcase our model's ability in real-world scenarios,
another dataset of 54 sounded videos of both violent and non-violent situations
was recorded. The model could successfully detect 52 out of 54 videos
correctly. The proposed method shows a promising performance on real scenarios.
Thus, it can be used for human action recognition and violence detection in
public places for security purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication, 10
  pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scenario-based Thermal Management Parametrization Through Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Rudolf, Philip Muhl, Sören Hohmann, Lutz Eckstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The thermal system of battery electric vehicles demands advanced control. Its
thermal management needs to effectively control active components across
varying operating conditions. While robust control function parametrization is
required, current methodologies show significant drawbacks. They consume
considerable time, human effort, and extensive real-world testing.
Consequently, there is a need for innovative and intelligent solutions that are
capable of autonomously parametrizing embedded controllers. Addressing this
issue, our paper introduces a learning-based tuning approach. We propose a
methodology that benefits from automated scenario generation for increased
robustness across vehicle usage scenarios. Our deep reinforcement learning
agent processes the tuning task context and incorporates an image-based
interpretation of embedded parameter sets. We demonstrate its applicability to
a valve controller parametrization task and verify it in real-world vehicle
testing. The results highlight the competitive performance to baseline methods.
This novel approach contributes to the shift towards virtual development of
thermal management functions, with promising potential of large-scale parameter
tuning in the automotive industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 2 tables, 1 algorithm, 10 equations, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Federated Learning on Heterogeneous and Long-Tailed Data
  via Expert Collaborative Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengling Lv, Xinyi Shang, Yang Zhou, Yiqun Zhang, Mengke Li, Yang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized Federated Learning (PFL) aims to acquire customized models for
each client without disclosing raw data by leveraging the collective knowledge
of distributed clients. However, the data collected in real-world scenarios is
likely to follow a long-tailed distribution. For example, in the medical
domain, it is more common for the number of general health notes to be much
larger than those specifically relatedto certain diseases. The presence of
long-tailed data can significantly degrade the performance of PFL models.
Additionally, due to the diverse environments in which each client operates,
data heterogeneity is also a classic challenge in federated learning. In this
paper, we explore the joint problem of global long-tailed distribution and data
heterogeneity in PFL and propose a method called Expert Collaborative Learning
(ECL) to tackle this problem. Specifically, each client has multiple experts,
and each expert has a different training subset, which ensures that each class,
especially the minority classes, receives sufficient training. Multiple experts
collaborate synergistically to produce the final prediction output. Without
special bells and whistles, the vanilla ECL outperforms other state-of-the-art
PFL methods on several benchmark datasets under different degrees of data
heterogeneity and long-tailed distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Representation Learning by Balanced Self Attention Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Shalam, Simon Korman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many leading self-supervised methods for unsupervised representation
learning, in particular those for embedding image features, are built on
variants of the instance discrimination task, whose optimization is known to be
prone to instabilities that can lead to feature collapse. Different techniques
have been devised to circumvent this issue, including the use of negative pairs
with different contrastive losses, the use of external memory banks, and
breaking of symmetry by using separate encoding networks with possibly
different structures. Our method, termed BAM, rather than directly matching
features of different views (augmentations) of input images, is based on
matching their self-attention vectors, which are the distributions of
similarities to the entire set of augmented images of a batch. We obtain rich
representations and avoid feature collapse by minimizing a loss that matches
these distributions to their globally balanced and entropy regularized version,
which is obtained through a simple self-optimal-transport computation. We
ablate and verify our method through a wide set of experiments that show
competitive performance with leading methods on both semi-supervised and
transfer-learning benchmarks. Our implementation and pre-trained models are
available at github.com/DanielShalam/BAM .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Automatic Hands-on-Keyboard Attack Detection Using LLMs in EDR
  Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Portnoy, Ehud Azikri, Shay Kels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endpoint Detection and Remediation (EDR) platforms are essential for
identifying and responding to cyber threats. This study presents a novel
approach using Large Language Models (LLMs) to detect Hands-on-Keyboard (HOK)
cyberattacks. Our method involves converting endpoint activity data into
narrative forms that LLMs can analyze to distinguish between normal operations
and potential HOK attacks. We address the challenges of interpreting endpoint
data by segmenting narratives into windows and employing a dual training
strategy. The results demonstrate that LLM-based models have the potential to
outperform traditional machine learning methods, offering a promising direction
for enhancing EDR capabilities and apply LLMs in cybersecurity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaWearS: A Shortcut in Wearable Systems Lifecycle with Only a Few
  Shots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Amirshahi, Maedeh H. Toosi, Siamak Mohammadi, Stefano Albini, Pasquale Davide Schiavone, Giovanni Ansaloni, Amir Aminifar, David Atienza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wearable systems provide continuous health monitoring and can lead to early
detection of potential health issues. However, the lifecycle of wearable
systems faces several challenges. First, effective model training for new
wearable devices requires substantial labeled data from various subjects
collected directly by the wearable. Second, subsequent model updates require
further extensive labeled data for retraining. Finally, frequent model updating
on the wearable device can decrease the battery life in long-term data
monitoring. Addressing these challenges, in this paper, we propose MetaWearS, a
meta-learning method to reduce the amount of initial data collection required.
Moreover, our approach incorporates a prototypical updating mechanism,
simplifying the update process by modifying the class prototype rather than
retraining the entire model. We explore the performance of MetaWearS in two
case studies, namely, the detection of epileptic seizures and the detection of
atrial fibrillation. We show that by fine-tuning with just a few samples, we
achieve 70% and 82% AUC for the detection of epileptic seizures and the
detection of atrial fibrillation, respectively. Compared to a conventional
approach, our proposed method performs better with up to 45% AUC. Furthermore,
updating the model with only 16 minutes of additional labeled data increases
the AUC by up to 5.3%. Finally, MetaWearS reduces the energy consumption for
model updates by 456x and 418x for epileptic seizure and AF detection,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiview learning with twin parametric margin SVM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Quadir, M. Tanveer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiview learning (MVL) seeks to leverage the benefits of diverse
perspectives to complement each other, effectively extracting and utilizing the
latent information within the dataset. Several twin support vector
machine-based MVL (MvTSVM) models have been introduced and demonstrated
outstanding performance in various learning tasks. However, MvTSVM-based models
face significant challenges in the form of computational complexity due to four
matrix inversions, the need to reformulate optimization problems in order to
employ kernel-generated surfaces for handling non-linear cases, and the
constraint of uniform noise assumption in the training data. Particularly in
cases where the data possesses a heteroscedastic error structure, these
challenges become even more pronounced. In view of the aforementioned
challenges, we propose multiview twin parametric margin support vector machine
(MvTPMSVM). MvTPMSVM constructs parametric hyperplanes with the goal of
maximizing the parametric margin between two classes, aiming to regulate and
manage the impact of the heteroscedastic noise structure existing within the
data. The proposed MvTPMSVM model avoids the explicit computation of matrix
inversions in the dual formulation, leading to enhanced computational
efficiency. We perform an extensive assessment of the MvTPMSVM model using
benchmark datasets such as UCI, KEEL, synthetic, and Animals with Attributes
(AwA). Our experimental results, coupled with rigorous statistical analyses,
confirm the superior generalization capabilities of the proposed MvTPMSVM model
compared to the baseline models. The source code of the proposed MvTPMSVM model
is available at \url{https://github.com/mtanveer1/MvTPMSVM}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shaping Rewards, Shaping Routes: On Multi-Agent Deep Q-Networks for
  Routing in Satellite Constellation Networks <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel M. H. Roth, Anupama Hegde, Thomas Delamotte, Andreas Knopp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective routing in satellite mega-constellations has become crucial to
facilitate the handling of increasing traffic loads, more complex network
architectures, as well as the integration into 6G networks. To enhance
adaptability as well as robustness to unpredictable traffic demands, and to
solve dynamic routing environments efficiently, machine learning-based
solutions are being considered. For network control problems, such as
optimizing packet forwarding decisions according to Quality of Service
requirements and maintaining network stability, deep reinforcement learning
techniques have demonstrated promising results. For this reason, we investigate
the viability of multi-agent deep Q-networks for routing in satellite
constellation networks. We focus specifically on reward shaping and quantifying
training convergence for joint optimization of latency and load balancing in
static and dynamic scenarios. To address identified drawbacks, we propose a
novel hybrid solution based on centralized learning and decentralized control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, to be published in proceedings of European Space
  Agency SPAICE Conference 2024, https://spaice.esa.int/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukinari Hisaki, Isao Ono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an off-policy deep reinforcement learning (DRL)
method utilizing the average reward criterion. While most existing DRL methods
employ the discounted reward criterion, this can potentially lead to a
discrepancy between the training objective and performance metrics in
continuing tasks, making the average reward criterion a recommended
alternative. We introduce RVI-SAC, an extension of the state-of-the-art
off-policy DRL method, Soft Actor-Critic (SAC), to the average reward
criterion. Our proposal consists of (1) Critic updates based on RVI Q-learning,
(2) Actor updates introduced by the average reward soft policy improvement
theorem, and (3) automatic adjustment of Reset Cost enabling the average reward
reinforcement learning to be applied to tasks with termination. We apply our
method to the Gymnasium's Mujoco tasks, a subset of locomotion tasks, and
demonstrate that RVI-SAC shows competitive performance compared to existing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2024; Code:
  https://github.com/yhisaki/average-reward-drl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A multi-task deep learning approach for lane-level pavement performance
  prediction with segment-level data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Wang, Wenbo Zhang, Yunpeng LI
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The elaborate pavement performance prediction is an important premise of
implementing preventive maintenance. Our survey reveals that in practice, the
pavement performance is usually measured at segment-level, where an unique
performance value is obtained for all lanes within one segment of 1km length.
It still lacks more elaborate performance analysis at lane-level due to costly
data collection and difficulty in prediction modeling. Therefore, this study
developed a multi-task deep learning approach to predict the lane-level
pavement performance with a large amount of historical segment-level
performance measurement data. The unified prediction framework can effectively
address inherent correlation and differences across lanes. In specific, the
prediction framework firstly employed an Long Short-Term Memory (LSTM) layer to
capture the segment-level pavement deterioration pattern. Then multiple
task-specific LSTM layers were designed based on number of lanes to capture
lane-level differences in pavement performance. Finally, we concatenated
multiple task-specific LSTM outputs with auxiliary features for further
training and obtained the lane-level predictions after fully connected layer.
The aforementioned prediction framework was validated with a real case in
China. It revealed a better model performance regardless of one-way 2-lane,
3-lane, and 4-lane scenarios, all lower than 10% in terms of mean absolute
percentage error. The proposed prediction framework also outperforms other
ensemble learning and shallow machine learning methods in almost every lane.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Top K Enhanced Reinforcement Learning Attacks on Heterogeneous Graph
  Node Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglin Gao, Gaoxi Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have attracted substantial interest due to their
exceptional performance on graph-based data. However, their robustness,
especially on heterogeneous graphs, remains underexplored, particularly against
adversarial attacks. This paper proposes HeteroKRLAttack, a targeted evasion
black-box attack method for heterogeneous graphs. By integrating reinforcement
learning with a Top-K algorithm to reduce the action space, our method
efficiently identifies effective attack strategies to disrupt node
classification tasks. We validate the effectiveness of HeteroKRLAttack through
experiments on multiple heterogeneous graph datasets, showing significant
reductions in classification accuracy compared to baseline methods. An ablation
study underscores the critical role of the Top-K algorithm in enhancing attack
performance. Our findings highlight potential vulnerabilities in current models
and provide guidance for future defense strategies against adversarial attacks
on heterogeneous graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Wolfe, Aayushi Dangol, Bill Howe, Alexis Hiniker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular and news media often portray teenagers with sensationalism, as both a
risk to society and at risk from society. As AI begins to absorb some of the
epistemic functions of traditional media, we study how teenagers in two
countries speaking two languages: 1) are depicted by AI, and 2) how they would
prefer to be depicted. Specifically, we study the biases about teenagers
learned by static word embeddings (SWEs) and generative language models (GLMs),
comparing these with the perspectives of adolescents living in the U.S. and
Nepal. We find English-language SWEs associate teenagers with societal
problems, and more than 50% of the 1,000 words most associated with teenagers
in the pretrained GloVe SWE reflect such problems. Given prompts about
teenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss
societal problems, most commonly violence, but also drug use, mental illness,
and sexual taboo. Nepali models, while not free of such associations, are less
dominated by social problems. Data from workshops with N=13 U.S. adolescents
and N=18 Nepalese adolescents show that AI presentations are disconnected from
teenage life, which revolves around activities like school and friendship.
Participant ratings of how well 20 trait words describe teens are decorrelated
from SWE associations, with Pearson's r=.02, n.s. in English FastText and
r=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in
GloVe. U.S. participants suggested AI could fairly present teens by
highlighting diversity, while Nepalese participants centered positivity.
Participants were optimistic that, if it learned from adolescents, rather than
media sources, AI could help mitigate stereotypes. Our work offers an
understanding of the ways SWEs and GLMs misrepresent a developmentally
vulnerable group and provides a template for less sensationalized
characterization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Artificial Intelligence, Ethics, and Society 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> Scale and Societal Consistency Mediate Facial Impression Bias in
  Vision-Language AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Wolfe, Aayushi Dangol, Alexis Hiniker, Bill Howe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal AI models capable of associating images and text hold promise for
numerous domains, ranging from automated image captioning to accessibility
applications for blind and low-vision users. However, uncertainty about bias
has in some cases limited their adoption and availability. In the present work,
we study 43 CLIP vision-language models to determine whether they learn
human-like facial impression biases, and we find evidence that such biases are
reflected across three distinct CLIP model families. We show for the first time
that the the degree to which a bias is shared across a society predicts the
degree to which it is reflected in a CLIP model. Human-like impressions of
visually unobservable attributes, like trustworthiness and sexuality, emerge
only in models trained on the largest dataset, indicating that a better fit to
uncurated cultural data results in the reproduction of increasingly subtle
social biases. Moreover, we use a hierarchical clustering approach to show that
dataset size predicts the extent to which the underlying structure of facial
impression bias resembles that of facial impression bias in humans. Finally, we
show that Stable Diffusion models employing CLIP as a text encoder learn facial
impression biases, and that these biases intersect with racial biases in Stable
Diffusion XL-Turbo. While pretrained CLIP models may prove useful for
scientific studies of bias, they will also require significant dataset curation
when intended for use as general-purpose models in a zero-shot setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Artificial Intelligence, Ethics, and Society 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Chen, Chenrui Tie, Ruihai Wu, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans perceive and interact with the world with the awareness of
equivariance, facilitating us in manipulating different objects in diverse
poses. For robotic manipulation, such equivariance also exists in many
scenarios. For example, no matter what the pose of a drawer is (translation,
rotation and tilt), the manipulation strategy is consistent (grasp the handle
and pull in a line). While traditional models usually do not have the awareness
of equivariance for robotic manipulation, which might result in more data for
training and poor performance in novel object poses, we propose our EqvAfford
framework, with novel designs to guarantee the equivariance in point-level
affordance learning for downstream robotic manipulation, with great performance
and generalization ability on representative tasks on objects in diverse poses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Evaluating Privacy-Utility Trade-off in Vertical
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03885v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03885v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Kang, Jiahuan Luo, Yuanqin He, Xiaojin Zhang, Lixin Fan, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has emerged as a practical solution to tackle data
silo issues without compromising user privacy. One of its variants, vertical
federated learning (VFL), has recently gained increasing attention as the VFL
matches the enterprises' demands of leveraging more valuable features to build
better machine learning models while preserving user privacy. Current works in
VFL concentrate on developing a specific protection or attack mechanism for a
particular VFL algorithm. In this work, we propose an evaluation framework that
formulates the privacy-utility evaluation problem. We then use this framework
as a guide to comprehensively evaluate a broad range of protection mechanisms
against most of the state-of-the-art privacy attacks for three widely deployed
VFL algorithms. These evaluations may help FL practitioners select appropriate
protection mechanisms given specific requirements. Our evaluation results
demonstrate that: the model inversion and most of the label inference attacks
can be thwarted by existing protection mechanisms; the model completion (MC)
attack is difficult to be prevented, which calls for more advanced MC-targeted
protection mechanisms. Based on our evaluation results, we offer concrete
advice on improving the privacy-preserving capability of VFL systems. The code
is available at https://github.com/yankang18/Attack-Defense-VFL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Principal Component Regression with Applications to Panel Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01357v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01357v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Agarwal, Keegan Harris, Justin Whitehouse, Zhiwei Steven Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal component regression (PCR) is a popular technique for fixed-design
error-in-variables regression, a generalization of the linear regression
setting in which the observed covariates are corrupted with random noise. We
provide the first time-uniform finite sample guarantees for (regularized) PCR
whenever data is collected adaptively. Since the proof techniques for analyzing
PCR in the fixed design setting do not readily extend to the online setting,
our results rely on adapting tools from modern martingale concentration to the
error-in-variables setting. We demonstrate the usefulness of our bounds by
applying them to the domain of panel data, a ubiquitous setting in econometrics
and statistics. As our first application, we provide a framework for experiment
design in panel data settings when interventions are assigned adaptively. Our
framework may be thought of as a generalization of the synthetic control and
synthetic interventions frameworks, where data is collected via an adaptive
intervention assignment policy. Our second application is a procedure for
learning such an intervention assignment policy in a setting where units arrive
sequentially to be treated. In addition to providing theoretical performance
guarantees (as measured by regret), we show that our method empirically
outperforms a baseline which does not leverage error-in-variables regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShieldGemma: Generative AI Content Moderation Based on Gemma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, Olivia Sturman, Oscar Wahltinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ShieldGemma, a comprehensive suite of LLM-based safety content
moderation models built upon Gemma2. These models provide robust,
state-of-the-art predictions of safety risks across key harm types (sexually
explicit, dangerous content, harassment, hate speech) in both user input and
LLM-generated output. By evaluating on both public and internal benchmarks, we
demonstrate superior performance compared to existing models, such as Llama
Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%).
Additionally, we present a novel LLM-based data curation pipeline, adaptable to
a variety of safety-related tasks and beyond. We have shown strong
generalization performance for model trained mainly on synthetic data. By
releasing ShieldGemma, we provide a valuable resource to the research
community, advancing LLM safety and enabling the creation of more effective
content moderation solutions for developers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-based subsurface multiphysics monitoring and forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinquan Huang, Fu Wang, Tariq Alkhalifah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Carbon capture and storage (CCS) plays a crucial role in mitigating
greenhouse gas emissions, particularly from industrial outputs. Using seismic
monitoring can aid in an accurate and robust monitoring system to ensure the
effectiveness of CCS and mitigate associated risks. However, conventional
seismic wave equation-based approaches are computationally demanding, which
hinders real-time applications. In addition to efficiency, forecasting and
uncertainty analysis are not easy to handle using such
numerical-simulation-based approaches. To this end, we propose a novel
subsurface multiphysics monitoring and forecasting framework utilizing video
diffusion models. This approach can generate high-quality representations of
CO$2$ evolution and associated changes in subsurface elastic properties. With
reconstruction guidance, forecasting and inversion can be achieved conditioned
on historical frames and/or observational data. Meanwhile, due to the
generative nature of the approach, we can quantify uncertainty in the
prediction. Tests based on the Compass model show that the proposed method
successfully captured the inherently complex physical phenomena associated with
CO$_2$ monitoring, and it can predict and invert the subsurface elastic
properties and CO$_2$ saturation with consistency in their evolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Markov Decision Processes under External Temporal Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranga Shaarad Ayyagari, Ambedkar Dukkipati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most reinforcement learning algorithms treat the context under which they
operate as a stationary, isolated, and undisturbed environment. However, in
real world applications, environments constantly change due to a variety of
external events. To address this problem, we study Markov Decision Processes
(MDP) under the influence of an external temporal process. We formalize this
notion and discuss conditions under which the problem becomes tractable with
suitable solutions. We propose a policy iteration algorithm to solve this
problem and theoretically analyze its performance. We derive results on the
sample complexity of the algorithm and study its dependency on the extent of
non-stationarity of the environment. We then conduct experiments to illustrate
our results in a classic control environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-clairvoyant Scheduling with Partial Predictions <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyad Benomar, Vianney Perchet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The non-clairvoyant scheduling problem has gained new interest within
learning-augmented algorithms, where the decision-maker is equipped with
predictions without any quality guarantees. In practical settings, access to
predictions may be reduced to specific instances, due to cost or data
limitations. Our investigation focuses on scenarios where predictions for only
$B$ job sizes out of $n$ are available to the algorithm. We first establish
near-optimal lower bounds and algorithms in the case of perfect predictions.
Subsequently, we present a learning-augmented algorithm satisfying the
robustness, consistency, and smoothness criteria, and revealing a novel
tradeoff between consistency and smoothness inherent in the scenario with a
restricted number of predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Randomization Can Reduce Both Bias and Variance: A Case Study in Random
  Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Liu, Rahul Mazumder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the often overlooked phenomenon, first noted in
\cite{breiman2001random}, that random forests appear to reduce bias compared to
bagging. Motivated by an interesting paper by \cite{mentch2020randomization},
where the authors argue that random forests reduce effective degrees of freedom
and only outperform bagging ensembles in low signal-to-noise ratio (SNR)
settings, we explore how random forests can uncover patterns in the data missed
by bagging. We empirically demonstrate that in the presence of such patterns,
random forests reduce bias along with variance and increasingly outperform
bagging ensembles when SNR is high. Our observations offer insights into the
real-world success of random forests across a range of SNRs and enhance our
understanding of the difference between random forests and bagging ensembles
with respect to the randomization injected into each split. Our investigations
also yield practical insights into the importance of tuning $mtry$ in random
forests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learnability of Parameter-Bounded Bayes Nets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Bhattacharyya, Davin Choo, Sutanu Gayen, Dimitrios Myrisiotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayes nets are extensively used in practice to efficiently represent joint
probability distributions over a set of random variables and capture dependency
relations. In a seminal paper, Chickering et al. (JMLR 2004) showed that given
a distribution $\mathbb{P}$, that is defined as the marginal distribution of a
Bayes net, it is $\mathsf{NP}$-hard to decide whether there is a
parameter-bounded Bayes net that represents $\mathbb{P}$. They called this
problem LEARN. In this work, we extend the $\mathsf{NP}$-hardness result of
LEARN and prove the $\mathsf{NP}$-hardness of a promise search variant of
LEARN, whereby the Bayes net in question is guaranteed to exist and one is
asked to find such a Bayes net. We complement our hardness result with a
positive result about the sample complexity that is sufficient to recover a
parameter-bounded Bayes net that is close (in TV distance) to a given
distribution $\mathbb{P}$, that is represented by some parameter-bounded Bayes
net, generalizing a degree-bounded sample complexity result of Brustle et al.
(EC 2020).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Novel <span class="highlight-title">GPT</span>-4 APIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kellin Pelrine, Mohammad Taufeeque, Michał Zając, Euan McLean, Adam Gleave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model attacks typically assume one of two extreme threat models:
full white-box access to model weights, or black-box access limited to a text
generation API. However, real-world APIs are often more flexible than just text
generation: these APIs expose "gray-box" access leading to new threat vectors.
To explore this, we red-team three new functionalities exposed in the GPT-4
APIs: fine-tuning, function calling and knowledge retrieval. We find that
fine-tuning a model on as few as 15 harmful examples or 100 benign examples can
remove core safeguards from GPT-4, enabling a range of harmful outputs.
Furthermore, we find that GPT-4 Assistants readily divulge the function call
schema and can be made to execute arbitrary function calls. Finally, we find
that knowledge retrieval can be hijacked by injecting instructions into
retrieval documents. These vulnerabilities highlight that any additions to the
functionality exposed by an API can create new vulnerabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mode-wise Principal Subspace Pursuit and Matrix Spiked Covariance Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00575v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00575v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runshi Tang, Ming Yuan, Anru R. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel framework called Mode-wise Principal Subspace
Pursuit (MOP-UP) to extract hidden variations in both the row and column
dimensions for matrix data. To enhance the understanding of the framework, we
introduce a class of matrix-variate spiked covariance models that serve as
inspiration for the development of the MOP-UP algorithm. The MOP-UP algorithm
consists of two steps: Average Subspace Capture (ASC) and Alternating
Projection (AP). These steps are specifically designed to capture the row-wise
and column-wise dimension-reduced subspaces which contain the most informative
features of the data. ASC utilizes a novel average projection operator as
initialization and achieves exact recovery in the noiseless setting. We analyze
the convergence and non-asymptotic error bounds of MOP-UP, introducing a
blockwise matrix eigenvalue perturbation bound that proves the desired bound,
where classic perturbation bounds fail. The effectiveness and practical merits
of the proposed framework are demonstrated through experiments on both
simulated and real datasets. Lastly, we discuss generalizations of our approach
to higher-order data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal of the Royal Statistical Society, Series B, to appear</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards smaller, faster decoder-only <span class="highlight-title">transformer</span>s: Architectural
  variants and their implications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14462v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14462v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sathya Krishnan Suresh, Shunmugapriya P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, the research on Large Language Models (LLMs) has grown
exponentially, predominantly focusing on models underpinned by the transformer
architecture, as established by [1], and further developed through the
decoder-only variations by [2]. Contemporary efforts in this field primarily
aim to enhance model capabilities by scaling up both the architecture and data
volumes utilized during training. However, the exploration into reduce these
model sizes while preserving their efficacy remains scant. In this study, we
introduce three modifications to the decoder-only transformer architecture,
namely ParallelGPT (pgpt), LinearGPT (lgpt), and ConvGPT (cgpt). These variants
demonstrate comparable performance to the conventional architecture in language
generation, yet benefit from reduced model sizes and faster training processes.
We open-source the model weights and the complete codebase for these
implementation for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decentralized Intelligence Network (DIN) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham Nash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized Intelligence Network (DIN) is a theoretical framework
addressing data fragmentation and siloing challenges, enabling scalable AI
through data sovereignty. It facilitates effective AI utilization within
sovereign networks by overcoming barriers to accessing diverse data sources,
leveraging: 1) personal data stores to ensure data sovereignty, where data
remains securely within Participants' control; 2) a scalable federated learning
protocol implemented on a public blockchain for decentralized AI training,
where only model parameter updates are shared, keeping data within the personal
data stores; and 3) a scalable, trustless cryptographic rewards mechanism on a
public blockchain to incentivize participation and ensure fair reward
distribution through a decentralized auditing protocol. This approach
guarantees that no entity can prevent or control access to training data or
influence financial benefits, as coordination and reward distribution are
managed on the public blockchain with an immutable record. The framework
supports effective AI training by allowing Participants to maintain control
over their data, benefit financially, and contribute to a decentralized,
scalable ecosystem that leverages collective AI to develop beneficial
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-sample Variational Inference of Bayesian Neural Networks with
  Arbitrary Nonlinearities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02063v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02063v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David J. Schodt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Neural Networks (BNNs) extend traditional neural networks to provide
uncertainties associated with their outputs. On the forward pass through a BNN,
predictions (and their uncertainties) are made either by Monte Carlo sampling
network weights from the learned posterior or by analytically propagating
statistical moments through the network. Though flexible, Monte Carlo sampling
is computationally expensive and can be infeasible or impractical under
resource constraints or for large networks. While moment propagation can
ameliorate the computational costs of BNN inference, it can be difficult or
impossible for networks with arbitrary nonlinearities, thereby restricting the
possible set of network layers permitted with such a scheme. In this work, we
demonstrate a simple yet effective approach for propagating statistical moments
through arbitrary nonlinearities with only 3 deterministic samples, enabling
few-sample variational inference of BNNs without restricting the set of network
layers used. Furthermore, we leverage this approach to demonstrate a novel
nonlinear activation function that we use to inject physics-informed prior
information into output nodes of a BNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comment 1: Fixed plot markers in figure 6 to match legend and to
  improve grayscale appearance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Evolutional Instant Interest Network for CTR Prediction in
  Trigger-Induced Recommendation <span class="chip">WSDM'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07769v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07769v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibo Xiao, Luwei Yang, Tao Zhang, Wen Jiang, Wei Ning, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recommendation has been playing a key role in many industries, e.g.,
e-commerce, streaming media, social media, etc. Recently, a new recommendation
scenario, called Trigger-Induced Recommendation (TIR), where users are able to
explicitly express their instant interests via trigger items, is emerging as an
essential role in many e-commerce platforms, e.g., Alibaba.com and Amazon.
Without explicitly modeling the user's instant interest, traditional
recommendation methods usually obtain sub-optimal results in TIR. Even though
there are a few methods considering the trigger and target items simultaneously
to solve this problem, they still haven't taken into account temporal
information of user behaviors, the dynamic change of user instant interest when
the user scrolls down and the interactions between the trigger and target
items. To tackle these problems, we propose a novel method -- Deep Evolutional
Instant Interest Network (DEI2N), for click-through rate prediction in TIR
scenarios. Specifically, we design a User Instant Interest Modeling Layer to
predict the dynamic change of the intensity of instant interest when the user
scrolls down. Temporal information is utilized in user behavior modeling.
Moreover, an Interaction Layer is introduced to learn better interactions
between the trigger and target items. We evaluate our method on several offline
and real-world industrial datasets. Experimental results show that our proposed
DEI2N outperforms state-of-the-art baselines. In addition, online A/B testing
demonstrates the superiority over the existing baseline in real-world
production environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, accepted by the 17th ACM International Conference
  on Web Search and Data Mining(WSDM'2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large AI Model Empowered Multimodal Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feibo Jiang, Li Dong, Yubo Peng, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal signals, including text, audio, image, and video, can be
integrated into Semantic Communication (SC) systems to provide an immersive
experience with low latency and high quality at the semantic level. However,
the multimodal SC has several challenges, including data heterogeneity,
semantic ambiguity, and signal distortion during transmission. Recent
advancements in large AI models, particularly in the Multimodal Language Model
(MLM) and Large Language Model (LLM), offer potential solutions for addressing
these issues. To this end, we propose a Large AI Model-based Multimodal SC
(LAM-MSC) framework, where we first present the MLM-based Multimodal Alignment
(MMA) that utilizes the MLM to enable the transformation between multimodal and
unimodal data while preserving semantic consistency. Then, a personalized
LLM-based Knowledge Base (LKB) is proposed, which allows users to perform
personalized semantic extraction or recovery through the LLM. This effectively
addresses the semantic ambiguity. Finally, we apply the Conditional Generative
adversarial network-based channel Estimation (CGE) for estimating the wireless
channel state information. This approach effectively mitigates the impact of
fading channels in SC. Finally, we conduct simulations that demonstrate the
superior performance of the LAM-MSC framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE CM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Survival Analysis with Adversarial Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16019v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16019v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Potter, Stefano Maxenti, Michael Everett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Survival Analysis (SA) models the time until an event occurs, with
applications in fields like medicine, defense, finance, and aerospace. Recent
work shows that Neural Networks (NNs) can capture complex relationships in SA.
However, dataset uncertainties (e.g., noisy measurements, human error) can
degrade model performance. To address this, we leverage NN verification
advances to create algorithms for robust, fully-parametric survival models. We
introduce a robust loss function and use CROWN-IBP regularization to handle
computational challenges in the Min-Max problem. Evaluating our approach on
SurvSet datasets, we find that our Survival Analysis with Adversarial
Regularization (SAWAR) method consistently outperforms baselines under various
perturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier
Score (IBS), and Concordance Index (CI). This demonstrates that adversarial
regularization enhances SA performance and calibration, mitigating data
uncertainty and improving generalization across diverse datasets up to 150%
across all perturbation magnitudes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, submission to IEEE Journal of Biomedical and
  Health Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Look at Value-Based Decision-Time vs. Background Planning Methods
  Across Different Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safa Alver, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In model-based reinforcement learning (RL), an agent can leverage a learned
model to improve its way of behaving in different ways. Two of the prevalent
ways to do this are through decision-time and background planning methods. In
this study, we are interested in understanding how the value-based versions of
these two planning methods will compare against each other across different
settings. Towards this goal, we first consider the simplest instantiations of
value-based decision-time and background planning methods and provide
theoretical results on which one will perform better in the regular RL and
transfer learning settings. Then, we consider the modern instantiations of them
and provide hypotheses on which one will perform better in the same settings.
Finally, we perform illustrative experiments to validate these theoretical
results and hypotheses. Overall, our findings suggest that even though
value-based versions of the two planning methods perform on par in their
simplest instantiations, the modern instantiations of value-based decision-time
planning methods can perform on par or better than the modern instantiations of
value-based background planning methods in both the regular RL and transfer
learning settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EWRL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Classification and Rejection: A One-versus-All Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifying patterns of known classes and rejecting ambiguous and novel (also
called as out-of-distribution (OOD)) inputs are involved in open world pattern
recognition. Deep neural network models usually excel in closed-set
classification while performs poorly in rejecting OOD inputs. To tackle this
problem, numerous methods have been designed to perform open set recognition
(OSR) or OOD rejection/detection tasks. Previous methods mostly take
post-training score transformation or hybrid models to ensure low scores on OOD
inputs while separating known classes. In this paper, we attempt to build a
unified framework for building open set classifiers for both classification and
OOD rejection. We formulate the open set recognition of $ K $-known-class as a
$ (K+1) $-class classification problem with model trained on known-class
samples only. By decomposing the $ K $-class problem into $ K $ one-versus-all
(OVA) binary classification tasks and binding some parameters, we show that
combining the scores of OVA classifiers can give $ (K+1) $-class posterior
probabilities, which enables classification and OOD rejection in a unified
framework. To maintain the closed-set classification accuracy of the OVA
trained classifier, we propose a hybrid training strategy combining OVA loss
and multi-class cross-entropy loss. We implement the OVA framework and hybrid
training strategy on the recently proposed convolutional prototype network and
prototype classifier on vision transformer (ViT) backbone. Experiments on
popular OSR and OOD detection datasets demonstrate that the proposed framework,
using a single multi-class classifier, yields competitive performance in
closed-set classification, OOD detection, and misclassification detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Machine Intelligence Research
  (https://link.springer.com/article/10.1007/s11633-024-1514-4)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PP-TIL: Personalized Planning for Autonomous Driving with Instance-based
  Transfer Imitation Learning <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18569v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18569v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangze Lin, Ying He, Fei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized motion planning holds significant importance within urban
automated driving, catering to the unique requirements of individual users.
Nevertheless, prior endeavors have frequently encountered difficulties in
simultaneously addressing two crucial aspects: personalized planning within
intricate urban settings and enhancing planning performance through data
utilization. The challenge arises from the expensive and limited nature of user
data, coupled with the scene state space tending towards infinity. These
factors contribute to overfitting and poor generalization problems during model
training. Henceforth, we propose an instance-based transfer imitation learning
approach. This method facilitates knowledge transfer from extensive expert
domain data to the user domain, presenting a fundamental resolution to these
issues. We initially train a pre-trained model using large-scale expert data.
Subsequently, during the fine-tuning phase, we feed the batch data, which
comprises expert and user data. Employing the inverse reinforcement learning
technique, we extract the style feature distribution from user demonstrations,
constructing the regularization term for the approximation of user style. In
our experiments, we conducted extensive evaluations of the proposed method.
Compared to the baseline methods, our approach mitigates the overfitting issue
caused by sparse user data. Furthermore, we discovered that integrating the
driving model with a differentiable nonlinear optimizer as a safety protection
layer for end-to-end personalized fine-tuning results in superior planning
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IROS 2024 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Review</span> on Knowledge Graphs for Healthcare: Resources, Applications,
  and Promises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04802v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04802v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carl Yang, Hejie Cui, Jiaying Lu, Shiyu Wang, Ran Xu, Wenjing Ma, Yue Yu, Shaojun Yu, Xuan Kan, Chen Ling, Tianfan Fu, Liang Zhao, Joyce Ho, Fei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Healthcare knowledge graphs (HKGs) are valuable tools for organizing
biomedical concepts and their relationships with interpretable structures. The
recent advent of large language models (LLMs) has paved the way for building
more comprehensive and accurate HKGs. This, in turn, can improve the
reliability of generated content and enable better evaluation of LLMs. However,
the challenges of HKGs such as regarding data heterogeneity and limited
coverage are not fully understood, highlighting the need for detailed reviews.
This work provides the first comprehensive review of HKGs. It summarizes the
pipeline and key techniques for HKG construction, as well as the common
utilization approaches, i.e., model-free and model-based. The existing HKG
resources are also organized based on the data types they capture and
application domains they cover, along with relevant statistical information
(Resource available at
https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase). At the
application level, we delve into the successful integration of HKGs across
various health domains, ranging from fine-grained basic science research to
high-level clinical decision support and public health. Lastly, the paper
highlights the opportunities for HKGs in the era of LLMs. This work aims to
serve as a valuable resource for understanding the potential and opportunities
of HKG in health research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, preprint submitted to ACM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Where Do We Go from Here? Multi-scale Allocentric Relational Inference
  from Natural Spatial Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzuf Paz-Argaman, Sayali Kulkarni, John Palowitch, Jason Baldridge, Reut Tsarfaty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When communicating routes in natural language, the concept of acquired
spatial knowledge is crucial for geographic information retrieval (GIR) and in
spatial cognitive research. However, NLP navigation studies often overlook the
impact of such acquired knowledge on textual descriptions. Current navigation
studies concentrate on egocentric local descriptions (e.g., `it will be on your
right') that require reasoning over the agent's local perception. These
instructions are typically given as a sequence of steps, with each action-step
explicitly mentioning and being followed by a landmark that the agent can use
to verify they are on the right path (e.g., `turn right and then you will
see...'). In contrast, descriptions based on knowledge acquired through a map
provide a complete view of the environment and capture its overall structure.
These instructions (e.g., `it is south of Central Park and a block north of a
police station') are typically non-sequential, contain allocentric relations,
with multiple spatial relations and implicit actions, without any explicit
verification. This paper introduces the Rendezvous (RVS) task and dataset,
which includes 10,404 examples of English geospatial instructions for reaching
a target location using map-knowledge. Our analysis reveals that RVS exhibits a
richer use of spatial allocentric relations, and requires resolving more
spatial relations simultaneously compared to previous text-based navigation
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time
  Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10768v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10768v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        GaoXiang Zhao, Li Zhou, XiaoQiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long time series forecasting aims to utilize historical information to
forecast future states over extended horizons. Traditional RNN-based series
forecasting methods struggle to effectively address long-term dependencies and
gradient issues in long time series problems. Recently, SegRNN has emerged as a
leading RNN-based model tailored for long-term series forecasting,
demonstrating state-of-the-art performance while maintaining a streamlined
architecture through innovative segmentation and parallel decoding techniques.
Nevertheless, SegRNN has several limitations: its fixed segmentation disrupts
data continuity and fails to effectively leverage information across different
segments, the segmentation strategy employed by SegRNN does not fundamentally
address the issue of information loss within the recurrent structure. To
address these issues, we propose the ISMRNN method with three key enhancements:
we introduce an implicit segmentation structure to decompose the time series
and map it to segmented hidden states, resulting in denser information exchange
during the segmentation phase. Additionally, we incorporate residual structures
in the encoding layer to mitigate information loss within the recurrent
structure. To extract information more effectively, we further integrate the
Mamba architecture to enhance time series information extraction. Experiments
on several real-world long time series forecasting datasets demonstrate that
our model surpasses the performance of current state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Graph Embedding based on Local Differential Privacy <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zening Li, Rong-Hua Li, Meihao Liao, Fusheng Jin, Guoren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph embedding has become a powerful tool for learning latent
representations of nodes in a graph. Despite its superior performance in
various graph-based machine learning tasks, serious privacy concerns arise when
the graph data contains personal or sensitive information. To address this
issue, we investigate and develop graph embedding algorithms that satisfy local
differential privacy (LDP). We introduce a novel privacy-preserving graph
embedding framework, named PrivGE, to protect node data privacy. Specifically,
we propose an LDP mechanism to obfuscate node data and utilize personalized
PageRank as the proximity measure to learn node representations. Furthermore,
we provide a theoretical analysis of the privacy guarantees and utility offered
by the PrivGE framework. Extensive experiments on several real-world graph
datasets demonstrate that PrivGE achieves an optimal balance between privacy
and utility, and significantly outperforms existing methods in node
classification and link prediction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Framework for Data-Use Auditing of ML Models <span class="chip">CCS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonghao Huang, Neil Zhenqiang Gong, Michael K. Reiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auditing the use of data in training machine-learning (ML) models is an
increasingly pressing challenge, as myriad ML practitioners routinely leverage
the effort of content creators to train models without their permission. In
this paper, we propose a general method to audit an ML model for the use of a
data-owner's data in training, without prior knowledge of the ML task for which
the data might be used. Our method leverages any existing black-box membership
inference method, together with a sequential hypothesis test of our own design,
to detect data use with a quantifiable, tunable false-detection rate. We show
the effectiveness of our proposed framework by applying it to audit data use in
two types of ML models, namely image classifiers and foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The full paper of "A General Framework for Data-Use Auditing of ML
  Models" accepted by CCS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Contamination Report from the 2024 CONDA Shared Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Sainz, Iker García-Ferrero, Alon Jacovi, Jon Ander Campos, Yanai Elazar, Eneko Agirre, Yoav Goldberg, Wei-Lin Chen, Jenny Chim, Leshem Choshen, Luca D'Amico-Wong, Melissa Dell, Run-Ze Fan, Shahriar Golchin, Yucheng Li, Pengfei Liu, Bhavish Pahwa, Ameya Prabhu, Suryansh Sharma, Emily Silcock, Kateryna Solonko, David Stap, Mihai Surdeanu, Yu-Min Tseng, Vishaal Udandarao, Zengzhi Wang, Ruijie Xu, Jinglin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant
aspects of data contamination in natural language processing, where data
contamination is understood as situations where evaluation data is included in
pre-training corpora used to train large scale models, compromising evaluation
results. The workshop fostered a shared task to collect evidence on data
contamination in current available datasets and models. The goal of the shared
task and associated database is to assist the community in understanding the
extent of the problem and to assist researchers in avoiding reporting
evaluation results on known contaminated resources. The shared task provides a
structured, centralized public database for the collection of contamination
evidence, open to contributions from the community via GitHub pool requests.
This first compilation paper is based on 566 reported entries over 91
contaminated sources from a total of 23 contributors. The details of the
individual contamination events are available in the platform. The platform
continues to be online, open to contributions from the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/spaces/CONDA-Workshop/Data-Contamination-Database</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-03T00:00:00Z">2024-08-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">40</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and
  Fisheye Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wu, Tianyi Cheng, Peixu Xin, Janusz Konrad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies aiming to optimize and bundle-adjust camera poses using
Neural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated
impressive capabilities in 3D scene reconstruction. However, these approaches
have been designed for pinhole-camera pose optimization and do not perform well
under radial image distortions such as those in fisheye cameras. Furthermore,
inaccurate depth initialization in DBARF results in erroneous geometric
information affecting the overall convergence and quality of results. In this
paper, we propose adaptive GRUs with a flexible bundle-adjustment method
adapted to radial distortions and incorporate feature-based recurrent neural
networks to generate continuous novel views from fisheye datasets. Other NeRF
methods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray
distance loss for distorted pose refinement, causing severe artifacts, long
rendering time, and are difficult to use in downstream tasks, where the dense
voxel representation generated by a NeRF method needs to be converted into a
mesh representation. We also address depth initialization issues by adding
MiDaS-based depth priors for pinhole images. Through extensive experiments, we
demonstrate the generalization capacity of FBINeRF and show high-fidelity
results for both pinhole-camera and fisheye-camera NeRFs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Generative Communication between Embodied Agents Good for Zero-Shot
  ObjectNav? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Sashank Dorbala, Vishnu Dutt Sharma, Pratap Tokekar, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Zero-Shot ObjectNav, an embodied ground agent is expected to navigate to a
target object specified by a natural language label without any
environment-specific fine-tuning. This is challenging, given the limited view
of a ground agent and its independent exploratory behavior. To address these
issues, we consider an assistive overhead agent with a bounded global view
alongside the ground agent and present two coordinated navigation schemes for
judicious exploration. We establish the influence of the Generative
Communication (GC) between the embodied agents equipped with Vision-Language
Models (VLMs) in improving zero-shot ObjectNav, achieving a 10% improvement in
the ground agent's ability to find the target object in comparison with an
unassisted setup in simulation. We further analyze the GC for unique traits
quantifying the presence of hallucination and cooperation. In particular, we
identify a unique trait of "preemptive hallucination" specific to our embodied
setting, where the overhead agent assumes that the ground agent has executed an
action in the dialogue when it is yet to move. Finally, we conduct real-world
inferences with GC and showcase qualitative examples where countering
pre-emptive hallucination via prompt finetuning improves real-world ObjectNav
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as
  Positive Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Gu Kwak, Hyungu Kahng, Seoung Bum Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning methods have shown promising results in solving many
practical problems when only a few labels are available. The existing methods
assume that the class distributions of labeled and unlabeled data are equal;
however, their performances are significantly degraded in class distribution
mismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled
data. Previous safe semi-supervised learning studies have addressed this
problem by making OOD data less likely to affect training based on labeled
data. However, even if the studies effectively filter out the unnecessary OOD
data, they can lose the basic information that all data share regardless of
class. To this end, we propose to apply a self-supervised contrastive learning
approach to fully exploit a large amount of unlabeled data. We also propose a
contrastive loss function with coefficient schedule to aggregate as an anchor
the labeled negative examples of the same class into positive examples. To
evaluate the performance of the proposed method, we conduct experiments on
image classification datasets - CIFAR-10, CIFAR-100, Tiny ImageNet, and
CIFAR-100+Tiny ImageNet - under various mismatch ratios. The results show that
self-supervised contrastive learning significantly improves classification
accuracy. Moreover, aggregating the in-distribution examples produces better
representation and consequently further improves classification accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Unfolding and Sampling for Transitory Video Summarization via
  Gershgorin Disc Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadid Sahami, Gene Cheung, Chia-Wen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-generated videos (UGVs) uploaded from mobile phones to social media
sites like YouTube and TikTok are short and non-repetitive. We summarize a
transitory UGV into several keyframes in linear time via fast graph sampling
based on Gershgorin disc alignment (GDA). Specifically, we first model a
sequence of $N$ frames in a UGV as an $M$-hop path graph $\mathcal{G}^o$ for $M
\ll N$, where the similarity between two frames within $M$ time instants is
encoded as a positive edge based on feature similarity. Towards efficient
sampling, we then "unfold" $\mathcal{G}^o$ to a $1$-hop path graph
$\mathcal{G}$, specified by a generalized graph Laplacian matrix $\mathcal{L}$,
via one of two graph unfolding procedures with provable performance bounds. We
show that maximizing the smallest eigenvalue $\lambda_{\min}(\mathbf{B})$ of a
coefficient matrix $\mathbf{B} = \textit{diag}\left(\mathbf{h}\right) + \mu
\mathcal{L}$, where $\mathbf{h}$ is the binary keyframe selection vector, is
equivalent to minimizing a worst-case signal reconstruction error. We maximize
instead the Gershgorin circle theorem (GCT) lower bound
$\lambda^-_{\min}(\mathbf{B})$ by choosing $\mathbf{h}$ via a new fast graph
sampling algorithm that iteratively aligns left-ends of Gershgorin discs for
all graph nodes (frames). Extensive experiments on multiple short video
datasets show that our algorithm achieves comparable or better video
summarization performance compared to state-of-the-art methods, at a
substantially reduced complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Image Translation from Visible to Infrared Domain for Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prahlad Anand, Qiranul Saadiyean, Aniruddh Sikdar, Nalini N, Suresh Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to learn a translation from visible to infrared imagery,
bridging the domain gap between the two modalities so as to improve accuracy on
downstream tasks including object detection. Previous approaches attempt to
perform bi-domain feature fusion through iterative optimization or end-to-end
deep convolutional networks. However, we pose the problem as similar to that of
image translation, adopting a two-stage training strategy with a Generative
Adversarial Network and an object detection model. The translation model learns
a conversion that preserves the structural detail of visible images while
preserving the texture and other characteristics of infrared images. Images so
generated are used to train standard object detection frameworks including
Yolov5, Mask and Faster RCNN. We also investigate the usefulness of integrating
a super-resolution step into our pipeline to further improve model accuracy,
and achieve an improvement of as high as 5.3% mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E$^3$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunshan Qi, Jia Li, Yifan Zhao, Yu Zhang, Lin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) achieve impressive rendering performance by
learning volumetric 3D representation from several images of different views.
However, it is difficult to reconstruct a sharp NeRF from blurry input as it
often occurs in the wild. To solve this problem, we propose a novel Efficient
Event-Enhanced NeRF (E$^3$NeRF) by utilizing the combination of RGB images and
event streams. To effectively introduce event streams into the neural
volumetric representation learning process, we propose an event-enhanced blur
rendering loss and an event rendering loss, which guide the network via
modeling the real blur process and event generation process, respectively.
Specifically, we leverage spatial-temporal information from the event stream to
evenly distribute learning attention over temporal blur while simultaneously
focusing on blurry texture through the spatial attention. Moreover, a camera
pose estimation framework for real-world data is built with the guidance of the
events to generalize the method to practical applications. Compared to previous
image-based or event-based NeRF, our framework makes more profound use of the
internal relationship between events and images. Extensive experiments on both
synthetic data and real-world data demonstrate that E$^3$NeRF can effectively
learn a sharp NeRF from blurry images, especially in non-uniform motion and
low-light scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TS-SAM: Fine-Tuning Segment-Anything Model for Downstream Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Chen Xu, Kai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapter based fine-tuning has been studied for improving the performance of
SAM on downstream tasks. However, there is still a significant performance gap
between fine-tuned SAMs and domain-specific models. To reduce the gap, we
propose Two-Stream SAM (TS-SAM). On the one hand, inspired by the side network
in Parameter-Efficient Fine-Tuning (PEFT), we designed a lightweight
Convolutional Side Adapter (CSA), which integrates the powerful features from
SAM into side network training for comprehensive feature fusion. On the other
hand, in line with the characteristics of segmentation tasks, we designed
Multi-scale Refinement Module (MRM) and Feature Fusion Decoder (FFD) to keep
both the detailed and semantic features. Extensive experiments on ten public
datasets from three tasks demonstrate that TS-SAM not only significantly
outperforms the recently proposed SAM-Adapter and SSOM, but achieves
competitive performance with the SOTA domain-specific models. Our code is
available at: https://github.com/maoyangou147/TS-SAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep CNN Model for Ringing Effect Attenuation of Vibroseis Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuang Jia, Wenkai Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of exploration geophysics, seismic vibrator is one of the widely
used seismic sources to acquire seismic data, which is usually named vibroseis.
"Ringing effect" is a common problem in vibroseis data processing due to the
limited frequency bandwidth of the vibrator, which degrades the performance of
first-break picking. In this paper, we proposed a novel deringing model for
vibroseis data using deep convolutional neural network (CNN). In this model we
use end-to-end training strategy to obtain the deringed data directly, and skip
connections to improve model training process and preserve the details of
vibroseis data. For real vibroseis deringing task we synthesize training data
and corresponding labels from real vibroseis data and utilize them to train the
deep CNN model. Experiments are conducted both on synthetic data and real
vibroseis data. The experiment results show that deep CNN model can attenuate
the ringing effect effectively and expand the bandwidth of vibroseis data. The
STA/LTA ratio method for first-break picking also shows improvement on deringed
vibroseis data using deep CNN model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ST-SACLF: Style Transfer Informed Self-Attention Classifier for
  Bias-Aware Painting Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mridula Vijendran, Frederick W. B. Li, Jingjing Deng, Hubert P. H. Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Painting classification plays a vital role in organizing, finding, and
suggesting artwork for digital and classic art galleries. Existing methods
struggle with adapting knowledge from the real world to artistic images during
training, leading to poor performance when dealing with different datasets. Our
innovation lies in addressing these challenges through a two-step process.
First, we generate more data using Style Transfer with Adaptive Instance
Normalization (AdaIN), bridging the gap between diverse styles. Then, our
classifier gains a boost with feature-map adaptive spatial attention modules,
improving its understanding of artistic details. Moreover, we tackle the
problem of imbalanced class representation by dynamically adjusting augmented
samples. Through a dual-stage process involving careful hyperparameter search
and model fine-tuning, we achieve an impressive 87.24\% accuracy using the
ResNet-50 backbone over 40 training epochs. Our study explores quantitative
analyses that compare different pretrained backbones, investigates model
optimization through ablation studies, and examines how varying augmentation
levels affect model performance. Complementing this, our qualitative
experiments offer valuable insights into the model's decision-making process
using spatial attention and its ability to differentiate between easy and
challenging samples based on confidence ranking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent
  Diffusion <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Lin, Lingyu Xiong, Xiandong Li, Wenxiong Kang, Xianjia Wu, Liang Peng, Songju Lei, Huang Xu, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D speech-driven facial animation generation has received much attention in
both industrial applications and academic research. Since the non-verbal facial
cues that exist across the face in reality are non-deterministic, the generated
results should be diverse. However, most recent methods are deterministic
models that cannot learn a many-to-many mapping between audio and facial motion
to generate diverse facial animations. To address this problem, we propose
GLDiTalker, which introduces a motion prior along with some stochasticity to
reduce the uncertainty of cross-modal mapping while increasing non-determinacy
of the non-verbal facial cues that reside throughout the face. Particularly,
GLDiTalker uses VQ-VAE to map facial motion mesh sequences into latent space in
the first stage, and then iteratively adds and removes noise to the latent
facial motion features in the second stage. In order to integrate different
levels of spatial information, the Spatial Pyramidal SpiralConv Encoder is also
designed to extract multi-scale features. Extensive qualitative and
quantitative experiments demonstrate that our method achieves the
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SkyDiffusion: Street-to-Satellite Image Synthesis with Diffusion Models
  and BEV Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Jinhua Yu, Haote Yang, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Street-to-satellite image synthesis focuses on generating realistic satellite
images from corresponding ground street-view images while maintaining a
consistent content layout, similar to looking down from the sky. The
significant differences in perspectives create a substantial domain gap between
the views, making this cross-view generation task particularly challenging. In
this paper, we introduce SkyDiffusion, a novel cross-view generation method for
synthesizing satellite images from street-view images, leveraging diffusion
models and Bird's Eye View (BEV) paradigm. First, we design a Curved-BEV method
to transform street-view images to the satellite view, reformulating the
challenging cross-domain image synthesis task into a conditional generation
problem. Curved-BEV also includes a "Multi-to-One" mapping strategy for
combining multiple street-view images within the same satellite coverage area,
effectively solving the occlusion issues in dense urban scenes. Next, we design
a BEV-controlled diffusion model to generate satellite images consistent with
the street-view content, which also incorporates a light manipulation module to
optimize the lighting condition of the synthesized image using a reference
satellite. Experimental results demonstrate that SkyDiffusion outperforms
state-of-the-art methods on both suburban (CVUSA & CVACT) and urban
(VIGOR-Chicago) cross-view datasets, with an average SSIM increase of 14.5% and
a FID reduction of 29.6%, achieving realistic and content-consistent satellite
image generation. The code and models of this work will be released at
https://opendatalab.github.io/skydiffusion/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MiniCPM-V: A <span class="highlight-title">GPT</span>-4V Level MLLM on Your Phone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent surge of Multimodal Large Language Models (MLLMs) has
fundamentally reshaped the landscape of AI research and industry, shedding
light on a promising path toward the next AI milestone. However, significant
challenges remain preventing MLLMs from being practical in real-world
applications. The most notable challenge comes from the huge cost of running an
MLLM with a massive number of parameters and extensive computation. As a
result, most MLLMs need to be deployed on high-performing cloud servers, which
greatly limits their application scopes such as mobile, offline,
energy-sensitive, and privacy-protective scenarios. In this work, we present
MiniCPM-V, a series of efficient MLLMs deployable on end-side devices. By
integrating the latest MLLM techniques in architecture, pretraining and
alignment, the latest MiniCPM-Llama3-V 2.5 has several notable features: (1)
Strong performance, outperforming GPT-4V-1106, Gemini Pro and Claude 3 on
OpenCompass, a comprehensive evaluation over 11 popular benchmarks, (2) strong
OCR capability and 1.8M pixel high-resolution image perception at any aspect
ratio, (3) trustworthy behavior with low hallucination rates, (4) multilingual
support for 30+ languages, and (5) efficient deployment on mobile phones. More
importantly, MiniCPM-V can be viewed as a representative example of a promising
trend: The model sizes for achieving usable (e.g., GPT-4V) level performance
are rapidly decreasing, along with the fast growth of end-side computation
capacity. This jointly shows that GPT-4V level MLLMs deployed on end devices
are becoming increasingly possible, unlocking a wider spectrum of real-world AI
applications in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NuLite -- Lightweight and Fast Model for Nuclei Instance Segmentation
  and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Tommasino, Cristiano Russo, Antonio Maria Rinaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In pathology, accurate and efficient analysis of Hematoxylin and Eosin (H\&E)
slides is crucial for timely and effective cancer diagnosis. Although many deep
learning solutions for nuclei instance segmentation and classification exist in
the literature, they often entail high computational costs and resource
requirements, thus limiting their practical usage in medical applications. To
address this issue, we introduce a novel convolutional neural network, NuLite,
a U-Net-like architecture designed explicitly on Fast-ViT, a state-of-the-art
(SOTA) lightweight CNN. We obtained three versions of our model, NuLite-S,
NuLite-M, and NuLite-H, trained on the PanNuke dataset. The experimental
results prove that our models equal CellViT (SOTA) in terms of panoptic quality
and detection. However, our lightest model, NuLite-S, is 40 times smaller in
terms of parameters and about 8 times smaller in terms of GFlops, while our
heaviest model is 17 times smaller in terms of parameters and about 7 times
smaller in terms of GFlops. Moreover, our model is up to about 8 times faster
than CellViT. Lastly, to prove the effectiveness of our solution, we provide a
robust comparison of external datasets, namely CoNseP, MoNuSeg, and GlySAC. Our
model is publicly available at https://github.com/CosmoIknosLab/NuLite
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver
  Attention to Predict Driver Behaviors Under Safety-Critical Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Xu, Yiran Luo, Tianle Lu, Qingfan Wang, Qing Zhou, Bingbing Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate behavior prediction for vehicles is essential but challenging for
autonomous driving. Most existing studies show satisfying performance under
regular scenarios, but most neglected safety-critical scenarios. In this study,
a spatio-temporal dual-encoder network named STDA for safety-critical scenarios
was developed. Considering the exceptional capabilities of human drivers in
terms of situational awareness and comprehending risks, driver attention was
incorporated into STDA to facilitate swift identification of the critical
regions, which is expected to improve both performance and interpretability.
STDA contains four parts: the driver attention prediction module, which
predicts driver attention; the fusion module designed to fuse the features
between driver attention and raw images; the temporary encoder module used to
enhance the capability to interpret dynamic scenes; and the behavior prediction
module to predict the behavior. The experiment data are used to train and
validate the model. The results show that STDA improves the G-mean from 0.659
to 0.719 when incorporating driver attention and adopting a temporal encoder
module. In addition, extensive experimentation has been conducted to validate
that the proposed module exhibits robust generalization capabilities and can be
seamlessly integrated into other mainstream models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of Embedded Spaces for Deep Learning Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Scholl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedded spaces are a key feature in deep learning. Good embedded spaces
represent the data well to support classification and advanced techniques such
as open-set recognition, few-short learning and explainability. This paper
presents a compact overview of different techniques to design embedded spaces
for classification. It compares different loss functions and constraints on the
network parameters with respect to the achievable geometric structure of the
embedded space. The techniques are demonstrated with two and three-dimensional
embeddings for the MNIST, Fashion MNIST and CIFAR-10 datasets, allowing visual
inspection of the embedded spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiFuser: Multimodal Fusion <span class="highlight-title">Transformer</span> for Enhanced Driver Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Wang, Wenqian Wang, Jianjun Gao, Dan Lin, Kim-Hui Yap, Bingbing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driver action recognition, aiming to accurately identify drivers' behaviours,
is crucial for enhancing driver-vehicle interactions and ensuring driving
safety. Unlike general action recognition, drivers' environments are often
challenging, being gloomy and dark, and with the development of sensors,
various cameras such as IR and depth cameras have emerged for analyzing
drivers' behaviors. Therefore, in this paper, we propose a novel multimodal
fusion transformer, named MultiFuser, which identifies cross-modal
interrelations and interactions among multimodal car cabin videos and
adaptively integrates different modalities for improved representations.
Specifically, MultiFuser comprises layers of Bi-decomposed Modules to model
spatiotemporal features, with a modality synthesizer for multimodal features
integration. Each Bi-decomposed Module includes a Modal Expertise ViT block for
extracting modality-specific features and a Patch-wise Adaptive Fusion block
for efficient cross-modal fusion. Extensive experiments are conducted on
Drive&Act dataset and the results demonstrate the efficacy of our proposed
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice
  Leaf Disease Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khairun Saddami, Yudha Nurdin, Mutia Zahramita, Muhammad Shahreeza Safiruz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rice plays a vital role as a primary food source for over half of the world's
population, and its production is critical for global food security.
Nevertheless, rice cultivation is frequently affected by various diseases that
can severely decrease yield and quality. Therefore, early and accurate
detection of rice diseases is necessary to prevent their spread and minimize
crop losses. In this research, we explore three mobile-compatible CNN
architectures, namely ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice
leaf disease classification. These models are selected due to their
compatibility with mobile devices, as they demand less computational power and
memory compared to other CNN models. To enhance the performance of the three
models, we added two fully connected layers separated by a dropout layer. We
used early stop creation to prevent the model from being overfiting. The
results of the study showed that the best performance was achieved by the
EfficientNet-B0 model with an accuracy of 99.8%. Meanwhile, MobileNetV2 and
ShuffleNet only achieved accuracies of 84.21% and 66.51%, respectively. This
study shows that EfficientNet-B0 when combined with the proposed layer and
early stop, can produce a high-accuracy model.
  Keywords: rice leaf detection; green AI; smart agriculture; EfficientNet;
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain penalisation for improved Out-of-Distribution Generalisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuvam Jena, Sushmetha Sumathi Rajendran, Karthik Seemakurthy, Sasithradevi A, Vijayalakshmi M, Prakash Poornachari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of object detection, domain generalisation (DG) aims to ensure
robust performance across diverse and unseen target domains by learning the
robust domain-invariant features corresponding to the objects of interest
across multiple source domains. While there are many approaches established for
performing DG for the task of classification, there has been a very little
focus on object detection. In this paper, we propose a domain penalisation (DP)
framework for the task of object detection, where the data is assumed to be
sampled from multiple source domains and tested on completely unseen test
domains. We assign penalisation weights to each domain, with the values updated
based on the detection networks performance on the respective source domains.
By prioritising the domains that needs more attention, our approach effectively
balances the training process. We evaluate our solution on the GWHD 2021
dataset, a component of the WiLDS benchmark and we compare against ERM and
GroupDRO as these are primarily loss function based. Our extensive experimental
results reveals that the proposed approach improves the accuracy by 0.3 percent
and 0.5 percent on validation and test out-of-distribution (OOD) sets,
respectively for FasterRCNN. We also compare the performance of our approach on
FCOS detector and show that our approach improves the baseline OOD performance
over the existing approaches by 1.3 percent and 1.4 percent on validation and
test sets, respectively. This study underscores the potential of performance
based domain penalisation in enhancing the generalisation ability of object
detection models across diverse environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAM3D: Leveraging Attention for Monocular 3D Object Detection <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diana-Alexandra Sas, Leandro Di Bella, Yangxintong Lyu, Florin Oniga, Adrian Munteanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the introduction of the self-attention mechanism and the adoption of
the Transformer architecture for Computer Vision tasks, the Vision
Transformer-based architectures gained a lot of popularity in the field, being
used for tasks such as image classification, object detection and image
segmentation. However, efficiently leveraging the attention mechanism in vision
transformers for the Monocular 3D Object Detection task remains an open
question. In this paper, we present LAM3D, a framework that Leverages
self-Attention mechanism for Monocular 3D object Detection. To do so, the
proposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as
feature extraction backbone and 2D/3D detection machinery. We evaluate the
proposed method on the KITTI 3D Object Detection Benchmark, proving the
applicability of the proposed solution in the autonomous driving domain and
outperforming reference methods. Moreover, due to the usage of self-attention,
LAM3D is able to systematically outperform the equivalent architecture that
does not employ self-attention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages. Accepted to MMSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Landmark-guided Diffusion Model for High-fidelity and Temporally
  Coherent Talking Head Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Tan, Xize Cheng, Lingyu Xiong, Lei Zhu, Xiandong Li, Xianjia Wu, Kai Gong, Minglei Li, Yi Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking head generation is a significant and challenging task
applicable to various fields such as virtual avatars, film production, and
online conferences. However, the existing GAN-based models emphasize generating
well-synchronized lip shapes but overlook the visual quality of generated
frames, while diffusion-based models prioritize generating high-quality frames
but neglect lip shape matching, resulting in jittery mouth movements. To
address the aforementioned problems, we introduce a two-stage diffusion-based
model. The first stage involves generating synchronized facial landmarks based
on the given speech. In the second stage, these generated landmarks serve as a
condition in the denoising process, aiming to optimize mouth jitter issues and
generate high-fidelity, well-synchronized, and temporally coherent talking head
videos. Extensive experiments demonstrate that our model yields the best
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Survey</span> on Emotion Recognition through Posture Detection and the
  possibility of its application in Virtual Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leina Elansary, Zaki Taha, Walaa Gad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A survey is presented focused on using pose estimation techniques in
Emotional recognition using various technologies normal cameras, and depth
cameras for real-time, and the potential use of VR and inputs including images,
videos, and 3-dimensional poses described in vector space. We discussed 19
research papers collected from selected journals and databases highlighting
their methodology, classification algorithm, and the used datasets that relate
to emotion recognition and pose estimation. A benchmark has been made according
to their accuracy as it was the most common performance measurement metric
used. We concluded that the multimodal Approaches overall made the best
accuracy and then we mentioned futuristic concerns that can improve the
development of this research topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Evaluation Framework for Image2Text Generation <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Hong Huang, Hongyi Zhu, Yixian Shen, Stevan Rudinac, Alessio M. Pacces, Evangelos Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the quality of automatically generated image descriptions is
challenging, requiring metrics that capture various aspects such as
grammaticality, coverage, correctness, and truthfulness. While human evaluation
offers valuable insights, its cost and time-consuming nature pose limitations.
Existing automated metrics like BLEU, ROUGE, METEOR, and CIDEr aim to bridge
this gap but often show weak correlations with human judgment. We address this
challenge by introducing a novel evaluation framework rooted in a modern large
language model (LLM), such as GPT-4 or Gemini, capable of image generation. In
our proposed framework, we begin by feeding an input image into a designated
image captioning model, chosen for evaluation, to generate a textual
description. Using this description, an LLM then creates a new image. By
extracting features from both the original and LLM-created images, we measure
their similarity using a designated similarity metric. A high similarity score
suggests that the image captioning model has accurately generated textual
descriptions, while a low similarity score indicates discrepancies, revealing
potential shortcomings in the model's performance. Human-annotated reference
captions are not required in our proposed evaluation framework, which serves as
a valuable tool for evaluating the effectiveness of image captioning models.
Its efficacy is confirmed through human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted for presentation at the 47th
  International ACM SIGIR Conference on Research and Development in Information
  Retrieval, specifically in the Large Language Model for Evaluation in IR
  (LLM4Eval) Workshop in 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual-Inertial SLAM for Agricultural Robotics: Benchmarking the
  Benefits and Computational Costs of Loop Closing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Schmidt, Constantin Blessing, Markus Enzweiler, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) is essential for mobile
robotics, enabling autonomous navigation in dynamic, unstructured outdoor
environments without relying on external positioning systems. In agricultural
applications, where environmental conditions can be particularly challenging
due to variable lighting or weather conditions, Visual-Inertial SLAM has
emerged as a potential solution. This paper benchmarks several open-source
Visual-Inertial SLAM systems, including ORB-SLAM3, VINS-Fusion, OpenVINS,
Kimera, and SVO Pro, to evaluate their performance in agricultural settings. We
focus on the impact of loop closing on localization accuracy and computational
demands, providing a comprehensive analysis of these systems' effectiveness in
real-world environments and especially their application to embedded systems in
agricultural robotics. Our contributions further include an assessment of
varying frame rates on localization accuracy and computational load. The
findings highlight the importance of loop closing in improving localization
accuracy while managing computational resources efficiently, offering valuable
insights for optimizing Visual-Inertial SLAM systems for practical outdoor
applications in mobile robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Ambiguity Model for Binary Edge Images with Edge Tracing and
  its Implementation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Hennig, Marc Leineke, Bärbel Mertsching
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a general and intuitive ambiguity model for intersections,
junctions and other structures in binary edge images. The model is combined
with edge tracing, where edges are ordered sequences of connected pixels. The
objective is to provide a versatile preprocessing method for tasks such as
figure-ground segmentation, object recognition, topological analysis, etc. By
using only a small set of straightforward principles, the results are intuitive
to describe. This helps to implement subsequent processing steps, such as
resolving ambiguous edge connections at junctions. By using an augmented edge
map, neighboring edges can be directly accessed using quick local search
operations. The edge tracing uses recursion, which leads to compact programming
code. We explain our algorithm using pseudocode, compare it with related
methods, and show how simple modular postprocessing steps can be used to
optimize the results. The complete algorithm, including all data structures,
requires less than 50 lines of pseudocode. We also provide a C++ implementation
of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVESFormer: Efficient <span class="highlight-title">Transformer</span> Design for Real-Time Audio-Visual
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zili Wang, Qi Yang, Linsu Shi, Jiazhong Yu, Qinghua Liang, Fei Li, Shiming Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, transformer-based models have demonstrated remarkable performance
on audio-visual segmentation (AVS) tasks. However, their expensive
computational cost makes real-time inference impractical. By characterizing
attention maps of the network, we identify two key obstacles in AVS models: 1)
attention dissipation, corresponding to the over-concentrated attention weights
by Softmax within restricted frames, and 2) inefficient, burdensome transformer
decoder, caused by narrow focus patterns in early stages. In this paper, we
introduce AVESFormer, the first real-time Audio-Visual Efficient Segmentation
transformer that achieves fast, efficient and light-weight simultaneously. Our
model leverages an efficient prompt query generator to correct the behaviour of
cross-attention. Additionally, we propose ELF decoder to bring greater
efficiency by facilitating convolutions suitable for local features to reduce
computational burdens. Extensive experiments demonstrate that our AVESFormer
significantly enhances model performance, achieving 79.9% on S4, 57.9% on MS3
and 31.2% on AVSS, outperforming previous state-of-the-art and achieving an
excellent trade-off between performance and speed. Code can be found at
https://github.com/MarkXCloud/AVESFormer.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Backwards: Minimal Synthetic <span class="highlight-title">Pre-train</span>ing? <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Nakamura, Ryu Tadokoro, Ryosuke Yamada, Yuki M. Asano, Iro Laina, Christian Rupprecht, Nakamasa Inoue, Rio Yokota, Hirokatsu Kataoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training and transfer learning are an important building block of current
computer vision systems. While pre-training is usually performed on large
real-world image datasets, in this paper we ask whether this is truly
necessary. To this end, we search for a minimal, purely synthetic pre-training
dataset that allows us to achieve performance similar to the 1 million images
of ImageNet-1k. We construct such a dataset from a single fractal with
perturbations. With this, we contribute three main findings. (i) We show that
pre-training is effective even with minimal synthetic images, with performance
on par with large-scale pre-training datasets like ImageNet-1k for full
fine-tuning. (ii) We investigate the single parameter with which we construct
artificial categories for our dataset. We find that while the shape differences
can be indistinguishable to humans, they are crucial for obtaining strong
performances. (iii) Finally, we investigate the minimal requirements for
successful pre-training. Surprisingly, we find that a substantial reduction of
synthetic images from 1k to 1 can even lead to an increase in pre-training
performance, a motivation to further investigate ''scaling backwards''.
Finally, we extend our method from synthetic images to real images to see if a
single real image can show similar pre-training effect through shape
augmentation. We find that the use of grayscale images and affine
transformations allows even real images to ''scale backwards''.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NiNformer: A Network in Network <span class="highlight-title">Transformer</span> with Token Mixing as a
  Gating Function Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02411v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02411v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Nazhat Abdullah, Tarkan Aydin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention mechanism is the main component of the transformer
architecture, and since its introduction, it has led to significant
advancements in deep learning that span many domains and multiple tasks. The
attention mechanism was utilized in computer vision as the Vision Transformer
ViT, and its usage has expanded into many tasks in the vision domain, such as
classification, segmentation, object detection, and image generation. While
this mechanism is very expressive and capable, it comes with the drawback of
being computationally expensive and requiring datasets of considerable size for
effective optimization. To address these shortcomings, many designs have been
proposed in the literature to reduce the computational burden and alleviate the
data size requirements. Examples of such attempts in the vision domain are the
MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper
introduces a new computational block as an alternative to the standard ViT
block that reduces the compute burdens by replacing the normal attention layers
with a Network in Network structure that enhances the static approach of the
MLP-Mixer with a dynamic system of learning an element-wise gating function by
a token mixing process. Extensive experimentation shows that the proposed
design provides better performance than the baseline architectures on multiple
datasets applied in the image classification task of the vision domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSA$^2$Net: Multi-scale Adaptive Attention-guided Network for Medical
  Image Segmentation <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Ghorbani Kolahi, Seyed Kamal Chaharsooghi, Toktam Khatibi, Afshin Bozorgpour, Reza Azad, Moein Heidari, Ilker Hacihaliloglu, Dorit Merhof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation involves identifying and separating object
instances in a medical image to delineate various tissues and structures, a
task complicated by the significant variations in size, shape, and density of
these features. Convolutional neural networks (CNNs) have traditionally been
used for this task but have limitations in capturing long-range dependencies.
Transformers, equipped with self-attention mechanisms, aim to address this
problem. However, in medical image segmentation it is beneficial to merge both
local and global features to effectively integrate feature maps across various
scales, capturing both detailed features and broader semantic elements for
dealing with variations in structures. In this paper, we introduce MSA$^2$Net,
a new deep segmentation framework featuring an expedient design of
skip-connections. These connections facilitate feature fusion by dynamically
weighting and combining coarse-grained encoder features with fine-grained
decoder feature maps. Specifically, we propose a Multi-Scale Adaptive Spatial
Attention Gate (MASAG), which dynamically adjusts the receptive field (Local
and Global contextual information) to ensure that spatially relevant features
are selectively highlighted while minimizing background distractions. Extensive
evaluations involving dermatology, and radiological datasets demonstrate that
our MSA$^2$Net outperforms state-of-the-art (SOTA) works or matches their
performance. The source code is publicly available at
https://github.com/xmindflow/MSA-2Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BMVC 2024. Supplementary materials included at the end of
  the main paper (3 pages, 2 figures, 1 table)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JOSENet: A Joint Stream Embedding Network for Violence Detection in
  Surveillance Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Nardelli, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing proliferation of video surveillance cameras and the escalating
demand for crime prevention have intensified interest in the task of violence
detection within the research community. Compared to other action recognition
tasks, violence detection in surveillance videos presents additional issues,
such as the wide variety of real fight scenes. Unfortunately, existing datasets
for violence detection are relatively small in comparison to those for other
action recognition tasks. Moreover, surveillance footage often features
different individuals in each video and varying backgrounds for each camera. In
addition, fast detection of violent actions in real-life surveillance videos is
crucial to prevent adverse outcomes, thus necessitating models that are
optimized for reduced memory usage and computational costs. These challenges
complicate the application of traditional action recognition methods. To tackle
all these issues, we introduce JOSENet, a novel self-supervised framework that
provides outstanding performance for violence detection in surveillance videos.
The proposed model processes two spatiotemporal video streams, namely RGB
frames and optical flows, and incorporates a new regularized self-supervised
learning approach for videos. JOSENet demonstrates improved performance
compared to state-of-the-art methods, while utilizing only one-fourth of the
frames per video segment and operating at a reduced frame rate. The source code
is available at https://github.com/ispamm/JOSENet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency-guided <span class="highlight-title">Prompt</span> Learning for Vision-Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01195v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01195v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuvendu Roy, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning
method for vision-language models. Our approach improves the generalization of
large foundation models when fine-tuned on downstream tasks in a few-shot
setting. The basic idea of CoPrompt is to enforce a consistency constraint in
the prediction of the trainable and pre-trained models to prevent overfitting
on the downstream task. Additionally, we introduce the following two components
into our consistency constraint to further boost the performance: enforcing
consistency on two perturbed inputs and combining two dominant paradigms of
tuning, prompting and adapter. Enforcing consistency on perturbed input serves
to further regularize the consistency constraint, thereby improving
generalization. Moreover, the integration of adapters and prompts not only
enhances performance on downstream tasks but also offers increased tuning
flexibility in both input and output spaces. This facilitates more effective
adaptation to downstream tasks in a few-shot learning setting. Experiments show
that CoPrompt outperforms existing methods on a range of evaluation suites,
including base-to-novel generalization, domain generalization, and
cross-dataset evaluation. On generalization, CoPrompt improves the
state-of-the-art on zero-shot tasks and the overall harmonic mean over 11
datasets. Detailed ablation studies show the effectiveness of each of the
components in CoPrompt. We make our code available at
https://github.com/ShuvenduRoy/CoPrompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating and Mitigating the Multimodal Hallucination Snowballing in
  Large Vision-Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00569v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00569v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihong Zhong, Xiaocheng Feng, Liang Zhao, Qiming Li, Lei Huang, Yuxuan Gu, Weitao Ma, Yuan Xu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though advanced in understanding visual information with human languages,
Large Vision-Language Models (LVLMs) still suffer from multimodal
hallucinations. A natural concern is that during multimodal interaction, the
generated hallucinations could influence the LVLMs' subsequent generation.
Thus, we raise a question: When presented with a query relevant to the
previously generated hallucination, will LVLMs be misled and respond
incorrectly, even though the ground visual information exists? To answer this,
we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when
encountering generated hallucinations, where LVLMs are required to answer
specific visual questions within a curated hallucinatory conversation.
Crucially, our experiment shows that the performance of open-source LVLMs drops
by at least $31\%$, indicating that LVLMs are prone to accept the generated
hallucinations and make false claims that they would not have supported without
distractions. We term this phenomenon Multimodal Hallucination Snowballing. To
mitigate this, we further propose a training-free method called Residual Visual
Decoding, where we revise the output distribution of LVLMs with the one derived
from the residual visual input, providing models with direct access to the
visual information. Experiments show that our method can mitigate more than
$24\%$ of the snowballed multimodal hallucination while maintaining
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024 Main Conference. 21 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FreeDrag: Feature Dragging for Reliable Point-based Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04684v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04684v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, Yi Jin, Jinjin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To serve the intricate and varied demands of image editing, precise and
flexible manipulation in image content is indispensable. Recently, Drag-based
editing methods have gained impressive performance. However, these methods
predominantly center on point dragging, resulting in two noteworthy drawbacks,
namely "miss tracking", where difficulties arise in accurately tracking the
predetermined handle points, and "ambiguous tracking", where tracked points are
potentially positioned in wrong regions that closely resemble the handle
points. To address the above issues, we propose FreeDrag, a feature dragging
methodology designed to free the burden on point tracking. The FreeDrag
incorporates two key designs, i.e., template feature via adaptive updating and
line search with backtracking, the former improves the stability against
drastic content change by elaborately controls feature updating scale after
each dragging, while the latter alleviates the misguidance from similar points
by actively restricting the search area in a line. These two technologies
together contribute to a more stable semantic dragging with higher efficiency.
Comprehensive experimental results substantiate that our approach significantly
outperforms pre-existing methodologies, offering reliable point-based editing
even in various complex scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>-Based Classification Outcome Prediction for Multimodal
  Stroke Treatment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danqing Ma, Meng Wang, Ao Xiang, Zongqing Qi, Qin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a multi-modal fusion framework Multitrans based on the
Transformer architecture and self-attention mechanism. This architecture
combines the study of non-contrast computed tomography (NCCT) images and
discharge diagnosis reports of patients undergoing stroke treatment, using a
variety of methods based on Transformer architecture approach to predicting
functional outcomes of stroke treatment. The results show that the performance
of single-modal text classification is significantly better than single-modal
image classification, but the effect of multi-modal combination is better than
any single modality. Although the Transformer model only performs worse on
imaging data, when combined with clinical meta-diagnostic information, both can
learn better complementary information and make good contributions to
accurately predicting stroke treatment effects..
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via
  SLM-Based <span class="highlight-title">Prompt</span> Engineering and Generative Adversary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjun Luo, Ziye Deng, Haoyu Huang, Xuecheng Liu, Ruizhe Chen, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Text-to-Image models, biases in human image
generation against demographic groups social attract more and more concerns.
Existing methods are designed based on certain models with fixed prompts,
unable to accommodate the trend of high-speed updating of Text-to-Image (T2I)
models and variable prompts in practical scenes. Additionally, they fail to
consider the possibility of hallucinations, leading to deviations between
expected and actual results. To address this issue, we introduce VersusDebias,
a novel and universal debiasing framework for biases in T2I models, consisting
of one generative adversarial mechanism (GAM) and one debiasing generation
mechanism using a small language model (SLM). The self-adaptive GAM generates
specialized attribute arrays for each prompts for diminishing the influence of
hallucinations from T2I models. The SLM uses prompt engineering to generate
debiased prompts for the T2I model, providing zero-shot debiasing ability and
custom optimization for different models. Extensive experiments demonstrate
VersusDebias's capability to rectify biases on arbitrary models across multiple
protected attributes simultaneously, including gender, race, and age.
Furthermore, VersusDebias outperforms existing methods in both zero-shot and
few-shot situations, illustrating its extraordinary utility. Our work is openly
accessible to the research community to ensure the reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Branch Generative Models for Multichannel Imaging with an
  Application to PET/CT Synergistic Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noel Jeffrey Pinton, Alexandre Bousse, Catherine Cheze-Le-Rest, Dimitris Visvikis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach for learned synergistic reconstruction
of medical images using multi-branch generative models. Leveraging variational
autoencoders (VAEs), our model learns from pairs of images simultaneously,
enabling effective denoising and reconstruction. Synergistic image
reconstruction is achieved by incorporating the trained models in a regularizer
that evaluates the distance between the images and the model. We demonstrate
the efficacy of our approach on both Modified National Institute of Standards
and Technology (MNIST) and positron emission tomography (PET)/computed
tomography (CT) datasets, showcasing improved image quality for low-dose
imaging. Despite challenges such as patch decomposition and model limitations,
our results underscore the potential of generative models for enhancing medical
imaging reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 16 figures, 2 tables, submitted to IEEE TRPMS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards A Generalizable Pathology Foundation Model via Unified Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabo Ma, Zhengrui Guo, Fengtao Zhou, Yihui Wang, Yingxue Xu, Yu Cai, Zhengjie Zhu, Cheng Jin, Yi Lin, Xinrui Jiang, Anjia Han, Li Liang, Ronald Cheong Kin Chan, Jiguang Wang, Kwang-Ting Cheng, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models pretrained on large-scale datasets are revolutionizing the
field of computational pathology (CPath). The generalization ability of
foundation models is crucial for the success in various downstream clinical
tasks. However, current foundation models have only been evaluated on a limited
type and number of tasks, leaving their generalization ability and overall
performance unclear. To address this gap, we established a most comprehensive
benchmark to evaluate the performance of off-the-shelf foundation models across
six distinct clinical task types, encompassing a total of 39 specific tasks.
Our findings reveal that existing foundation models excel at certain task types
but struggle to effectively handle the full breadth of clinical tasks. To
improve the generalization of pathology foundation models, we propose a unified
knowledge distillation framework consisting of both expert and self knowledge
distillation, where the former allows the model to learn from the knowledge of
multiple expert models, while the latter leverages self-distillation to enable
image representation learning via local-global alignment. Based on this
framework, a Generalizable Pathology Foundation Model (GPFM) is pretrained on a
large-scale dataset consisting of 190 million images from around 86,000 public
H&E whole slides across 34 major tissue types. Evaluated on the established
benchmark, GPFM achieves an impressive average rank of 1.36, with 29 tasks
ranked 1st, while the the second-best model, UNI, attains an average rank of
2.96, with only 4 tasks ranked 1st. The superior generalization of GPFM
demonstrates its exceptional modeling capabilities across a wide range of
clinical tasks, positioning it as a new cornerstone for feature representation
in CPath.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CM2-Net: Continual Cross-Modal Mapping Network for Driver Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11340v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11340v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Wang, Chen Cai, Wenqian Wang, Jianjun Gao, Dan Lin, Wenyang Liu, Kim-Hui Yap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driver action recognition has significantly advanced in enhancing
driver-vehicle interactions and ensuring driving safety by integrating multiple
modalities, such as infrared and depth. Nevertheless, compared to RGB modality
only, it is always laborious and costly to collect extensive data for all types
of non-RGB modalities in car cabin environments. Therefore, previous works have
suggested independently learning each non-RGB modality by fine-tuning a model
pre-trained on RGB videos, but these methods are less effective in extracting
informative features when faced with newly-incoming modalities due to large
domain gaps. In contrast, we propose a Continual Cross-Modal Mapping Network
(CM2-Net) to continually learn each newly-incoming modality with instructive
prompts from the previously-learned modalities. Specifically, we have developed
Accumulative Cross-modal Mapping Prompting (ACMP), to map the discriminative
and informative features learned from previous modalities into the feature
space of newly-incoming modalities. Then, when faced with newly-incoming
modalities, these mapped features are able to provide effective prompts for
which features should be extracted and prioritized. These prompts are
accumulating throughout the continual learning process, thereby boosting
further recognition performances. Extensive experiments conducted on the
Drive&Act dataset demonstrate the performance superiority of CM2-Net on both
uni- and multi-modal driver action recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Reduction Strategy for Non Line of Sight Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunbo Shim, In Cho, Daekyu Kwon, Seon Joo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel optimization-based method for non-line-of-sight
(NLOS) imaging that aims to reconstruct hidden scenes under general setups with
significantly reduced reconstruction time. In NLOS imaging, the visible
surfaces of the target objects are notably sparse. To mitigate unnecessary
computations arising from empty regions, we design our method to render the
transients through partial propagations from a continuously sampled set of
points from the hidden space. Our method is capable of accurately and
efficiently modeling the view-dependent reflectance using surface normals,
which enables us to obtain surface geometry as well as albedo. In this
pipeline, we propose a novel domain reduction strategy to eliminate superfluous
computations in empty regions. During the optimization process, our domain
reduction procedure periodically prunes the empty regions from our sampling
domain in a coarse-to-fine manner, leading to substantial improvement in
efficiency. We demonstrate the effectiveness of our method in various NLOS
scenarios with sparse scanning patterns. Experiments conducted on both
synthetic and real-world data support the efficacy in general NLOS scenarios,
and the improved efficiency of our method compared to the previous
optimization-based solutions. Our code is available at
https://github.com/hyunbo9/domain-reduction-strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral
  Image Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01726v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01726v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanyiman Fu, Fengchao Xiong, Jianfeng Lu, Jun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising is a crucial preprocessing step for hyperspectral images (HSIs) due
to noise arising from intra-imaging mechanisms and environmental factors.
Long-range spatial-spectral correlation modeling is beneficial for HSI
denoising but often comes with high computational complexity. Based on the
state space model (SSM), Mamba is known for its remarkable long-range
dependency modeling capabilities and computational efficiency. Building on
this, we introduce a memory-efficient spatial-spectral UMamba (SSUMamba) for
HSI denoising, with the spatial-spectral continuous scan (SSCS) Mamba being the
core component. SSCS Mamba alternates the row, column, and band in six
different orders to generate the sequence and uses the bidirectional SSM to
exploit long-range spatial-spectral dependencies. In each order, the images are
rearranged between adjacent scans to ensure spatial-spectral continuity.
Additionally, 3D convolutions are embedded into the SSCS Mamba to enhance local
spatial-spectral modeling. Experiments demonstrate that SSUMamba achieves
superior denoising results with lower memory consumption per batch compared to
transformer-based methods. The source code is available at
https://github.com/lronkitty/SSUMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIP: Versatile Image Outpainting Empowered by Multimodal Large Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Yang, Haoran Wang, Zining Zhu, Chenglong Liu, Meng Wymond Wu, Zeke Xie, Zhong Ji, Jungong Han, Mingming Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on resolving the problem of image outpainting, which
aims to extrapolate the surrounding parts given the center contents of an
image. Although recent works have achieved promising performance, the lack of
versatility and customization hinders their practical applications in broader
scenarios. Therefore, this work presents a novel image outpainting framework
that is capable of customizing the results according to the requirement of
users. First of all, we take advantage of a Multimodal Large Language Model
(MLLM) that automatically extracts and organizes the corresponding textual
descriptions of the masked and unmasked part of a given image. Accordingly, the
obtained text prompts are introduced to endow our model with the capacity to
customize the outpainting results. In addition, a special Cross-Attention
module, namely Center-Total-Surrounding (CTS), is elaborately designed to
enhance further the the interaction between specific space regions of the image
and corresponding parts of the text prompts. Note that unlike most existing
methods, our approach is very resource-efficient since it is just slightly
fine-tuned on the off-the-shelf stable diffusion (SD) model rather than being
trained from scratch. Finally, the experimental results on three commonly used
datasets, i.e. Scenery, Building, and WikiArt, demonstrate our model
significantly surpasses the SoTA methods. Moreover, versatile outpainting
results are listed to show its customized ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our source code is available at: https://github.com/ucasyjz/VIP, 15
  pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-02T00:00:00Z">2024-08-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">80</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Talk Less, Interact Better: Evaluating In-context Conversational
  Adaptation in Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Hua, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans spontaneously use increasingly efficient language as interactions
progress, by adapting and forming ad-hoc conventions. This phenomenon has been
studied extensively using reference games, showing properties of human language
that go beyond relaying intents. It remains unexplored whether multimodal large
language models (MLLMs) similarly increase communication efficiency during
interactions, and what mechanisms they may adopt for this purpose. We introduce
ICCA, an automated framework to evaluate such conversational adaptation as an
in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and
observe that while they may understand the increasingly efficient language of
their interlocutor, they do not spontaneously make their own language more
efficient over time. This latter ability can only be elicited in some models
(e.g., GPT-4) with heavy-handed prompting. This shows that this property of
linguistic interaction does not arise from current training regimes, even
though it is a common hallmark of human language. ICCA is available at
https://github.com/lil-lab/ICCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NOLO: Navigate Only Look Once 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Zhou, Jiangxing Wang, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The in-context learning ability of Transformer models has brought new
possibilities to visual navigation. In this paper, we focus on the video
navigation setting, where an in-context navigation policy needs to be learned
purely from videos in an offline manner, without access to the actual
environment. For this setting, we propose Navigate Only Look Once (NOLO), a
method for learning a navigation policy that possesses the in-context ability
and adapts to new scenes by taking corresponding context videos as input
without finetuning or re-training. To enable learning from videos, we first
propose a pseudo action labeling procedure using optical flow to recover the
action label from egocentric videos. Then, offline reinforcement learning is
applied to learn the navigation policy. Through extensive experiments on
different scenes, we show that our algorithm outperforms baselines by a large
margin, which demonstrates the in-context learning ability of the learned
policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial-Spectral Morphological Mamba for Hyperspectral Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ahmad, Muhammad Hassaan Farooq Butt, Muhammad Usama, Adil Mehmood Khan, Manual Mazzara, Salvatore Distenano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Transformers have garnered significant attention for
Hyperspectral Image Classification (HSIC) due to their self-attention
mechanism, which provides strong classification performance. However, these
models face major challenges in computational efficiency, as their complexity
increases quadratically with the sequence length. The Mamba architecture,
leveraging a State Space Model, offers a more efficient alternative to
Transformers. This paper introduces the Spatial-Spectral Morphological Mamba
(MorpMamba) model. In the MorpMamba model, a token generation module first
converts the Hyperspectral Image (HSI) patch into spatial-spectral tokens.
These tokens are then processed by a morphology block, which computes
structural and shape information using depthwise separable convolutional
operations. The extracted information is enhanced in a feature enhancement
module that adjusts the spatial and spectral tokens based on the center region
of the HSI sample, allowing for effective information fusion within each block.
Subsequently, the tokens are refined in a multi-head self-attention block to
further improve the feature space. Finally, the combined information is fed
into the state space block for classification and the creation of the ground
truth map. Experiments on widely used Hyperspectral (HS) datasets demonstrate
that the MorpMamba model outperforms (parametric efficiency) both CNN and
Transformer models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using
  Windowed Nonlinear Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runze Yuan, Tao Liu, Zijia Dai, Yi-Fan Zuo, Laurent Kneip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are an interesting visual exteroceptive sensor that reacts to
brightness changes rather than integrating absolute image intensities. Owing to
this design, the sensor exhibits strong performance in situations of
challenging dynamics and illumination conditions. While event-based
simultaneous tracking and mapping remains a challenging problem, a number of
recent works have pointed out the sensor's suitability for prior map-based
tracking. By making use of cross-modal registration paradigms, the camera's
ego-motion can be tracked across a large spectrum of illumination and dynamics
conditions on top of accurate maps that have been created a priori by more
traditional sensors. The present paper follows up on a recently introduced
event-based geometric semi-dense tracking paradigm, and proposes the addition
of inertial signals in order to robustify the estimation. More specifically,
the added signals provide strong cues for pose initialization as well as
regularization during windowed, multi-frame tracking. As a result, the proposed
framework achieves increased performance under challenging illumination
conditions as well as a reduction of the rate at which intermediate event
representations need to be registered in order to maintain stable tracking
across highly dynamic sequences. Our evaluation focuses on a diverse set of
real world sequences and comprises a comparison of our proposed method against
a purely event-based alternative running at different rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 3 tables, International Conference on Intelligent
  Robots and Systems 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoxuan Feng, Di Hu, Wenke Ma, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess a remarkable talent for flexibly alternating to different
senses when interacting with the environment. Picture a chef skillfully gauging
the timing of ingredient additions and controlling the heat according to the
colors, sounds, and aromas, seamlessly navigating through every stage of the
complex cooking process. This ability is founded upon a thorough comprehension
of task stages, as achieving the sub-goal within each stage can necessitate the
utilization of different senses. In order to endow robots with similar ability,
we incorporate the task stages divided by sub-goals into the imitation learning
process to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a
stage-guided dynamic multi-sensory fusion method with coarse-to-fine stage
understanding, which dynamically adjusts the priority of modalities based on
the fine-grained state within the predicted current stage. We train a robot
system equipped with visual, auditory, and tactile sensors to accomplish
challenging robotic manipulation tasks: pouring and peg insertion with keyway.
Experimental results indicate that our approach enables more effective and
explainable dynamic fusion, aligning more closely with the human fusion process
than existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Automatic Relevance Judgment using Vision--Language Models for
  Image--Text Retrieval Evaluation <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jheng-Hong Yang, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision--Language Models (VLMs) have demonstrated success across diverse
applications, yet their potential to assist in relevance judgments remains
uncertain. This paper assesses the relevance estimation capabilities of VLMs,
including CLIP, LLaVA, and GPT-4V, within a large-scale \textit{ad hoc}
retrieval task tailored for multimedia content creation in a zero-shot fashion.
Preliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,
encompassing open-source and closed-source visual-instruction-tuned Large
Language Models (LLMs), achieve notable Kendall's $\tau \sim 0.4$ when compared
to human relevance judgments, surpassing the CLIPScore metric. (2) While
CLIPScore is strongly preferred, LLMs are less biased towards CLIP-based
retrieval systems. (3) GPT-4V's score distribution aligns more closely with
human judgments than other models, achieving a Cohen's $\kappa$ value of around
0.08, which outperforms CLIPScore at approximately -0.096. These findings
underscore the potential of LLM-powered VLMs in enhancing relevance judgments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM SIGIR 2024 LLM4Eval Workshop:
  https://llm4eval.github.io/papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balanced Residual Distillation Learning for 3D Point Cloud
  Class-Incremental Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanzhi Su, Siyuan Chen, Yuan-Gen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) thrives due to its success in processing the
influx of information by learning from continuously added new classes while
preventing catastrophic forgetting about the old ones. It is essential for the
performance breakthrough of CIL to effectively refine past knowledge from the
base model and balance it with new learning. However, such an issue has not yet
been considered in current research. In this work, we explore the potential of
CIL from these perspectives and propose a novel balanced residual distillation
framework (BRD-CIL) to push the performance bar of CIL to a new higher level.
Specifically, BRD-CIL designs a residual distillation learning strategy, which
can dynamically expand the network structure to capture the residuals between
the base and target models, effectively refining the past knowledge.
Furthermore, BRD-CIL designs a balanced pseudo-label learning strategy by
generating a guidance mask to reduce the preference for old classes, ensuring
balanced learning from new and old classes. We apply the proposed BRD-CIL to a
challenging 3D point cloud semantic segmentation task where the data are
unordered and unstructured. Extensive experimental results demonstrate that
BRD-CIL sets a new benchmark with an outstanding balance capability in
class-biased scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy
  Correspondence Learning in Cross-Modal Retrieval <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Duan, Zhangxuan Gu, Zhenzhe Ying, Lei Qi, Changhua Meng, Yinghuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of cross-modal retrieval, seamlessly integrating diverse
modalities within multimedia remains a formidable challenge, especially given
the complexities introduced by noisy correspondence learning (NCL). Such noise
often stems from mismatched data pairs, which is a significant obstacle
distinct from traditional noisy labels. This paper introduces
Pseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address
this challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an
auxiliary "pseudo-classification" task that interprets captions as categorical
labels, steering the model to learn image-text semantic similarity through a
non-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,
capitalizing on PC$^2$'s pseudo-classification capability, we generate
pseudo-captions to provide more informative and tangible supervision for each
mismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed
to assistant the correction of correspondence. In addition to technical
contributions, we develop a realistic NCL dataset called Noise of Web (NoW),
which could be a new powerful NCL benchmark where noise exists naturally.
Empirical evaluations of PC$^2$ showcase marked improvements over existing
state-of-the-art robust cross-modal retrieval techniques on both simulated and
realistic datasets with various NCL settings. The contributed dataset and
source code are released at https://github.com/alipay/PC2-NoiseofWeb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingyu Li, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal semantic segmentation shows significant potential for enhancing
segmentation accuracy in complex scenes. However, current methods often
incorporate specialized feature fusion modules tailored to specific modalities,
thereby restricting input flexibility and increasing the number of training
parameters. To address these challenges, we propose StitchFusion, a
straightforward yet effective modal fusion framework that integrates
large-scale pre-trained models directly as encoders and feature fusers. This
approach facilitates comprehensive multi-modal and multi-scale feature fusion,
accommodating any visual modal inputs. Specifically, Our framework achieves
modal integration during encoding by sharing multi-modal visual information. To
enhance information exchange across modalities, we introduce a
multi-directional adapter module (MultiAdapter) to enable cross-modal
information transfer during encoding. By leveraging MultiAdapter to propagate
multi-scale information across pre-trained encoders during the encoding
process, StitchFusion achieves multi-modal visual information integration
during encoding. Extensive comparative experiments demonstrate that our model
achieves state-of-the-art performance on four multi-modal segmentation datasets
with minimal additional parameters. Furthermore, the experimental integration
of MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their
complementary nature. Our code is available at StitchFusion_repo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Backbone for Long-Horizon Robot Task Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuai Chen, Wei Chen, Dongmyoung Lee, Yukun Ge, Nicolas Rojas, Petar Kormushev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end robot learning, particularly for long-horizon tasks, often results
in unpredictable outcomes and poor generalization. To address these challenges,
we propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot
task understanding and transferability. This framework uses therbligs (basic
action elements) as the backbone to decompose high-level robot tasks into
elemental robot configurations, which are then integrated with current
foundation models to improve task understanding. The approach consists of two
stages: offline training and online testing. During the offline training stage,
we developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig
segmentation across various tasks. In the online testing stage, after a
one-shot demonstration of a new task is collected, our MGSF network extracts
high-level knowledge, which is then encoded into the image using Action
Registration (ActionREG). Additionally, the Large Language Model
(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure
precise action execution, facilitating trajectory transfer in novel robot
scenarios. Experimental results validate these methods, achieving 94.37% recall
in therblig segmentation and success rates of 94.4% and 80% in real-world
online robot testing for simple and complex scenarios, respectively.
Supplementary material is available at:
https://sites.google.com/view/therbligsbasedbackbone/home
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures. This work is intended to be submitted to IEEE
  Robotics and Automation Letters (RA-L) for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty
  and Semantic Object Cues for Gaze Guidance in Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vito Mengers, Nicolas Roth, Oliver Brock, Klaus Obermayer, Martin Rolfs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How we perceive objects around us depends on what we actively attend to, yet
our eye movements depend on the perceived objects. Still, object segmentation
and gaze behavior are typically treated as two independent processes. Drawing
on an information processing pattern from robotics, we present a mechanistic
model that simulates these processes for dynamic real-world scenes. Our
image-computable model uses the current scene segmentation for object-based
saccadic decision-making while using the foveated object to refine its scene
segmentation recursively. To model this refinement, we use a Bayesian filter,
which also provides an uncertainty estimate for the segmentation that we use to
guide active scene exploration. We demonstrate that this model closely
resembles observers' free viewing behavior, measured by scanpath statistics,
including foveation duration and saccade amplitude distributions used for
parameter fitting and higher-level statistics not used for fitting. These
include how object detections, inspections, and returns are balanced and a
delay of returning saccades without an explicit implementation of such temporal
inhibition of return. Extensive simulations and ablation studies show that
uncertainty promotes balanced exploration and that semantic object cues are
crucial to form the perceptual units used in object-based attention. Moreover,
we show how our model's modular design allows for extensions, such as
incorporating saccadic momentum or pre-saccadic attention, to further align its
output with human scanpaths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35+16 pages, 8+4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TopoNAS: Boosting Search Efficiency of Gradient-based NAS via
  Topological Simplification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danpei Zhao, Zhuoran Liu, Bo Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving search efficiency serves as one of the crucial objectives of Neural
Architecture Search (NAS). However, many current approaches ignore the
universality of the search strategy and fail to reduce the computational
redundancy during the search process, especially in one-shot NAS architectures.
Besides, current NAS methods show invalid reparameterization in non-linear
search space, leading to poor efficiency in common search spaces like DARTS. In
this paper, we propose TopoNAS, a model-agnostic approach for gradient-based
one-shot NAS that significantly reduces searching time and memory usage by
topological simplification of searchable paths. Firstly, we model the
non-linearity in search spaces to reveal the parameterization difficulties. To
improve the search efficiency, we present a topological simplification method
and iteratively apply module-sharing strategies to simplify the topological
structure of searchable paths. In addition, a kernel normalization technique is
also proposed to preserve the search accuracy. Experimental results on the
NASBench201 benchmark with various search spaces demonstrate the effectiveness
of our method. It proves the proposed TopoNAS enhances the performance of
various architectures in terms of search efficiency while maintaining a high
level of accuracy. The project page is available at
https://xdedss.github.io/topo_simplification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Underwater Object Detection Enhancement via Channel Stabilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ali, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complex marine environment exacerbates the challenges of object detection
manifold. Marine trash endangers the aquatic ecosystem, presenting a persistent
challenge. Accurate detection of marine deposits is crucial for mitigating this
harm. Our work addresses underwater object detection by enhancing image quality
and evaluating detection methods. We use Detectron2's backbone with various
base models and configurations for this task.
  We propose a novel channel stabilization technique alongside a simplified
image enhancement model to reduce haze and color cast in training images,
improving multi-scale object detection. Following image processing, we test
different Detectron2 backbones for optimal detection accuracy. Additionally, we
apply a sharpening filter with augmentation techniques to highlight object
profiles for easier recognition.
  Results are demonstrated on the TrashCan Dataset, both instance and material
versions. The best-performing backbone method incorporates our channel
stabilization and augmentation techniques. We also compare our Detectron2
detection results with the Deformable Transformer. In the instance version of
TrashCan 1.0, our method achieves a 9.53% absolute increase in average
precision for small objects and a 7% absolute gain in bounding box detection
compared to the baseline. The code will be available on Code:
https://github.com/aliman80/Underwater-
Object-Detection-via-Channel-Stablization
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DPX: Progressive 2D-to-3D Oral Image Reconstruction with Hybrid MLP-CNN
  Networks <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuang Li, Mingyuan Meng, Zimo Huang, Lei Bi, Eduardo Delamare, Dagan Feng, Bin Sheng, Jinman Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic X-ray (PX) is a prevalent modality in dental practice for its wide
availability and low cost. However, as a 2D projection image, PX does not
contain 3D anatomical information, and therefore has limited use in dental
applications that can benefit from 3D information, e.g., tooth angular
misa-lignment detection and classification. Reconstructing 3D structures
directly from 2D PX has recently been explored to address limitations with
existing methods primarily reliant on Convolutional Neural Networks (CNNs) for
direct 2D-to-3D mapping. These methods, however, are unable to correctly infer
depth-axis spatial information. In addition, they are limited by the in-trinsic
locality of convolution operations, as the convolution kernels only capture the
information of immediate neighborhood pixels. In this study, we propose a
progressive hybrid Multilayer Perceptron (MLP)-CNN pyra-mid network (3DPX) for
2D-to-3D oral PX reconstruction. We introduce a progressive reconstruction
strategy, where 3D images are progressively re-constructed in the 3DPX with
guidance imposed on the intermediate recon-struction result at each pyramid
level. Further, motivated by the recent ad-vancement of MLPs that show promise
in capturing fine-grained long-range dependency, our 3DPX integrates MLPs and
CNNs to improve the semantic understanding during reconstruction. Extensive
experiments on two large datasets involving 464 studies demonstrate that our
3DPX outperforms state-of-the-art 2D-to-3D oral reconstruction methods,
including standalone MLP and transformers, in reconstruction quality, and also
im-proves the performance of downstream angular misalignment classification
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and
  Resampling <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huo, Zixin Guo, Xinxin Zuo, Zhihao Shi, Juwei Lu, Peng Dai, Songcen Xu, Li Cheng, Yee-Hong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a 3D mesh, we aim to synthesize 3D textures that correspond to
arbitrary textual descriptions. Current methods for generating and assembling
textures from sampled views often result in prominent seams or excessive
smoothing. To tackle these issues, we present TexGen, a novel multi-view
sampling and resampling framework for texture generation leveraging a
pre-trained text-to-image diffusion model. For view consistent sampling, first
of all we maintain a texture map in RGB space that is parameterized by the
denoising step and updated after each sampling step of the diffusion model to
progressively reduce the view discrepancy. An attention-guided multi-view
sampling strategy is exploited to broadcast the appearance information across
views. To preserve texture details, we develop a noise resampling technique
that aids in the estimation of noise, generating inputs for subsequent
denoising steps, as directed by the text prompt and current texture map.
Through an extensive amount of qualitative and quantitative evaluations, we
demonstrate that our proposed method produces significantly better texture
quality for diverse 3D objects with a high degree of view consistency and rich
appearance details, outperforming current state-of-the-art methods.
Furthermore, our proposed texture generation technique can also be applied to
texture editing while preserving the original identity. More experimental
results are available at https://dong-huo.github.io/TexGen/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>European Conference on Computer Vision (ECCV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning based Visually Rich Document Content Understanding: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Ding, Jean Lee, Soyeon Caren Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visually Rich Documents (VRDs) are essential in academia, finance, medical
fields, and marketing due to their multimodal information content. Traditional
methods for extracting information from VRDs depend on expert knowledge and
manual labor, making them costly and inefficient. The advent of deep learning
has revolutionized this process, introducing models that leverage multimodal
information vision, text, and layout along with pretraining tasks to develop
comprehensive document representations. These models have achieved
state-of-the-art performance across various downstream tasks, significantly
enhancing the efficiency and accuracy of information extraction from VRDs. In
response to the growing demands and rapid developments in Visually Rich
Document Understanding (VRDU), this paper provides a comprehensive review of
deep learning-based VRDU frameworks. We systematically survey and analyze
existing methods and benchmark datasets, categorizing them based on adopted
strategies and downstream tasks. Furthermore, we compare different techniques
used in VRDU models, focusing on feature representation and fusion, model
architecture, and pretraining methods, while highlighting their strengths,
limitations, and appropriate scenarios. Finally, we identify emerging trends
and challenges in VRDU, offering insights into future research directions and
practical applications. This survey aims to provide a thorough understanding of
VRDU advancements, benefiting both academic and industrial sectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot
  Learning: A General Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liuyuan Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Zero-Shot Learning (GZSL) is a challenging task requiring
accurate classification of both seen and unseen classes. Within this domain,
Audio-visual GZSL emerges as an extremely exciting yet difficult task, given
the inclusion of both visual and acoustic features as multi-modal inputs.
Existing efforts in this field mostly utilize either embedding-based or
generative-based methods. However, generative training is difficult and
unstable, while embedding-based methods often encounter domain shift problem.
Thus, we find it promising to integrate both methods into a unified framework
to leverage their advantages while mitigating their respective disadvantages.
Our study introduces a general framework employing out-of-distribution (OOD)
detection, aiming to harness the strengths of both approaches. We first employ
generative adversarial networks to synthesize unseen features, enabling the
training of an OOD detector alongside classifiers for seen and unseen classes.
This detector determines whether a test feature belongs to seen or unseen
classes, followed by classification utilizing separate classifiers for each
feature type. We test our framework on three popular audio-visual datasets and
observe a significant improvement comparing to existing state-of-the-art works.
Codes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wave-Mamba: Wavelet State Space Model for Ultra-High-Definition
  Low-Light Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbin Zou, Hongxia Gao, Weipeng Yang, Tongtong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultra-high-definition (UHD) technology has attracted widespread attention due
to its exceptional visual quality, but it also poses new challenges for
low-light image enhancement (LLIE) techniques. UHD images inherently possess
high computational complexity, leading existing UHD LLIE methods to employ
high-magnification downsampling to reduce computational costs, which in turn
results in information loss. The wavelet transform not only allows downsampling
without loss of information, but also separates the image content from the
noise. It enables state space models (SSMs) to avoid being affected by noise
when modeling long sequences, thus making full use of the long-sequence
modeling capability of SSMs. On this basis, we propose Wave-Mamba, a novel
approach based on two pivotal insights derived from the wavelet domain: 1) most
of the content information of an image exists in the low-frequency component,
less in the high-frequency component. 2) The high-frequency component exerts a
minimal influence on the outcomes of low-light enhancement. Specifically, to
efficiently model global content information on UHD images, we proposed a
low-frequency state space block (LFSSBlock) by improving SSMs to focus on
restoring the information of low-frequency sub-bands. Moreover, we propose a
high-frequency enhance block (HFEBlock) for high-frequency sub-band
information, which uses the enhanced low-frequency information to correct the
high-frequency information and effectively restore the correct high-frequency
details. Through comprehensive evaluation, our method has demonstrated superior
performance, significantly outshining current leading techniques while
maintaining a more streamlined architecture. The code is available at
https://github.com/AlexZou14/Wave-Mamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, ACMMM2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Framework to Boost 3D GS Initialization for Text-to-3D
  Generation by Lexical Richness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lutao Jiang, Hangyu Li, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-3D content creation has recently received much attention, especially
with the prevalence of 3D Gaussians Splatting. In general, GS-based methods
comprise two key stages: initialization and rendering optimization. To achieve
initialization, existing works directly apply random sphere initialization or
3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such
strategies suffer from two critical yet challenging problems: 1) the final
shapes are still similar to the initial ones even after training; 2) shapes can
be produced only from simple texts, e.g., "a dog", not for lexically richer
texts, e.g., "a dog is sitting on the top of the airplane". To address these
problems, this paper proposes a novel general framework to boost the 3D GS
Initialization for text-to-3D generation upon the lexical richness. Our key
idea is to aggregate 3D Gaussians into spatially uniform voxels to represent
complex shapes while enabling the spatial interaction among the 3D Gaussians
and semantic interaction between Gaussians and texts. Specifically, we first
construct a voxelized representation, where each voxel holds a 3D Gaussian with
its position, scale, and rotation fixed while setting opacity as the sole
factor to determine a position's occupancy. We then design an initialization
network mainly consisting of two novel components: 1) Global Information
Perception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design
enables each 3D Gaussian to assimilate the spatial information from other areas
and semantic information from texts. Extensive experiments show the superiority
of our framework of high-quality 3D GS initialization against the existing
methods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.
Also, our framework can be seamlessly plugged into SoTA training frameworks,
e.g., LucidDreamer, for semantically consistent text-to-3D generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP4Sketch: Enhancing Sketch to Mugshot Matching through <span class="highlight-title">Dataset</span>
  Augmentation using Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushal Kumar Jain, Steve Grosz, Anoop M. Namboodiri, Anil K. Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forensic sketch-to-mugshot matching is a challenging task in face
recognition, primarily hindered by the scarcity of annotated forensic sketches
and the modality gap between sketches and photographs. To address this, we
propose CLIP4Sketch, a novel approach that leverages diffusion models to
generate a large and diverse set of sketch images, which helps in enhancing the
performance of face recognition systems in sketch-to-mugshot matching. Our
method utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate
sketches with explicit control over identity and style. We combine CLIP and
Adaface embeddings of a reference mugshot, along with textual descriptions of
style, as the conditions to the diffusion model. We demonstrate the efficacy of
our approach by generating a comprehensive dataset of sketches corresponding to
mugshots and training a face recognition model on our synthetic data. Our
results show significant improvements in sketch-to-mugshot matching accuracy
over training on an existing, limited amount of real face sketch data,
validating the potential of diffusion models in enhancing the performance of
face recognition systems across modalities. We also compare our dataset with
datasets generated using GAN-based methods to show its superiority.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WaveMamba: Spatial-Spectral Wavelet Mamba for Hyperspectral Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ahmad, Muhammad Usama, Manual Mazzara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral Imaging (HSI) has proven to be a powerful tool for capturing
detailed spectral and spatial information across diverse applications. Despite
the advancements in Deep Learning (DL) and Transformer architectures for HSI
Classification (HSIC), challenges such as computational efficiency and the need
for extensive labeled data persist. This paper introduces WaveMamba, a novel
approach that integrates wavelet transformation with the Spatial-Spectral Mamba
architecture to enhance HSIC. WaveMamba captures both local texture patterns
and global contextual relationships in an end-to-end trainable model. The
Wavelet-based enhanced features are then processed through the state-space
architecture to model spatial-spectral relationships and temporal dependencies.
The experimental results indicate that WaveMamba surpasses existing models,
achieving an accuracy improvement of 4.5\% on the University of Houston dataset
and a 2.0\% increase on the Pavia University dataset. These findings validate
its effectiveness in addressing the complex data interactions inherent in HSIs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Caldarella, Massimiliano Mancini, Elisa Ricci, Rahaf Aljundi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) combine visual and textual understanding,
rendering them well-suited for diverse tasks like generating image captions and
answering visual questions across various domains. However, these capabilities
are built upon training on large amount of uncurated data crawled from the web.
The latter may include sensitive information that VLMs could memorize and leak,
raising significant privacy concerns. In this paper, we assess whether these
vulnerabilities exist, focusing on identity leakage. Our study leads to three
key findings: (i) VLMs leak identity information, even when the vision-language
alignment and the fine-tuning use anonymized data; (ii) context has little
influence on identity leakage; (iii) simple, widely used anonymization
techniques, like blurring, are not sufficient to address the problem. These
findings underscore the urgent need for robust privacy protection strategies
when deploying VLMs. Ethical awareness and responsible development practices
are essential to mitigate these risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-head Spatial-Spectral Mamba for Hyperspectral Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ahmad, Muhammad Hassaan Farooq Butt, Muhammad Usama, Hamad Ahmed Altuwaijri, Manual Mazzara, Salvatore Distenano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial-Spectral Mamba (SSM) improves computational efficiency and captures
long-range dependencies, addressing Transformer limitations. However,
traditional Mamba models overlook rich spectral information in HSIs and
struggle with high dimensionality and sequential data. To address these issues,
we propose the SSM with multi-head self-attention and token enhancement
(MHSSMamba). This model integrates spectral and spatial information by
enhancing spectral tokens and using multi-head attention to capture complex
relationships between spectral bands and spatial locations. It also manages
long-range dependencies and the sequential nature of HSI data, preserving
contextual information across spectral bands. MHSSMamba achieved remarkable
classification accuracies of 97.62\% on Pavia University, 96.92\% on the
University of Houston, 96.85\% on Salinas, and 99.49\% on Wuhan-longKou
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from
  a Single Sketch <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zidu Wang, Xiangyu Zhu, Jiang Yu, Tianshuo Zhang, Zhen Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D textured face reconstruction from sketches applicable in many scenarios
such as animation, 3D avatars, artistic design, missing people search, etc., is
a highly promising but underdeveloped research topic. On the one hand, the
stylistic diversity of sketches leads to existing sketch-to-3D-face methods
only being able to handle pose-limited and realistically shaded sketches. On
the other hand, texture plays a vital role in representing facial appearance,
yet sketches lack this information, necessitating additional texture control in
the reconstruction process. This paper proposes a novel method for
reconstructing controllable textured and detailed 3D faces from sketches, named
S2TD-Face. S2TD-Face introduces a two-stage geometry reconstruction framework
that directly reconstructs detailed geometry from the input sketch. To keep
geometry consistent with the delicate strokes of the sketch, we propose a novel
sketch-to-geometry loss that ensures the reconstruction accurately fits the
input features like dimples and wrinkles. Our training strategies do not rely
on hard-to-obtain 3D face scanning data or labor-intensive hand-drawn sketches.
Furthermore, S2TD-Face introduces a texture control module utilizing text
prompts to select the most suitable textures from a library and seamlessly
integrate them into the geometry, resulting in a 3D detailed face with
controllable texture. S2TD-Face surpasses existing state-of-the-art methods in
extensive quantitative and qualitative experiments. Our project is available at
https://github.com/wang-zidu/S2TD-Face .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Weakly Supervised and Globally Explainable Learning Framework for
  Brain Tumor Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruitao Xie, Limai Jiang, Xiaoxi He, Yi Pan, Yunpeng Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-based brain tumor segmentation can help doctors make better
diagnoses. However, the complex structure of brain tumors and expensive
pixel-level annotations present challenges for automatic tumor segmentation. In
this paper, we propose a counterfactual generation framework that not only
achieves exceptional brain tumor segmentation performance without the need for
pixel-level annotations, but also provides explainability. Our framework
effectively separates class-related features from class-unrelated features of
the samples, and generate new samples that preserve identity features while
altering class attributes by embedding different class-related features. We
perform topological data analysis on the extracted class-related features and
obtain a globally explainable manifold, and for each abnormal sample to be
segmented, a meaningful normal sample could be effectively generated with the
guidance of the rule-based paths designed within the manifold for comparison
for identifying the tumor regions. We evaluate our proposed method on two
datasets, which demonstrates superior performance of brain tumor segmentation.
The code is available at https://github.com/xrt11/tumor-segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Multimedia and Expo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, Xingyu Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  VAR is a new generation paradigm that employs 'next-scale prediction' as
opposed to 'next-token prediction'. This innovative transformation enables
auto-regressive (AR) transformers to rapidly learn visual distributions and
achieve robust generalization. However, the original VAR model is constrained
to class-conditioned synthesis, relying solely on textual captions for
guidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model
that integrates Visual Auto-Regressive techniques with the capabilities of
CLIP. The VAR-CLIP framework encodes captions into text embeddings, which are
then utilized as textual conditions for image generation. To facilitate
training on extensive datasets, such as ImageNet, we have constructed a
substantial image-text dataset leveraging BLIP2. Furthermore, we delve into the
significance of word positioning within CLIP for the purpose of caption
guidance. Extensive experiments confirm VAR-CLIP's proficiency in generating
fantasy images with high fidelity, textual congruence, and aesthetic
excellence. Our project page are https://github.com/daixiangzi/VAR-CLIP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>total 10 pages, code:https://github.com/daixiangzi/VAR-CLIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking <span class="highlight-title">Pre-train</span>ed Feature Extractor Selection in Multiple Instance
  Learning for Whole Slide Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Wong, Mun Yong Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL) has become a preferred method for
classifying gigapixel whole slide images (WSIs), without requiring patch label
annotation. The focus of the current MIL research stream is on the
embedding-based MIL approach, which involves extracting feature vectors from
patches using a pre-trained feature extractor. These feature vectors are then
fed into an MIL aggregator for slide-level prediction. Despite prior research
suggestions on enhancing the most commonly used ResNet50 supervised model
pre-trained on ImageNet-1K, there remains a lack of clear guidance on selecting
the optimal feature extractor to maximize WSI performance. This study aims at
addressing this gap by examining MIL feature extractors across three
dimensions: pre-training dataset, backbone model, and pre-training method.
Extensive experiments were carried out on the two public WSI datasets
(TCGA-NSCLC and Camelyon16) using four SOTA MIL models. The main findings
indicate the following: 1) Performance significantly improves with larger and
more varied pre-training datasets in both CNN and Transformer backbones. 2)
`Modern and deeper' backbones greatly outperform `standard' backbones (ResNet
and ViT), with performance improvements more guaranteed in Transformer-based
backbones. 3) The choice of self-supervised learning (SSL) method is crucial,
with the most significant benefits observed when applied to the Transformer
(ViT) backbone. The study findings have practical implications, including
designing more effective pathological foundation models. Our code is available
at: https://anonymous.4open.science/r/MIL-Feature-Extractor-Selection
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PreMix: Boosting Multiple Instance Learning in Digital Histopathology
  through <span class="highlight-title">Pre-train</span>ing with Intra-Batch Slide Mixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Wong, Mun Yong Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification of gigapixel-sized whole slide images (WSIs), digital
representations of histological slides obtained via a high-resolution scanner,
faces significant challenges associated with the meticulous and time-consuming
nature of fine-grained labeling. While weakly-supervised multiple instance
learning (MIL) has emerged as a promising approach, current MIL methods are
constrained by their limited ability to leverage the wealth of information
embedded within unlabeled WSIs. This limitation often necessitates training MIL
feature aggregators from scratch after the feature extraction process,
hindering efficiency and accuracy. PreMix extends the general MIL framework by
pre-training the MIL aggregator with an intra-batch slide mixing approach.
Specifically, PreMix incorporates Barlow Twins Slide Mixing during
pre-training, enhancing its ability to handle diverse WSI sizes and maximizing
the utility of unlabeled WSIs. Combined with Mixup and Manifold Mixup during
fine-tuning, PreMix achieves a mean of 4.7% performance improvement over the
baseline MIL framework, the hierarchical image pyramid transformer (HIPT), on
the Camelyon16 dataset. The observed improvement across a range of active
learning acquisition functions and WSI-labeled training budgets highlights the
framework's adaptability to diverse datasets and varying resource constraints.
Ultimately, PreMix paves the way for more efficient and accurate WSI
classification under limited WSI-labeled datasets, encouraging the broader
adoption of unlabeled WSI data in histopathological research. The code is
available at https://anonymous.4open.science/r/PreMix
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Curve Detection in Volumetric Medical Imaging via Attraction
  Field <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farukh Yaushev, Daria Nogina, Valentin Samokhin, Mariya Dugova, Ekaterina Petrash, Dmitry Sevryukov, Mikhail Belyaev, Maxim Pisov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding body part geometry is crucial for precise medical diagnostics.
Curves effectively describe anatomical structures and are widely used in
medical imaging applications related to cardiovascular, respiratory, and
skeletal diseases. Traditional curve detection methods are often task-specific,
relying heavily on domain-specific features, limiting their broader
applicability. This paper introduces a novel approach for detecting
non-branching curves, which does not require prior knowledge of the object's
orientation, shape, or position. Our method uses neural networks to predict (1)
an attraction field, which offers subpixel accuracy, and (2) a closeness map,
which limits the region of interest and essentially eliminates outliers far
from the desired curve. We tested our curve detector on several clinically
relevant tasks with diverse morphologies and achieved impressive subpixel-level
accuracy results that surpass existing methods, highlighting its versatility
and robustness. Additionally, to support further advancements in this field, we
provide our private annotations of aortic centerlines and masks, which can
serve as a benchmark for future research. The dataset can be found at
https://github.com/neuro-ml/curve-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ShapeMI MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting Global Perturbation Robustness of Image Models using
  Axiomatic Spectral Importance Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Róisín Luo, James McDermott, Colm O'Riordan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perturbation robustness evaluates the vulnerabilities of models, arising from
a variety of perturbations, such as data corruptions and adversarial attacks.
Understanding the mechanisms of perturbation robustness is critical for global
interpretability. We present a model-agnostic, global mechanistic
interpretability method to interpret the perturbation robustness of image
models. This research is motivated by two key aspects. First, previous global
interpretability works, in tandem with robustness benchmarks, e.g. mean
corruption error (mCE), are not designed to directly interpret the mechanisms
of perturbation robustness within image models. Second, we notice that the
spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially
decay over the frequency. This power-law-like decay implies that: Low-frequency
signals are generally more robust than high-frequency signals -- yet high
classification accuracy can not be achieved by low-frequency signals alone. By
applying Shapley value theory, our method axiomatically quantifies the
predictive powers of robust features and non-robust features within an
information theory framework. Our method, dubbed as \textbf{I-ASIDE}
(\textbf{I}mage \textbf{A}xiomatic \textbf{S}pectral \textbf{I}mportance
\textbf{D}ecomposition \textbf{E}xplanation), provides a unique insight into
model robustness mechanisms. We conduct extensive experiments over a variety of
vision models pre-trained on ImageNet to show that \textbf{I-ASIDE} can not
only \textbf{measure} the perturbation robustness but also \textbf{provide
interpretations} of its mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Transactions on Machine Learning Research (TMLR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changqun Xia, Chenxi Xie, Zhentao He, Tianshu Yu, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an advanced study on more challenging high-resolution salient
object detection (HRSOD) from both dataset and network framework perspectives.
To compensate for the lack of HRSOD dataset, we thoughtfully collect a
large-scale high resolution salient object detection dataset, called UHRSD,
containing 5,920 images from real-world complex scenarios at 4K-8K resolutions.
All the images are finely annotated in pixel-level, far exceeding previous
low-resolution SOD datasets. Aiming at overcoming the contradiction between the
sampling depth and the receptive field size in the past methods, we propose a
novel one-stage framework for HR-SOD task using pyramid grafting mechanism. In
general, transformer-based and CNN-based backbones are adopted to extract
features from different resolution images independently and then these features
are grafted from transformer branch to CNN branch. An attention-based
Cross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine
broken detailed information more holistically, guided by different source
feature during decoding process. Moreover, we design an Attention Guided Loss
(AGL) to explicitly supervise the attention matrix generated by CMGM to help
the network better interact with the attention from different branches.
Comprehensive experiments on UHRSD and widely-used SOD datasets demonstrate
that our method can simultaneously locate salient object and preserve rich
details, outperforming state-of-the-art methods. To verify the generalization
ability of the proposed framework, we apply it to the camouflaged object
detection (COD) task. Notably, our method performs superior to most
state-of-the-art COD methods without bells and whistles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IG-SLAM: Instant Gaussian SLAM <span class="chip">3DV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furkan Aykut Sarikamis, Abdullah Aydin Alatan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting has recently shown promising results as an alternative
scene representation in SLAM systems to neural implicit representations.
However, current methods either lack dense depth maps to supervise the mapping
process or detailed training designs that consider the scale of the
environment. To address these drawbacks, we present IG-SLAM, a dense RGB-only
SLAM system that employs robust Dense-SLAM methods for tracking and combines
them with Gaussian Splatting. A 3D map of the environment is constructed using
accurate pose and dense depth provided by tracking. Additionally, we utilize
depth uncertainty in map optimization to improve 3D reconstruction. Our decay
strategy in map optimization enhances convergence and allows the system to run
at 10 fps in a single process. We demonstrate competitive performance with
state-of-the-art RGB-only SLAM systems while achieving faster operation speeds.
We present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC
datasets. The system achieves photo-realistic 3D reconstruction in large-scale
sequences, particularly in the EuRoC dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 page ref, 5 figures, 3DV submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient and Effective <span class="highlight-title">Transformer</span> Decoder-Based Framework for
  Multi-Task Visual Grounding <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chen, Long Chen, Yu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most advanced visual grounding methods rely on Transformers for
visual-linguistic feature fusion. However, these Transformer-based approaches
encounter a significant drawback: the computational costs escalate
quadratically due to the self-attention mechanism in the Transformer Encoder,
particularly when dealing with high-resolution images or long context
sentences. This quadratic increase in computational burden restricts the
applicability of visual grounding to more intricate scenes, such as
conversation-based reasoning segmentation, which involves lengthy language
expressions. In this paper, we propose an efficient and effective multi-task
visual grounding (EEVG) framework based on Transformer Decoder to address this
issue, which reduces the cost in both language and visual aspects. In the
language aspect, we employ the Transformer Decoder to fuse visual and
linguistic features, where linguistic features are input as memory and visual
features as queries. This allows fusion to scale linearly with language
expression length. In the visual aspect, we introduce a parameter-free approach
to reduce computation by eliminating background visual tokens based on
attention scores. We then design a light mask head to directly predict
segmentation masks from the remaining sparse feature maps. Extensive results
and ablation studies on benchmarks demonstrate the efficiency and effectiveness
of our approach. Code is available in https://github.com/chenwei746/EEVG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21pages, 10 figures, 9 tables. Accepted to ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contribution-based Low-Rank Adaptation with <span class="highlight-title">Pre-train</span>ing Model for Real
  Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donwon Park, Hayeon Kim, Se Young Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, pre-trained model and efficient parameter tuning have achieved
remarkable success in natural language processing and high-level computer
vision with the aid of masked modeling and prompt tuning. In low-level computer
vision, however, there have been limited investigations on pre-trained models
and even efficient fine-tuning strategy has not yet been explored despite its
importance and benefit in various real-world tasks such as alleviating memory
inflation issue when integrating new tasks on AI edge devices. Here, we propose
a novel efficient parameter tuning approach dubbed contribution-based low-rank
adaptation (CoLoRA) for multiple image restorations along with effective
pre-training method with random order degradations (PROD). Unlike prior arts
that tune all network parameters, our CoLoRA effectively fine-tunes small
amount of parameters by leveraging LoRA (low-rank adaptation) for each new
vision task with our contribution-based method to adaptively determine layer by
layer capacity for that task to yield comparable performance to full tuning.
Furthermore, our PROD strategy allows to extend the capability of pre-trained
models with improved performance as well as robustness to bridge synthetic
pre-training and real-world fine-tuning. Our CoLoRA with PROD has demonstrated
its superior performance in various image restoration tasks across diverse
degradation types on both synthetic and real-world datasets for known and novel
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 15 figures, for homepage see this url :
  https://janeyeon.github.io/colora/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Partial Optimal Transport for Universal Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Yang, Xiang Gu, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal domain adaptation (UniDA) aims to transfer knowledge from a labeled
source domain to an unlabeled target domain without requiring the same label
sets of both domains. The existence of domain and category shift makes the task
challenging and requires us to distinguish "known" samples (i.e., samples whose
labels exist in both domains) and "unknown" samples (i.e., samples whose labels
exist in only one domain) in both domains before reducing the domain gap. In
this paper, we consider the problem from the point of view of distribution
matching which we only need to align two distributions partially. A novel
approach, dubbed mini-batch Prototypical Partial Optimal Transport (m-PPOT), is
proposed to conduct partial distribution alignment for UniDA. In training
phase, besides minimizing m-PPOT, we also leverage the transport plan of m-PPOT
to reweight source prototypes and target samples, and design reweighted entropy
loss and reweighted cross-entropy loss to distinguish "known" and "unknown"
samples. Experiments on four benchmarks show that our method outperforms the
previous state-of-the-art UniDA methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effect of Fog Particle Size Distribution on 3D Object Detection Under
  Adverse Weather Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajinkya Shinde, Gaurav Sharma, Manisha Pattanaik, Sri Niwas Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based sensors employing optical spectrum signals play a vital role in
providing significant information about the target objects in autonomous
driving vehicle systems. However, the presence of fog in the atmosphere
severely degrades the overall system's performance. This manuscript analyzes
the role of fog particle size distributions in 3D object detection under
adverse weather conditions. We utilise Mie theory and meteorological optical
range (MOR) to calculate the attenuation and backscattering coefficient values
for point cloud generation and analyze the overall system's accuracy in Car,
Cyclist, and Pedestrian case scenarios under easy, medium and hard detection
difficulties. Gamma and Junge (Power-Law) distributions are employed to
mathematically model the fog particle size distribution under strong and
moderate advection fog environments. Subsequently, we modified the KITTI
dataset based on the backscattering coefficient values and trained it on the
PV-RCNN++ deep neural network model for Car, Cyclist, and Pedestrian cases
under different detection difficulties. The result analysis shows a significant
variation in the system's accuracy concerning the changes in target object
dimensionality, the nature of the fog environment and increasing detection
difficulties, with the Car exhibiting the highest accuracy of around 99% and
the Pedestrian showing the lowest accuracy of around 73%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FCDFusion: a Fast, Low Color Deviation Method for Fusing Visible and
  Infrared Image Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hesong Li, Ying Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible and infrared image fusion (VIF) aims to combine information from
visible and infrared images into a single fused image. Previous VIF methods
usually employ a color space transformation to keep the hue and saturation from
the original visible image. However, for fast VIF methods, this operation
accounts for the majority of the calculation and is the bottleneck preventing
faster processing. In this paper, we propose a fast fusion method, FCDFusion,
with little color deviation. It preserves color information without color space
transformations, by directly operating in RGB color space. It incorporates
gamma correction at little extra cost, allowing color and contrast to be
rapidly improved. We regard the fusion process as a scaling operation on 3D
color vectors, greatly simplifying the calculations. A theoretical analysis and
experiments show that our method can achieve satisfactory results in only 7
FLOPs per pixel. Compared to state-of-the-art fast, color-preserving methods
using HSV color space, our method provides higher contrast at only half of the
computational cost. We further propose a new metric, color deviation, to
measure the ability of a VIF method to preserve color. It is specifically
designed for VIF tasks with color visible-light images, and overcomes
deficiencies of existing VIF metrics used for this purpose. Our code is
available at https://github.com/HeasonLee/FCDFusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted by Computational Visual Media</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhysMamba: Leveraging Dual-Stream Cross-Attention SSD for Remote
  Physiological Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Yan, Yan Zhong, Wenjun Zhang, Lin Shu, Hongbin Xu, Wenxiong Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote Photoplethysmography (rPPG) is a non-contact technique for extracting
physiological signals from facial videos, used in applications like emotion
monitoring, medical assistance, and anti-face spoofing. Unlike controlled
laboratory settings, real-world environments often contain motion artifacts and
noise, affecting the performance of existing methods. To address this, we
propose PhysMamba, a dual-stream time-frequency interactive model based on
Mamba. PhysMamba integrates the state-of-the-art Mamba-2 model and employs a
dual-stream architecture to learn diverse rPPG features, enhancing robustness
in noisy conditions. Additionally, we designed the Cross-Attention State Space
Duality (CASSD) module to improve information exchange and feature
complementarity between the two streams. We validated PhysMamba using PURE,
UBFC-rPPG and MMPD. Experimental results show that PhysMamba achieves
state-of-the-art performance across various scenarios, particularly in complex
environments, demonstrating its potential in practical remote heart rate
monitoring applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting the Semantic Knowledge of <span class="highlight-title">Pre-train</span>ed Text-Encoders for
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Yu, Zhe Tao, Hantao Yao, Joost Van de Weijer, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) excel on fixed datasets but struggle with
incremental and shifting data in real-world scenarios. Continual learning
addresses this challenge by allowing models to learn from new data while
retaining previously learned knowledge. Existing methods mainly rely on visual
features, often neglecting the rich semantic information encoded in text. The
semantic knowledge available in the label information of the images, offers
important semantic information that can be related with previously acquired
knowledge of semantic classes. Consequently, effectively leveraging this
information throughout continual learning is expected to be beneficial. To
address this, we propose integrating semantic guidance within and across tasks
by capturing semantic similarity using text embeddings. We start from a
pre-trained CLIP model, employ the \emph{Semantically-guided Representation
Learning (SG-RL)} module for a soft-assignment towards all current task
classes, and use the Semantically-guided Knowledge Distillation (SG-KD) module
for enhanced knowledge transfer. Experimental results demonstrate the
superiority of our method on general and fine-grained datasets. Our code can be
found in
https://github.com/aprilsveryown/semantically-guided-continual-learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amodal Segmentation for Laparoscopic Surgery Video Instruments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohua Shi, Zhaochen Liu, Lingyu Duan, Tingting Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of surgical instruments is crucial for enhancing surgeon
performance and ensuring patient safety. Conventional techniques such as
binary, semantic, and instance segmentation share a common drawback: they do
not accommodate the parts of instruments obscured by tissues or other
instruments. Precisely predicting the full extent of these occluded instruments
can significantly improve laparoscopic surgeries by providing critical guidance
during operations and assisting in the analysis of potential surgical errors,
as well as serving educational purposes. In this paper, we introduce Amodal
Segmentation to the realm of surgical instruments in the medical field. This
technique identifies both the visible and occluded parts of an object. To
achieve this, we introduce a new Amoal Instruments Segmentation (AIS) dataset,
which was developed by reannotating each instrument with its complete mask,
utilizing the 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge
dataset. Additionally, we evaluate several leading amodal segmentation methods
to establish a benchmark for this new dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Gaze Object Prediction via Pixel-level Supervision from Vision
  Foundation Model <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Jin, Lei Zhang, Shi Yan, Bin Fan, Binglu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaze object prediction (GOP) aims to predict the category and location of the
object that a human is looking at. Previous methods utilized box-level
supervision to identify the object that a person is looking at, but struggled
with semantic ambiguity, ie, a single box may contain several items since
objects are close together. The Vision foundation model (VFM) has improved in
object segmentation using box prompts, which can reduce confusion by more
precisely locating objects, offering advantages for fine-grained prediction of
gaze objects. This paper presents a more challenging gaze object segmentation
(GOS) task, which involves inferring the pixel-level mask corresponding to the
object captured by human gaze behavior. In particular, we propose that the
pixel-level supervision provided by VFM can be integrated into gaze object
prediction to mitigate semantic ambiguity. This leads to our gaze object
detection and segmentation framework that enables accurate pixel-level
predictions. Different from previous methods that require additional head input
or ignore head features, we propose to automatically obtain head features from
scene features to ensure the model's inference efficiency and flexibility in
the real world. Moreover, rather than directly fuse features to predict gaze
heatmap as in existing methods, which may overlook spatial location and subtle
details of the object, we develop a space-to-object gaze regression method to
facilitate human-object gaze interaction. Specifically, it first constructs an
initial human-object spatial connection, then refines this connection by
interacting with semantically clear features in the segmentation branch,
ultimately predicting a gaze heatmap for precise localization. Extensive
experiments on GOO-Synth and GOO-Real datasets demonstrate the effectiveness of
our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Split Learning with Vision <span class="highlight-title">Transformer</span>s using
  Patch-Wise Random and Noisy CutMix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungeun Oh, Sihun Baek, Jihong Park, Hyelin Nam, Praneeth Vepakomma, Ramesh Raskar, Mehdi Bennis, Seong-Lyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, the vision transformer (ViT) has increasingly superseded
the convolutional neural network (CNN) for improved accuracy and robustness.
However, ViT's large model sizes and high sample complexity make it difficult
to train on resource-constrained edge devices. Split learning (SL) emerges as a
viable solution, leveraging server-side resources to train ViTs while utilizing
private data from distributed devices. However, SL requires additional
information exchange for weight updates between the device and the server,
which can be exposed to various attacks on private training data. To mitigate
the risk of data breaches in classification tasks, inspired from the CutMix
regularization, we propose a novel privacy-preserving SL framework that injects
Gaussian noise into smashed data and mixes randomly chosen patches of smashed
data across clients, coined DP-CutMixSL. Our analysis demonstrates that
DP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy
protection against membership inference attacks during forward propagation.
Through simulations, we show that DP-CutMixSL improves privacy protection
against membership inference attacks, reconstruction attacks, and label
inference attacks, while also improving accuracy compared to DP-SL and
DP-MixSL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, 8 tables, to be published in Transactions on
  Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MambaST: A Plug-and-Play Cross-Spectral Spatial-Temporal Fuser for
  Efficient Pedestrian Detection <span class="chip">SC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangbo Gao, Asiegbu Miracle Kanu-Asiegbu, Xiaoxiao Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes MambaST, a plug-and-play cross-spectral spatial-temporal
fusion pipeline for efficient pedestrian detection. Several challenges exist
for pedestrian detection in autonomous driving applications. First, it is
difficult to perform accurate detection using RGB cameras under dark or
low-light conditions. Cross-spectral systems must be developed to integrate
complementary information from multiple sensor modalities, such as thermal and
visible cameras, to improve the robustness of the detections. Second,
pedestrian detection models are latency-sensitive. Efficient and easy-to-scale
detection models with fewer parameters are highly desirable for real-time
applications such as autonomous driving. Third, pedestrian video data provides
spatial-temporal correlations of pedestrian movement. It is beneficial to
incorporate temporal as well as spatial information to enhance pedestrian
detection. This work leverages recent advances in the state space model (Mamba)
and proposes a novel Multi-head Hierarchical Patching and Aggregation (MHHPA)
structure to extract both fine-grained and coarse-grained information from both
RGB and thermal imagery. Experimental results show that the proposed MHHPA is
an effective and efficient alternative to a Transformer model for
cross-spectral pedestrian detection. Our proposed model also achieves superior
performance on small-scale pedestrian detection. The code is available at
https://github.com/XiangboGaoBarry/MambaST}{https://github.com/XiangboGaoBarry/MambaST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ITSC 2024 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure from Motion-based Motion Estimation and 3D Reconstruction of
  Unknown Shaped Space Debris 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kentaro Uno, Takehiro Matsuoka, Akiyoshi Uchida, Kazuya Yoshida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the boost in the number of spacecraft launches in the current decades,
the space debris problem is daily becoming significantly crucial. For
sustainable space utilization, the continuous removal of space debris is the
most severe problem for humanity. To maximize the reliability of the debris
capture mission in orbit, accurate motion estimation of the target is
essential. Space debris has lost its attitude and orbit control capabilities,
and its shape is unknown due to the break. This paper proposes the Structure
from Motion-based algorithm to perform unknown shaped space debris motion
estimation with limited resources, where only 2D images are required as input.
The method then outputs the reconstructed shape of the unknown object and the
relative pose trajectory between the target and the camera simultaneously,
which are exploited to estimate the target's motion. The method is
quantitatively validated with the realistic image dataset generated by the
microgravity experiment in a 2D air-floating testbed and 3D kinematic
simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 10 figures. Manuscript accepted at the 2024 IEEE 20th
  International Conference on Automation Science and Engineerin (CASE 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POA: <span class="highlight-title">Pre-train</span>ing Once for Models of All Sizes <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingying Zhang, Xin Guo, Jiangwei Lao, Lei Yu, Lixiang Ru, Jian Wang, Guo Ye, Huimei He, Jingdong Chen, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale self-supervised pre-training has paved the way for one foundation
model to handle many different vision tasks. Most pre-training methodologies
train a single model of a certain size at one time. Nevertheless, various
computation or storage constraints in real-world scenarios require substantial
efforts to develop a series of models with different sizes to deploy. Thus, in
this study, we propose a novel tri-branch self-supervised training framework,
termed as POA (Pre-training Once for All), to tackle this aforementioned issue.
Our approach introduces an innovative elastic student branch into a modern
self-distillation paradigm. At each pre-training step, we randomly sample a
sub-network from the original student to form the elastic student and train all
branches in a self-distilling fashion. Once pre-trained, POA allows the
extraction of pre-trained models of diverse sizes for downstream tasks.
Remarkably, the elastic student facilitates the simultaneous pre-training of
multiple models with different sizes, which also acts as an additional ensemble
of models of various sizes to enhance representation learning. Extensive
experiments, including k-nearest neighbors, linear probing evaluation and
assessments on multiple downstream tasks demonstrate the effectiveness and
advantages of our POA. It achieves state-of-the-art performance using ViT, Swin
Transformer and ResNet backbones, producing around a hundred models with
different sizes through a single pre-training session. The code is available
at: https://github.com/Qichuzyy/POA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PINNs for Medical Image Analysis: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chayan Banerjee, Kien Nguyen, Olivier Salvado, Truyen Tran, Clinton Fookes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The incorporation of physical information in machine learning frameworks is
transforming medical image analysis (MIA). By integrating fundamental knowledge
and governing physical laws, these models achieve enhanced robustness and
interpretability. In this work, we explore the utility of physics-informed
approaches for MIA (PIMIA) tasks such as registration, generation,
classification, and reconstruction. We present a systematic literature review
of over 80 papers on physics-informed methods dedicated to MIA. We propose a
unified taxonomy to investigate what physics knowledge and processes are
modelled, how they are represented, and the strategies to incorporate them into
MIA models. We delve deep into a wide range of image analysis tasks, from
imaging, generation, prediction, inverse imaging (super-resolution and
reconstruction), registration, and image analysis (segmentation and
classification). For each task, we thoroughly examine and present in a tabular
format the central physics-guided operation, the region of interest (with
respect to human anatomy), the corresponding imaging modality, the dataset used
for model training, the deep network architecture employed, and the primary
physical process, equation, or principle utilized. Additionally, we also
introduce a novel metric to compare the performance of PIMIA methods across
different tasks and datasets. Based on this review, we summarize and distil our
perspectives on the challenges, open research questions, and directions for
future research. We highlight key open challenges in PIMIA, including selecting
suitable physics priors and establishing a standardized benchmarking platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EIUP: A Training-Free Approach to Erase Non-Compliant Concepts
  Conditioned on Implicit Unsafe <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Die Chen, Zhiwen Li, Mingyuan Fan, Cen Chen, Wenmeng Zhou, Yaliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have shown the ability to learn a diverse
range of concepts. However, it is worth noting that they may also generate
undesirable outputs, consequently giving rise to significant security concerns.
Specifically, issues such as Not Safe for Work (NSFW) content and potential
violations of style copyright may be encountered. Since image generation is
conditioned on text, prompt purification serves as a straightforward solution
for content safety. Similar to the approach taken by LLM, some efforts have
been made to control the generation of safe outputs by purifying prompts.
However, it is also important to note that even with these efforts, non-toxic
text still carries a risk of generating non-compliant images, which is referred
to as implicit unsafe prompts. Furthermore, some existing works fine-tune the
models to erase undesired concepts from model weights. This type of method
necessitates multiple training iterations whenever the concept is updated,
which can be time-consuming and may potentially lead to catastrophic
forgetting. To address these challenges, we propose a simple yet effective
approach that incorporates non-compliant concepts into an erasure prompt. This
erasure prompt proactively participates in the fusion of image spatial features
and text embeddings. Through attention mechanisms, our method is capable of
identifying feature representations of non-compliant concepts in the image
space. We re-weight these features to effectively suppress the generation of
unsafe images conditioned on original implicit unsafe prompts. Our method
exhibits superior erasure effectiveness while achieving high scores in image
fidelity compared to the state-of-the-art baselines. WARNING: This paper
contains model outputs that may be offensive.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features
  for Highly Controllable Text-Driven Image Translation <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Gao, Jiaying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models have been a revolutionary
milestone in the evolution of generative AI and multimodal technology, allowing
extraordinary image generation based on natural-language text prompts. However,
the issue of lacking controllability of such models restricts their practical
applicability for real-life content creation, for which attention has been
focused on leveraging a reference image to control text-to-image synthesis. Due
to the close correlation between the reference image and the generated image,
this problem can also be regarded as the task of manipulating (or editing) the
reference image as per the text, namely text-driven image-to-image translation.
This paper contributes a novel, concise, and efficient approach that adapts the
pre-trained large-scale text-to-image (T2I) diffusion model to the
image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality
and versatile text-driven I2I translation without any model training, model
fine-tuning, or online optimization process. To guide T2I generation with a
reference image, we propose to model diverse guiding factors with
correspondingly different frequency bands of diffusion features in the DCT
spectral space, and accordingly devise a novel frequency band substitution
layer that dynamically substitutes a certain DCT frequency band of the
diffusion features with the corresponding counterpart of the reference image
along the reverse sampling process. We demonstrate that our method flexibly
enables highly controllable text-driven I2I translation both in the guiding
factor and guiding intensity of the reference image, simply by tuning the type
and bandwidth of the substituted frequency band, respectively. Extensive
qualitative and quantitative experiments verify the superiority of our approach
over related methods in I2I translation visual quality, versatility, and
controllability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted conference paper of ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visible-Thermal Multiple Object Tracking: Large-scale Video <span class="highlight-title">Dataset</span> and
  Progressive Fusion Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhu, Qianwu Wang, Chenglong Li, Jin Tang, Zhixiang Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complementary benefits from visible and thermal infrared data are widely
utilized in various computer vision task, such as visual tracking, semantic
segmentation and object detection, but rarely explored in Multiple Object
Tracking (MOT). In this work, we contribute a large-scale Visible-Thermal video
benchmark for MOT, called VT-MOT. VT-MOT has the following main advantages. 1)
The data is large scale and high diversity. VT-MOT includes 582 video sequence
pairs, 401k frame pairs from surveillance, drone, and handheld platforms. 2)
The cross-modal alignment is highly accurate. We invite several professionals
to perform both spatial and temporal alignment frame by frame. 3) The
annotation is dense and high-quality. VT-MOT has 3.99 million annotation boxes
annotated and double-checked by professionals, including heavy occlusion and
object re-acquisition (object disappear and reappear) challenges. To provide a
strong baseline, we design a simple yet effective tracking framework, which
effectively fuses temporal information and complementary information of two
modalities in a progressive manner, for robust visible-thermal MOT. A
comprehensive experiment are conducted on VT-MOT and the results prove the
superiority and effectiveness of the proposed method compared with
state-of-the-art methods. From the evaluation results and analysis, we specify
several potential future directions for visible-thermal MOT. The project is
released in https://github.com/wqw123wqw/PFTrack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Object Heights From LiDAR & Aerial Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus Guerrero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work shows a procedural method for extracting object heights from LiDAR
and aerial imagery. We discuss how to get heights and the future of LiDAR and
imagery processing. SOTA object segmentation allows us to take get object
heights with no deep learning background. Engineers will be keeping track of
world data across generations and reprocessing them. They will be using older
procedural methods like this paper and newer ones discussed here. SOTA methods
are going beyond analysis and into generative AI. We cover both a procedural
methodology and the newer ones performed with language models. These include
point cloud, imagery and text encoding allowing for spatially aware AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIS-ME: A Multi-modal Framework for Soil Moisture Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Rakib, Adil Aman Mohammed, Cole Diggins, Sumit Sharma, Jeff Michael Sadler, Tyson Ochsner, Arun Bagavathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soil moisture estimation is an important task to enable precision agriculture
in creating optimal plans for irrigation, fertilization, and harvest. It is
common to utilize statistical and machine learning models to estimate soil
moisture from traditional data sources such as weather forecasts, soil
properties, and crop properties. However, there is a growing interest in
utilizing aerial and geospatial imagery to estimate soil moisture. Although
these images capture high-resolution crop details, they are expensive to curate
and challenging to interpret. Imagine, an AI-enhanced software tool that
predicts soil moisture using visual cues captured by smartphones and
statistical data given by weather forecasts. This work is a first step towards
that goal of developing a multi-modal approach for soil moisture estimation. In
particular, we curate a dataset consisting of real-world images taken from
ground stations and their corresponding weather data. We also propose MIS-ME -
Meteorological & Image based Soil Moisture Estimator, a multi-modal framework
for soil moisture estimation. Our extensive analysis shows that MIS-ME achieves
a MAPE of 10.79%, outperforming traditional unimodal approaches with a
reduction of 2.6% in MAPE for meteorological data and 1.5% in MAPE for image
data, highlighting the effectiveness of tailored multi-modal approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DSAA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Trajectory Prediction with Multi-View Data Integration in
  Cooperative Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Rahul Bhadani, Larry Head
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research on trajectory prediction primarily relies on data collected
by onboard sensors of an ego vehicle. With the rapid advancement in connected
technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure
(V2I) communication, valuable information from alternate views becomes
accessible via wireless networks. The integration of information from
alternative views has the potential to overcome the inherent limitations
associated with a single viewpoint, such as occlusions and limited field of
view. In this work, we introduce V2INet, a novel trajectory prediction
framework designed to model multi-view data by extending existing single-view
models. Unlike previous approaches where the multi-view data is manually fused
or formulated as a separate training stage, our model supports end-to-end
training, enhancing both flexibility and performance. Moreover, the predicted
multimodal trajectories are calibrated by a post-hoc conformal prediction
module to get valid and efficient confidence regions. We evaluated the entire
framework using the real-world V2I dataset V2X-Seq. Our results demonstrate
superior performance in terms of Final Displacement Error (FDE) and Miss Rate
(MR) using a single GPU. The code is publicly available at:
\url{https://github.com/xichennn/V2I_trajectory_prediction}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth-Wise Convolutions in Vision <span class="highlight-title">Transformer</span>s for Efficient Training on
  Small <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiao Zhang, Wenju Xu, Bo Luo, Guanghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Vision Transformer (ViT) leverages the Transformer's encoder to capture
global information by dividing images into patches and achieves superior
performance across various computer vision tasks. However, the self-attention
mechanism of ViT captures the global context from the outset, overlooking the
inherent relationships between neighboring pixels in images or videos.
Transformers mainly focus on global information while ignoring the fine-grained
local details. Consequently, ViT lacks inductive bias during image or video
dataset training. In contrast, convolutional neural networks (CNNs), with their
reliance on local filters, possess an inherent inductive bias, making them more
efficient and quicker to converge than ViT with less data. In this paper, we
present a lightweight Depth-Wise Convolution module as a shortcut in ViT
models, bypassing entire Transformer blocks to ensure the models capture both
local and global information with minimal overhead. Additionally, we introduce
two architecture variants, allowing the Depth-Wise Convolution modules to be
applied to multiple Transformer blocks for parameter savings, and incorporating
independent parallel Depth-Wise Convolution modules with different kernels to
enhance the acquisition of local information. The proposed approach
significantly boosts the performance of ViT models on image classification,
object detection and instance segmentation by a large margin, especially on
small datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet
for image classification, and COCO for object detection and instance
segmentation. The source code can be accessed at
https://github.com/ZTX-100/Efficient_ViT_with_DW.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Self-training Framework for Fine-grained Scene Graph Generation <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09786v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09786v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kibum Kim, Kanghoon Yoon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene graph generation (SGG) models have suffered from inherent problems
regarding the benchmark datasets such as the long-tailed predicate distribution
and missing annotation problems. In this work, we aim to alleviate the
long-tailed problem of SGG by utilizing unannotated triplets. To this end, we
introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels
for unannotated triplets based on which the SGG models are trained. While there
has been significant progress in self-training for image recognition, designing
a self-training framework for the SGG task is more challenging due to its
inherent nature such as the semantic ambiguity and the long-tailed distribution
of predicate classes. Hence, we propose a novel pseudo-labeling technique for
SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is
a model-agnostic framework that can be applied to any existing SGG models.
Furthermore, we devise a graph structure learner (GSL) that is beneficial when
adopting our proposed self-training framework to the state-of-the-art
message-passing neural network (MPNN)-based SGG models. Our extensive
experiments verify the effectiveness of ST-SGG on various SGG models,
particularly in enhancing the performance on fine-grained predicate classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages; ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11652v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11652v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny Gupta, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a privacy-preserving approach to train models
on decentralized data. Its potential in healthcare is significant, but
challenges arise due to cross-client variations in medical image data,
exacerbated by limited annotations. This paper introduces Cross-Client
Variations Adaptive Federated Learning (CCVA-FL) to address these issues.
CCVA-FL aims to minimize cross-client variations by transforming images into a
common feature space. It involves expert annotation of a subset of images from
each client, followed by the selection of a client with the least data
complexity as the target. Synthetic medical images are then generated using
Scalable Diffusion Models with Transformers (DiT) based on the target client's
annotated images. These synthetic images, capturing diversity and representing
the original data, are shared with other clients. Each client then translates
its local images into the target image space using image-to-image translation.
The translated images are subsequently used in a federated learning setting to
develop a server model. Our results demonstrate that CCVA-FL outperforms
Vanilla Federated Averaging by effectively addressing data distribution
differences across clients without compromising privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I found critical errors in the manuscript affecting its validity. I
  need to correct these before resubmitting. Major changes to methodology and
  results are underway, significantly altering the content. I will resubmit the
  revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Importance of Downstream Networks in Digital Pathology Foundation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustav Bredell, Marcel Fischer, Przemyslaw Szostak, Samaneh Abbasi-Sureshjani, Alvaro Gomariz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital pathology has significantly advanced disease detection and
pathologist efficiency through the analysis of gigapixel whole-slide images
(WSI). In this process, WSIs are first divided into patches, for which a
feature extractor model is applied to obtain feature vectors, which are
subsequently processed by an aggregation model to predict the respective WSI
label. With the rapid evolution of representation learning, numerous new
feature extractor models, often termed foundational models, have emerged.
Traditional evaluation methods rely on a static downstream aggregation model
setup, encompassing a fixed architecture and hyperparameters, a practice we
identify as potentially biasing the results. Our study uncovers a sensitivity
of feature extractor models towards aggregation model configurations,
indicating that performance comparability can be skewed based on the chosen
configurations. By accounting for this sensitivity, we find that the
performance of many current feature extractor models is notably similar. We
support this insight by evaluating seven feature extractor models across three
different datasets with 162 different aggregation model configurations. This
comprehensive approach provides a more nuanced understanding of the feature
extractors' sensitivity to various aggregation model configurations, leading to
a fairer and more accurate assessment of new foundation models in digital
pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion-aware Latent Diffusion Models for Video Frame Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13534v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13534v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilin Huang, Yijie Yu, Ling Yang, Chujun Qin, Bing Zheng, Xiawu Zheng, Zikun Zhou, Yaowei Wang, Wenming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of AIGC, video frame interpolation (VFI) has become a
crucial component in existing video generation frameworks, attracting
widespread research interest. For the VFI task, the motion estimation between
neighboring frames plays a crucial role in avoiding motion ambiguity. However,
existing VFI methods always struggle to accurately predict the motion
information between consecutive frames, and this imprecise estimation leads to
blurred and visually incoherent interpolated frames. In this paper, we propose
a novel diffusion framework, motion-aware latent diffusion models (MADiff),
which is specifically designed for the VFI task. By incorporating motion priors
between the conditional neighboring frames with the target interpolated frame
predicted throughout the diffusion sampling procedure, MADiff progressively
refines the intermediate outcomes, culminating in generating both visually
smooth and realistic results. Extensive experiments conducted on benchmark
datasets demonstrate that our method achieves state-of-the-art performance
significantly outperforming existing approaches, especially under challenging
scenarios involving dynamic textures with complex motion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heitor R. Medeiros, David Latortue, Eric Granger, Marco Pedersoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, using multiple modalities like visible (RGB) and
infrared (IR) can greatly improve the performance of a predictive task such as
object detection (OD). Multimodal learning is a common way to leverage these
modalities, where multiple modality-specific encoders and a fusion module are
used to improve performance. In this paper, we tackle a different way to employ
RGB and IR modalities, where only one modality or the other is observed by a
single shared vision encoder. This realistic setting requires a lower memory
footprint and is more suitable for applications such as autonomous driving and
surveillance, which commonly rely on RGB and IR data. However, when learning a
single encoder on multiple modalities, one modality can dominate the other,
producing uneven recognition results. This work investigates how to efficiently
leverage RGB and IR modalities to train a common transformer-based OD vision
encoder, while countering the effects of modality imbalance. For this, we
introduce a novel training technique to Mix Patches (MiPa) from the two
modalities, in conjunction with a patch-wise modality agnostic module, for
learning a common representation of both modalities. Our experiments show that
MiPa can learn a representation to reach competitive results on traditional
RGB/IR benchmarks while only requiring a single modality during inference. Our
code is available at: https://github.com/heitorrapela/MiPa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Geo-diversity of Generated Images with Contextualized Vendi
  Score Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reyhane Askari Hemmat, Melissa Hall, Alicia Sun, Candace Ross, Michal Drozdzal, Adriana Romero-Soriano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing popularity of text-to-image generative models, there has
been increasing focus on understanding their risks and biases. Recent work has
found that state-of-the-art models struggle to depict everyday objects with the
true diversity of the real world and have notable gaps between geographic
regions. In this work, we aim to increase the diversity of generated images of
common objects such that per-region variations are representative of the real
world. We introduce an inference time intervention, contextualized Vendi Score
Guidance (c-VSG), that guides the backwards steps of latent diffusion models to
increase the diversity of a sample as compared to a "memory bank" of previously
generated images while constraining the amount of variation within that of an
exemplar set of real-world contextualizing images. We evaluate c-VSG with two
geographically representative datasets and find that it substantially increases
the diversity of generated images, both for the worst performing regions and on
average, while simultaneously maintaining or improving image quality and
consistency. Additionally, qualitative analyses reveal that diversity of
generated images is significantly improved, including along the lines of
reductive region portrayals present in the original model. We hope that this
work is a step towards text-to-image generative models that reflect the true
geographic diversity of the world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate and Efficient Event-based Semantic Segmentation Using Adaptive
  Spiking Encoder-Decoder Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11857v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11857v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhang, Luziwei Leng, Kaiwei Che, Hu Zhang, Jie Cheng, Qinghai Guo, Jiangxing Liao, Ran Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs), known for their low-power, event-driven
computation and intrinsic temporal dynamics, are emerging as promising
solutions for processing dynamic, asynchronous signals from event-based
sensors. Despite their potential, SNNs face challenges in training and
architectural design, resulting in limited performance in challenging
event-based dense prediction tasks compared to artificial neural networks
(ANNs). In this work, we develop an efficient spiking encoder-decoder network
(SpikingEDN) for large-scale event-based semantic segmentation tasks. To
enhance the learning efficiency from dynamic event streams, we harness the
adaptive threshold which improves network accuracy, sparsity and robustness in
streaming inference. Moreover, we develop a dual-path Spiking
Spatially-Adaptive Modulation module, which is specifically tailored to enhance
the representation of sparse events and multi-modal inputs, thereby
considerably improving network performance. Our SpikingEDN attains a mean
intersection over union (MIoU) of 72.57\% on the DDD17 dataset and 58.32\% on
the larger DSEC-Semantic dataset, showing competitive results to the
state-of-the-art ANNs while requiring substantially fewer computational
resources. Our results shed light on the untapped potential of SNNs in
event-based vision applications. The source code will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Neural Networks and
  Learning Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPIdepth: Strengthened Pose Information for <span class="highlight-title">Self-supervised</span> Monocular
  Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mykola Lavreniuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised monocular depth estimation has garnered considerable
attention for its applications in autonomous driving and robotics. While recent
methods have made strides in leveraging techniques like the Self Query Layer
(SQL) to infer depth from motion, they often overlook the potential of
strengthening pose information. In this paper, we introduce SPIdepth, a novel
approach that prioritizes enhancing the pose network for improved depth
estimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the
importance of pose information in capturing fine-grained scene structures. By
enhancing the pose network's capabilities, SPIdepth achieves remarkable
advancements in scene understanding and depth estimation. Experimental results
on benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth's
state-of-the-art performance, surpassing previous methods by significant
margins. Specifically, SPIdepth tops the self-supervised KITTI benchmark.
Additionally, SPIdepth achieves the lowest AbsRel (0.029), SqRel (0.069), and
RMSE (1.394) on KITTI, establishing new state-of-the-art results. On
Cityscapes, SPIdepth shows improvements over SQLdepth of 21.7% in AbsRel, 36.8%
in SqRel, and 16.5% in RMSE, even without using motion masks. On Make3D,
SPIdepth in zero-shot outperforms all other models. Remarkably, SPIdepth
achieves these results using only a single image for inference, surpassing even
methods that utilize video sequences for inference, thus demonstrating its
efficacy and efficiency in real-world applications. Our approach represents a
significant leap forward in self-supervised monocular depth estimation,
underscoring the importance of strengthening pose information for advancing
scene understanding in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeong Hun Yeo, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel Face-to-Face spoken dialogue model. It
processes audio-visual speech from user input and generates audio-visual speech
as the response, marking the initial step towards creating an avatar chatbot
system without relying on intermediate text. To this end, we newly introduce
MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken
dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded
based on the open domain dialogue dataset, TopicalChat. The MultiDialog
contains parallel audio-visual recordings of conversation partners acting
according to the given script with emotion annotations, which we expect to open
up research opportunities in multimodal synthesis. Our Face-to-Face spoken
dialogue model incorporates a textually pretrained large language model and
adapts it into the audio-visual spoken dialogue domain by incorporating
speech-text joint pretraining. Through extensive experiments, we validate the
effectiveness of our model in facilitating a face-to-face conversation. Demo
and data are available at https://multidialog.github.io and
https://huggingface.co/datasets/IVLLab/MultiDialog, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incremental Object-Based Novelty Detection with Feedback Loop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09004v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09004v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Caldarella, Elisa Ricci, Rahaf Aljundi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-based Novelty Detection (ND) aims to identify unknown objects that do
not belong to classes seen during training by an object detection model. The
task is particularly crucial in real-world applications, as it allows to avoid
potentially harmful behaviours, e.g. as in the case of object detection models
adopted in a self-driving car or in an autonomous robot. Traditional approaches
to ND focus on one time offline post processing of the pretrained object
detection output, leaving no possibility to improve the model robustness after
training and discarding the abundant amount of out-of-distribution data
encountered during deployment. In this work, we propose a novel framework for
object-based ND, assuming that human feedback can be requested on the predicted
output and later incorporated to refine the ND model without negatively
affecting the main object detection performance. This refinement operation is
repeated whenever new feedback is available. To tackle this new formulation of
the problem for object detection, we propose a lightweight ND module attached
on top of a pre-trained object detection model, which is incrementally updated
through a feedback loop. We also propose a new benchmark to evaluate methods on
this new setting and test extensively our ND approach against baselines,
showing increased robustness and a successful incorporation of the received
feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision <span class="highlight-title">Transformer</span>s: From Semantic Segmentation to Dense Prediction <span class="chip">CVPR 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09339v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09339v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhang, Jiachen Lu, Sixiao Zheng, Xinxuan Zhao, Xiatian Zhu, Yanwei Fu, Tao Xiang, Jianfeng Feng, Philip H. S. Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of vision transformers (ViTs) in image classification has
shifted the methodologies for visual representation learning. In particular,
ViTs learn visual representation at full receptive field per layer across all
the image patches, in comparison to the increasing receptive fields of CNNs
across layers and other alternatives (e.g., large kernels and atrous
convolution). In this work, for the first time we explore the global context
learning potentials of ViTs for dense visual prediction (e.g., semantic
segmentation). Our motivation is that through learning global context at full
receptive field layer by layer, ViTs may capture stronger long-range dependency
information, critical for dense prediction tasks. We first demonstrate that
encoding an image as a sequence of patches, a vanilla ViT without local
convolution and resolution reduction can yield stronger visual representation
for semantic segmentation. For example, our model, termed as SEgmentation
TRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the
test leaderboard on the day of submission) and performs competitively on
Cityscapes. However, the basic ViT architecture falls short in broader dense
prediction applications, such as object detection and instance segmentation,
due to its lack of a pyramidal structure, high computational demand, and
insufficient local context. For tackling general dense visual prediction tasks
in a cost-effective manner, we further formulate a family of Hierarchical
Local-Global (HLG) Transformers, characterized by local attention within
windows and global-attention across windows in a pyramidal architecture.
Extensive experiments show that our methods achieve appealing performance on a
variety of dense prediction tasks (e.g., object detection and instance
segmentation and semantic segmentation) as well as image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of CVPR 2021 paper arXiv:2012.15840 Published on
  International Journal of Computer Vision (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image
  <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Wu, Yutong Xie, Zeyu Zhang, Minh Hieu Phan, Qi Chen, Ling Chen, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-language pretraining (VLP) in the medical field utilizes
contrastive learning on image-text pairs to achieve effective transfer across
tasks. Yet, current VLP approaches with the masked modelling strategy face two
challenges when applied to the medical domain. First, current models struggle
to accurately reconstruct key pathological features due to the scarcity of
medical data. Second, most methods only adopt either paired image-text or
image-only data, failing to exploit the combination of both paired and unpaired
data. To this end, this paper proposes a XLIP (Masked modelling for medical
Language-Image Pre-training) framework to enhance pathological learning and
feature learning via unpaired data. First, we introduce the attention-masked
image modelling (AttMIM) and entity-driven masked language modelling module
(EntMLM), which learns to reconstruct pathological visual and textual tokens
via multi-modal feature interaction, thus improving medical-enhanced features.
The AttMIM module masks a portion of the image features that are highly
responsive to textual features. This allows XLIP to improve the reconstruction
of highly similar image data in medicine efficiency. Second, our XLIP
capitalizes unpaired data to enhance multimodal learning by introducing
disease-kind prompts. The experimental results show that XLIP achieves SOTA for
zero-shot and fine-tuning classification performance on five datasets. Our code
will be available at https://github.com/White65534/XLIP
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Optimization Framework to Enforce Multi-View Consistency for
  Texturing 3D Meshes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Zhao, Chen Song, Xiaodong Gu, Yuan Dong, Qi Zuo, Weihao Yuan, Liefeng Bo, Zilong Dong, Qixing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental problem in the texturing of 3D meshes using pre-trained
text-to-image models is to ensure multi-view consistency. State-of-the-art
approaches typically use diffusion models to aggregate multi-view inputs, where
common issues are the blurriness caused by the averaging operation in the
aggregation step or inconsistencies in local features. This paper introduces an
optimization framework that proceeds in four stages to achieve multi-view
consistency. Specifically, the first stage generates an over-complete set of 2D
textures from a predefined set of viewpoints using an MV-consistent diffusion
process. The second stage selects a subset of views that are mutually
consistent while covering the underlying 3D model. We show how to achieve this
goal by solving semi-definite programs. The third stage performs non-rigid
alignment to align the selected views across overlapping regions. The fourth
stage solves an MRF problem to associate each mesh face with a selected view.
In particular, the third and fourth stages are iterated, with the cuts obtained
in the fourth stage encouraging non-rigid alignment in the third stage to focus
on regions close to the cuts. Experimental results show that our approach
significantly outperforms baseline approaches both qualitatively and
quantitatively. Project page: https://aigc3d.github.io/ConsistenTex.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness-Aware 3D Object Detection in Autonomous Driving: A <span class="highlight-title">Review</span> and
  Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziying Song, Lin Liu, Feiyang Jia, Yadan Luo, Guoxin Zhang, Lei Yang, Li Wang, Caiyan Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of modern autonomous driving, the perception system is
indispensable for accurately assessing the state of the surrounding
environment, thereby enabling informed prediction and planning. The key step to
this system is related to 3D object detection that utilizes vehicle-mounted
sensors such as LiDAR and cameras to identify the size, the category, and the
location of nearby objects. Despite the surge in 3D object detection methods
aimed at enhancing detection precision and efficiency, there is a gap in the
literature that systematically examines their resilience against environmental
variations, noise, and weather changes. This study emphasizes the importance of
robustness, alongside accuracy and latency, in evaluating perception systems
under practical scenarios. Our work presents an extensive survey of
camera-only, LiDAR-only, and multi-modal 3D object detection algorithms,
thoroughly evaluating their trade-off between accuracy, latency, and
robustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair
comparisons. Among these, multi-modal 3D detection approaches exhibit superior
robustness, and a novel taxonomy is introduced to reorganize the literature for
enhanced clarity. This survey aims to offer a more practical perspective on the
current capabilities and the constraints of 3D object detection algorithms in
real-world applications, thus steering future research towards
robustness-centric advancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAM-guided Graph Cut for 3D Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08372v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08372v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Guo, He Zhu, Sida Peng, Yuang Wang, Yujun Shen, Ruizhen Hu, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of 3D instance segmentation by
simultaneously leveraging 3D geometric and multi-view image information. Many
previous works have applied deep learning techniques to 3D point clouds for
instance segmentation. However, these methods often failed to generalize to
various types of scenes due to the scarcity and low-diversity of labeled 3D
point cloud data. Some recent works have attempted to lift 2D instance
segmentations to 3D within a bottom-up framework. The inconsistency in 2D
instance segmentations among views can substantially degrade the performance of
3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to
effectively exploit 2D segmentation models for 3D instance segmentation.
Specifically, we pre-segment the scene into several superpoints in 3D,
formulating the task into a graph cut problem. The superpoint graph is
constructed based on 2D segmentation models, where node features are obtained
from multi-view image features and edge weights are computed based on
multi-view segmentation results, enabling the better generalization ability. To
process the graph, we train a graph neural network using pseudo 3D labels from
2D segmentation models. Experimental results on the ScanNet, ScanNet++ and
KITTI-360 datasets demonstrate that our method achieves robust segmentation
performance and can generalize across different types of scenes. Our project
page is available at https://zju3dv.github.io/sam_graph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/sam_graph</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised
  Change Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04788v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04788v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyu Li, Xiangyong Cao, Yupeng Deng, Junmin Liu, Deyu Meng, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change Detection (CD) aims to identify pixels with semantic changes between
images. However, annotating massive numbers of pixel-level images is
labor-intensive and costly, especially for multi-temporal images, which require
pixel-wise comparisons by human experts. Considering the excellent performance
of visual language models (VLMs) for zero-shot, open-vocabulary, etc. with
prompt-based reasoning, it is promising to utilize VLMs to make better CD under
limited labeled data. In this paper, we propose a VLM guidance-based
semi-supervised CD method, namely SemiCD-VL. The insight of SemiCD-VL is to
synthesize free change labels using VLMs to provide additional supervision
signals for unlabeled data. However, almost all current VLMs are designed for
single-temporal images and cannot be directly applied to bi- or multi-temporal
images. Motivated by this, we first propose a VLM-based mixed change event
generation (CEG) strategy to yield pseudo labels for unlabeled CD data. Since
the additional supervised signals provided by these VLM-driven pseudo labels
may conflict with the pseudo labels from the consistency regularization
paradigm (e.g. FixMatch), we propose the dual projection head for de-entangling
different signal sources. Further, we explicitly decouple the bi-temporal
images semantic representation through two auxiliary segmentation decoders,
which are also guided by VLM. Finally, to make the model more adequately
capture change representations, we introduce metric-aware supervision by
feature-level contrastive loss in auxiliary branches. Extensive experiments
show the advantage of SemiCD-VL. For instance, SemiCD-VL improves the FixMatch
baseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels. In
addition, our CEG strategy, in an un-supervised manner, can achieve performance
far superior to state-of-the-art un-supervised CD methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenVid-1M: A Large-Scale High-Quality <span class="highlight-title">Dataset</span> for Text-to-video
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, Ying Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video (T2V) generation has recently garnered significant attention
thanks to the large multi-modality model Sora. However, T2V generation still
faces two important challenges: 1) Lacking a precise open sourced high-quality
dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,
are either with low quality or too large for most research institutions.
Therefore, it is challenging but crucial to collect a precise high-quality
text-video pairs for T2V generation. 2) Ignoring to fully utilize textual
information. Recent T2V methods have focused on vision transformers, using a
simple cross attention module for video generation, which falls short of
thoroughly extracting semantic information from text prompt. To address these
issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive
captions. This open-scenario dataset contains over 1 million text-video pairs,
facilitating research on T2V generation. Furthermore, we curate 433K 1080p
videos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition
video generation. Additionally, we propose a novel Multi-modal Video Diffusion
Transformer (MVDiT) capable of mining both structure information from visual
tokens and semantic information from text tokens. Extensive experiments and
ablation studies verify the superiority of OpenVid-1M over previous datasets
and the effectiveness of our MVDiT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptive Lung Nodule Detection in X-ray Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haifeng Zhao, Lixiang Jiang, Leilei Ma, Dengdi Sun, Yanping Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images from different healthcare centers exhibit varied data
distributions, posing significant challenges for adapting lung nodule detection
due to the domain shift between training and application phases. Traditional
unsupervised domain adaptive detection methods often struggle with this shift,
leading to suboptimal outcomes. To overcome these challenges, we introduce a
novel domain adaptive approach for lung nodule detection that leverages mean
teacher self-training and contrastive learning. First, we propose a
hierarchical contrastive learning strategy to refine nodule representations and
enhance the distinction between nodules and background. Second, we introduce a
nodule-level domain-invariant feature learning (NDL) module to capture
domain-invariant features through adversarial learning across different
domains. Additionally, we propose a new annotated dataset of X-ray images to
aid in advancing lung nodule detection research. Extensive experiments
conducted on multiple X-ray datasets demonstrate the efficacy of our approach
in mitigating domain shift impacts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper will submit to IEEE SMC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap
  Multiview Point Cloud Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Liu, Jinjun Shan, Amaldev Haridevan, Shuo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud registration is a prerequisite for many applications in computer
vision and robotics. Most existing methods focus on pairwise registration of
two point clouds with high overlap. Although there have been some methods for
low overlap cases, they struggle in degraded scenarios. This paper introduces a
novel framework dubbed L-PR, designed to register unordered low overlap
multiview point clouds leveraging LiDAR fiducial markers. We refer to them as
LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco
markers, thin sheets of paper that do not affect the 3D geometry of the
environment. We first propose an improved adaptive threshold marker detection
method to provide robust detection results when the viewpoints among point
clouds change dramatically. Then, we formulate the unordered multiview point
cloud registration problem as a maximum a-posteriori (MAP) problem and develop
a framework consisting of two levels of graphs to address it. The first-level
graph, constructed as a weighted graph, is designed to efficiently and
optimally infer initial values of scan poses from the unordered set. The
second-level graph is constructed as a factor graph. By globally optimizing the
variables on the graph, including scan poses, marker poses, and marker corner
positions, we tackle the MAP problem. We conduct both qualitative and
quantitative experiments to demonstrate that the proposed method surpasses
previous state-of-the-art (SOTA) methods and to showcase that L-PR can serve as
a low-cost and efficient tool for 3D asset collection and training data
collection. In particular, we collect a new dataset named Livox-3DMatch using
L-PR and incorporate it into the training of the SOTA learning-based method,
SGHR, which brings evident improvements for SGHR on various benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISORF: A Distributed Online 3D Reconstruction Framework for Mobile
  Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00228v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00228v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunlin Li, Hanrui Fan, Xiaorui Huang, Ruofan Liang, Sankeerth Durvasula, Nandita Vijaykumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework, DISORF, to enable online 3D reconstruction and
visualization of scenes captured by resource-constrained mobile robots and edge
devices. To address the limited computing capabilities of edge devices and
potentially limited network availability, we design a framework that
efficiently distributes computation between the edge device and the remote
server. We leverage on-device SLAM systems to generate posed keyframes and
transmit them to remote servers that can perform high-quality 3D reconstruction
and visualization at runtime by leveraging recent advances in neural 3D
methods. We identify a key challenge with online training where naive image
sampling strategies can lead to significant degradation in rendering quality.
We propose a novel shifted exponential frame sampling method that addresses
this challenge for online training. We demonstrate the effectiveness of our
framework in enabling high-quality real-time reconstruction and visualization
of unknown scenes as they are captured and streamed from cameras in mobile
robots and edge devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forecasting Future Videos from Novel Views via Disentangled 3D Scene
  Representation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sudhir Yarram, Junsong Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video extrapolation in space and time (VEST) enables viewers to forecast a 3D
scene into the future and view it from novel viewpoints. Recent methods propose
to learn an entangled representation, aiming to model layered scene geometry,
motion forecasting and novel view synthesis together, while assuming simplified
affine motion and homography-based warping at each scene layer, leading to
inaccurate video extrapolation. Instead of entangled scene representation and
rendering, our approach chooses to disentangle scene geometry from scene
motion, via lifting the 2D scene to 3D point clouds, which enables high quality
rendering of future videos from novel views. To model future 3D scene motion,
we propose a disentangled two-stage approach that initially forecasts
ego-motion and subsequently the residual motion of dynamic objects (e.g., cars,
people). This approach ensures more precise motion predictions by reducing
inaccuracies from entanglement of ego-motion with dynamic object motion, where
better ego-motion forecasting could significantly enhance the visual outcomes.
Extensive experimental analysis on two urban scene datasets demonstrate
superior performance of our proposed method in comparison to strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024. Project Page:
  https://skrya.github.io/projects/ffn-dsr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenRC: Generative 3D Room Completion from Sparse Image Collections <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12939v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12939v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming-Feng Li, Yueh-Feng Ku, Hong-Xuan Yen, Chi Liu, Yu-Lun Liu, Albert Y. C. Chen, Cheng-Hao Kuo, Min Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse RGBD scene completion is a challenging task especially when
considering consistent textures and geometries throughout the entire scene.
Different from existing solutions that rely on human-designed text prompts or
predefined camera trajectories, we propose GenRC, an automated training-free
pipeline to complete a room-scale 3D mesh with high-fidelity textures. To
achieve this, we first project the sparse RGBD images to a highly incomplete 3D
mesh. Instead of iteratively generating novel views to fill in the void, we
utilized our proposed E-Diffusion to generate a view-consistent panoramic RGBD
image which ensures global geometry and appearance consistency. Furthermore, we
maintain the input-output scene stylistic consistency through textual inversion
to replace human-designed text prompts. To bridge the domain gap among
datasets, E-Diffusion leverages models trained on large-scale datasets to
generate diverse appearances. GenRC outperforms state-of-the-art methods under
most appearance and geometric metrics on ScanNet and ARKitScenes datasets, even
though GenRC is not trained on these datasets nor using predefined camera
trajectories. Project page: https://minfenli.github.io/GenRC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Spatial-spectral Neural Network for Hyperspectral Image Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liang,  Chengjie, Kun Li, Xin Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) denoising is an essential procedure for HSI
applications. Unfortunately, the existing Transformer-based methods mainly
focus on non-local modeling, neglecting the importance of locality in image
denoising. Moreover, deep learning methods employ complex spectral learning
mechanisms, thus introducing large computation costs.
  To address these problems, we propose a hybrid spatial-spectral denoising
network (HSSD), in which we design a novel hybrid dual-path network inspired by
CNN and Transformer characteristics, leading to capturing both local and
non-local spatial details while suppressing noise efficiently. Furthermore, to
reduce computational complexity, we adopt a simple but effective decoupling
strategy that disentangles the learning of space and spectral channels, where
multilayer perception with few parameters is utilized to learn the global
correlations among spectra. The synthetic and real experiments demonstrate that
our proposed method outperforms state-of-the-art methods on spatial and
spectral reconstruction. The code and details are available on
https://github.com/HLImg/HSSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are some errors in professional theory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Learned Image Compression: Context is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11590v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11590v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jixiang Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since LIC has made rapid progress recently compared to traditional methods,
this paper attempts to discuss the question about 'Where is the boundary of
Learned Image Compression(LIC)?'. Thus this paper splits the above problem into
two sub-problems:1)Where is the boundary of rate-distortion performance of
PSNR? 2)How to further improve the compression gain and achieve the boundary?
Therefore this paper analyzes the effectiveness of scaling parameters for
encoder, decoder and context model, which are the three components of LIC. Then
we conclude that scaling for LIC is to scale for context model and decoder
within LIC. Extensive experiments demonstrate that overfitting can actually
serve as an effective context. By optimizing the context, this paper further
improves PSNR and achieves state-of-the-art performance, showing a performance
gain of 14.39% with BD-RATE over VVC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Guidance Network for Missing-Modality Inference in Content
  Moderation <span class="chip">ICME 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuokai Zhao, Harish Palani, Tianyi Liu, Lena Evans, Ruth Toner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal deep learning, especially vision-language models, have gained
significant traction in recent years, greatly improving performance on many
downstream tasks, including content moderation and violence detection. However,
standard multimodal approaches often assume consistent modalities between
training and inference, limiting applications in many real-world use cases, as
some modalities may not be available during inference. While existing research
mitigates this problem through reconstructing the missing modalities, they
unavoidably increase unnecessary computational cost, which could be just as
critical, especially for large, deployed infrastructures in industry. To this
end, we propose a novel guidance network that promotes knowledge sharing during
training, taking advantage of the multimodal representations to train better
single-modality models to be used for inference. Real-world experiments in
violence detection shows that our proposed framework trains single-modality
models that significantly outperform traditionally trained counterparts, while
avoiding increases in computational cost for inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICME 2024 Camera Ready. Code is available at
  https://github.com/zhuokaizhao/multimodal-guidance-network</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applications of Spiking Neural Networks in Visual Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somayeh Hussaini, Michael Milford, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for
their largely-unrealized potential energy efficiency and low latency
particularly when implemented on neuromorphic hardware. Our paper highlights
three advancements for SNNs in Visual Place Recognition (VPR). Firstly, we
propose Modular SNNs, where each SNN represents a set of non-overlapping
geographically distinct places, enabling scalable networks for large
environments. Secondly, we present Ensembles of Modular SNNs, where multiple
networks represent the same place, significantly enhancing accuracy compared to
single-network models. Each of our Modular SNN modules is compact, comprising
only 1500 neurons and 474k synapses, making them ideally suited for ensembling
due to their small size. Lastly, we investigate the role of sequence matching
in SNN-based VPR, a technique where consecutive images are used to refine place
recognition. We analyze the responsiveness of SNNs to ensembling and sequence
matching compared to other VPR techniques. Our contributions highlight the
viability of SNNs for VPR, offering scalable and robust solutions, and paving
the way for their application in various energy-sensitive robotic tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Gap in Data Augmentation: Insights from Illumination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqiang Xiao, Weiwen Guo, Junfeng Liu, Mengze Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of computer vision, data augmentation is widely used to enrich
the feature complexity of training datasets with deep learning techniques.
However, regarding the generalization capabilities of models, the difference in
artificial features generated by data augmentation and natural visual features
has not been fully revealed. This study introduces the concept of "visual
representation variables" to define the possible visual variations in a task as
a joint distribution of these variables. We focus on the visual representation
variable "illumination", by simulating its distribution degradation and
examining how data augmentation techniques enhance model performance on a
classification task. Our goal is to investigate the differences in
generalization between models trained with augmented data and those trained
under real-world illumination conditions. Results indicate that after applying
various data augmentation methods, model performance has significantly
improved. Yet, a noticeable generalization gap still exists after utilizing
various data augmentation methods, emphasizing the critical role of feature
diversity in the training set for enhancing model generalization.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mission Impossible: A Statistical Perspective on Jailbreaking LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtong Su, Julia Kempe, Karen Ullrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are trained on a deluge of text data with
limited quality control. As a result, LLMs can exhibit unintended or even
harmful behaviours, such as leaking information, fake news or hate speech.
Countermeasures, commonly referred to as preference alignment, include
fine-tuning the pretrained LLMs with carefully crafted text examples of desired
behaviour. Even then, empirical evidence shows preference aligned LLMs can be
enticed to harmful behaviour. This so called jailbreaking of LLMs is typically
achieved by adversarially modifying the input prompt to the LLM. Our paper
provides theoretical insights into the phenomenon of preference alignment and
jailbreaking from a statistical perspective. Under our framework, we first show
that pretrained LLMs will mimic harmful behaviour if present in the training
corpus. Under that same framework, we then introduce a statistical notion of
alignment, and lower-bound the jailbreaking probability, showing that it is
unpreventable under reasonable assumptions. Based on our insights, we propose
an alteration to the currently prevalent alignment strategy RLHF. Specifically,
we introduce a simple modification to the RLHF objective, we call E-RLHF, that
aims to increase the likelihood of safe responses. E-RLHF brings no additional
training cost, and is compatible with other methods. Empirically, we
demonstrate that E-RLHF outperforms RLHF on all alignment problems put forward
by the AdvBench and HarmBench project without sacrificing model performance as
measured by the MT-Bench project.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Talk Less, Interact Better: Evaluating In-context Conversational
  Adaptation in Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Hua, Yoav Artzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans spontaneously use increasingly efficient language as interactions
progress, by adapting and forming ad-hoc conventions. This phenomenon has been
studied extensively using reference games, showing properties of human language
that go beyond relaying intents. It remains unexplored whether multimodal large
language models (MLLMs) similarly increase communication efficiency during
interactions, and what mechanisms they may adopt for this purpose. We introduce
ICCA, an automated framework to evaluate such conversational adaptation as an
in-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and
observe that while they may understand the increasingly efficient language of
their interlocutor, they do not spontaneously make their own language more
efficient over time. This latter ability can only be elicited in some models
(e.g., GPT-4) with heavy-handed prompting. This shows that this property of
linguistic interaction does not arise from current training regimes, even
though it is a common hallmark of human language. ICCA is available at
https://github.com/lil-lab/ICCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Quest for the Right Mediator: A History, <span class="highlight-title">Survey</span>, and Theoretical
  Grounding of Causal Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability provides a toolset for understanding how and why neural
networks behave in certain ways. However, there is little unity in the field:
most studies employ ad-hoc evaluations and do not share theoretical
foundations, making it difficult to measure progress and compare the pros and
cons of different techniques. Furthermore, while mechanistic understanding is
frequently discussed, the basic causal units underlying these mechanisms are
often not explicitly defined. In this paper, we propose a perspective on
interpretability research grounded in causal mediation analysis. Specifically,
we describe the history and current state of interpretability taxonomized
according to the types of causal units (mediators) employed, as well as methods
used to search over mediators. We discuss the pros and cons of each mediator,
providing insights as to when particular kinds of mediators and search methods
are most appropriate depending on the goals of a given study. We argue that
this framing yields a more cohesive narrative of the field, as well as
actionable insights for future work. Specifically, we recommend a focus on
discovering new mediators with better trade-offs between human-interpretability
and compute-efficiency, and which can uncover more sophisticated abstractions
from neural networks than the primarily linear mediators employed in current
work. We also argue for more standardized evaluations that enable principled
comparisons across mediator types, such that we can better understand when
particular causal units are better suited to particular use cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional LoRA Parameter Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Jin, Kai Wang, Dongwen Tang, Wangbo Zhao, Yukun Zhou, Junshu Tang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have achieved remarkable success in image, video, and text
domains. Inspired by this, researchers have explored utilizing generative
models to generate neural network parameters. However, these efforts have been
limited by the parameter size and the practicality of generating
high-performance parameters. In this paper, we propose COND P-DIFF, a novel
approach that demonstrates the feasibility of controllable high-performance
parameter generation, particularly for LoRA (Low-Rank Adaptation) weights,
during the fine-tuning process. Specifically, we employ an autoencoder to
extract efficient latent representations for parameters. We then train a
conditional latent diffusion model to synthesize high-performing model
parameters from random noise based on specific task conditions. Experimental
results in both computer vision and natural language processing domains
consistently demonstrate that COND P-DIFF can generate high-performance
parameters conditioned on the given task. Moreover, we observe that the
parameter distribution generated by COND P-DIFF exhibits differences compared
to the distribution obtained through normal optimization methods, indicating a
certain level of generalization capability. Our work paves the way for further
exploration of condition-driven parameter generation, offering a promising
direction for task-specific adaptation of neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Derivation of Back-propagation for Graph Convolutional Networks using
  Matrix Calculus and its Application to Explainable Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Che Hsiao, Rongting Yue, Abhishek Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a comprehensive and detailed derivation of the
backpropagation algorithm for graph convolutional neural networks using matrix
calculus. The derivation is extended to include arbitrary element-wise
activation functions and an arbitrary number of layers. The study addresses two
fundamental problems, namely node classification and link prediction. To
validate our method, we compare it with reverse-mode automatic differentiation.
The experimental results demonstrate that the median sum of squared errors of
the updated weight matrices, when comparing our method to the approach using
reverse-mode automatic differentiation, falls within the range of $10^{-18}$ to
$10^{-14}$. These outcomes are obtained from conducting experiments on a
five-layer graph convolutional network, applied to a node classification
problem on Zachary's karate club social network and a link prediction problem
on a drug-drug interaction network. Finally, we show how the derived
closed-form solution can facilitate the development of explainable AI and
sensitivity analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ed Language Models Improve the Few-shot <span class="highlight-title">Prompt</span> Ability of
  Decision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yang, Pan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision Transformer (DT) has emerged as a promising class of algorithms in
offline reinforcement learning (RL) tasks, leveraging pre-collected datasets
and Transformer's capability to model long sequences. Recent works have
demonstrated that using parts of trajectories from training tasks as prompts in
DT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.
However, collecting data from specific environments can be both costly and
unsafe in many scenarios, leading to suboptimal performance and limited
few-shot prompt abilities due to the data-hungry nature of Transformer-based
models. Additionally, the limited datasets used in pre-training make it
challenging for Prompt-DT type of methods to distinguish between various RL
tasks through prompts alone. To address these challenges, we introduce the
Language model-initialized Prompt Decision Transformer (LPDT), which leverages
pre-trained language models for meta-RL tasks and fine-tunes the model using
Low-rank Adaptation (LoRA). We further incorporate prompt regularization to
effectively differentiate between tasks based on prompt feature
representations. Our approach integrates pre-trained language model and RL
tasks seamlessly. Extensive empirical studies demonstrate that initializing
with a pre-trained language model significantly enhances the performance of
Prompt-DT on unseen tasks compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 figures, 8 tables. Accepted by the Training Agents with Foundation
  Models Workshop at RLC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FT K-Means: A High-Performance K-Means on GPU with Fault Tolerance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixun Wu, Yitong Ding, Yujia Zhai, Jinyang Liu, Jiajun Huang, Zizhe Jian, Huangliang Dai, Sheng Di, Bryan M. Wong, Zizhong Chen, Franck Cappello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  K-Means is a widely used algorithm in clustering, however, its efficiency is
primarily constrained by the computational cost of distance computing. Existing
implementations suffer from suboptimal utilization of computational units and
lack resilience against soft errors. To address these challenges, we introduce
FT K-Means, a high-performance GPU-accelerated implementation of K-Means with
online fault tolerance. We first present a stepwise optimization strategy that
achieves competitive performance compared to NVIDIA's cuML library. We further
improve FT K-Means with a template-based code generation framework that
supports different data types and adapts to different input shapes. A novel
warp-level tensor-core error correction scheme is proposed to address the
failure of existing fault tolerance methods due to memory asynchronization
during copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100
GPU demonstrate that FT K-Means without fault tolerance outperforms cuML's
K-Means implementation, showing a performance increase of 10\%-300\% in
scenarios involving irregular data shapes. Moreover, the fault tolerance
feature of FT K-Means introduces only an overhead of 11\%, maintaining robust
performance even with tens of errors injected per second.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuralBeta: Estimating Beta Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Liu, Jimin Lin, Achintya Gopal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to estimating beta in finance often involve rigid
assumptions and fail to adequately capture beta dynamics, limiting their
effectiveness in use cases like hedging. To address these limitations, we have
developed a novel method using neural networks called NeuralBeta, which is
capable of handling both univariate and multivariate scenarios and tracking the
dynamic behavior of beta. To address the issue of interpretability, we
introduce a new output layer inspired by regularized weighted linear
regression, which provides transparency into the model's decision-making
process. We conducted extensive experiments on both synthetic and market data,
demonstrating NeuralBeta's superior performance compared to benchmark methods
across various scenarios, especially instances where beta is highly
time-varying, e.g., during regime shifts in the market. This model not only
represents an advancement in the field of beta estimation, but also shows
potential for applications in other financial contexts that assume linear
relationships.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining a probabilistic prediction on the simplex with Shapley
  compositions <span class="chip">ECAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul-Gauthier Noé, Miquel Perelló-Nieto, Jean-François Bonastre, Peter Flach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Originating in game theory, Shapley values are widely used for explaining a
machine learning model's prediction by quantifying the contribution of each
feature's value to the prediction. This requires a scalar prediction as in
binary classification, whereas a multiclass probabilistic prediction is a
discrete probability distribution, living on a multidimensional simplex. In
such a multiclass setting the Shapley values are typically computed separately
on each class in a one-vs-rest manner, ignoring the compositional nature of the
output distribution. In this paper, we introduce Shapley compositions as a
well-founded way to properly explain a multiclass probabilistic prediction,
using the Aitchison geometry from compositional data analysis. We prove that
the Shapley composition is the unique quantity satisfying linearity, symmetry
and efficiency on the Aitchison simplex, extending the corresponding axiomatic
properties of the standard Shapley value. We demonstrate this proper multiclass
treatment in a range of scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in ECAI2024's proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resampling and averaging coordinates on data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew J. Blumberg, Mathieu Carriere, Jun Hou Fung, Michael A. Mandell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce algorithms for robustly computing intrinsic coordinates on point
clouds. Our approach relies on generating many candidate coordinates by
subsampling the data and varying hyperparameters of the embedding algorithm
(e.g., manifold learning). We then identify a subset of representative
embeddings by clustering the collection of candidate coordinates and using
shape descriptors from topological data analysis. The final output is the
embedding obtained as an average of the representative embeddings using
generalized Procrustes analysis. We validate our algorithm on both synthetic
data and experimental measurements from genomics, demonstrating robustness to
noise and outliers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Recruitment Resource Allocation to Improve Cohort
  Representativeness in Participatory Biomedical <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Borza, Andrew Estornell, Ellen Wright Clayton, Chien-Ju Ho, Russell Rothman, Yevgeniy Vorobeychik, Bradley Malin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large participatory biomedical studies, studies that recruit individuals to
join a dataset, are gaining popularity and investment, especially for analysis
by modern AI methods. Because they purposively recruit participants, these
studies are uniquely able to address a lack of historical representation, an
issue that has affected many biomedical datasets. In this work, we define
representativeness as the similarity to a target population distribution of a
set of attributes and our goal is to mirror the U.S. population across
distributions of age, gender, race, and ethnicity. Many participatory studies
recruit at several institutions, so we introduce a computational approach to
adaptively allocate recruitment resources among sites to improve
representativeness. In simulated recruitment of 10,000-participant cohorts from
medical centers in the STAR Clinical Research Network, we show that our
approach yields a more representative cohort than existing baselines. Thus, we
highlight the value of computational modeling in guiding recruitment efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the American Medical Informatics
  Association Annual Symposium 2024, 10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Coordinate Descent for Efficient Neural Network Learning Using
  Line Search and Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Che Hsiao, Abhishek Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel coordinate descent algorithm leveraging a
combination of one-directional line search and gradient information for
parameter updates for a squared error loss function. Each parameter undergoes
updates determined by either the line search or gradient method, contingent
upon whether the modulus of the gradient of the loss with respect to that
parameter surpasses a predefined threshold. Notably, a larger threshold value
enhances algorithmic efficiency. Despite the potentially slower nature of the
line search method relative to gradient descent, its parallelizability
facilitates computational time reduction. Experimental validation conducted on
a 2-layer Rectified Linear Unit network with synthetic data elucidates the
impact of hyperparameters on convergence rates and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Debugging is NP-hard for Classifiers Trained with SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizheng Guo, Pengyu Chen, Yanzhang Fu, Dongjing Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data debugging is to find a subset of the training data such that the model
obtained by retraining on the subset has a better accuracy. A bunch of
heuristic approaches are proposed, however, none of them are guaranteed to
solve this problem effectively. This leaves an open issue whether there exists
an efficient algorithm to find the subset such that the model obtained by
retraining on it has a better accuracy. To answer this open question and
provide theoretical basis for further study on developing better algorithms for
data debugging, we investigate the computational complexity of the problem
named Debuggable. Given a machine learning model $\mathcal{M}$ obtained by
training on dataset $D$ and a test instance
$(\mathbf{x}_\text{test},y_\text{test})$ where
$\mathcal{M}(\mathbf{x}_\text{test})\neq y_\text{test}$, Debuggable is to
determine whether there exists a subset $D^\prime$ of $D$ such that the model
$\mathcal{M}^\prime$ obtained by retraining on $D^\prime$ satisfies
$\mathcal{M}^\prime(\mathbf{x}_\text{test})=y_\text{test}$. To cover a wide
range of commonly used models, we take SGD-trained linear classifier as the
model and derive the following main results. (1) If the loss function and the
dimension of the model are not fixed, Debuggable is NP-complete regardless of
the training order in which all the training samples are processed during SGD.
(2) For hinge-like loss functions, a comprehensive analysis on the
computational complexity of Debuggable is provided; (3) If the loss function is
a linear function, Debuggable can be solved in linear time, that is, data
debugging can be solved easily in this case. These results not only highlight
the limitations of current approaches but also offer new insights into data
debugging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autoencoders in Function Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Bunker, Mark Girolami, Hefin Lambley, Andrew M. Stuart, T. J. Sullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders have found widespread application, in both their original
deterministic form and in their variational formulation (VAEs). In scientific
applications it is often of interest to consider data that are comprised of
functions; the same perspective is useful in image processing. In practice,
discretisation (of differential equations arising in the sciences) or
pixellation (of images) renders problems finite dimensional, but conceiving
first of algorithms that operate on functions, and only then discretising or
pixellating, leads to better algorithms that smoothly operate between different
levels of discretisation or pixellation. In this paper function-space versions
of the autoencoder (FAE) and variational autoencoder (FVAE) are introduced,
analysed, and deployed. Well-definedness of the objective function governing
VAEs is a subtle issue, even in finite dimension, and more so on function
space. The FVAE objective is well defined whenever the data distribution is
compatible with the chosen generative model; this happens, for example, when
the data arise from a stochastic differential equation. The FAE objective is
valid much more broadly, and can be straightforwardly applied to data governed
by differential equations. Pairing these objectives with neural operator
architectures, which can thus be evaluated on any mesh, enables new
applications of autoencoders to inpainting, superresolution, and generative
modelling of scientific data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages, 25 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy
  Correspondence Learning in Cross-Modal Retrieval <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Duan, Zhangxuan Gu, Zhenzhe Ying, Lei Qi, Changhua Meng, Yinghuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of cross-modal retrieval, seamlessly integrating diverse
modalities within multimedia remains a formidable challenge, especially given
the complexities introduced by noisy correspondence learning (NCL). Such noise
often stems from mismatched data pairs, which is a significant obstacle
distinct from traditional noisy labels. This paper introduces
Pseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address
this challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an
auxiliary "pseudo-classification" task that interprets captions as categorical
labels, steering the model to learn image-text semantic similarity through a
non-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,
capitalizing on PC$^2$'s pseudo-classification capability, we generate
pseudo-captions to provide more informative and tangible supervision for each
mismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed
to assistant the correction of correspondence. In addition to technical
contributions, we develop a realistic NCL dataset called Noise of Web (NoW),
which could be a new powerful NCL benchmark where noise exists naturally.
Empirical evaluations of PC$^2$ showcase marked improvements over existing
state-of-the-art robust cross-modal retrieval techniques on both simulated and
realistic datasets with various NCL settings. The contributed dataset and
source code are released at https://github.com/alipay/PC2-NoiseofWeb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingyu Li, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal semantic segmentation shows significant potential for enhancing
segmentation accuracy in complex scenes. However, current methods often
incorporate specialized feature fusion modules tailored to specific modalities,
thereby restricting input flexibility and increasing the number of training
parameters. To address these challenges, we propose StitchFusion, a
straightforward yet effective modal fusion framework that integrates
large-scale pre-trained models directly as encoders and feature fusers. This
approach facilitates comprehensive multi-modal and multi-scale feature fusion,
accommodating any visual modal inputs. Specifically, Our framework achieves
modal integration during encoding by sharing multi-modal visual information. To
enhance information exchange across modalities, we introduce a
multi-directional adapter module (MultiAdapter) to enable cross-modal
information transfer during encoding. By leveraging MultiAdapter to propagate
multi-scale information across pre-trained encoders during the encoding
process, StitchFusion achieves multi-modal visual information integration
during encoding. Extensive comparative experiments demonstrate that our model
achieves state-of-the-art performance on four multi-modal segmentation datasets
with minimal additional parameters. Furthermore, the experimental integration
of MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their
complementary nature. Our code is available at StitchFusion_repo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benno Weck, Ilaria Manco, Emmanouil Benetos, Elio Quinton, George Fazekas, Dmitry Bogdanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal models that jointly process audio and language hold great promise
in audio understanding and are increasingly being adopted in the music domain.
By allowing users to query via text and obtain information about a given audio
input, these models have the potential to enable a variety of music
understanding tasks via language-based interfaces. However, their evaluation
poses considerable challenges, and it remains unclear how to effectively assess
their ability to correctly interpret music-related inputs with current methods.
Motivated by this, we introduce MuChoMusic, a benchmark for evaluating music
understanding in multimodal language models focused on audio. MuChoMusic
comprises 1,187 multiple-choice questions, all validated by human annotators,
on 644 music tracks sourced from two publicly available music datasets, and
covering a wide variety of genres. Questions in the benchmark are crafted to
assess knowledge and reasoning abilities across several dimensions that cover
fundamental musical concepts and their relation to cultural and functional
contexts. Through the holistic analysis afforded by the benchmark, we evaluate
five open-source models and identify several pitfalls, including an
over-reliance on the language modality, pointing to a need for better
multimodal integration. Data and code are open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ISMIR 2024. Data: https://doi.org/10.5281/zenodo.12709974
  Code: https://github.com/mulab-mir/muchomusic Supplementary material:
  https://mulab-mir.github.io/muchomusic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Linear Regression when Noises and Covariates are Heavy-Tailed and
  Contaminated by Outliers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takeyuki Sasai, Hironori Fujisawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a problem estimating coefficients of linear regression under
sparsity assumption when covariates and noises are sampled from heavy tailed
distributions. Additionally, we consider the situation where not only
covariates and noises are sampled from heavy tailed distributions but also
contaminated by outliers. Our estimators can be computed efficiently, and
exhibit sharp error bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This research builds on and improves the results of arxiv:2206.07594.
  There will be no further update for the earlier manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HMDN: Hierarchical Multi-Distribution Network for Click-Through Rate
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Lou, Yu Yang, Kuiyao Dong, Heyuan Huang, Wenyi Yu, Ping Wang, Xiu Li, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the recommendation service needs to address increasingly diverse
distributions, such as multi-population, multi-scenario, multitarget, and
multi-interest, more and more recent works have focused on multi-distribution
modeling and achieved great progress. However, most of them only consider
modeling in a single multi-distribution manner, ignoring that mixed
multi-distributions often coexist and form hierarchical relationships. To
address these challenges, we propose a flexible modeling paradigm, named
Hierarchical Multi-Distribution Network (HMDN), which efficiently models these
hierarchical relationships and can seamlessly integrate with existing
multi-distribution methods, such as Mixture of-Experts (MoE) and Dynamic-Weight
(DW) models. Specifically, we first design a hierarchical multi-distribution
representation refinement module, employing a multi-level residual quantization
to obtain fine-grained hierarchical representation. Then, the refined
hierarchical representation is integrated into the existing single
multi-distribution models, seamlessly expanding them into mixed
multi-distribution models. Experimental results on both public and industrial
datasets validate the effectiveness and flexibility of HMDN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnifiedNN: Efficient Neural Network Training on the Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sifat Ut Taki, Spyridon Mastorakis, Arthi Padmanabhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, cloud-based services are widely favored over the traditional
approach of locally training a Neural Network (NN) model. Oftentimes, a cloud
service processes multiple requests from users--thus training multiple NN
models concurrently. However, training NN models concurrently is a challenging
process, which typically requires significant amounts of available computing
resources and takes a long time to complete. In this paper, we present
UnifiedNN to effectively train multiple NN models concurrently on the cloud.
UnifiedNN effectively "combines" multiple NN models and features several memory
and time conservation mechanisms to train multiple NN models simultaneously
without impacting the accuracy of the training process. Specifically, UnifiedNN
merges multiple NN models and creates a large singular unified model in order
to efficiently train all models at once. We have implemented a prototype of
UnifiedNN in PyTorch and we have compared its performance with relevant
state-of-the-art frameworks. Our experimental results demonstrate that
UnifiedNN can reduce memory consumption by up to 53% and training time by up to
81% when compared with vanilla PyTorch without impacting the model training and
testing accuracy. Finally, our results indicate that UnifiedNN can reduce
memory consumption by up to 52% and training time by up to 41% when compared to
state-of-the-art frameworks when training multiple models concurrently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Prediction for Streaming Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleena Chanda, N. V. Vinodchandran, Bertrand Clarke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present two new approaches for point prediction with streaming data. One
is based on the Count-Min sketch (CMS) and the other is based on Gaussian
process priors with a random bias. These methods are intended for the most
general predictive problems where no true model can be usefully formulated for
the data stream. In statistical contexts, this is often called the
$\mathcal{M}$-open problem class. Under the assumption that the data consists
of i.i.d samples from a fixed distribution function $F$, we show that the
CMS-based estimates of the distribution function are consistent.
  We compare our new methods with two established predictors in terms of
cumulative $L^1$ error. One is based on the Shtarkov solution (often called the
normalized maximum likelihood) in the normal experts setting and the other is
based on Dirichlet process priors. These comparisons are for two cases. The
first is one-pass meaning that the updating of the predictors is done using the
fact that the CMS is a sketch. For predictors that are not one-pass, we use
streaming $K$-means to give a representative subset of fixed size that can be
updated as data accumulate.
  Preliminary computational work suggests that the one-pass median version of
the CMS method is rarely outperformed by the other methods for sufficiently
complex data. We also find that predictors based on Gaussian process priors
with random biases perform well. The Shtarkov predictors we use here did not
perform as well probably because we were only using the simplest example. The
other predictors seemed to perform well mainly when the data did not look like
they came from an M-open data generator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, two figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Smoothing ADMM for Quantile Regression with Non-Convex
  Sparse Penalties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Mirzaeifard, Diyako Ghaderyan, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving internet-of-things (IoT) ecosystem, effective data
analysis techniques are crucial for handling distributed data generated by
sensors. Addressing the limitations of existing methods, such as the
sub-gradient approach, which fails to distinguish between active and non-active
coefficients effectively, this paper introduces the decentralized smoothing
alternating direction method of multipliers (DSAD) for penalized quantile
regression. Our method leverages non-convex sparse penalties like the minimax
concave penalty (MCP) and smoothly clipped absolute deviation (SCAD), improving
the identification and retention of significant predictors. DSAD incorporates a
total variation norm within a smoothing ADMM framework, achieving consensus
among distributed nodes and ensuring uniform model performance across disparate
data sources. This approach overcomes traditional convergence challenges
associated with non-convex penalties in decentralized settings. We present
theoretical proofs and extensive simulation results to validate the
effectiveness of the DSAD, demonstrating its superiority in achieving reliable
convergence and enhancing estimation accuracy compared with prior methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Decision-driven Methodology for Designing Uncertainty-aware AI
  Self-Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregory Canal, Vladimir Leung, Philip Sage, Eric Heim, I-Jeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has revolutionized decision-making processes and
systems throughout society and, in particular, has emerged as a significant
technology in high-impact scenarios of national interest. Yet, despite AI's
impressive predictive capabilities in controlled settings, it still suffers
from a range of practical setbacks preventing its widespread use in various
critical scenarios. In particular, it is generally unclear if a given AI
system's predictions can be trusted by decision-makers in downstream
applications. To address the need for more transparent, robust, and trustworthy
AI systems, a suite of tools has been developed to quantify the uncertainty of
AI predictions and, more generally, enable AI to "self-assess" the reliability
of its predictions. In this manuscript, we categorize methods for AI
self-assessment along several key dimensions and provide guidelines for
selecting and designing the appropriate method for a practitioner's needs. In
particular, we focus on uncertainty estimation techniques that consider the
impact of self-assessment on the choices made by downstream decision-makers and
on the resulting costs and benefits of decision outcomes. To demonstrate the
utility of our methodology for self-assessment design, we illustrate its use
for two realistic national-interest scenarios. This manuscript is a practical
guide for machine learning engineers and AI system users to select the ideal
self-assessment techniques for each problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Robustness of Machine Learning Models using Covariate
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Prakash R, Anwesha Bhattacharyya, Joel Vaughan, Vijayan N. Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning models become increasingly prevalent in critical
decision-making models and systems in fields like finance, healthcare, etc.,
ensuring their robustness against adversarial attacks and changes in the input
data is paramount, especially in cases where models potentially overfit. This
paper proposes a comprehensive framework for assessing the robustness of
machine learning models through covariate perturbation techniques. We explore
various perturbation strategies to assess robustness and examine their impact
on model predictions, including separate strategies for numeric and non-numeric
variables, summaries of perturbations to assess and compare model robustness
across different scenarios, and local robustness diagnosis to identify any
regions in the data where a model is particularly unstable. Through empirical
studies on real world dataset, we demonstrate the effectiveness of our approach
in comparing robustness across models, identifying the instabilities in the
model, and enhancing model robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 11 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Mixed Integer Linear Optimization Trained Multivariate
  Classification Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Alston, Illya V. Hicks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate decision trees are powerful machine learning tools for
classification and regression that attract many researchers and industry
professionals. An optimal binary tree has two types of vertices, (i) branching
vertices which have exactly two children and where datapoints are assessed on a
set of discrete features and (ii) leaf vertices at which datapoints are given a
prediction, and can be obtained by solving a biobjective optimization problem
that seeks to (i) maximize the number of correctly classified datapoints and
(ii) minimize the number of branching vertices. Branching vertices are linear
combinations of training features and therefore can be thought of as
hyperplanes. In this paper, we propose two cut-based mixed integer linear
optimization (MILO) formulations for designing optimal binary classification
trees (leaf vertices assign discrete classes). Our models leverage on-the-fly
identification of minimal infeasible subsystems (MISs) from which we derive
cutting planes that hold the form of packing constraints. We show theoretical
improvements on the strongest flow-based MILO formulation currently in the
literature and conduct experiments on publicly available datasets to show our
models' ability to scale, strength against traditional branch and bound
approaches, and robustness in out-of-sample test performance. Our code and data
are available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2206.04857</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Clock: High-Dimensional Effects in Two-Dimensional Plots <span class="chip">IEEE VIS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Ovcharenko, Rita Sevastjanova, Valentina Boeva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans struggle to perceive and interpret high-dimensional data. Therefore,
high-dimensional data are often projected into two dimensions for
visualization. Many applications benefit from complex nonlinear dimensionality
reduction techniques, but the effects of individual high-dimensional features
are hard to explain in the two-dimensional space. Most visualization solutions
use multiple two-dimensional plots, each showing the effect of one
high-dimensional feature in two dimensions; this approach creates a need for a
visual inspection of k plots for a k-dimensional input space. Our solution,
Feature Clock, provides a novel approach that eliminates the need to inspect
these k plots to grasp the influence of original features on the data structure
depicted in two dimensions. Feature Clock enhances the explainability and
compactness of visualizations of embedded data and is available in an
open-source Python library.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE VIS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tiny Supervised ODL Core with Auto Data Pruning for Human Activity
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroki Matsutani, Radu Marculescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a low-cost and low-power tiny supervised
on-device learning (ODL) core that can address the distributional shift of
input data for human activity recognition. Although ODL for resource-limited
edge devices has been studied recently, how exactly to provide the training
labels to these devices at runtime remains an open-issue. To address this
problem, we propose to combine an automatic data pruning with supervised ODL to
reduce the number queries needed to acquire predicted labels from a nearby
teacher device and thus save power consumption during model retraining. The
data pruning threshold is automatically tuned, eliminating a manual threshold
tuning. As a tinyML solution at a few mW for the human activity recognition, we
design a supervised ODL core that supports our automatic data pruning using a
45nm CMOS process technology. We show that the required memory size for the
core is smaller than the same-shaped multilayer perceptron (MLP) and the power
consumption is only 3.39mW. Experiments using a human activity recognition
dataset show that the proposed automatic data pruning reduces the communication
volume by 55.7% and power consumption accordingly with only 0.9% accuracy loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE BSN 2024 (accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certified Robust Invariant Polytope Training in Neural Controlled ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Harapanahalli, Samuel Coogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a nonlinear control system modeled as an ordinary differential
equation subject to disturbance, with a state feedback controller parameterized
as a feedforward neural network. We propose a framework for training
controllers with certified robust forward invariant polytopes, where any
trajectory initialized inside the polytope remains within the polytope,
regardless of the disturbance. First, we parameterize a family of lifted
control systems in a higher dimensional space, where the original neural
controlled system evolves on an invariant subspace of each lifted system. We
use interval analysis and neural network verifiers to further construct a
family of lifted embedding systems, carefully capturing the knowledge of this
invariant subspace. If the vector field of any lifted embedding system
satisfies a sign constraint at a single point, then a certain convex polytope
of the original system is robustly forward invariant. Treating the neural
network controller and the lifted system parameters as variables, we propose an
algorithm to train controllers with certified forward invariant polytopes in
the closed-loop control system. Through two examples, we demonstrate how the
simplicity of the sign constraint allows our approach to scale with system
dimension to over $50$ states, and outperform state-of-the-art Lyapunov-based
sampling approaches in runtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection and Characterization of Coordinated Online Behavior: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Mannocci, Michele Mazza, Anna Monreale, Maurizio Tesconi, Stefano Cresci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordination is a fundamental aspect of life. The advent of social media has
made it integral also to online human interactions, such as those that
characterize thriving online communities and social movements. At the same
time, coordination is also core to effective disinformation, manipulation, and
hate campaigns. This survey collects, categorizes, and critically discusses the
body of work produced as a result of the growing interest on coordinated online
behavior. We reconcile industry and academic definitions, propose a
comprehensive framework to study coordinated online behavior, and review and
critically discuss the existing detection and characterization methods. Our
analysis identifies open challenges and promising directions of research,
serving as a guide for scholars, practitioners, and policymakers in
understanding and addressing the complexities inherent to online coordination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep progressive reinforcement learning-based flexible resource
  scheduling framework for IRS and UAV-assisted MEC system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Dong, Feibo Jiang, Minjie Wang, Yubo Peng, Xiaolong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intelligent reflection surface (IRS) and unmanned aerial vehicle
(UAV)-assisted mobile edge computing (MEC) system is widely used in temporary
and emergency scenarios. Our goal is to minimize the energy consumption of the
MEC system by jointly optimizing UAV locations, IRS phase shift, task
offloading, and resource allocation with a variable number of UAVs. To this
end, we propose a Flexible REsource Scheduling (FRES) framework by employing a
novel deep progressive reinforcement learning which includes the following
innovations: Firstly, a novel multi-task agent is presented to deal with the
mixed integer nonlinear programming (MINLP) problem. The multi-task agent has
two output heads designed for different tasks, in which a classified head is
employed to make offloading decisions with integer variables while a fitting
head is applied to solve resource allocation with continuous variables.
Secondly, a progressive scheduler is introduced to adapt the agent to the
varying number of UAVs by progressively adjusting a part of neurons in the
agent. This structure can naturally accumulate experiences and be immune to
catastrophic forgetting. Finally, a light taboo search (LTS) is introduced to
enhance the global search of the FRES. The numerical results demonstrate the
superiority of the FRES framework which can make real-time and optimal resource
scheduling even in dynamic MEC systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Classification of Dry Bean Varieties Using XGBoost and SVM
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramtin Ardeshirifar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comparative study on the automated classification of
seven different varieties of dry beans using machine learning models.
Leveraging a dataset of 12,909 dry bean samples, reduced from an initial 13,611
through outlier removal and feature extraction, we applied Principal Component
Analysis (PCA) for dimensionality reduction and trained two multiclass
classifiers: XGBoost and Support Vector Machine (SVM). The models were
evaluated using nested cross-validation to ensure robust performance assessment
and hyperparameter tuning. The XGBoost and SVM models achieved overall correct
classification rates of 94.00% and 94.39%, respectively. The results underscore
the efficacy of these machine learning approaches in agricultural applications,
particularly in enhancing the uniformity and efficiency of seed classification.
This study contributes to the growing body of work on precision agriculture,
demonstrating that automated systems can significantly support seed quality
control and crop yield optimization. Future work will explore incorporating
more diverse datasets and advanced algorithms to further improve classification
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figurs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tailoring Graph Neural Network-based Flow-guided Localization to
  Individual Bloodstreams and Activities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Galván, Filip Lemic, Gerard Calvo Bartra, Sergi Abadal, Xavier Costa Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow-guided localization using in-body nanodevices in the bloodstream is
expected to be beneficial for early disease detection, continuous monitoring of
biological conditions, and targeted treatment. The nanodevices face size and
power constraints that produce erroneous raw data for localization purposes.
On-body anchors receive this data, and use it to derive the locations of
diagnostic events of interest. Different Machine Learning (ML) approaches have
been recently proposed for this task, yet they are currently restricted to a
reference bloodstream of a resting patient. As such, they are unable to deal
with the physical diversity of patients' bloodstreams and cannot provide
continuous monitoring due to changes in individual patient's activities. Toward
addressing these issues for the current State-of-the-Art (SotA) flow-guided
localization approach based on Graph Neural Networks (GNNs), we propose a
pipeline for GNN adaptation based on individual physiological indicators
including height, weight, and heart rate. Our results indicate that the
proposed adaptions are beneficial in reconciling the individual differences
between bloodstreams and activities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, 2 tables, 16 references, accepted at ACM
  NanoCom'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HeteroMorpheus: Universal Control Based on Morphological Heterogeneity
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YiFan Hao, Yang Yang, Junru Song, Wei Peng, Weien Zhou, Tingsong Jiang, Wen Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of robotic control, designing individual controllers for each
robot leads to high computational costs. Universal control policies, applicable
across diverse robot morphologies, promise to mitigate this challenge.
Predominantly, models based on Graph Neural Networks (GNN) and Transformers are
employed, owing to their effectiveness in capturing relational dynamics across
a robot's limbs. However, these models typically employ homogeneous graph
structures that overlook the functional diversity of different limbs. To bridge
this gap, we introduce HeteroMorpheus, a novel method based on heterogeneous
graph Transformer. This method uniquely addresses limb heterogeneity, fostering
better representation of robot dynamics of various morphologies. Through
extensive experiments we demonstrate the superiority of HeteroMorpheus against
state-of-the-art methods in the capability of policy generalization, including
zero-shot generalization and sample-efficient transfer to unfamiliar robot
morphologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZNorm: Z-Score Gradient Normalization for Accelerating Neural Network
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyoung Yun, Hoyoung Kim, Suin Cho, Hangil Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in deep learning necessitate efficient training
methods for deep neural networks (DNNs). As models grow in complexity,
vanishing and exploding gradients impede convergence and performance. We
propose Z-Score Normalization for Gradient Descent (ZNorm), an innovative
technique that adjusts only the gradients to enhance training efficiency and
improve model performance. ZNorm normalizes the overall gradients, providing
consistent gradient scaling across layers, thereby reducing the risks of
vanishing and exploding gradients. Our extensive experiments on CIFAR-10 and
medical datasets demonstrate that ZNorm not only accelerates convergence but
also enhances performance metrics. ZNorm consistently outperforms existing
methods, achieving superior results using the same computational settings. In
medical imaging applications, ZNorm improves tumor prediction and segmentation
performances, underscoring its practical utility. These findings highlight
ZNorm's potential as a robust and versatile tool for improving the efficiency
and effectiveness of deep neural network training across a wide range of
architectures and applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certifiably Robust Encoding Schemes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Saxena, Tom Wollschläger, Nicola Franco, Jeanette Miriam Lorenz, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning uses principles from quantum mechanics to process
data, offering potential advances in speed and performance. However, previous
work has shown that these models are susceptible to attacks that manipulate
input data or exploit noise in quantum circuits. Following this, various
studies have explored the robustness of these models. These works focus on the
robustness certification of manipulations of the quantum states. We extend this
line of research by investigating the robustness against perturbations in the
classical data for a general class of data encoding schemes. We show that for
such schemes, the addition of suitable noise channels is equivalent to
evaluating the mean value of the noiseless classifier at the smoothed data,
akin to Randomized Smoothing from classical machine learning. Using our general
framework, we show that suitable additions of phase-damping noise channels
improve empirical and provable robustness for the considered class of encoding
schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Variational Quantum Circuits Using Metaheuristic Strategies
  in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Kölle, Daniel Seidl, Maximilian Zorn, Philipp Altmann, Jonas Stein, Thomas Gabor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum Reinforcement Learning (QRL) offers potential advantages over
classical Reinforcement Learning, such as compact state space representation
and faster convergence in certain scenarios. However, practical benefits
require further validation. QRL faces challenges like flat solution landscapes,
where traditional gradient-based methods are inefficient, necessitating the use
of gradient-free algorithms. This work explores the integration of
metaheuristic algorithms -- Particle Swarm Optimization, Ant Colony
Optimization, Tabu Search, Genetic Algorithm, Simulated Annealing, and Harmony
Search -- into QRL. These algorithms provide flexibility and efficiency in
parameter optimization. Evaluations in $5\times5$ MiniGrid Reinforcement
Learning environments show that, all algorithms yield near-optimal results,
with Simulated Annealing and Particle Swarm Optimization performing best. In
the Cart Pole environment, Simulated Annealing, Genetic Algorithms, and
Particle Swarm Optimization achieve optimal results, while the others perform
slightly better than random action selection. These findings demonstrate the
potential of Particle Swarm Optimization and Simulated Annealing for efficient
QRL learning, emphasizing the need for careful algorithm selection and
adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at QCE24 - QCRL24 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nested Music <span class="highlight-title">Transformer</span>: Sequentially Decoding Compound Tokens in
  Symbolic Music and Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Ryu, Hao-Wen Dong, Jongmin Jung, Dasaem Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing symbolic music with compound tokens, where each token consists
of several different sub-tokens representing a distinct musical feature or
attribute, offers the advantage of reducing sequence length. While previous
research has validated the efficacy of compound tokens in music sequence
modeling, predicting all sub-tokens simultaneously can lead to suboptimal
results as it may not fully capture the interdependencies between them. We
introduce the Nested Music Transformer (NMT), an architecture tailored for
decoding compound tokens autoregressively, similar to processing flattened
tokens, but with low memory usage. The NMT consists of two transformers: the
main decoder that models a sequence of compound tokens and the sub-decoder for
modeling sub-tokens of each compound token. The experiment results showed that
applying the NMT to compound tokens can enhance the performance in terms of
better perplexity in processing various symbolic music datasets and discrete
audio tokens from the MAESTRO dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 25th International Society for Music Information
  Retrieval Conference (ISMIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sustainable Diffusion-based Incentive Mechanism for Generative AI-driven
  Digital Twins in Industrial Cyber-Physical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinbo Wen, Jiawen Kang, Dusit Niyato, Yang Zhang, Shiwen Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial Cyber-Physical Systems (ICPSs) are an integral component of modern
manufacturing and industries. By digitizing data throughout the product life
cycle, Digital Twins (DTs) in ICPSs enable a shift from current industrial
infrastructures to intelligent and adaptive infrastructures. Thanks to data
process capability, Generative Artificial Intelligence (GAI) can drive the
construction and update of DTs to improve predictive accuracy and prepare for
diverse smart manufacturing. However, mechanisms that leverage sensing
Industrial Internet of Things (IIoT) devices to share data for the construction
of DTs are susceptible to adverse selection problems. In this paper, we first
develop a GAI-driven DT architecture for ICPSs. To address the adverse
selection problem caused by information asymmetry, we propose a contract theory
model and develop the sustainable diffusion-based soft actor-critic algorithm
to identify the optimal feasible contract. Specifically, we leverage the
dynamic structured pruning technique to reduce parameter numbers of actor
networks, allowing sustainability and efficient implementation of the proposed
algorithm. Finally, numerical results demonstrate the effectiveness of the
proposed scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation-Enhanced Searchlight: Enabling brain decoding from
  visual perception to mental imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Olza, David Soto, Roberto Santana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cognitive neuroscience and brain-computer interface research, accurately
predicting imagined stimuli is crucial. This study investigates the
effectiveness of Domain Adaptation (DA) in enhancing imagery prediction using
primarily visual data from fMRI scans of 18 subjects. Initially, we train a
baseline model on visual stimuli to predict imagined stimuli, utilizing data
from 14 brain regions. We then develop several models to improve imagery
prediction, comparing different DA methods. Our results demonstrate that DA
significantly enhances imagery prediction, especially with the Regular Transfer
approach. We then conduct a DA-enhanced searchlight analysis using Regular
Transfer, followed by permutation-based statistical tests to identify brain
regions where imagery decoding is consistently above chance across subjects.
Our DA-enhanced searchlight predicts imagery contents in a highly distributed
set of brain regions, including the visual cortex and the frontoparietal
cortex, thereby outperforming standard cross-domain classification methods. The
complete code and data for this paper have been made openly available for the
use of the scientific community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TCR-<span class="highlight-title">GPT</span>: Integrating Autoregressive Model and Reinforcement Learning for
  T-Cell Receptor Repertoires Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Lin, Dandan Zhang, Yun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  T-cell receptors (TCRs) play a crucial role in the immune system by
recognizing and binding to specific antigens presented by infected or cancerous
cells. Understanding the sequence patterns of TCRs is essential for developing
targeted immune therapies and designing effective vaccines. Language models,
such as auto-regressive transformers, offer a powerful solution to this problem
by learning the probability distributions of TCR repertoires, enabling the
generation of new TCR sequences that inherit the underlying patterns of the
repertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only
transformer architecture, designed to uncover and replicate sequence patterns
in TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring
sequence probability distributions measured by Pearson correlation coefficient.
Furthermore, by leveraging Reinforcement Learning(RL), we adapted the
distribution of TCR sequences to generate TCRs capable of recognizing specific
peptides, offering significant potential for advancing targeted immune
therapies and vaccine development. With the efficacy of RL, fine-tuned
pretrained TCR-GPT models demonstrated the ability to produce TCR repertoires
likely to bind specific peptides, illustrating RL's efficiency in enhancing the
model's adaptability to the probability distributions of biologically relevant
TCR sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Prediction of Ventilator-Associated Pneumonia in Patients with
  Traumatic Brain Injury Using Advanced Machine Learning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negin Ashrafi, Armin Abdollahi, Maryam Pishgar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Ventilator-associated pneumonia (VAP) in traumatic brain injury
(TBI) patients poses a significant mortality risk and imposes a considerable
financial burden on patients and healthcare systems. Timely detection and
prognostication of VAP in TBI patients are crucial to improve patient outcomes
and alleviate the strain on healthcare resources.
  Methods: We implemented six machine learning models using the MIMIC-III
database. Our methodology included preprocessing steps, such as feature
selection with CatBoost and expert opinion, addressing class imbalance with the
Synthetic Minority Oversampling Technique (SMOTE), and rigorous model tuning
through 5-fold cross-validation to optimize hyperparameters. Key models
evaluated included SVM, Logistic Regression, Random Forest, XGBoost, ANN, and
AdaBoost. Additionally, we conducted SHAP analysis to determine feature
importance and performed an ablation study to assess feature impacts on model
performance.
  Results: XGBoost outperformed the baseline models and the best existing
literature. We used metrics, including AUC, Accuracy, Specificity, Sensitivity,
F1 Score, PPV, and NPV. XGBoost demonstrated the highest performance with an
AUC of 0.940 and an Accuracy of 0.875, which are 23.4% and 23.5% higher than
the best results in the existing literature, with an AUC of 0.706 and an
Accuracy of 0.640, respectively. This enhanced performance underscores the
models' effectiveness in clinical settings.
  Conclusions: This study enhances the predictive modeling of VAP in TBI
patients, improving early detection and intervention potential. Refined feature
selection and advanced ensemble techniques significantly boosted model accuracy
and reliability, offering promising directions for future clinical applications
and medical diagnostics research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning topological energy braiding of non-Bloch bands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuwei Shi, Shibing Chu, Yuee Xie, Yuanping Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has been used to identify phase transitions in a variety of
physical systems. However, there is still a lack of relevant research on
non-Bloch energy braiding in non-Hermitian systems. In this work, we study
non-Bloch energy braiding in one-dimensional non-Hermitian systems using
unsupervised and supervised methods. In unsupervised learning, we use diffusion
maps to successfully identify non-Bloch energy braiding without any prior
knowledge and combine it with k-means to cluster different topological elements
into clusters, such as Unlink and Hopf link. In supervised learning, we train a
Convolutional Neural Network (CNN) based on Bloch energy data to predict not
only Bloch energy braiding but also non-Bloch energy braiding with an accuracy
approaching 100%. By analysing the CNN, we can ascertain that the network has
successfully acquired the ability to recognise the braiding topology of the
energy bands. The present study demonstrates the considerable potential of
machine learning in the identification of non-Hermitian topological phases and
energy braiding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Xin Xu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning, as a vital technique, has sparked a notable revolution in
artificial intelligence. As the most representative architecture, Transformers
have empowered numerous advanced models, especially the large language models
that comprise billions of parameters, becoming a cornerstone in deep learning.
Despite the impressive achievements, Transformers still face inherent
limitations, particularly the time-consuming inference resulting from the
quadratic computation complexity of attention calculation. Recently, a novel
architecture named Mamba, drawing inspiration from classical state space
models, has emerged as a promising alternative for building foundation models,
delivering comparable modeling abilities to Transformers while preserving
near-linear scalability concerning sequence length. This has sparked an
increasing number of studies actively exploring Mamba's potential to achieve
impressive performance across diverse domains. Given such rapid evolution,
there is a critical need for a systematic review that consolidates existing
Mamba-empowered models, offering a comprehensive understanding of this emerging
model architecture. In this survey, we therefore conduct an in-depth
investigation of recent Mamba-associated studies, covering from three main
aspects: the advancements of Mamba-based models, the techniques of adapting
Mamba to diverse data, and the applications where Mamba can excel.
Specifically, we first recall the foundational knowledge of various
representative deep learning models and the details of Mamba as preliminaries.
Then, to showcase the significance of Mamba, we comprehensively review the
related studies focusing on Mamba models' architecture design, data
adaptability, and applications. Finally, we present an discussion of current
limitations and explore various promising research directions to provide deeper
insights for future investigations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Encoding--Searching Separation Perspective on Bi-Encoder Neural
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung-Nghiep Tran, Akiko Aizawa, Atsuhiro Takasu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews, analyzes, and proposes a new perspective on the
bi-encoder architecture for neural search. While the bi-encoder architecture is
widely used due to its simplicity and scalability at test time, it has some
notable issues such as low performance on seen datasets and weak zero-shot
performance on new datasets. In this paper, we analyze these issues and
summarize two main critiques: the encoding information bottleneck problem and
limitations of the basic assumption of embedding search. We then construct a
thought experiment to logically analyze the encoding and searching operations
and challenge the basic assumption of embedding search. Building on these
observations, we propose a new perspective on the bi-encoder architecture
called the \textit{encoding--searching separation} perspective, which
conceptually and practically separates the encoding and searching operations.
This new perspective is applied to explain the root cause of the identified
issues and discuss ways to mitigate the problems. Finally, we discuss the
implications of the ideas underlying the new perspective, the design surface
that it exposes and the potential research directions arising from it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universality of kernel random matrices and kernel regression in the
  quadratic regime 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parthe Pandit, Zhichao Wang, Yizhe Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel ridge regression (KRR) is a popular class of machine learning models
that has become an important tool for understanding deep learning. Much of the
focus has been on studying the proportional asymptotic regime, $n \asymp d$,
where $n$ is the number of training samples and $d$ is the dimension of the
dataset. In this regime, under certain conditions on the data distribution, the
kernel random matrix involved in KRR exhibits behavior akin to that of a linear
kernel. In this work, we extend the study of kernel regression to the quadratic
asymptotic regime, where $n \asymp d^2$. In this regime, we demonstrate that a
broad class of inner-product kernels exhibit behavior similar to a quadratic
kernel. Specifically, we establish an operator norm approximation bound for the
difference between the original kernel random matrix and a quadratic kernel
random matrix with additional correction terms compared to the Taylor expansion
of the kernel functions. The approximation works for general data distributions
under a Gaussian-moment-matching assumption with a covariance structure. This
new approximation is utilized to obtain a limiting spectral distribution of the
original kernel matrix and characterize the precise asymptotic training and
generalization errors for KRR in the quadratic regime when $n/d^2$ converges to
a non-zero constant. The generalization errors are obtained for both
deterministic and random teacher models. Our proof techniques combine moment
methods, Wick's formula, orthogonal polynomials, and resolvent analysis of
random matrices with correlated entries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>75 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Hyperparameters on Large Language Model Inference
  Performance: An Evaluation of vLLM and HuggingFace Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent surge of open-source large language models (LLMs) enables
developers to create AI-based solutions while maintaining control over aspects
such as privacy and compliance, thereby providing governance and ownership of
the model deployment process. To utilize these LLMs, inference engines are
needed. These engines load the model's weights onto available resources, such
as GPUs, and process queries to generate responses. The speed of inference, or
performance, of the LLM, is critical for real-time applications, as it computes
millions or billions of floating point operations per inference. Recently,
advanced inference engines such as vLLM have emerged, incorporating novel
mechanisms such as efficient memory management to achieve state-of-the-art
performance. In this paper, we analyze the performance, particularly the
throughput (tokens generated per unit of time), of 20 LLMs using two inference
libraries: vLLM and HuggingFace's pipelines. We investigate how various
hyperparameters, which developers must configure, influence inference
performance. Our results reveal that throughput landscapes are irregular, with
distinct peaks, highlighting the importance of hyperparameter optimization to
achieve maximum performance. We also show that applying hyperparameter
optimization when upgrading or downgrading the GPU model used for inference can
improve throughput from HuggingFace pipelines by an average of 9.16% and 13.7%,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Split Learning with Vision <span class="highlight-title">Transformer</span>s using
  Patch-Wise Random and Noisy CutMix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungeun Oh, Sihun Baek, Jihong Park, Hyelin Nam, Praneeth Vepakomma, Ramesh Raskar, Mehdi Bennis, Seong-Lyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, the vision transformer (ViT) has increasingly superseded
the convolutional neural network (CNN) for improved accuracy and robustness.
However, ViT's large model sizes and high sample complexity make it difficult
to train on resource-constrained edge devices. Split learning (SL) emerges as a
viable solution, leveraging server-side resources to train ViTs while utilizing
private data from distributed devices. However, SL requires additional
information exchange for weight updates between the device and the server,
which can be exposed to various attacks on private training data. To mitigate
the risk of data breaches in classification tasks, inspired from the CutMix
regularization, we propose a novel privacy-preserving SL framework that injects
Gaussian noise into smashed data and mixes randomly chosen patches of smashed
data across clients, coined DP-CutMixSL. Our analysis demonstrates that
DP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy
protection against membership inference attacks during forward propagation.
Through simulations, we show that DP-CutMixSL improves privacy protection
against membership inference attacks, reconstruction attacks, and label
inference attacks, while also improving accuracy compared to DP-SL and
DP-MixSL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, 8 tables, to be published in Transactions on
  Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling interpretable causal trees from causal forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Rehill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods for estimating treatment effect heterogeneity
promise greater flexibility than existing methods that test a few pre-specified
hypotheses. However, one problem these methods can have is that it can be
challenging to extract insights from complicated machine learning models. A
high-dimensional distribution of conditional average treatment effects may give
accurate, individual-level estimates, but it can be hard to understand the
underlying patterns; hard to know what the implications of the analysis are.
This paper proposes the Distilled Causal Tree, a method for distilling a
single, interpretable causal tree from a causal forest. This compares well to
existing methods of extracting a single tree, particularly in noisy data or
high-dimensional data where there are many correlated features. Here it even
outperforms the base causal forest in most simulations. Its estimates are
doubly robust and asymptotically normal just as those of the causal forest are.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Family of Distributions of Random Subsets for Controlling Positive and
  Negative Dependence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiro Kawashima, Hideitsu Hino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positive and negative dependence are fundamental concepts that characterize
the attractive and repulsive behavior of random subsets. Although some
probabilistic models are known to exhibit positive or negative dependence, it
is challenging to seamlessly bridge them with a practicable probabilistic
model. In this study, we introduce a new family of distributions, named the
discrete kernel point process (DKPP), which includes determinantal point
processes and parts of Boltzmann machines. We also develop some computational
methods for probabilistic operations and inference with DKPPs, such as
calculating marginal and conditional probabilities and learning the parameters.
Our numerical experiments demonstrate the controllability of positive and
negative dependence and the effectiveness of the computational methods for
DKPPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GNN-MolKAN: Harnessing the Power of KAN to Advance Molecular
  Representation Learning with GNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective molecular representation learning is crucial for molecular property
prediction and drug design. However, existing approaches struggle with
limitations in insufficient annotations and suboptimal architecture design. For
instance, Graph Neural Networks (GNNs) suffer from over-squashing, causing the
loss of important structural details in molecules, thus impairing molecular
representations. In this work, we propose a new class of GNNs, GNN-MolKAN and
its augmented variant, GNN-MolKAN+, that integrate the Kolmogorov-Arnold
Networks (KAN) architecture from AI + Science into GNNs to address these
challenges. Additionally, we introduce Adaptive FastKAN (AdFastKAN), an
advanced KAN that offers increased stability and speed, further enhancing the
performance of standard GNNs. Notably, our approach holds three key benefits:
1) Superior Performance: GNN-MolKAN and GNN-MolKAN+ demonstrate superior
prediction ability, robust generalization to unseen scaffolds, and versatile
transferability across different GNN architectures. 2) Efficiency: These models
require less computational time and fewer parameters while matching or
surpassing the state-of-the-art (SOTA) self-supervised methods. 3) Few-shot
Learning Ability: GNN-MolKAN demonstrates great potential in few-shot learning
scenarios, achieving an average improvement of 6.97% across few-shot
benchmarks. Overall, we validate our architecture on 6 classification datasets,
6 regression datasets, and 4 few-shot learning datasets, consistently achieving
highly competitive results across all of them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IBB Traffic Graph Data: Benchmarking and Road Traffic Prediction Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eren Olug, Kiymet Kaya, Resul Tugay, Sule Gunduz Oguducu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road traffic congestion prediction is a crucial component of intelligent
transportation systems, since it enables proactive traffic management, enhances
suburban experience, reduces environmental impact, and improves overall safety
and efficiency. Although there are several public datasets, especially for
metropolitan areas, these datasets may not be applicable to practical scenarios
due to insufficiency in the scale of data (i.e. number of sensors and road
links) and several external factors like different characteristics of the
target area such as urban, highways and the data collection location. To
address this, this paper introduces a novel IBB Traffic graph dataset as an
alternative benchmark dataset to mitigate these limitations and enrich the
literature with new geographical characteristics. IBB Traffic graph dataset
covers the sensor data collected at 2451 distinct locations. Moreover, we
propose a novel Road Traffic Prediction Model that strengthens temporal links
through feature engineering, node embedding with GLEE to represent
inter-related relationships within the traffic network, and traffic prediction
with ExtraTrees. The results indicate that the proposed model consistently
outperforms the baseline models, demonstrating an average accuracy improvement
of 4%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with
  Accelerated LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afia Anjum, Maksim E. Eren, Ismael Boureima, Boian Alexandrov, Manish Bhattarai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities across a wide range of natural language processing (NLP) tasks,
such as question-answering, sentiment analysis, text summarization, and machine
translation. However, the ever-growing complexity of LLMs demands immense
computational resources, hindering the broader research and application of
these models. To address this, various parameter-efficient fine-tuning
strategies, such as Low-Rank Approximation (LoRA) and Adapters, have been
developed. Despite their potential, these methods often face limitations in
compressibility. Specifically, LoRA struggles to scale effectively with the
increasing number of trainable parameters in modern large scale LLMs.
Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which
utilizes tensor train decomposition, has not yet achieved the level of
compression necessary for fine-tuning very large scale models with limited
resources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA),
a novel parameter-efficient fine-tuning (PEFT) approach that extends LoRETTA
with optimized tensor train (TT) decomposition integration. By eliminating
Adapters and traditional LoRA-based structures, TT-LoRA achieves greater model
compression without compromising downstream task performance, along with
reduced inference latency and computational overhead. We conduct an exhaustive
parameter search to establish benchmarks that highlight the trade-off between
model compression and performance. Our results demonstrate significant
compression of LLMs while maintaining comparable performance to larger models,
facilitating their deployment on resource-constraint platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LA-UR-24-28177</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Financial Market Predictions: Causality-Driven Feature
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Liang, Zhengyang Li, Weitong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the FinSen dataset that revolutionizes financial market
analysis by integrating economic and financial news articles from 197 countries
with stock market data. The dataset's extensive coverage spans 15 years from
2007 to 2023 with temporal information, offering a rich, global perspective
with 160,000 records on financial market news. Our study leverages causally
validated sentiment scores and LSTM models to enhance market forecast accuracy
and reliability. Utilizing the FinSen dataset, we introduce an innovative Focal
Calibration Loss, reducing Expected Calibration Error (ECE) to 3.34 percent
with the DAN 3 model. This not only improves prediction accuracy but also
aligns probabilistic forecasts closely with real outcomes, crucial for the
financial sector where predicted probability is paramount. Our approach
demonstrates the effectiveness of combining sentiment analysis with precise
calibration techniques for trustworthy financial forecasting where the cost of
misinterpretation can be high. Finsen Data can be found at [this github
URL](https://github.com/EagleAdelaide/FinSen_Dataset.git).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 20th International Conference Advanced Data Mining
  and Applications 2024 (ADMA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Two-Stage Cloud Resource Scaling via Hierarchical
  Multi-Indicator Forecasting and Bayesian Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Luo, Shiyu Wang, Zhemeng Yu, Wei Lu, Xiaofeng Gao, Lintao Ma, Guihai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The surging demand for cloud computing resources, driven by the rapid growth
of sophisticated large-scale models and data centers, underscores the critical
importance of efficient and adaptive resource allocation. As major tech
enterprises deploy massive infrastructures with thousands of GPUs, existing
cloud platforms still struggle with low resource utilization due to key
challenges: capturing hierarchical indicator structures, modeling non-Gaussian
distributions, and decision-making under uncertainty. To address these
challenges, we propose HRAMONY, an adaptive Hierarchical Attention-based
Resource Modeling and Decision-Making System. HARMONY combines hierarchical
multi-indicator distribution forecasting and uncertainty-aware Bayesian
decision-making. It introduces a novel hierarchical attention mechanism that
comprehensively models complex inter-indicator dependencies, enabling accurate
predictions that can adapt to evolving environment states. By transforming
Gaussian projections into adaptive non-Gaussian distributions via Normalizing
Flows. Crucially, HARMONY leverages the full predictive distributions in an
adaptive Bayesian process, proactively incorporating uncertainties to optimize
resource allocation while robustly meeting SLA constraints under varying
conditions. Extensive evaluations across four large-scale cloud datasets
demonstrate HARMONY's state-of-the-art performance, significantly outperforming
nine established methods. A month-long real-world deployment validated
HARMONY's substantial practical impact, realizing over 35,000 GPU hours in
savings and translating to $100K+ in cost reduction, showcasing its remarkable
economic value through adaptive, uncertainty-aware scaling. Our code is
available at https://github.com/Floating-LY/HARMONY1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IncidentNet: Traffic Incident Detection, Localization and Severity
  Estimation with Sparse Sensing <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Shashank Peddiraju, Kaustubh Harapanahalli, Edward Andert, Aviral Shrivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior art in traffic incident detection relies on high sensor coverage and is
primarily based on decision-tree and random forest models that have limited
representation capacity and, as a result, cannot detect incidents with high
accuracy. This paper presents IncidentNet - a novel approach for classifying,
localizing, and estimating the severity of traffic incidents using deep
learning models trained on data captured from sparsely placed sensors in urban
environments. Our model works on microscopic traffic data that can be collected
using cameras installed at traffic intersections. Due to the unavailability of
datasets that provide microscopic traffic details and traffic incident details
simultaneously, we also present a methodology to generate a synthetic
microscopic traffic dataset that matches given macroscopic traffic data.
IncidentNet achieves a traffic incident detection rate of 98%, with false alarm
rates of less than 7% in 197 seconds on average in urban environments with
cameras on less than 20% of the traffic intersections.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 2024 IEEE 27th International Conference on
  Intelligent Transportation Systems (ITSC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconstructing Richtmyer-Meshkov instabilities from noisy radiographs
  using low dimensional features and attention-based neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel A. Serino, Marc L. Klasky, Balasubramanya T. Nadiga, Xiaojian Xu, Trevor Wilcox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A trained attention-based transformer network can robustly recover the
complex topologies given by the Richtmyer-Meshkoff instability from a sequence
of hydrodynamic features derived from radiographic images corrupted with blur,
scatter, and noise. This approach is demonstrated on ICF-like double shell
hydrodynamic simulations. The key component of this network is a transformer
encoder that acts on a sequence of features extracted from noisy radiographs.
This encoder includes numerous self-attention layers that act to learn temporal
dependencies in the input sequences and increase the expressiveness of the
model. This approach is demonstrated to exhibit an excellent ability to
accurately recover the Richtmyer-Meshkov instability growth rates, even despite
the gas-metal interface being greatly obscured by radiographic noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ META-ANOVA: Screening interactions for interpretable machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchan Choi, Seokhun Park, Chanmoo Park, Dongha Kim, Yongdai Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are two things to be considered when we evaluate predictive models. One
is prediction accuracy,and the other is interpretability. Over the recent
decades, many prediction models of high performance, such as ensemble-based
models and deep neural networks, have been developed. However, these models are
often too complex, making it difficult to intuitively interpret their
predictions. This complexity in interpretation limits their use in many
real-world fields that require accountability, such as medicine, finance, and
college admissions. In this study, we develop a novel method called Meta-ANOVA
to provide an interpretable model for any given prediction model. The basic
idea of Meta-ANOVA is to transform a given black-box prediction model to the
functional ANOVA model. A novel technical contribution of Meta-ANOVA is a
procedure of screening out unnecessary interaction before transforming a given
black-box model to the functional ANOVA model. This screening procedure allows
the inclusion of higher order interactions in the transformed functional ANOVA
model without computational difficulties. We prove that the screening procedure
is asymptotically consistent. Through various experiments with synthetic and
real-world datasets, we empirically demonstrate the superiority of Meta-ANOVA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIS-ME: A Multi-modal Framework for Soil Moisture Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Rakib, Adil Aman Mohammed, Cole Diggins, Sumit Sharma, Jeff Michael Sadler, Tyson Ochsner, Arun Bagavathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soil moisture estimation is an important task to enable precision agriculture
in creating optimal plans for irrigation, fertilization, and harvest. It is
common to utilize statistical and machine learning models to estimate soil
moisture from traditional data sources such as weather forecasts, soil
properties, and crop properties. However, there is a growing interest in
utilizing aerial and geospatial imagery to estimate soil moisture. Although
these images capture high-resolution crop details, they are expensive to curate
and challenging to interpret. Imagine, an AI-enhanced software tool that
predicts soil moisture using visual cues captured by smartphones and
statistical data given by weather forecasts. This work is a first step towards
that goal of developing a multi-modal approach for soil moisture estimation. In
particular, we curate a dataset consisting of real-world images taken from
ground stations and their corresponding weather data. We also propose MIS-ME -
Meteorological & Image based Soil Moisture Estimator, a multi-modal framework
for soil moisture estimation. Our extensive analysis shows that MIS-ME achieves
a MAPE of 10.79%, outperforming traditional unimodal approaches with a
reduction of 2.6% in MAPE for meteorological data and 1.5% in MAPE for image
data, highlighting the effectiveness of tailored multi-modal approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DSAA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning applied to Insurance Portfolio Pursuit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward James Young, Alistair Rogers, Elliott Tong, James Jordon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When faced with a new customer, many factors contribute to an insurance
firm's decision of what offer to make to that customer. In addition to the
expected cost of providing the insurance, the firm must consider the other
offers likely to be made to the customer, and how sensitive the customer is to
differences in price. Moreover, firms often target a specific portfolio of
customers that could depend on, e.g., age, location, and occupation. Given such
a target portfolio, firms may choose to modulate an individual customer's offer
based on whether the firm desires the customer within their portfolio. We term
the problem of modulating offers to achieve a desired target portfolio the
portfolio pursuit problem. Having formulated the portfolio pursuit problem as a
sequential decision making problem, we devise a novel reinforcement learning
algorithm for its solution. We test our method on a complex synthetic market
environment, and demonstrate that it outperforms a baseline method which mimics
current industry approaches to portfolio pursuit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Infrequent Resolving Algorithm for Online Linear Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guokai Li, Zizhuo Wang, Jingwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online linear programming (OLP) has gained significant attention from both
researchers and practitioners due to its extensive applications, such as online
auction, network revenue management and advertising. Existing OLP algorithms
fall into two categories: LP-based algorithms and LP-free algorithms. The
former one typically guarantees better performance, even offering a constant
regret, but requires solving a large number of LPs, which could be
computationally expensive. In contrast, LP-free algorithm only requires
first-order computations but induces a worse performance, lacking a constant
regret bound. In this work, we bridge the gap between these two extremes by
proposing an algorithm that achieves a constant regret while solving LPs only
$O(\log\log T)$ times over the time horizon $T$. Moreover, when we are allowed
to solve LPs only $M$ times, we propose an algorithm that can guarantee an
$O\left(T^{(1/2+\epsilon)^{M-1}}\right)$ regret. Furthermore, when the arrival
probabilities are known at the beginning, our algorithm can guarantee a
constant regret by solving LPs $O(\log\log T)$ times, and an
$O\left(T^{(1/2+\epsilon)^{M}}\right)$ regret by solving LPs only $M$ times.
Numerical experiments are conducted to demonstrate the efficiency of the
proposed algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Trajectory Prediction with Multi-View Data Integration in
  Cooperative Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Rahul Bhadani, Larry Head
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research on trajectory prediction primarily relies on data collected
by onboard sensors of an ego vehicle. With the rapid advancement in connected
technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure
(V2I) communication, valuable information from alternate views becomes
accessible via wireless networks. The integration of information from
alternative views has the potential to overcome the inherent limitations
associated with a single viewpoint, such as occlusions and limited field of
view. In this work, we introduce V2INet, a novel trajectory prediction
framework designed to model multi-view data by extending existing single-view
models. Unlike previous approaches where the multi-view data is manually fused
or formulated as a separate training stage, our model supports end-to-end
training, enhancing both flexibility and performance. Moreover, the predicted
multimodal trajectories are calibrated by a post-hoc conformal prediction
module to get valid and efficient confidence regions. We evaluated the entire
framework using the real-world V2I dataset V2X-Seq. Our results demonstrate
superior performance in terms of Final Displacement Error (FDE) and Miss Rate
(MR) using a single GPU. The code is publicly available at:
\url{https://github.com/xichennn/V2I_trajectory_prediction}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Data Selection for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16827v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16827v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major factor in the recent success of large language models is the use of
enormous and ever-growing text datasets for unsupervised pre-training. However,
naively training a model on all available data may not be optimal (or
feasible), as the quality of available text data can vary. Filtering out data
can also decrease the carbon footprint and financial costs of training models
by reducing the amount of training required. Data selection methods aim to
determine which candidate data points to include in the training dataset and
how to appropriately sample from the selected data points. The promise of
improved data selection methods has caused the volume of research in the area
to rapidly expand. However, because deep learning is mostly driven by empirical
evidence and experimentation on large-scale data is expensive, few
organizations have the resources for extensive data selection research.
Consequently, knowledge of effective data selection practices has become
concentrated within a few organizations, many of which do not openly share
their findings and methodologies. To narrow this gap in knowledge, we present a
comprehensive review of existing literature on data selection methods and
related research areas, providing a taxonomy of existing approaches. By
describing the current landscape of research, this work aims to accelerate
progress in data selection by establishing an entry point for new and
established researchers. Additionally, throughout this review we draw attention
to noticeable holes in the literature and conclude the paper by proposing
promising avenues for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper list available at
  https://github.com/alon-albalak/data-selection-survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doubly Robust Interval Estimation for Optimal Policy Evaluation in
  Online Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15501v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15501v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Shen, Hengrui Cai, Rui Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the performance of an ongoing policy plays a vital role in many
areas such as medicine and economics, to provide crucial instructions on the
early-stop of the online experiment and timely feedback from the environment.
Policy evaluation in online learning thus attracts increasing attention by
inferring the mean outcome of the optimal policy (i.e., the value) in
real-time. Yet, such a problem is particularly challenging due to the dependent
data generated in the online environment, the unknown optimal policy, and the
complex exploration and exploitation trade-off in the adaptive experiment. In
this paper, we aim to overcome these difficulties in policy evaluation for
online learning. We explicitly derive the probability of exploration that
quantifies the probability of exploring non-optimal actions under commonly used
bandit algorithms. We use this probability to conduct valid inference on the
online conditional mean estimator under each action and develop the doubly
robust interval estimation (DREAM) method to infer the value under the
estimated optimal policy in online learning. The proposed value estimator
provides double protection for consistency and is asymptotically normal with a
Wald-type confidence interval provided. Extensive simulation studies and real
data applications are conducted to demonstrate the empirical validity of the
proposed DREAM method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "A Good Bot Always Knows Its Limitations": Assessing Autonomous System
  Decision-making Competencies through Factorized Machine Self-confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brett Israelsen, Nisar R. Ahmed, Matthew Aitken, Eric W. Frew, Dale A. Lawrence, Brian M. Argrow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can intelligent machines assess their competencies in completing tasks?
This question has come into focus for autonomous systems that algorithmically
reason and make decisions under uncertainty. It is argued here that machine
self-confidence - a form of meta-reasoning based on self-assessments of an
agent's knowledge about the state of the world and itself, as well as its
ability to reason about and execute tasks - leads to many eminently computable
and useful competency indicators for such agents. This paper presents a
culmination of work on this concept in the form of a computational framework
called Factorized Machine Self-confidence (FaMSeC), which provides a holistic
engineering-focused description of factors driving an algorithmic
decision-making process, including: outcome assessment, solver quality, model
quality, alignment quality, and past experience. In FaMSeC, self confidence
indicators are derived from hierarchical `problem-solving statistics' embedded
within broad classes of probabilistic decision-making algorithms such as Markov
decision processes. The problem-solving statistics are obtained by evaluating
and grading probabilistic exceedance margins with respect to given competency
standards, which are specified for each of the various decision-making
competency factors by the informee (e.g. a non-expert user or an expert system
designer). This approach allows `algorithmic goodness of fit' evaluations to be
easily incorporated into the design of many kinds of autonomous agents in the
form of human-interpretable competency self-assessment reports. Detailed
descriptions and application examples for a Markov decision process agent show
how two of the FaMSeC factors (outcome assessment and solver quality) can be
computed and reported for a range of possible tasking contexts through novel
use of meta-utility functions, behavior simulations, and surrogate prediction
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>59 pages, 22 figures, draft to be submitted for journal review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Visual Quadrupedal Loco-Manipulation from Demonstrations <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengmao He, Kun Lei, Yanjie Ze, Koushil Sreenath, Zhongyu Li, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadruped robots are progressively being integrated into human environments.
Despite the growing locomotion capabilities of quadrupedal robots, their
interaction with objects in realistic scenes is still limited. While additional
robotic arms on quadrupedal robots enable manipulating objects, they are
sometimes redundant given that a quadruped robot is essentially a mobile unit
equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,
we aim to empower a quadruped robot to execute real-world manipulation tasks
using only its legs. We decompose the loco-manipulation process into a
low-level reinforcement learning (RL)-based controller and a high-level
Behavior Cloning (BC)-based planner. By parameterizing the manipulation
trajectory, we synchronize the efforts of the upper and lower layers, thereby
leveraging the advantages of both RL and BC. Our approach is validated through
simulations and real-world experiments, demonstrating the robot's ability to
perform tasks that demand mobility and high precision, such as lifting a basket
from the ground while moving, closing a dishwasher, pressing a button, and
pushing a door. Project website: https://zhengmaohe.github.io/leg-manip
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IROS 2024. Project website:
  https://zhengmaohe.github.io/leg-manip</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11652v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11652v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny Gupta, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a privacy-preserving approach to train models
on decentralized data. Its potential in healthcare is significant, but
challenges arise due to cross-client variations in medical image data,
exacerbated by limited annotations. This paper introduces Cross-Client
Variations Adaptive Federated Learning (CCVA-FL) to address these issues.
CCVA-FL aims to minimize cross-client variations by transforming images into a
common feature space. It involves expert annotation of a subset of images from
each client, followed by the selection of a client with the least data
complexity as the target. Synthetic medical images are then generated using
Scalable Diffusion Models with Transformers (DiT) based on the target client's
annotated images. These synthetic images, capturing diversity and representing
the original data, are shared with other clients. Each client then translates
its local images into the target image space using image-to-image translation.
The translated images are subsequently used in a federated learning setting to
develop a server model. Our results demonstrate that CCVA-FL outperforms
Vanilla Federated Averaging by effectively addressing data distribution
differences across clients without compromising privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I found critical errors in the manuscript affecting its validity. I
  need to correct these before resubmitting. Major changes to methodology and
  results are underway, significantly altering the content. I will resubmit the
  revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Geo-diversity of Generated Images with Contextualized Vendi
  Score Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reyhane Askari Hemmat, Melissa Hall, Alicia Sun, Candace Ross, Michal Drozdzal, Adriana Romero-Soriano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing popularity of text-to-image generative models, there has
been increasing focus on understanding their risks and biases. Recent work has
found that state-of-the-art models struggle to depict everyday objects with the
true diversity of the real world and have notable gaps between geographic
regions. In this work, we aim to increase the diversity of generated images of
common objects such that per-region variations are representative of the real
world. We introduce an inference time intervention, contextualized Vendi Score
Guidance (c-VSG), that guides the backwards steps of latent diffusion models to
increase the diversity of a sample as compared to a "memory bank" of previously
generated images while constraining the amount of variation within that of an
exemplar set of real-world contextualizing images. We evaluate c-VSG with two
geographically representative datasets and find that it substantially increases
the diversity of generated images, both for the worst performing regions and on
average, while simultaneously maintaining or improving image quality and
consistency. Additionally, qualitative analyses reveal that diversity of
generated images is significantly improved, including along the lines of
reductive region portrayals present in the original model. We hope that this
work is a step towards text-to-image generative models that reflect the true
geographic diversity of the world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Reparameterized Discrete Diffusion Model for Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05737v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05737v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Zheng, Jianbo Yuan, Lei Yu, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies discrete diffusion probabilistic models with applications
to natural language generation. We derive an alternative yet equivalent
formulation of the sampling from discrete diffusion processes and leverage this
insight to develop a family of reparameterized discrete diffusion models. The
derived generic framework is highly flexible, offers a fresh perspective of the
generation process in discrete diffusion models, and features more effective
training and decoding techniques. We conduct extensive experiments to evaluate
the text generation capability of our model, demonstrating significant
improvements over existing diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLM 2024; Code available at
  https://github.com/hkunlp/reparam-discrete-diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A multi-criteria approach for selecting an explanation from the set of
  counterfactuals produced by an ensemble of explainers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ignacy Stępka, Mateusz Lango, Jerzy Stefanowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactuals are widely used to explain ML model predictions by providing
alternative scenarios for obtaining the more desired predictions. They can be
generated by a variety of methods that optimize different, sometimes
conflicting, quality measures and produce quite different solutions. However,
choosing the most appropriate explanation method and one of the generated
counterfactuals is not an easy task. Instead of forcing the user to test many
different explanation methods and analysing conflicting solutions, in this
paper, we propose to use a multi-stage ensemble approach that will select
single counterfactual based on the multiple-criteria analysis. It offers a
compromise solution that scores well on several popular quality measures. This
approach exploits the dominance relation and the ideal point decision aid
method, which selects one counterfactual from the Pareto front. The conducted
experiments demonstrated that the proposed approach generates fully actionable
counterfactuals with attractive compromise values of the considered quality
measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hyperparameter Study for Quantum Kernel Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11891v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11891v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Egginger, Alona Sakhnenko, Jeanette Miriam Lorenz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum kernel methods are a promising method in quantum machine learning
thanks to the guarantees connected to them. Their accessibility for analytic
considerations also opens up the possibility of prescreening datasets based on
their potential for a quantum advantage. To do so, earlier works developed the
geometric difference, which can be understood as a closeness measure between
two kernel-based machine learning approaches, most importantly between a
quantum kernel and a classical kernel. This metric links the quantum and
classical model complexities, and it was developed to bound generalization
error. Therefore, it raises the question of how this metric behaves in an
empirical setting. In this work, we investigate the effects of hyperparameter
choice on the model performance and the generalization gap between classical
and quantum kernels. The importance of hyperparameters is well known also for
classical machine learning. Of special interest are hyperparameters associated
with the quantum Hamiltonian evolution feature map, as well as the number of
qubits to trace out before computing a projected quantum kernel. We conduct a
thorough investigation of the hyperparameters across 11 datasets and we
identify certain aspects that can be exploited. Analyzing the effects of
certain hyperparameter settings on the empirical performance, as measured by
cross validation accuracy, and generalization ability, as measured by geometric
difference described above, brings us one step closer to understanding the
potential of quantum kernel methods on classical datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Expanded implications of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Reinforcement Learning for Traveling Purchaser Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02476v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02476v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofeng Yuan, Rongping Zhu, Wanlu Yang, Shiji Song, Keyou You, Yuli Zhang, C. L. Philip Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traveling purchaser problem (TPP) is an important combinatorial
optimization problem with broad applications. Due to the coupling between
routing and purchasing, existing works on TPPs commonly address route
construction and purchase planning simultaneously, which, however, leads to
exact methods with high computational cost and heuristics with sophisticated
design but limited performance. In sharp contrast, we propose a novel approach
based on deep reinforcement learning (DRL), which addresses route construction
and purchase planning separately, while evaluating and optimizing the solution
from a global perspective. The key components of our approach include a
bipartite graph representation for TPPs to capture the market-product
relations, and a policy network that extracts information from the bipartite
graph and uses it to sequentially construct the route. One significant benefit
of our framework is that we can efficiently construct the route using the
policy network, and once the route is determined, the associated purchasing
plan can be easily derived through linear programming, while, leveraging DRL,
we can train the policy network to optimize the global solution objective.
Furthermore, by introducing a meta-learning strategy, the policy network can be
trained stably on large-sized TPP instances, and generalize well across
instances of varying sizes and distributions, even to much larger instances
that are never seen during training. Experiments on various synthetic TPP
instances and the TPPLIB benchmark demonstrate that our DRL-based approach can
significantly outperform well-established TPP heuristics, reducing the
optimality gap by 40%-90%, and also showing an advantage in runtime, especially
on large-sized instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed moving average field guided learning for spatio-temporal data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00736v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00736v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imma Valentina Curato, Orkun Furat, Lorenzo Proietti, Bennet Stroeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Influenced mixed moving average fields are a versatile modeling class for
spatio-temporal data. However, their predictive distribution is not generally
known. Under this modeling assumption, we define a novel spatio-temporal
embedding and a theory-guided machine learning approach that employs a
generalized Bayesian algorithm to make ensemble forecasts. We use Lipschitz
predictors and determine fixed-time and any-time PAC Bayesian bounds in the
batch learning setting. Performing causal forecast is a highlight of our
methodology as its potential application to data with spatial and temporal
short and long-range dependence. We then test the performance of our learning
methodology by using linear predictors and data sets simulated from a
spatio-temporal Ornstein-Uhlenbeck process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model
  Training Data Through Knowledge Graph Comparison 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devam Mondal, Carlo Lipizzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In light of recent legal allegations brought by publishers, newspapers, and
other creators of copyrighted corpora against large language model developers
who use their copyrighted materials for training or fine-tuning purposes, we
propose a novel system, a variant of a plagiarism detection system, that
assesses whether a knowledge source has been used in the training or
fine-tuning of a large language model. Unlike current methods, we utilize an
approach that uses Resource Description Framework (RDF) triples to create
knowledge graphs from both a source document and an LLM continuation of that
document. These graphs are then analyzed with respect to content using cosine
similarity and with respect to structure using a normalized version of graph
edit distance that shows the degree of isomorphism. Unlike traditional
plagiarism systems that focus on content matching and keyword identification
between a source and a target corpus, our approach enables a broader and more
accurate evaluation of similarity between a source document and LLM
continuation by focusing on relationships between ideas and their organization
with regards to others. Additionally, our approach does not require access to
LLM metrics like perplexity that may be unavailable in closed large language
model "black-box" systems, as well as the training corpus. We thus assess
whether an LLM has "plagiarized" a corpus in its continuation through
similarity measures. A prototype of our system will be found on a hyperlinked
GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bond Graphs for multi-physics informed Neural Networks for multi-variate
  time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis-Raja Brachet, Pierre-Yves Richard, Céline Hudelot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the trend of hybrid Artificial Intelligence techniques, Physical-Informed
Machine Learning has seen a growing interest. It operates mainly by imposing
data, learning, or architecture bias with simulation data, Partial Differential
Equations, or equivariance and invariance properties. While it has shown great
success on tasks involving one physical domain, such as fluid dynamics,
existing methods are not adapted to tasks with complex multi-physical and
multi-domain phenomena. In addition, it is mainly formulated as an end-to-end
learning scheme. To address these challenges, we propose to leverage Bond
Graphs, a multi-physics modeling approach, together with Message Passing Graph
Neural Networks. We propose a Neural Bond graph Encoder (NBgE) producing
multi-physics-informed representations that can be fed into any task-specific
model. It provides a unified way to integrate both data and architecture biases
in deep learning. Our experiments on two challenging multi-domain physical
systems - a Direct Current Motor and the Respiratory System - demonstrate the
effectiveness of our approach on a multivariate time-series forecasting task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CP-<span class="highlight-title">Prompt</span>: Composition-Based Cross-modal <span class="highlight-title">Prompt</span>ing for
  Domain-Incremental Continual Learning <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Feng, Zhen Tian, Yifan Zhu, Zongfu Han, Haoran Luo, Guangwei Zhang, Meina Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key challenge of cross-modal domain-incremental learning (DIL) is to
enable the learning model to continuously learn from novel data with different
feature distributions under the same task without forgetting old ones. However,
existing top-performing methods still cause high forgetting rates, by lacking
intra-domain knowledge extraction and inter-domain common prompting strategy.
In this paper, we propose a simple yet effective framework, CP-Prompt, by
training limited parameters to instruct a pre-trained model to learn new
domains and avoid forgetting existing feature distributions. CP-Prompt captures
intra-domain knowledge by compositionally inserting personalized prompts on
multi-head self-attention layers and then learns the inter-domain knowledge
with a common prompting strategy. CP-Prompt shows superiority compared with
state-of-the-art baselines among three widely evaluated DIL tasks. The source
code is available at https://github.com/dannis97500/CP_Prompt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Routoo: Learning to Route to Large Language Models Effectively 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Mohammadshahi, Arshad Rafiq Shaikh, Majid Yazdani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing foundational large language models (LLMs) is becoming increasingly
costly and inefficient. Also, closed-source and larger open-source models
generally offer better response quality but come with higher inference costs
than smaller models. In this paper, we introduce Routoo, an architecture
designed to optimize the selection of LLMs for specific prompts based on
performance, cost, and efficiency. Routoo consists of two key components: a
performance predictor and a cost-aware decoding. The performance predictor is a
lightweight LLM that estimates the performance of various underlying LLMs
without needing to execute and evaluate them. The cost-aware decoding then
selects the most suitable model based on these predictions and other
constraints like cost and latency. We evaluated Routoo using the MMLU benchmark
across 57 domains employing open-source models. Our results show that Routoo
matches the performance of the Mixtral 8x7b model while reducing inference
costs by one-third. Additionally, by allowing increased costs, Routoo surpasses
Mixtral's accuracy by over 5% at equivalent costs, achieving an accuracy of
75.9%. When integrating GPT4 into our model pool, Routoo nearly matches GPT4's
performance at half the cost and exceeds it with a 25% cost reduction. These
outcomes highlight Routoo's potential to create new SOTA in a cost-effective
manner by leveraging the collective knowledge of multiple LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Waste Your Time: Early Stopping Cross-Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Bergman, Lennart Purucker, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art automated machine learning systems for tabular data often
employ cross-validation; ensuring that measured performances generalize to
unseen data, or that subsequent ensembling does not overfit. However, using
k-fold cross-validation instead of holdout validation drastically increases the
computational cost of validating a single configuration. While ensuring better
generalization and, by extension, better performance, the additional cost is
often prohibitive for effective model selection within a time budget. We aim to
make model selection with cross-validation more effective. Therefore, we study
early stopping the process of cross-validation during model selection. We
investigate the impact of early stopping on random search for two algorithms,
MLP and random forest, across 36 classification datasets. We further analyze
the impact of the number of folds by considering 3-, 5-, and 10-folds. In
addition, we investigate the impact of early stopping with Bayesian
optimization instead of random search and also repeated cross-validation. Our
exploratory study shows that even a simple-to-understand and easy-to-implement
method consistently allows model selection to converge faster; in ~94% of all
datasets, on average by ~214%. Moreover, stopping cross-validation enables
model selection to explore the search space more exhaustively by considering
+167% configurations on average within one hour, while also obtaining better
overall performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Third International Conference on Automated Machine
  Learning (AutoML 2024); for code, see
  https://github.com/automl/DontWasteYourTime-early-stopping</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recent Advances in Generative AI and Large Language Models: Current
  Status, Challenges, and Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14962v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14962v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Desta Haileselassie Hagos, Rick Battle, Danda B. Rawat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Generative Artificial Intelligence (AI) and Large Language
Models (LLMs) has marked a new era of Natural Language Processing (NLP),
introducing unprecedented capabilities that are revolutionizing various
domains. This paper explores the current state of these cutting-edge
technologies, demonstrating their remarkable advancements and wide-ranging
applications. Our paper contributes to providing a holistic perspective on the
technical foundations, practical applications, and emerging challenges within
the evolving landscape of Generative AI and LLMs. We believe that understanding
the generative capabilities of AI systems and the specific context of LLMs is
crucial for researchers, practitioners, and policymakers to collaboratively
shape the responsible and ethical integration of these technologies into
various domains. Furthermore, we identify and address main research gaps,
providing valuable insights to guide future research endeavors within the AI
research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version is accepted for publication in the journal of IEEE
  Transactions on Artificial Intelligence (TAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive
  Learning for Cross-Subject EEG-based Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weishan Ye, Zhiguo Zhang, Fei Teng, Min Zhang, Jianhong Wang, Dong Ni, Fali Li, Peng Xu, Zhen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) is an objective tool for emotion recognition
with promising applications. However, the scarcity of labeled data remains a
major challenge in this field, limiting the widespread use of EEG-based emotion
recognition. In this paper, a semi-supervised Dual-stream Self-Attentive
Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed
to tackle the challenge of limited labeled data in cross-subject EEG-based
emotion recognition. The DS-AGC framework includes two parallel streams for
extracting non-structural and structural EEG features. The non-structural
stream incorporates a semi-supervised multi-domain adaptation method to
alleviate distribution discrepancy among labeled source domain, unlabeled
source domain, and unknown target domain. The structural stream develops a
graph contrastive learning method to extract effective graph-based feature
representation from multiple EEG channels in a semi-supervised manner. Further,
a self-attentive fusion module is developed for feature fusion, sample
selection, and emotion recognition, which highlights EEG features more relevant
to emotions and data samples in the labeled source domain that are closer to
the target domain. Extensive experiments conducted on two benchmark databases
(SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-out
cross-validation evaluation scheme show that the proposed model outperforms
existing methods under different incomplete label conditions (with an average
improvement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating its
effectiveness in addressing the label scarcity problem in cross-subject
EEG-based emotion recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2304.06496</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DragD3D: Realistic Mesh Editing with Rigidity Control Driven by 2D
  Diffusion Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao Xie, Eugene Belilovsky, Sudhir Mudur, Tiberiu Popa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct mesh editing and deformation are key components in the geometric
modeling and animation pipeline. Mesh editing methods are typically framed as
optimization problems combining user-specified vertex constraints with a
regularizer that determines the position of the rest of the vertices. The
choice of the regularizer is key to the realism and authenticity of the final
result. Physics and geometry-based regularizers are not aware of the global
context and semantics of the object, and the more recent deep learning priors
are limited to a specific class of 3D object deformations. Our main
contribution is a vertex-based mesh editing method called DragD3D based on (1)
a novel optimization formulation that decouples the rotation and stretch
components of the deformation and combines a 3D geometric regularizer with (2)
the recently introduced DDS loss which scores the faithfulness of the rendered
2D image to one from a diffusion model. Thus, our deformation method achieves
globally realistic shape deformation which is not restricted to any class of
objects. Our new formulation optimizes directly the transformation of the
neural Jacobian field explicitly separating the rotational and stretching
components. The objective function of the optimization combines the approximate
gradients of DDS and the gradients from the geometric loss to satisfy the
vertex constraints. Additional user control over desired global shape
deformation is made possible by allowing explicit per-triangle deformation
control as well as explicit separation of rotational and stretching components
of the deformation. We show that our deformations can be controlled to yield
realistic shape deformations that are aware of the global context of the
objects, and provide better results than just using geometric regularizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, project page:
  https://tianhaoxie.github.io/project/DragD3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Library of Variational LSE Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nico Meyer, Martin Röhn, Jakob Murauer, Axel Plinge, Christopher Mutschler, Daniel D. Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear systems of equations can be found in various mathematical domains, as
well as in the field of machine learning. By employing noisy intermediate-scale
quantum devices, variational solvers promise to accelerate finding solutions
for large systems. Although there is a wealth of theoretical research on these
algorithms, only fragmentary implementations exist. To fill this gap, we have
developed the variational-lse-solver framework, which realizes existing
approaches in literature, and introduces several enhancements. The
user-friendly interface is designed for researchers that work at the
abstraction level of identifying and developing end-to-end applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 2nd International Workshop on Quantum Machine
  Learning: From Research to Practice (QML@QCE 2024), Montr\'eal, Qu\'ebec,
  Canada. 4 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data
  and eXpert model predictions <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hareem Nisar, Syed Muhammad Anwar, Zhifan Jiang, Abhijeet Parida, Ramon Sanchez-Jacob, Vishwesh Nath, Holger R. Roth, Marius George Linguraru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision language models (VLMs) have progressed incredibly from research
to applicability for general-purpose use cases. LLaVA-Med, a pioneering large
language and vision assistant for biomedicine, can perform multi-modal
biomedical image and data analysis to provide a natural language interface for
radiologists. While it is highly generalizable and works with multi-modal data,
it is currently limited by well-known challenges that exist in the large
language model space. Hallucinations and imprecision in responses can lead to
misdiagnosis which currently hinder the clinical adaptability of VLMs. To
create precise, user-friendly models in healthcare, we propose D-Rax -- a
domain-specific, conversational, radiologic assistance tool that can be used to
gain insights about a particular radiologic image. In this study, we enhance
the conversational analysis of chest X-ray (CXR) images to support radiological
reporting, offering comprehensive insights from medical imaging and aiding in
the formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the
LLaVA-Med architecture on our curated enhanced instruction-following data,
comprising of images, instructions, as well as disease diagnosis and
demographic predictions derived from MIMIC-CXR imaging data, CXR-related visual
question answer (VQA) pairs, and predictive outcomes from multiple expert AI
models. We observe statistically significant improvement in responses when
evaluated for both open and close-ended conversations. Leveraging the power of
state-of-the-art diagnostic models combined with VLMs, D-Rax empowers
clinicians to interact with medical images using natural language, which could
potentially streamline their decision-making process, enhance diagnostic
accuracy, and conserve their time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to the MICCAI 2024 Second International Workshop on
  Foundation Models for General Medical AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-grained Attention in Hierarchical <span class="highlight-title">Transformer</span>s for Tabular
  Time-series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Azorin, Zied Ben Houidi, Massimo Gallo, Alessandro Finamore, Pietro Michiardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data is ubiquitous in many real-life systems. In particular,
time-dependent tabular data, where rows are chronologically related, is
typically used for recording historical events, e.g., financial transactions,
healthcare records, or stock history. Recently, hierarchical variants of the
attention mechanism of transformer architectures have been used to model
tabular time-series data. At first, rows (or columns) are encoded separately by
computing attention between their fields. Subsequently, encoded rows (or
columns) are attended to one another to model the entire tabular time-series.
While efficient, this approach constrains the attention granularity and limits
its ability to learn patterns at the field-level across separate rows, or
columns. We take a first step to address this gap by proposing Fieldy, a
fine-grained hierarchical model that contextualizes fields at both the row and
column levels. We compare our proposal against state of the art models on
regression and classification tasks using public tabular time-series datasets.
Our results show that combining row-wise and column-wise attention improves
performance without increasing model size. Code and data are available at
https://github.com/raphaaal/fieldy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages; Camera Ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Insights from the Design Space Exploration of Flow-Guided Nanoscale
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Lemic, Gerard Calvo Bartra, Arnau Brosa López, Jorge Torres Gómez, Jakob Struye, Falko Dressler, Sergi Abadal, Xavier Costa Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanodevices with Terahertz (THz)-based wireless communication capabilities
are providing a primer for flow-guided localization within the human
bloodstreams. Such localization is allowing for assigning the locations of
sensed events with the events themselves, providing benefits along the lines of
early and precise diagnostics, and reduced costs and invasiveness. Flow-guided
localization is still in a rudimentary phase, with only a handful of works
targeting the problem. Nonetheless, the performance assessments of the proposed
solutions are already carried out in a non-standardized way, usually along a
single performance metric, and ignoring various aspects that are relevant at
such a scale (e.g., nanodevices' limited energy) and for such a challenging
environment (e.g., extreme attenuation of in-body THz propagation). As such,
these assessments feature low levels of realism and cannot be compared in an
objective way. Toward addressing this issue, we account for the environmental
and scale-related peculiarities of the scenario and assess the performance of
two state-of-the-art flow-guided localization approaches along a set of
heterogeneous performance metrics such as the accuracy and reliability of
localization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 tables, 14 references, accepted at ACM
  NanoCom'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSMA: Multi-agent Trajectory Prediction in Connected and Autonomous
  Vehicle Environment with Multi-source Data Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Rahul Bhadani, Zhanbo Sun, Larry Head
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prediction of surrounding vehicle trajectories is crucial for
collision-free path planning. In this study, we focus on a scenario where a
connected and autonomous vehicle (CAV) serves as the central agent, utilizing
both sensors and communication technologies to perceive its surrounding
traffics consisting of autonomous vehicles (AVs), connected vehicles (CVs), and
human-driven vehicles (HDVs). Our trajectory prediction task is aimed at all
the detected surrounding vehicles. To effectively integrate the multi-source
data from both sensor and communication technologies, we propose a deep
learning framework called MSMA utilizing a cross-attention module for
multi-source data fusion. Vector map data is utilized to provide contextual
information. The trajectory dataset is collected in CARLA simulator with
synthesized data errors introduced. Numerical experiments demonstrate that in a
mixed traffic flow scenario, the integration of data from different sources
enhances our understanding of the environment. This notably improves trajectory
prediction accuracy, particularly in situations with a high CV market
penetration rate. The code is available at: https://github.com/xichennn/MSMA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time gravitational-wave inference for binary neutron stars using
  machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Dax, Stephen R. Green, Jonathan Gair, Nihar Gupte, Michael Pürrer, Vivien Raymond, Jonas Wildberger, Jakob H. Macke, Alessandra Buonanno, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mergers of binary neutron stars (BNSs) emit signals in both the
gravitational-wave (GW) and electromagnetic (EM) spectra. Famously, the 2017
multi-messenger observation of GW170817 led to scientific discoveries across
cosmology, nuclear physics, and gravity. Central to these results were the sky
localization and distance obtained from GW data, which, in the case of
GW170817, helped to identify the associated EM transient, AT 2017gfo, 11 hours
after the GW signal. Fast analysis of GW data is critical for directing
time-sensitive EM observations; however, due to challenges arising from the
length and complexity of signals, it is often necessary to make approximations
that sacrifice accuracy. Here, we present a machine learning framework that
performs complete BNS inference in just one second without making any such
approximations. Our approach enhances multi-messenger observations by providing
(i) accurate localization even before the merger; (ii) improved localization
precision by $\sim30\%$ compared to approximate low-latency methods; and (iii)
detailed information on luminosity distance, inclination, and masses, which can
be used to prioritize expensive telescope time. Additionally, the flexibility
and reduced cost of our method open new opportunities for equation-of-state
studies. Finally, we demonstrate that our method scales to extremely long
signals, up to an hour in length, thus serving as a blueprint for data analysis
for next-generation ground- and space-based detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8+8 pages, 3+7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G
  Subnetworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Abode, Ramoni Adeogun, Lou Salaün, Renato Abreu, Thomas Jacobsen, Gilberto Berardinelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an unsupervised approach for frequency sub-band
allocation in wireless networks using graph-based learning. We consider a dense
deployment of subnetworks in the factory environment with a limited number of
sub-bands which must be optimally allocated to coordinate inter-subnetwork
interference. We model the subnetwork deployment as a conflict graph and
propose an unsupervised learning approach inspired by the graph colouring
heuristic and the Potts model to optimize the sub-band allocation using graph
neural networks. The numerical evaluation shows that the proposed method
achieves close performance to the centralized greedy colouring sub-band
allocation heuristic with lower computational time complexity. In addition, it
incurs reduced signalling overhead compared to iterative optimization
heuristics that require all the mutual interfering channel information. We
further demonstrate that the method is robust to different network settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in VTC Fall 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Sentence Embeddings with Automatic Generation of Training Data
  Using Few-shot Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soma Sato, Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoder-based large language models (LLMs) have shown high performance on
many tasks in natural language processing. This is also true for sentence
embedding learning, where a decoder-based model, PromptEOL, has achieved the
best performance on semantic textual similarity (STS) tasks. However, PromptEOL
requires a manually annotated natural language inference (NLI) dataset for
fine-tuning. We aim to improve sentence embeddings without using large manually
annotated datasets by automatically generating an NLI dataset with an LLM and
using it for fine-tuning of PromptEOL. To achieve this, we explore methods of
data generation suitable for sentence embedding learning in this study.
Specifically, we will focus on automatic dataset generation through few-shot
learning and explore the appropriate methods to leverage few-shot examples.
Experimental results on the STS tasks demonstrate that our approach outperforms
existing models in settings without large manually annotated datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Operating Modes of In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqian Lin, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,
acquiring a new skill from in-context samples, and task retrieval, i.e.,
locating and activating a relevant pretrained skill. Recent theoretical work
investigates various mathematical models to analyze ICL, but existing models
explain only one operating mode at a time. We introduce a probabilistic model,
with which one can explain the dual operating modes of ICL simultaneously.
Focusing on in-context learning of linear functions, we extend existing models
for pretraining data by introducing multiple task groups and task-dependent
input distributions. We then analyze the behavior of the optimally pretrained
model under the squared loss, i.e., the MMSE estimator of the label given
in-context examples. Regarding pretraining task distribution as prior and
in-context examples as the observation, we derive the closed-form expression of
the task posterior distribution. With the closed-form expression, we obtain a
quantitative understanding of the two operating modes of ICL. Furthermore, we
shed light on an unexplained phenomenon observed in practice: under certain
settings, the ICL risk initially increases and then decreases with more
in-context examples. Our model offers a plausible explanation for this "early
ascent" phenomenon: a limited number of in-context samples may lead to the
retrieval of an incorrect skill, thereby increasing the risk, which will
eventually diminish as task learning takes effect with more in-context samples.
We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,
where in-context examples are assigned random labels. Lastly, we validate our
findings and predictions via experiments involving Transformers and large
language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Tree Search Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dixant Mittal, Wee Sun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In decision-making problems with limited training data, policy functions
approximated using deep neural networks often exhibit suboptimal performance.
An alternative approach involves learning a world model from the limited data
and determining actions through online search. However, the performance is
adversely affected by compounding errors arising from inaccuracies in the
learned world model. While methods like TreeQN have attempted to address these
inaccuracies by incorporating algorithmic inductive biases into the neural
network architectures, the biases they introduce are often weak and
insufficient for complex decision-making tasks. In this work, we introduce
Differentiable Tree Search Network (D-TSN), a novel neural network architecture
that significantly strengthens the inductive bias by embedding the algorithmic
structure of a best-first online search algorithm. D-TSN employs a learned
world model to conduct a fully differentiable online search. The world model is
jointly optimized with the search algorithm, enabling the learning of a robust
world model and mitigating the effect of prediction inaccuracies. Further, we
note that a naive incorporation of best-first search could lead to a
discontinuous loss function in the parameter space. We address this issue by
adopting a stochastic tree expansion policy, formulating search tree expansion
as another decision-making task, and introducing an effective variance
reduction technique for the gradient computation. We evaluate D-TSN in an
offline-RL setting with a limited training data scenario on Procgen games and
grid navigation task, and demonstrate that D-TSN outperforms popular model-free
and model-based baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Perturbed States for Transformed Input-robust Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tung M. Luu, Haeyong Kang, Tri Ton, Thanh Nguyen, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) agents demonstrating proficiency in a training
environment exhibit vulnerability to adversarial perturbations in input
observations during deployment. This underscores the importance of building a
robust agent before its real-world deployment. To alleviate the challenging
point, prior works focus on developing robust training-based procedures,
encompassing efforts to fortify the deep neural network component's robustness
or subject the agent to adversarial training against potent attacks. In this
work, we propose a novel method referred to as Transformed Input-robust RL
(TIRL), which explores another avenue to mitigate the impact of adversaries by
employing input transformation-based defenses. Specifically, we introduce two
principles for applying transformation-based defenses in learning robust RL
agents: (1) autoencoder-styled denoising to reconstruct the original state and
(2) bounded transformations (bit-depth reduction and vector quantization (VQ))
to achieve close transformed inputs. The transformations are applied to the
state before feeding it into the policy network. Extensive experiments on
multiple MuJoCo environments demonstrate that input transformation-based
defenses, i.e., VQ, defend against several adversaries in the state
observations. The official code is available at
https://github.com/tunglm2203/tirl
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages (Code: https://github.com/tunglm2203/tirl)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Adaptive Gradient Regularization Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16944v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16944v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huixiu Jiang, Ling Yang, Yu Bao, Rutong Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizer plays an important role in neural network training with high
efficiency and performance. Weight update based on its gradient is the central
part of the optimizer. It has been shown that normalization and standardization
operation on weight and gradient can accelerate the training process and
improve performance such as Weight Standardization (WS), weight normalization
(WN) and gradient normalization (GN); there is also gradient centralization
(GC). In this work, we introduce a new optimization technique based on the
gradient magnitude in a gradient vector named adaptive gradient regularization
(AGR), which normalizes the gradient vector in all dimensions as a coefficient
vector and subtracts the product of the gradient and its coefficient vector by
the vanilla gradient. It can be viewed as an adaptive gradient clipping method.
We show that the AGR can improve the loss function Lipschitzness with a more
stable training process and better generalization performance. AGR is very
simple to be embedded into vanilla optimizers such as Adan and AdamW with only
three lines of code. Our experiments are conducted in image generation, image
classification and language representation, which shows that our AGR improves
the training result.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weighed l1 on the simplex: Compressive sensing meets locality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.13894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.13894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abiy Tasissa, Pranay Tankala, Demba Ba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse manifold learning algorithms combine techniques in manifold learning
and sparse optimization to learn features that could be utilized for downstream
tasks. The standard setting of compressive sensing can not be immediately
applied to this setup. Due to the intrinsic geometric structure of data,
dictionary atoms might be redundant and do not satisfy the restricted isometry
property or coherence condition. In addition, manifold learning emphasizes
learning local geometry which is not reflected in a standard $\ell_1$
minimization problem. We propose weighted $\ell_0$ and weighted $\ell_1$
metrics that encourage representation via neighborhood atoms suited for
dictionary based manifold learning. Assuming that the data is generated from
Delaunay triangulation, we show the equivalence of weighted $\ell_0$ and
weighted $\ell_1$. We discuss an optimization program that learns the
dictionaries and sparse coefficients and demonstrate the utility of our
regularization on synthetic and real datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures. The proof of theorem 1 in v1 does not hold true
  in general without additional assumptions. This version fixes this problem.
  For more details, we refer the interested reader to arXiv:2012.02134 which is
  the journal version of the workshop paper v1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Millimeter Beamforming via <span class="highlight-title">Self-Supervised</span> Hybrid Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12653v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12653v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenghao Zhu, Bohao Wang, Zhaohui Yang, Chongwen Huang, Zhaoyang Zhang, George C. Alexandropoulos, Chau Yuen, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beamforming with large-scale antenna arrays has been widely used in recent
years, which is acknowledged as an important part in 5G and incoming 6G. Thus,
various techniques are leveraged to improve its performance, e.g., deep
learning, advanced optimization algorithms, etc. Although its performance in
many previous research scenarios with deep learning is quite attractive,
usually it drops rapidly when the environment or dataset is changed. Therefore,
designing effective beamforming network with strong robustness is an open issue
for the intelligent wireless communications. In this paper, we propose a robust
beamforming self-supervised network, and verify it in two kinds of different
datasets with various scenarios. Simulation results show that the proposed
self-supervised network with hybrid learning performs well in both classic
DeepMIMO and new WAIR-D dataset with the strong robustness under the various
environments. Also, we present the principle to explain the rationality of this
kind of hybrid learning, which is instructive to apply with more kinds of
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Model-Free LQR Control over Rate-Limited Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritra Mitra, Lintao Ye, Vijay Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the success of model-free methods for control design in many problem
settings, it is natural to ask how things will change if realistic
communication channels are utilized for the transmission of gradients or
policies. While the resulting problem has analogies with the formulations
studied under the rubric of networked control systems, the rich literature in
that area has typically assumed that the model of the system is known. As a
step towards bridging the fields of model-free control design and networked
control systems, we ask: \textit{Is it possible to solve basic control problems
- such as the linear quadratic regulator (LQR) problem - in a model-free manner
over a rate-limited channel?} Toward answering this question, we study a
setting where a worker agent transmits quantized policy gradients (of the LQR
cost) to a server over a noiseless channel with a finite bit-rate. We propose a
new algorithm titled Adaptively Quantized Gradient Descent (\texttt{AQGD}), and
prove that above a certain finite threshold bit-rate, \texttt{AQGD} guarantees
exponentially fast convergence to the globally optimal policy, with \textit{no
deterioration of the exponent relative to the unquantized setting}. More
generally, our approach reveals the benefits of adaptive quantization in
preserving fast linear convergence rates, and, as such, may be of independent
interest to the literature on compressed optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for an Oral Presentation at the 6th Annual Learning for
  Dynamics & Control Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on LoRA of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation~(LoRA), which updates the dense neural network layers
with pluggable low-rank matrices, is one of the best performed parameter
efficient fine-tuning paradigms. Furthermore, it has significant advantages in
cross-task generalization and privacy-preserving. Hence, LoRA has gained much
attention recently, and the number of related literature demonstrates
exponential growth. It is necessary to conduct a comprehensive overview of the
current progress on LoRA. This survey categorizes and reviews the progress from
the perspectives of (1) downstream adaptation improving variants that improve
LoRA's performance on downstream tasks; (2) cross-task generalization methods
that mix multiple LoRA plugins to achieve cross-task generalization; (3)
efficiency-improving methods that boost the computation-efficiency of LoRA; (4)
data privacy-preserving methods that use LoRA in federated learning; (5)
application. Besides, this survey also discusses the future directions in this
field. At last, we provide a Github page
(https://github.com/ZJU-LLMs/Awesome-LoRAs.git) for readers to check the
updates and initiate discussions on this survey paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SemiSFL: Split Federated Learning on Unlabeled and Non-IID Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15870v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15870v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xu, Yunming Liao, Hongli Xu, Zhipeng Sun, Liusheng Huang, Chunming Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged to allow multiple clients to
collaboratively train machine learning models on their private data at the
network edge. However, training and deploying large-scale models on
resource-constrained devices is challenging. Fortunately, Split Federated
Learning (SFL) offers a feasible solution by alleviating the computation and/or
communication burden on clients. However, existing SFL works often assume
sufficient labeled data on clients, which is usually impractical. Besides, data
non-IIDness poses another challenge to ensure efficient model training. To our
best knowledge, the above two issues have not been simultaneously addressed in
SFL. Herein, we propose a novel Semi-supervised SFL system, termed SemiSFL,
which incorporates clustering regularization to perform SFL with unlabeled and
non-IID client data. Moreover, our theoretical and experimental investigations
into model convergence reveal that the inconsistent training processes on
labeled and unlabeled data have an influence on the effectiveness of clustering
regularization. To mitigate the training inconsistency, we develop an algorithm
for dynamically adjusting the global updating frequency, so as to improve
training performance. Extensive experiments on benchmark models and datasets
show that our system provides a 3.8x speed-up in training time, reduces the
communication cost by about 70.3% while reaching the target accuracy, and
achieves up to 5.8% improvement in accuracy under non-IID scenarios compared to
the state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rule-Based Error Detection and Correction to Operationalize Movement
  Trajectory Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14250v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14250v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Xi, Kevin Scaria, Divyagna Bavikadi, Paulo Shakarian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification of movement trajectories has many applications in
transportation and is a key component for large-scale movement trajectory
generation and anomaly detection which has key safety applications in the
aftermath of a disaster or other external shock. However, the current
state-of-the-art (SOTA) are based on supervised deep learning - which leads to
challenges when the distribution of trajectories changes due to such a shock.
We provide a neuro-symbolic rule-based framework to conduct error correction
and detection of these models to integrate into our movement trajectory
platform. We provide a suite of experiments on several recent SOTA models where
we show highly accurate error detection, the ability to improve accuracy with a
changing test distribution, and accuracy improvement for the base use case in
addition to a suite of theoretical properties that informed algorithm
development. Specifically, we show an F1 scores for predicting errors of up to
0.984, significant performance increase for out-of distribution accuracy (8.51%
improvement over SOTA for zero-shot accuracy), and accuracy improvement over
the SOTA model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Guidance Network for Missing-Modality Inference in Content
  Moderation <span class="chip">ICME 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuokai Zhao, Harish Palani, Tianyi Liu, Lena Evans, Ruth Toner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal deep learning, especially vision-language models, have gained
significant traction in recent years, greatly improving performance on many
downstream tasks, including content moderation and violence detection. However,
standard multimodal approaches often assume consistent modalities between
training and inference, limiting applications in many real-world use cases, as
some modalities may not be available during inference. While existing research
mitigates this problem through reconstructing the missing modalities, they
unavoidably increase unnecessary computational cost, which could be just as
critical, especially for large, deployed infrastructures in industry. To this
end, we propose a novel guidance network that promotes knowledge sharing during
training, taking advantage of the multimodal representations to train better
single-modality models to be used for inference. Real-world experiments in
violence detection shows that our proposed framework trains single-modality
models that significantly outperform traditionally trained counterparts, while
avoiding increases in computational cost for inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICME 2024 Camera Ready. Code is available at
  https://github.com/zhuokaizhao/multimodal-guidance-network</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-State TD Target for Model-Free Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16522v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16522v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuhao Wang, Zhiyong Chen, Lepeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal difference (TD) learning is a fundamental technique in reinforcement
learning that updates value estimates for states or state-action pairs using a
TD target. This target represents an improved estimate of the true value by
incorporating both immediate rewards and the estimated value of subsequent
states. Traditionally, TD learning relies on the value of a single subsequent
state. We propose an enhanced multi-state TD (MSTD) target that utilizes the
estimated values of multiple subsequent states. Building on this new MSTD
concept, we develop complete actor-critic algorithms that include management of
replay buffers in two modes, and integrate with deep deterministic policy
optimization (DDPG) and soft actor-critic (SAC). Experimental results
demonstrate that algorithms employing the MSTD target significantly improve
learning performance compared to traditional methods.The code is provided on
GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-08-09T02:09:25.752094719Z">
            2024-08-09 02:09:25 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
