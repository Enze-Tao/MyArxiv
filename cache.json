{"2024-08-02T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.00374v2","updated":"2024-08-02T13:00:46Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19394v3","updated":"2024-08-02T10:05:03Z","published":"2024-07-28T04:23:40Z","title":"Depth-Wise Convolutions in Vision Transformers for Efficient Training on\n  Small Datasets","summary":"  The Vision Transformer (ViT) leverages the Transformer's encoder to capture\nglobal information by dividing images into patches and achieves superior\nperformance across various computer vision tasks. However, the self-attention\nmechanism of ViT captures the global context from the outset, overlooking the\ninherent relationships between neighboring pixels in images or videos.\nTransformers mainly focus on global information while ignoring the fine-grained\nlocal details. Consequently, ViT lacks inductive bias during image or video\ndataset training. In contrast, convolutional neural networks (CNNs), with their\nreliance on local filters, possess an inherent inductive bias, making them more\nefficient and quicker to converge than ViT with less data. In this paper, we\npresent a lightweight Depth-Wise Convolution module as a shortcut in ViT\nmodels, bypassing entire Transformer blocks to ensure the models capture both\nlocal and global information with minimal overhead. Additionally, we introduce\ntwo architecture variants, allowing the Depth-Wise Convolution modules to be\napplied to multiple Transformer blocks for parameter savings, and incorporating\nindependent parallel Depth-Wise Convolution modules with different kernels to\nenhance the acquisition of local information. The proposed approach\nsignificantly boosts the performance of ViT models on image classification,\nobject detection and instance segmentation by a large margin, especially on\nsmall datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet\nfor image classification, and COCO for object detection and instance\nsegmentation. The source code can be accessed at\nhttps://github.com/ZTX-100/Efficient_ViT_with_DW.\n","authors":["Tianxiao Zhang","Wenju Xu","Bo Luo","Guanghui Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19394v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09786v5","updated":"2024-08-02T01:22:46Z","published":"2024-01-18T08:10:34Z","title":"Adaptive Self-training Framework for Fine-grained Scene Graph Generation","summary":"  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n","authors":["Kibum Kim","Kanghoon Yoon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2401.09786v5.pdf","comment":"9 pages; ICLR 2024"},{"id":"http://arxiv.org/abs/2408.01417v1","updated":"2024-08-02T17:51:57Z","published":"2024-08-02T17:51:57Z","title":"Talk Less, Interact Better: Evaluating In-context Conversational\n  Adaptation in Multimodal LLMs","summary":"  Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.\n","authors":["Yilun Hua","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.01417v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.01384v1","updated":"2024-08-02T16:41:34Z","published":"2024-08-02T16:41:34Z","title":"NOLO: Navigate Only Look Once","summary":"  The in-context learning ability of Transformer models has brought new\npossibilities to visual navigation. In this paper, we focus on the video\nnavigation setting, where an in-context navigation policy needs to be learned\npurely from videos in an offline manner, without access to the actual\nenvironment. For this setting, we propose Navigate Only Look Once (NOLO), a\nmethod for learning a navigation policy that possesses the in-context ability\nand adapts to new scenes by taking corresponding context videos as input\nwithout finetuning or re-training. To enable learning from videos, we first\npropose a pseudo action labeling procedure using optical flow to recover the\naction label from egocentric videos. Then, offline reinforcement learning is\napplied to learn the navigation policy. Through extensive experiments on\ndifferent scenes, we show that our algorithm outperforms baselines by a large\nmargin, which demonstrates the in-context learning ability of the learned\npolicy.\n","authors":["Bohan Zhou","Jiangxing Wang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11652v3","updated":"2024-08-02T16:30:55Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v3.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.01372v1","updated":"2024-08-02T16:28:51Z","published":"2024-08-02T16:28:51Z","title":"Spatial-Spectral Morphological Mamba for Hyperspectral Image\n  Classification","summary":"  In recent years, Transformers have garnered significant attention for\nHyperspectral Image Classification (HSIC) due to their self-attention\nmechanism, which provides strong classification performance. However, these\nmodels face major challenges in computational efficiency, as their complexity\nincreases quadratically with the sequence length. The Mamba architecture,\nleveraging a State Space Model, offers a more efficient alternative to\nTransformers. This paper introduces the Spatial-Spectral Morphological Mamba\n(MorpMamba) model. In the MorpMamba model, a token generation module first\nconverts the Hyperspectral Image (HSI) patch into spatial-spectral tokens.\nThese tokens are then processed by a morphology block, which computes\nstructural and shape information using depthwise separable convolutional\noperations. The extracted information is enhanced in a feature enhancement\nmodule that adjusts the spatial and spectral tokens based on the center region\nof the HSI sample, allowing for effective information fusion within each block.\nSubsequently, the tokens are refined in a multi-head self-attention block to\nfurther improve the feature space. Finally, the combined information is fed\ninto the state space block for classification and the creation of the ground\ntruth map. Experiments on widely used Hyperspectral (HS) datasets demonstrate\nthat the MorpMamba model outperforms (parametric efficiency) both CNN and\nTransformer models.\n","authors":["Muhammad Ahmad","Muhammad Hassaan Farooq Butt","Muhammad Usama","Adil Mehmood Khan","Manual Mazzara","Salvatore Distenano"],"pdf_url":"https://arxiv.org/pdf/2408.01372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01370v1","updated":"2024-08-02T16:24:55Z","published":"2024-08-02T16:24:55Z","title":"EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using\n  Windowed Nonlinear Optimization","summary":"  Event cameras are an interesting visual exteroceptive sensor that reacts to\nbrightness changes rather than integrating absolute image intensities. Owing to\nthis design, the sensor exhibits strong performance in situations of\nchallenging dynamics and illumination conditions. While event-based\nsimultaneous tracking and mapping remains a challenging problem, a number of\nrecent works have pointed out the sensor's suitability for prior map-based\ntracking. By making use of cross-modal registration paradigms, the camera's\nego-motion can be tracked across a large spectrum of illumination and dynamics\nconditions on top of accurate maps that have been created a priori by more\ntraditional sensors. The present paper follows up on a recently introduced\nevent-based geometric semi-dense tracking paradigm, and proposes the addition\nof inertial signals in order to robustify the estimation. More specifically,\nthe added signals provide strong cues for pose initialization as well as\nregularization during windowed, multi-frame tracking. As a result, the proposed\nframework achieves increased performance under challenging illumination\nconditions as well as a reduction of the rate at which intermediate event\nrepresentations need to be registered in order to maintain stable tracking\nacross highly dynamic sequences. Our evaluation focuses on a diverse set of\nreal world sequences and comprises a comparison of our proposed method against\na purely event-based alternative running at different rates.\n","authors":["Runze Yuan","Tao Liu","Zijia Dai","Yi-Fan Zuo","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2408.01370v1.pdf","comment":"8 pages, 5 figures, 3 tables, International Conference on Intelligent\n  Robots and Systems 2024"},{"id":"http://arxiv.org/abs/2408.01366v1","updated":"2024-08-02T16:20:56Z","published":"2024-08-02T16:20:56Z","title":"Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic\n  Manipulation","summary":"  Humans possess a remarkable talent for flexibly alternating to different\nsenses when interacting with the environment. Picture a chef skillfully gauging\nthe timing of ingredient additions and controlling the heat according to the\ncolors, sounds, and aromas, seamlessly navigating through every stage of the\ncomplex cooking process. This ability is founded upon a thorough comprehension\nof task stages, as achieving the sub-goal within each stage can necessitate the\nutilization of different senses. In order to endow robots with similar ability,\nwe incorporate the task stages divided by sub-goals into the imitation learning\nprocess to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a\nstage-guided dynamic multi-sensory fusion method with coarse-to-fine stage\nunderstanding, which dynamically adjusts the priority of modalities based on\nthe fine-grained state within the predicted current stage. We train a robot\nsystem equipped with visual, auditory, and tactile sensors to accomplish\nchallenging robotic manipulation tasks: pouring and peg insertion with keyway.\nExperimental results indicate that our approach enables more effective and\nexplainable dynamic fusion, aligning more closely with the human fusion process\nthan existing methods.\n","authors":["Ruoxuan Feng","Di Hu","Wenke Ma","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17804v3","updated":"2024-08-02T16:17:35Z","published":"2023-11-29T16:54:25Z","title":"The Importance of Downstream Networks in Digital Pathology Foundation\n  Models","summary":"  Digital pathology has significantly advanced disease detection and\npathologist efficiency through the analysis of gigapixel whole-slide images\n(WSI). In this process, WSIs are first divided into patches, for which a\nfeature extractor model is applied to obtain feature vectors, which are\nsubsequently processed by an aggregation model to predict the respective WSI\nlabel. With the rapid evolution of representation learning, numerous new\nfeature extractor models, often termed foundational models, have emerged.\nTraditional evaluation methods rely on a static downstream aggregation model\nsetup, encompassing a fixed architecture and hyperparameters, a practice we\nidentify as potentially biasing the results. Our study uncovers a sensitivity\nof feature extractor models towards aggregation model configurations,\nindicating that performance comparability can be skewed based on the chosen\nconfigurations. By accounting for this sensitivity, we find that the\nperformance of many current feature extractor models is notably similar. We\nsupport this insight by evaluating seven feature extractor models across three\ndifferent datasets with 162 different aggregation model configurations. This\ncomprehensive approach provides a more nuanced understanding of the feature\nextractors' sensitivity to various aggregation model configurations, leading to\na fairer and more accurate assessment of new foundation models in digital\npathology.\n","authors":["Gustav Bredell","Marcel Fischer","Przemyslaw Szostak","Samaneh Abbasi-Sureshjani","Alvaro Gomariz"],"pdf_url":"https://arxiv.org/pdf/2311.17804v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01363v1","updated":"2024-08-02T16:15:25Z","published":"2024-08-02T16:15:25Z","title":"Toward Automatic Relevance Judgment using Vision--Language Models for\n  Image--Text Retrieval Evaluation","summary":"  Vision--Language Models (VLMs) have demonstrated success across diverse\napplications, yet their potential to assist in relevance judgments remains\nuncertain. This paper assesses the relevance estimation capabilities of VLMs,\nincluding CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc}\nretrieval task tailored for multimedia content creation in a zero-shot fashion.\nPreliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,\nencompassing open-source and closed-source visual-instruction-tuned Large\nLanguage Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared\nto human relevance judgments, surpassing the CLIPScore metric. (2) While\nCLIPScore is strongly preferred, LLMs are less biased towards CLIP-based\nretrieval systems. (3) GPT-4V's score distribution aligns more closely with\nhuman judgments than other models, achieving a Cohen's $\\kappa$ value of around\n0.08, which outperforms CLIPScore at approximately -0.096. These findings\nunderscore the potential of LLM-powered VLMs in enhancing relevance judgments.\n","authors":["Jheng-Hong Yang","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2408.01363v1.pdf","comment":"Accepted by ACM SIGIR 2024 LLM4Eval Workshop:\n  https://llm4eval.github.io/papers"},{"id":"http://arxiv.org/abs/2404.13534v3","updated":"2024-08-02T16:14:46Z","published":"2024-04-21T05:09:56Z","title":"Motion-aware Latent Diffusion Models for Video Frame Interpolation","summary":"  With the advancement of AIGC, video frame interpolation (VFI) has become a\ncrucial component in existing video generation frameworks, attracting\nwidespread research interest. For the VFI task, the motion estimation between\nneighboring frames plays a crucial role in avoiding motion ambiguity. However,\nexisting VFI methods always struggle to accurately predict the motion\ninformation between consecutive frames, and this imprecise estimation leads to\nblurred and visually incoherent interpolated frames. In this paper, we propose\na novel diffusion framework, motion-aware latent diffusion models (MADiff),\nwhich is specifically designed for the VFI task. By incorporating motion priors\nbetween the conditional neighboring frames with the target interpolated frame\npredicted throughout the diffusion sampling procedure, MADiff progressively\nrefines the intermediate outcomes, culminating in generating both visually\nsmooth and realistic results. Extensive experiments conducted on benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance\nsignificantly outperforming existing approaches, especially under challenging\nscenarios involving dynamic textures with complex motion.\n","authors":["Zhilin Huang","Yijie Yu","Ling Yang","Chujun Qin","Bing Zheng","Xiawu Zheng","Zikun Zhou","Yaowei Wang","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2404.13534v3.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.18849v2","updated":"2024-08-02T16:13:40Z","published":"2024-04-29T16:42:58Z","title":"MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection","summary":"  In real-world scenarios, using multiple modalities like visible (RGB) and\ninfrared (IR) can greatly improve the performance of a predictive task such as\nobject detection (OD). Multimodal learning is a common way to leverage these\nmodalities, where multiple modality-specific encoders and a fusion module are\nused to improve performance. In this paper, we tackle a different way to employ\nRGB and IR modalities, where only one modality or the other is observed by a\nsingle shared vision encoder. This realistic setting requires a lower memory\nfootprint and is more suitable for applications such as autonomous driving and\nsurveillance, which commonly rely on RGB and IR data. However, when learning a\nsingle encoder on multiple modalities, one modality can dominate the other,\nproducing uneven recognition results. This work investigates how to efficiently\nleverage RGB and IR modalities to train a common transformer-based OD vision\nencoder, while countering the effects of modality imbalance. For this, we\nintroduce a novel training technique to Mix Patches (MiPa) from the two\nmodalities, in conjunction with a patch-wise modality agnostic module, for\nlearning a common representation of both modalities. Our experiments show that\nMiPa can learn a representation to reach competitive results on traditional\nRGB/IR benchmarks while only requiring a single modality during inference. Our\ncode is available at: https://github.com/heitorrapela/MiPa.\n","authors":["Heitor R. Medeiros","David Latortue","Eric Granger","Marco Pedersoli"],"pdf_url":"https://arxiv.org/pdf/2404.18849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04551v2","updated":"2024-08-02T16:09:49Z","published":"2024-06-06T23:35:51Z","title":"Improving Geo-diversity of Generated Images with Contextualized Vendi\n  Score Guidance","summary":"  With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.\n","authors":["Reyhane Askari Hemmat","Melissa Hall","Alicia Sun","Candace Ross","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2406.04551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01356v1","updated":"2024-08-02T16:09:06Z","published":"2024-08-02T16:09:06Z","title":"Balanced Residual Distillation Learning for 3D Point Cloud\n  Class-Incremental Semantic Segmentation","summary":"  Class-incremental learning (CIL) thrives due to its success in processing the\ninflux of information by learning from continuously added new classes while\npreventing catastrophic forgetting about the old ones. It is essential for the\nperformance breakthrough of CIL to effectively refine past knowledge from the\nbase model and balance it with new learning. However, such an issue has not yet\nbeen considered in current research. In this work, we explore the potential of\nCIL from these perspectives and propose a novel balanced residual distillation\nframework (BRD-CIL) to push the performance bar of CIL to a new higher level.\nSpecifically, BRD-CIL designs a residual distillation learning strategy, which\ncan dynamically expand the network structure to capture the residuals between\nthe base and target models, effectively refining the past knowledge.\nFurthermore, BRD-CIL designs a balanced pseudo-label learning strategy by\ngenerating a guidance mask to reduce the preference for old classes, ensuring\nbalanced learning from new and old classes. We apply the proposed BRD-CIL to a\nchallenging 3D point cloud semantic segmentation task where the data are\nunordered and unstructured. Extensive experimental results demonstrate that\nBRD-CIL sets a new benchmark with an outstanding balance capability in\nclass-biased scenarios.\n","authors":["Yuanzhi Su","Siyuan Chen","Yuan-Gen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01349v1","updated":"2024-08-02T15:54:49Z","published":"2024-08-02T15:54:49Z","title":"PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy\n  Correspondence Learning in Cross-Modal Retrieval","summary":"  In the realm of cross-modal retrieval, seamlessly integrating diverse\nmodalities within multimedia remains a formidable challenge, especially given\nthe complexities introduced by noisy correspondence learning (NCL). Such noise\noften stems from mismatched data pairs, which is a significant obstacle\ndistinct from traditional noisy labels. This paper introduces\nPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address\nthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an\nauxiliary \"pseudo-classification\" task that interprets captions as categorical\nlabels, steering the model to learn image-text semantic similarity through a\nnon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,\ncapitalizing on PC$^2$'s pseudo-classification capability, we generate\npseudo-captions to provide more informative and tangible supervision for each\nmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed\nto assistant the correction of correspondence. In addition to technical\ncontributions, we develop a realistic NCL dataset called Noise of Web (NoW),\nwhich could be a new powerful NCL benchmark where noise exists naturally.\nEmpirical evaluations of PC$^2$ showcase marked improvements over existing\nstate-of-the-art robust cross-modal retrieval techniques on both simulated and\nrealistic datasets with various NCL settings. The contributed dataset and\nsource code are released at https://github.com/alipay/PC2-NoiseofWeb.\n","authors":["Yue Duan","Zhangxuan Gu","Zhenzhe Ying","Lei Qi","Changhua Meng","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01349v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2304.11857v3","updated":"2024-08-02T15:43:33Z","published":"2023-04-24T07:12:50Z","title":"Accurate and Efficient Event-based Semantic Segmentation Using Adaptive\n  Spiking Encoder-Decoder Network","summary":"  Spiking neural networks (SNNs), known for their low-power, event-driven\ncomputation and intrinsic temporal dynamics, are emerging as promising\nsolutions for processing dynamic, asynchronous signals from event-based\nsensors. Despite their potential, SNNs face challenges in training and\narchitectural design, resulting in limited performance in challenging\nevent-based dense prediction tasks compared to artificial neural networks\n(ANNs). In this work, we develop an efficient spiking encoder-decoder network\n(SpikingEDN) for large-scale event-based semantic segmentation tasks. To\nenhance the learning efficiency from dynamic event streams, we harness the\nadaptive threshold which improves network accuracy, sparsity and robustness in\nstreaming inference. Moreover, we develop a dual-path Spiking\nSpatially-Adaptive Modulation module, which is specifically tailored to enhance\nthe representation of sparse events and multi-modal inputs, thereby\nconsiderably improving network performance. Our SpikingEDN attains a mean\nintersection over union (MIoU) of 72.57\\% on the DDD17 dataset and 58.32\\% on\nthe larger DSEC-Semantic dataset, showing competitive results to the\nstate-of-the-art ANNs while requiring substantially fewer computational\nresources. Our results shed light on the untapped potential of SNNs in\nevent-based vision applications. The source code will be made publicly\navailable.\n","authors":["Rui Zhang","Luziwei Leng","Kaiwei Che","Hu Zhang","Jie Cheng","Qinghai Guo","Jiangxing Liao","Ran Cheng"],"pdf_url":"https://arxiv.org/pdf/2304.11857v3.pdf","comment":"Accepted for publication in IEEE Transactions on Neural Networks and\n  Learning Systems"},{"id":"http://arxiv.org/abs/2408.01343v1","updated":"2024-08-02T15:41:16Z","published":"2024-08-02T15:41:16Z","title":"StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal\n  Semantic Segmentation","summary":"  Multimodal semantic segmentation shows significant potential for enhancing\nsegmentation accuracy in complex scenes. However, current methods often\nincorporate specialized feature fusion modules tailored to specific modalities,\nthereby restricting input flexibility and increasing the number of training\nparameters. To address these challenges, we propose StitchFusion, a\nstraightforward yet effective modal fusion framework that integrates\nlarge-scale pre-trained models directly as encoders and feature fusers. This\napproach facilitates comprehensive multi-modal and multi-scale feature fusion,\naccommodating any visual modal inputs. Specifically, Our framework achieves\nmodal integration during encoding by sharing multi-modal visual information. To\nenhance information exchange across modalities, we introduce a\nmulti-directional adapter module (MultiAdapter) to enable cross-modal\ninformation transfer during encoding. By leveraging MultiAdapter to propagate\nmulti-scale information across pre-trained encoders during the encoding\nprocess, StitchFusion achieves multi-modal visual information integration\nduring encoding. Extensive comparative experiments demonstrate that our model\nachieves state-of-the-art performance on four multi-modal segmentation datasets\nwith minimal additional parameters. Furthermore, the experimental integration\nof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their\ncomplementary nature. Our code is available at StitchFusion_repo.\n","authors":["Bingyu Li","Da Zhang","Zhiyuan Zhao","Junyu Gao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01334v1","updated":"2024-08-02T15:32:42Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v1.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2404.12501v2","updated":"2024-08-02T15:28:19Z","published":"2024-04-18T20:43:33Z","title":"SPIdepth: Strengthened Pose Information for Self-supervised Monocular\n  Depth Estimation","summary":"  Self-supervised monocular depth estimation has garnered considerable\nattention for its applications in autonomous driving and robotics. While recent\nmethods have made strides in leveraging techniques like the Self Query Layer\n(SQL) to infer depth from motion, they often overlook the potential of\nstrengthening pose information. In this paper, we introduce SPIdepth, a novel\napproach that prioritizes enhancing the pose network for improved depth\nestimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the\nimportance of pose information in capturing fine-grained scene structures. By\nenhancing the pose network's capabilities, SPIdepth achieves remarkable\nadvancements in scene understanding and depth estimation. Experimental results\non benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth's\nstate-of-the-art performance, surpassing previous methods by significant\nmargins. Specifically, SPIdepth tops the self-supervised KITTI benchmark.\nAdditionally, SPIdepth achieves the lowest AbsRel (0.029), SqRel (0.069), and\nRMSE (1.394) on KITTI, establishing new state-of-the-art results. On\nCityscapes, SPIdepth shows improvements over SQLdepth of 21.7% in AbsRel, 36.8%\nin SqRel, and 16.5% in RMSE, even without using motion masks. On Make3D,\nSPIdepth in zero-shot outperforms all other models. Remarkably, SPIdepth\nachieves these results using only a single image for inference, surpassing even\nmethods that utilize video sequences for inference, thus demonstrating its\nefficacy and efficiency in real-world applications. Our approach represents a\nsignificant leap forward in self-supervised monocular depth estimation,\nunderscoring the importance of strengthening pose information for advancing\nscene understanding in real-world applications.\n","authors":["Mykola Lavreniuk"],"pdf_url":"https://arxiv.org/pdf/2404.12501v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01322v1","updated":"2024-08-02T15:20:34Z","published":"2024-08-02T15:20:34Z","title":"A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty\n  and Semantic Object Cues for Gaze Guidance in Dynamic Scenes","summary":"  How we perceive objects around us depends on what we actively attend to, yet\nour eye movements depend on the perceived objects. Still, object segmentation\nand gaze behavior are typically treated as two independent processes. Drawing\non an information processing pattern from robotics, we present a mechanistic\nmodel that simulates these processes for dynamic real-world scenes. Our\nimage-computable model uses the current scene segmentation for object-based\nsaccadic decision-making while using the foveated object to refine its scene\nsegmentation recursively. To model this refinement, we use a Bayesian filter,\nwhich also provides an uncertainty estimate for the segmentation that we use to\nguide active scene exploration. We demonstrate that this model closely\nresembles observers' free viewing behavior, measured by scanpath statistics,\nincluding foveation duration and saccade amplitude distributions used for\nparameter fitting and higher-level statistics not used for fitting. These\ninclude how object detections, inspections, and returns are balanced and a\ndelay of returning saccades without an explicit implementation of such temporal\ninhibition of return. Extensive simulations and ablation studies show that\nuncertainty promotes balanced exploration and that semantic object cues are\ncrucial to form the perceptual units used in object-based attention. Moreover,\nwe show how our model's modular design allows for extensions, such as\nincorporating saccadic momentum or pre-saccadic attention, to further align its\noutput with human scanpaths.\n","authors":["Vito Mengers","Nicolas Roth","Oliver Brock","Klaus Obermayer","Martin Rolfs"],"pdf_url":"https://arxiv.org/pdf/2408.01322v1.pdf","comment":"35+16 pages, 8+4 figures"},{"id":"http://arxiv.org/abs/2406.07867v2","updated":"2024-08-02T15:05:47Z","published":"2024-06-12T04:48:36Z","title":"Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation","summary":"  In this paper, we introduce a novel Face-to-Face spoken dialogue model. It\nprocesses audio-visual speech from user input and generates audio-visual speech\nas the response, marking the initial step towards creating an avatar chatbot\nsystem without relying on intermediate text. To this end, we newly introduce\nMultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken\ndialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded\nbased on the open domain dialogue dataset, TopicalChat. The MultiDialog\ncontains parallel audio-visual recordings of conversation partners acting\naccording to the given script with emotion annotations, which we expect to open\nup research opportunities in multimodal synthesis. Our Face-to-Face spoken\ndialogue model incorporates a textually pretrained large language model and\nadapts it into the audio-visual spoken dialogue domain by incorporating\nspeech-text joint pretraining. Through extensive experiments, we validate the\neffectiveness of our model in facilitating a face-to-face conversation. Demo\nand data are available at https://multidialog.github.io and\nhttps://huggingface.co/datasets/IVLLab/MultiDialog, respectively.\n","authors":["Se Jin Park","Chae Won Kim","Hyeongseop Rha","Minsu Kim","Joanna Hong","Jeong Hun Yeo","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2406.07867v2.pdf","comment":"Accepted to ACL 2024 (Oral)"},{"id":"http://arxiv.org/abs/2408.01311v1","updated":"2024-08-02T15:01:29Z","published":"2024-08-02T15:01:29Z","title":"TopoNAS: Boosting Search Efficiency of Gradient-based NAS via\n  Topological Simplification","summary":"  Improving search efficiency serves as one of the crucial objectives of Neural\nArchitecture Search (NAS). However, many current approaches ignore the\nuniversality of the search strategy and fail to reduce the computational\nredundancy during the search process, especially in one-shot NAS architectures.\nBesides, current NAS methods show invalid reparameterization in non-linear\nsearch space, leading to poor efficiency in common search spaces like DARTS. In\nthis paper, we propose TopoNAS, a model-agnostic approach for gradient-based\none-shot NAS that significantly reduces searching time and memory usage by\ntopological simplification of searchable paths. Firstly, we model the\nnon-linearity in search spaces to reveal the parameterization difficulties. To\nimprove the search efficiency, we present a topological simplification method\nand iteratively apply module-sharing strategies to simplify the topological\nstructure of searchable paths. In addition, a kernel normalization technique is\nalso proposed to preserve the search accuracy. Experimental results on the\nNASBench201 benchmark with various search spaces demonstrate the effectiveness\nof our method. It proves the proposed TopoNAS enhances the performance of\nvarious architectures in terms of search efficiency while maintaining a high\nlevel of accuracy. The project page is available at\nhttps://xdedss.github.io/topo_simplification.\n","authors":["Danpei Zhao","Zhuoran Liu","Bo Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.01311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01293v1","updated":"2024-08-02T14:28:49Z","published":"2024-08-02T14:28:49Z","title":"Underwater Object Detection Enhancement via Channel Stabilization","summary":"  The complex marine environment exacerbates the challenges of object detection\nmanifold. Marine trash endangers the aquatic ecosystem, presenting a persistent\nchallenge. Accurate detection of marine deposits is crucial for mitigating this\nharm. Our work addresses underwater object detection by enhancing image quality\nand evaluating detection methods. We use Detectron2's backbone with various\nbase models and configurations for this task.\n  We propose a novel channel stabilization technique alongside a simplified\nimage enhancement model to reduce haze and color cast in training images,\nimproving multi-scale object detection. Following image processing, we test\ndifferent Detectron2 backbones for optimal detection accuracy. Additionally, we\napply a sharpening filter with augmentation techniques to highlight object\nprofiles for easier recognition.\n  Results are demonstrated on the TrashCan Dataset, both instance and material\nversions. The best-performing backbone method incorporates our channel\nstabilization and augmentation techniques. We also compare our Detectron2\ndetection results with the Deformable Transformer. In the instance version of\nTrashCan 1.0, our method achieves a 9.53% absolute increase in average\nprecision for small objects and a 7% absolute gain in bounding box detection\ncompared to the baseline. The code will be available on Code:\nhttps://github.com/aliman80/Underwater-\nObject-Detection-via-Channel-Stablization\n","authors":["Muhammad Ali","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2408.01293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01292v1","updated":"2024-08-02T14:28:10Z","published":"2024-08-02T14:28:10Z","title":"3DPX: Progressive 2D-to-3D Oral Image Reconstruction with Hybrid MLP-CNN\n  Networks","summary":"  Panoramic X-ray (PX) is a prevalent modality in dental practice for its wide\navailability and low cost. However, as a 2D projection image, PX does not\ncontain 3D anatomical information, and therefore has limited use in dental\napplications that can benefit from 3D information, e.g., tooth angular\nmisa-lignment detection and classification. Reconstructing 3D structures\ndirectly from 2D PX has recently been explored to address limitations with\nexisting methods primarily reliant on Convolutional Neural Networks (CNNs) for\ndirect 2D-to-3D mapping. These methods, however, are unable to correctly infer\ndepth-axis spatial information. In addition, they are limited by the in-trinsic\nlocality of convolution operations, as the convolution kernels only capture the\ninformation of immediate neighborhood pixels. In this study, we propose a\nprogressive hybrid Multilayer Perceptron (MLP)-CNN pyra-mid network (3DPX) for\n2D-to-3D oral PX reconstruction. We introduce a progressive reconstruction\nstrategy, where 3D images are progressively re-constructed in the 3DPX with\nguidance imposed on the intermediate recon-struction result at each pyramid\nlevel. Further, motivated by the recent ad-vancement of MLPs that show promise\nin capturing fine-grained long-range dependency, our 3DPX integrates MLPs and\nCNNs to improve the semantic understanding during reconstruction. Extensive\nexperiments on two large datasets involving 464 studies demonstrate that our\n3DPX outperforms state-of-the-art 2D-to-3D oral reconstruction methods,\nincluding standalone MLP and transformers, in reconstruction quality, and also\nim-proves the performance of downstream angular misalignment classification\ntasks.\n","authors":["Xiaoshuang Li","Mingyuan Meng","Zimo Huang","Lei Bi","Eduardo Delamare","Dagan Feng","Bin Sheng","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01292v1.pdf","comment":"accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.01291v1","updated":"2024-08-02T14:24:40Z","published":"2024-08-02T14:24:40Z","title":"TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and\n  Resampling","summary":"  Given a 3D mesh, we aim to synthesize 3D textures that correspond to\narbitrary textual descriptions. Current methods for generating and assembling\ntextures from sampled views often result in prominent seams or excessive\nsmoothing. To tackle these issues, we present TexGen, a novel multi-view\nsampling and resampling framework for texture generation leveraging a\npre-trained text-to-image diffusion model. For view consistent sampling, first\nof all we maintain a texture map in RGB space that is parameterized by the\ndenoising step and updated after each sampling step of the diffusion model to\nprogressively reduce the view discrepancy. An attention-guided multi-view\nsampling strategy is exploited to broadcast the appearance information across\nviews. To preserve texture details, we develop a noise resampling technique\nthat aids in the estimation of noise, generating inputs for subsequent\ndenoising steps, as directed by the text prompt and current texture map.\nThrough an extensive amount of qualitative and quantitative evaluations, we\ndemonstrate that our proposed method produces significantly better texture\nquality for diverse 3D objects with a high degree of view consistency and rich\nappearance details, outperforming current state-of-the-art methods.\nFurthermore, our proposed texture generation technique can also be applied to\ntexture editing while preserving the original identity. More experimental\nresults are available at https://dong-huo.github.io/TexGen/\n","authors":["Dong Huo","Zixin Guo","Xinxin Zuo","Zhihao Shi","Juwei Lu","Peng Dai","Songcen Xu","Li Cheng","Yee-Hong Yang"],"pdf_url":"https://arxiv.org/pdf/2408.01291v1.pdf","comment":"European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2408.01287v1","updated":"2024-08-02T14:19:34Z","published":"2024-08-02T14:19:34Z","title":"Deep Learning based Visually Rich Document Content Understanding: A\n  Survey","summary":"  Visually Rich Documents (VRDs) are essential in academia, finance, medical\nfields, and marketing due to their multimodal information content. Traditional\nmethods for extracting information from VRDs depend on expert knowledge and\nmanual labor, making them costly and inefficient. The advent of deep learning\nhas revolutionized this process, introducing models that leverage multimodal\ninformation vision, text, and layout along with pretraining tasks to develop\ncomprehensive document representations. These models have achieved\nstate-of-the-art performance across various downstream tasks, significantly\nenhancing the efficiency and accuracy of information extraction from VRDs. In\nresponse to the growing demands and rapid developments in Visually Rich\nDocument Understanding (VRDU), this paper provides a comprehensive review of\ndeep learning-based VRDU frameworks. We systematically survey and analyze\nexisting methods and benchmark datasets, categorizing them based on adopted\nstrategies and downstream tasks. Furthermore, we compare different techniques\nused in VRDU models, focusing on feature representation and fusion, model\narchitecture, and pretraining methods, while highlighting their strengths,\nlimitations, and appropriate scenarios. Finally, we identify emerging trends\nand challenges in VRDU, offering insights into future research directions and\npractical applications. This survey aims to provide a thorough understanding of\nVRDU advancements, benefiting both academic and industrial sectors.\n","authors":["Yihao Ding","Jean Lee","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2408.01287v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2408.01284v1","updated":"2024-08-02T14:10:20Z","published":"2024-08-02T14:10:20Z","title":"Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot\n  Learning: A General Framework","summary":"  Generalized Zero-Shot Learning (GZSL) is a challenging task requiring\naccurate classification of both seen and unseen classes. Within this domain,\nAudio-visual GZSL emerges as an extremely exciting yet difficult task, given\nthe inclusion of both visual and acoustic features as multi-modal inputs.\nExisting efforts in this field mostly utilize either embedding-based or\ngenerative-based methods. However, generative training is difficult and\nunstable, while embedding-based methods often encounter domain shift problem.\nThus, we find it promising to integrate both methods into a unified framework\nto leverage their advantages while mitigating their respective disadvantages.\nOur study introduces a general framework employing out-of-distribution (OOD)\ndetection, aiming to harness the strengths of both approaches. We first employ\ngenerative adversarial networks to synthesize unseen features, enabling the\ntraining of an OOD detector alongside classifiers for seen and unseen classes.\nThis detector determines whether a test feature belongs to seen or unseen\nclasses, followed by classification utilizing separate classifiers for each\nfeature type. We test our framework on three popular audio-visual datasets and\nobserve a significant improvement comparing to existing state-of-the-art works.\nCodes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.\n","authors":["Liuyuan Wen"],"pdf_url":"https://arxiv.org/pdf/2408.01284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01276v1","updated":"2024-08-02T14:01:34Z","published":"2024-08-02T14:01:34Z","title":"Wave-Mamba: Wavelet State Space Model for Ultra-High-Definition\n  Low-Light Image Enhancement","summary":"  Ultra-high-definition (UHD) technology has attracted widespread attention due\nto its exceptional visual quality, but it also poses new challenges for\nlow-light image enhancement (LLIE) techniques. UHD images inherently possess\nhigh computational complexity, leading existing UHD LLIE methods to employ\nhigh-magnification downsampling to reduce computational costs, which in turn\nresults in information loss. The wavelet transform not only allows downsampling\nwithout loss of information, but also separates the image content from the\nnoise. It enables state space models (SSMs) to avoid being affected by noise\nwhen modeling long sequences, thus making full use of the long-sequence\nmodeling capability of SSMs. On this basis, we propose Wave-Mamba, a novel\napproach based on two pivotal insights derived from the wavelet domain: 1) most\nof the content information of an image exists in the low-frequency component,\nless in the high-frequency component. 2) The high-frequency component exerts a\nminimal influence on the outcomes of low-light enhancement. Specifically, to\nefficiently model global content information on UHD images, we proposed a\nlow-frequency state space block (LFSSBlock) by improving SSMs to focus on\nrestoring the information of low-frequency sub-bands. Moreover, we propose a\nhigh-frequency enhance block (HFEBlock) for high-frequency sub-band\ninformation, which uses the enhanced low-frequency information to correct the\nhigh-frequency information and effectively restore the correct high-frequency\ndetails. Through comprehensive evaluation, our method has demonstrated superior\nperformance, significantly outshining current leading techniques while\nmaintaining a more streamlined architecture. The code is available at\nhttps://github.com/AlexZou14/Wave-Mamba.\n","authors":["Wenbin Zou","Hongxia Gao","Weipeng Yang","Tongtong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01276v1.pdf","comment":"10 pages, 8 figures, ACMMM2024 accepted"},{"id":"http://arxiv.org/abs/2408.01269v1","updated":"2024-08-02T13:46:15Z","published":"2024-08-02T13:46:15Z","title":"A General Framework to Boost 3D GS Initialization for Text-to-3D\n  Generation by Lexical Richness","summary":"  Text-to-3D content creation has recently received much attention, especially\nwith the prevalence of 3D Gaussians Splatting. In general, GS-based methods\ncomprise two key stages: initialization and rendering optimization. To achieve\ninitialization, existing works directly apply random sphere initialization or\n3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such\nstrategies suffer from two critical yet challenging problems: 1) the final\nshapes are still similar to the initial ones even after training; 2) shapes can\nbe produced only from simple texts, e.g., \"a dog\", not for lexically richer\ntexts, e.g., \"a dog is sitting on the top of the airplane\". To address these\nproblems, this paper proposes a novel general framework to boost the 3D GS\nInitialization for text-to-3D generation upon the lexical richness. Our key\nidea is to aggregate 3D Gaussians into spatially uniform voxels to represent\ncomplex shapes while enabling the spatial interaction among the 3D Gaussians\nand semantic interaction between Gaussians and texts. Specifically, we first\nconstruct a voxelized representation, where each voxel holds a 3D Gaussian with\nits position, scale, and rotation fixed while setting opacity as the sole\nfactor to determine a position's occupancy. We then design an initialization\nnetwork mainly consisting of two novel components: 1) Global Information\nPerception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design\nenables each 3D Gaussian to assimilate the spatial information from other areas\nand semantic information from texts. Extensive experiments show the superiority\nof our framework of high-quality 3D GS initialization against the existing\nmethods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.\nAlso, our framework can be seamlessly plugged into SoTA training frameworks,\ne.g., LucidDreamer, for semantically consistent text-to-3D generation.\n","authors":["Lutao Jiang","Hangyu Li","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09004v2","updated":"2024-08-02T13:27:09Z","published":"2023-11-15T14:46:20Z","title":"Incremental Object-Based Novelty Detection with Feedback Loop","summary":"  Object-based Novelty Detection (ND) aims to identify unknown objects that do\nnot belong to classes seen during training by an object detection model. The\ntask is particularly crucial in real-world applications, as it allows to avoid\npotentially harmful behaviours, e.g. as in the case of object detection models\nadopted in a self-driving car or in an autonomous robot. Traditional approaches\nto ND focus on one time offline post processing of the pretrained object\ndetection output, leaving no possibility to improve the model robustness after\ntraining and discarding the abundant amount of out-of-distribution data\nencountered during deployment. In this work, we propose a novel framework for\nobject-based ND, assuming that human feedback can be requested on the predicted\noutput and later incorporated to refine the ND model without negatively\naffecting the main object detection performance. This refinement operation is\nrepeated whenever new feedback is available. To tackle this new formulation of\nthe problem for object detection, we propose a lightweight ND module attached\non top of a pre-trained object detection model, which is incrementally updated\nthrough a feedback loop. We also propose a new benchmark to evaluate methods on\nthis new setting and test extensively our ND approach against baselines,\nshowing increased robustness and a successful incorporation of the received\nfeedback.\n","authors":["Simone Caldarella","Elisa Ricci","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2311.09004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09339v4","updated":"2024-08-02T13:13:47Z","published":"2022-07-19T15:49:35Z","title":"Vision Transformers: From Semantic Segmentation to Dense Prediction","summary":"  The emergence of vision transformers (ViTs) in image classification has\nshifted the methodologies for visual representation learning. In particular,\nViTs learn visual representation at full receptive field per layer across all\nthe image patches, in comparison to the increasing receptive fields of CNNs\nacross layers and other alternatives (e.g., large kernels and atrous\nconvolution). In this work, for the first time we explore the global context\nlearning potentials of ViTs for dense visual prediction (e.g., semantic\nsegmentation). Our motivation is that through learning global context at full\nreceptive field layer by layer, ViTs may capture stronger long-range dependency\ninformation, critical for dense prediction tasks. We first demonstrate that\nencoding an image as a sequence of patches, a vanilla ViT without local\nconvolution and resolution reduction can yield stronger visual representation\nfor semantic segmentation. For example, our model, termed as SEgmentation\nTRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the\ntest leaderboard on the day of submission) and performs competitively on\nCityscapes. However, the basic ViT architecture falls short in broader dense\nprediction applications, such as object detection and instance segmentation,\ndue to its lack of a pyramidal structure, high computational demand, and\ninsufficient local context. For tackling general dense visual prediction tasks\nin a cost-effective manner, we further formulate a family of Hierarchical\nLocal-Global (HLG) Transformers, characterized by local attention within\nwindows and global-attention across windows in a pyramidal architecture.\nExtensive experiments show that our methods achieve appealing performance on a\nvariety of dense prediction tasks (e.g., object detection and instance\nsegmentation and semantic segmentation) as well as image classification.\n","authors":["Li Zhang","Jiachen Lu","Sixiao Zheng","Xinxuan Zhao","Xiatian Zhu","Yanwei Fu","Tao Xiang","Jianfeng Feng","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2207.09339v4.pdf","comment":"Extended version of CVPR 2021 paper arXiv:2012.15840 Published on\n  International Journal of Computer Vision (2024)"},{"id":"http://arxiv.org/abs/2408.01233v1","updated":"2024-08-02T12:48:36Z","published":"2024-08-02T12:48:36Z","title":"CLIP4Sketch: Enhancing Sketch to Mugshot Matching through Dataset\n  Augmentation using Diffusion Models","summary":"  Forensic sketch-to-mugshot matching is a challenging task in face\nrecognition, primarily hindered by the scarcity of annotated forensic sketches\nand the modality gap between sketches and photographs. To address this, we\npropose CLIP4Sketch, a novel approach that leverages diffusion models to\ngenerate a large and diverse set of sketch images, which helps in enhancing the\nperformance of face recognition systems in sketch-to-mugshot matching. Our\nmethod utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate\nsketches with explicit control over identity and style. We combine CLIP and\nAdaface embeddings of a reference mugshot, along with textual descriptions of\nstyle, as the conditions to the diffusion model. We demonstrate the efficacy of\nour approach by generating a comprehensive dataset of sketches corresponding to\nmugshots and training a face recognition model on our synthetic data. Our\nresults show significant improvements in sketch-to-mugshot matching accuracy\nover training on an existing, limited amount of real face sketch data,\nvalidating the potential of diffusion models in enhancing the performance of\nface recognition systems across modalities. We also compare our dataset with\ndatasets generated using GAN-based methods to show its superiority.\n","authors":["Kushal Kumar Jain","Steve Grosz","Anoop M. Namboodiri","Anil K. Jain"],"pdf_url":"https://arxiv.org/pdf/2408.01233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01231v1","updated":"2024-08-02T12:44:07Z","published":"2024-08-02T12:44:07Z","title":"WaveMamba: Spatial-Spectral Wavelet Mamba for Hyperspectral Image\n  Classification","summary":"  Hyperspectral Imaging (HSI) has proven to be a powerful tool for capturing\ndetailed spectral and spatial information across diverse applications. Despite\nthe advancements in Deep Learning (DL) and Transformer architectures for HSI\nClassification (HSIC), challenges such as computational efficiency and the need\nfor extensive labeled data persist. This paper introduces WaveMamba, a novel\napproach that integrates wavelet transformation with the Spatial-Spectral Mamba\narchitecture to enhance HSIC. WaveMamba captures both local texture patterns\nand global contextual relationships in an end-to-end trainable model. The\nWavelet-based enhanced features are then processed through the state-space\narchitecture to model spatial-spectral relationships and temporal dependencies.\nThe experimental results indicate that WaveMamba surpasses existing models,\nachieving an accuracy improvement of 4.5\\% on the University of Houston dataset\nand a 2.0\\% increase on the Pavia University dataset. These findings validate\nits effectiveness in addressing the complex data interactions inherent in HSIs.\n","authors":["Muhammad Ahmad","Muhammad Usama","Manual Mazzara"],"pdf_url":"https://arxiv.org/pdf/2408.01231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01228v1","updated":"2024-08-02T12:36:13Z","published":"2024-08-02T12:36:13Z","title":"The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models","summary":"  Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.\n","authors":["Simone Caldarella","Massimiliano Mancini","Elisa Ricci","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2408.01228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01224v1","updated":"2024-08-02T12:27:15Z","published":"2024-08-02T12:27:15Z","title":"Multi-head Spatial-Spectral Mamba for Hyperspectral Image Classification","summary":"  Spatial-Spectral Mamba (SSM) improves computational efficiency and captures\nlong-range dependencies, addressing Transformer limitations. However,\ntraditional Mamba models overlook rich spectral information in HSIs and\nstruggle with high dimensionality and sequential data. To address these issues,\nwe propose the SSM with multi-head self-attention and token enhancement\n(MHSSMamba). This model integrates spectral and spatial information by\nenhancing spectral tokens and using multi-head attention to capture complex\nrelationships between spectral bands and spatial locations. It also manages\nlong-range dependencies and the sequential nature of HSI data, preserving\ncontextual information across spectral bands. MHSSMamba achieved remarkable\nclassification accuracies of 97.62\\% on Pavia University, 96.92\\% on the\nUniversity of Houston, 96.85\\% on Salinas, and 99.49\\% on Wuhan-longKou\ndatasets.\n","authors":["Muhammad Ahmad","Muhammad Hassaan Farooq Butt","Muhammad Usama","Hamad Ahmed Altuwaijri","Manual Mazzara","Salvatore Distenano"],"pdf_url":"https://arxiv.org/pdf/2408.01224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01218v1","updated":"2024-08-02T12:16:07Z","published":"2024-08-02T12:16:07Z","title":"S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from\n  a Single Sketch","summary":"  3D textured face reconstruction from sketches applicable in many scenarios\nsuch as animation, 3D avatars, artistic design, missing people search, etc., is\na highly promising but underdeveloped research topic. On the one hand, the\nstylistic diversity of sketches leads to existing sketch-to-3D-face methods\nonly being able to handle pose-limited and realistically shaded sketches. On\nthe other hand, texture plays a vital role in representing facial appearance,\nyet sketches lack this information, necessitating additional texture control in\nthe reconstruction process. This paper proposes a novel method for\nreconstructing controllable textured and detailed 3D faces from sketches, named\nS2TD-Face. S2TD-Face introduces a two-stage geometry reconstruction framework\nthat directly reconstructs detailed geometry from the input sketch. To keep\ngeometry consistent with the delicate strokes of the sketch, we propose a novel\nsketch-to-geometry loss that ensures the reconstruction accurately fits the\ninput features like dimples and wrinkles. Our training strategies do not rely\non hard-to-obtain 3D face scanning data or labor-intensive hand-drawn sketches.\nFurthermore, S2TD-Face introduces a texture control module utilizing text\nprompts to select the most suitable textures from a library and seamlessly\nintegrate them into the geometry, resulting in a 3D detailed face with\ncontrollable texture. S2TD-Face surpasses existing state-of-the-art methods in\nextensive quantitative and qualitative experiments. Our project is available at\nhttps://github.com/wang-zidu/S2TD-Face .\n","authors":["Zidu Wang","Xiangyu Zhu","Jiang Yu","Tianshuo Zhang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2408.01218v1.pdf","comment":"ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.01191v1","updated":"2024-08-02T11:18:32Z","published":"2024-08-02T11:18:32Z","title":"A Weakly Supervised and Globally Explainable Learning Framework for\n  Brain Tumor Segmentation","summary":"  Machine-based brain tumor segmentation can help doctors make better\ndiagnoses. However, the complex structure of brain tumors and expensive\npixel-level annotations present challenges for automatic tumor segmentation. In\nthis paper, we propose a counterfactual generation framework that not only\nachieves exceptional brain tumor segmentation performance without the need for\npixel-level annotations, but also provides explainability. Our framework\neffectively separates class-related features from class-unrelated features of\nthe samples, and generate new samples that preserve identity features while\naltering class attributes by embedding different class-related features. We\nperform topological data analysis on the extracted class-related features and\nobtain a globally explainable manifold, and for each abnormal sample to be\nsegmented, a meaningful normal sample could be effectively generated with the\nguidance of the rule-based paths designed within the manifold for comparison\nfor identifying the tumor regions. We evaluate our proposed method on two\ndatasets, which demonstrates superior performance of brain tumor segmentation.\nThe code is available at https://github.com/xrt11/tumor-segmentation.\n","authors":["Ruitao Xie","Limai Jiang","Xiaoxi He","Yi Pan","Yunpeng Cai"],"pdf_url":"https://arxiv.org/pdf/2408.01191v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo"},{"id":"http://arxiv.org/abs/2408.01181v1","updated":"2024-08-02T11:03:22Z","published":"2024-08-02T11:03:22Z","title":"VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling","summary":"  VAR is a new generation paradigm that employs 'next-scale prediction' as\nopposed to 'next-token prediction'. This innovative transformation enables\nauto-regressive (AR) transformers to rapidly learn visual distributions and\nachieve robust generalization. However, the original VAR model is constrained\nto class-conditioned synthesis, relying solely on textual captions for\nguidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model\nthat integrates Visual Auto-Regressive techniques with the capabilities of\nCLIP. The VAR-CLIP framework encodes captions into text embeddings, which are\nthen utilized as textual conditions for image generation. To facilitate\ntraining on extensive datasets, such as ImageNet, we have constructed a\nsubstantial image-text dataset leveraging BLIP2. Furthermore, we delve into the\nsignificance of word positioning within CLIP for the purpose of caption\nguidance. Extensive experiments confirm VAR-CLIP's proficiency in generating\nfantasy images with high fidelity, textual congruence, and aesthetic\nexcellence. Our project page are https://github.com/daixiangzi/VAR-CLIP\n","authors":["Qian Zhang","Xiangzi Dai","Ninghua Yang","Xiang An","Ziyong Feng","Xingyu Ren"],"pdf_url":"https://arxiv.org/pdf/2408.01181v1.pdf","comment":"total 10 pages, code:https://github.com/daixiangzi/VAR-CLIP"},{"id":"http://arxiv.org/abs/2407.19546v2","updated":"2024-08-02T10:53:37Z","published":"2024-07-28T17:38:21Z","title":"XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image\n  Pre-Training","summary":"  Vision-and-language pretraining (VLP) in the medical field utilizes\ncontrastive learning on image-text pairs to achieve effective transfer across\ntasks. Yet, current VLP approaches with the masked modelling strategy face two\nchallenges when applied to the medical domain. First, current models struggle\nto accurately reconstruct key pathological features due to the scarcity of\nmedical data. Second, most methods only adopt either paired image-text or\nimage-only data, failing to exploit the combination of both paired and unpaired\ndata. To this end, this paper proposes a XLIP (Masked modelling for medical\nLanguage-Image Pre-training) framework to enhance pathological learning and\nfeature learning via unpaired data. First, we introduce the attention-masked\nimage modelling (AttMIM) and entity-driven masked language modelling module\n(EntMLM), which learns to reconstruct pathological visual and textual tokens\nvia multi-modal feature interaction, thus improving medical-enhanced features.\nThe AttMIM module masks a portion of the image features that are highly\nresponsive to textual features. This allows XLIP to improve the reconstruction\nof highly similar image data in medicine efficiency. Second, our XLIP\ncapitalizes unpaired data to enhance multimodal learning by introducing\ndisease-kind prompts. The experimental results show that XLIP achieves SOTA for\nzero-shot and fine-tuning classification performance on five datasets. Our code\nwill be available at https://github.com/White65534/XLIP\n","authors":["Biao Wu","Yutong Xie","Zeyu Zhang","Minh Hieu Phan","Qi Chen","Ling Chen","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2407.19546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01167v1","updated":"2024-08-02T10:34:23Z","published":"2024-08-02T10:34:23Z","title":"Rethinking Pre-trained Feature Extractor Selection in Multiple Instance\n  Learning for Whole Slide Image Classification","summary":"  Multiple instance learning (MIL) has become a preferred method for\nclassifying gigapixel whole slide images (WSIs), without requiring patch label\nannotation. The focus of the current MIL research stream is on the\nembedding-based MIL approach, which involves extracting feature vectors from\npatches using a pre-trained feature extractor. These feature vectors are then\nfed into an MIL aggregator for slide-level prediction. Despite prior research\nsuggestions on enhancing the most commonly used ResNet50 supervised model\npre-trained on ImageNet-1K, there remains a lack of clear guidance on selecting\nthe optimal feature extractor to maximize WSI performance. This study aims at\naddressing this gap by examining MIL feature extractors across three\ndimensions: pre-training dataset, backbone model, and pre-training method.\nExtensive experiments were carried out on the two public WSI datasets\n(TCGA-NSCLC and Camelyon16) using four SOTA MIL models. The main findings\nindicate the following: 1) Performance significantly improves with larger and\nmore varied pre-training datasets in both CNN and Transformer backbones. 2)\n`Modern and deeper' backbones greatly outperform `standard' backbones (ResNet\nand ViT), with performance improvements more guaranteed in Transformer-based\nbackbones. 3) The choice of self-supervised learning (SSL) method is crucial,\nwith the most significant benefits observed when applied to the Transformer\n(ViT) backbone. The study findings have practical implications, including\ndesigning more effective pathological foundation models. Our code is available\nat: https://anonymous.4open.science/r/MIL-Feature-Extractor-Selection\n","authors":["Bryan Wong","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2408.01167v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.01162v1","updated":"2024-08-02T10:24:35Z","published":"2024-08-02T10:24:35Z","title":"PreMix: Boosting Multiple Instance Learning in Digital Histopathology\n  through Pre-training with Intra-Batch Slide Mixing","summary":"  The classification of gigapixel-sized whole slide images (WSIs), digital\nrepresentations of histological slides obtained via a high-resolution scanner,\nfaces significant challenges associated with the meticulous and time-consuming\nnature of fine-grained labeling. While weakly-supervised multiple instance\nlearning (MIL) has emerged as a promising approach, current MIL methods are\nconstrained by their limited ability to leverage the wealth of information\nembedded within unlabeled WSIs. This limitation often necessitates training MIL\nfeature aggregators from scratch after the feature extraction process,\nhindering efficiency and accuracy. PreMix extends the general MIL framework by\npre-training the MIL aggregator with an intra-batch slide mixing approach.\nSpecifically, PreMix incorporates Barlow Twins Slide Mixing during\npre-training, enhancing its ability to handle diverse WSI sizes and maximizing\nthe utility of unlabeled WSIs. Combined with Mixup and Manifold Mixup during\nfine-tuning, PreMix achieves a mean of 4.7% performance improvement over the\nbaseline MIL framework, the hierarchical image pyramid transformer (HIPT), on\nthe Camelyon16 dataset. The observed improvement across a range of active\nlearning acquisition functions and WSI-labeled training budgets highlights the\nframework's adaptability to diverse datasets and varying resource constraints.\nUltimately, PreMix paves the way for more efficient and accurate WSI\nclassification under limited WSI-labeled datasets, encouraging the broader\nadoption of unlabeled WSI data in histopathological research. The code is\navailable at https://anonymous.4open.science/r/PreMix\n","authors":["Bryan Wong","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2408.01162v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2408.01159v1","updated":"2024-08-02T10:21:10Z","published":"2024-08-02T10:21:10Z","title":"Robust Curve Detection in Volumetric Medical Imaging via Attraction\n  Field","summary":"  Understanding body part geometry is crucial for precise medical diagnostics.\nCurves effectively describe anatomical structures and are widely used in\nmedical imaging applications related to cardiovascular, respiratory, and\nskeletal diseases. Traditional curve detection methods are often task-specific,\nrelying heavily on domain-specific features, limiting their broader\napplicability. This paper introduces a novel approach for detecting\nnon-branching curves, which does not require prior knowledge of the object's\norientation, shape, or position. Our method uses neural networks to predict (1)\nan attraction field, which offers subpixel accuracy, and (2) a closeness map,\nwhich limits the region of interest and essentially eliminates outliers far\nfrom the desired curve. We tested our curve detector on several clinically\nrelevant tasks with diverse morphologies and achieved impressive subpixel-level\naccuracy results that surpass existing methods, highlighting its versatility\nand robustness. Additionally, to support further advancements in this field, we\nprovide our private annotations of aortic centerlines and masks, which can\nserve as a benchmark for future research. The dataset can be found at\nhttps://github.com/neuro-ml/curve-detection.\n","authors":["Farukh Yaushev","Daria Nogina","Valentin Samokhin","Mariya Dugova","Ekaterina Petrash","Dmitry Sevryukov","Mikhail Belyaev","Maxim Pisov"],"pdf_url":"https://arxiv.org/pdf/2408.01159v1.pdf","comment":"Accepted to ShapeMI MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.15559v2","updated":"2024-08-02T10:19:34Z","published":"2024-03-22T18:28:04Z","title":"An Optimization Framework to Enforce Multi-View Consistency for\n  Texturing 3D Meshes","summary":"  A fundamental problem in the texturing of 3D meshes using pre-trained\ntext-to-image models is to ensure multi-view consistency. State-of-the-art\napproaches typically use diffusion models to aggregate multi-view inputs, where\ncommon issues are the blurriness caused by the averaging operation in the\naggregation step or inconsistencies in local features. This paper introduces an\noptimization framework that proceeds in four stages to achieve multi-view\nconsistency. Specifically, the first stage generates an over-complete set of 2D\ntextures from a predefined set of viewpoints using an MV-consistent diffusion\nprocess. The second stage selects a subset of views that are mutually\nconsistent while covering the underlying 3D model. We show how to achieve this\ngoal by solving semi-definite programs. The third stage performs non-rigid\nalignment to align the selected views across overlapping regions. The fourth\nstage solves an MRF problem to associate each mesh face with a selected view.\nIn particular, the third and fourth stages are iterated, with the cuts obtained\nin the fourth stage encouraging non-rigid alignment in the third stage to focus\non regions close to the cuts. Experimental results show that our approach\nsignificantly outperforms baseline approaches both qualitatively and\nquantitatively. Project page: https://aigc3d.github.io/ConsistenTex.\n","authors":["Zhengyi Zhao","Chen Song","Xiaodong Gu","Yuan Dong","Qi Zuo","Weihao Yuan","Liefeng Bo","Zilong Dong","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.15559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06542v2","updated":"2024-08-02T10:04:09Z","published":"2024-01-12T12:35:45Z","title":"Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and\n  Outlook","summary":"  In the realm of modern autonomous driving, the perception system is\nindispensable for accurately assessing the state of the surrounding\nenvironment, thereby enabling informed prediction and planning. The key step to\nthis system is related to 3D object detection that utilizes vehicle-mounted\nsensors such as LiDAR and cameras to identify the size, the category, and the\nlocation of nearby objects. Despite the surge in 3D object detection methods\naimed at enhancing detection precision and efficiency, there is a gap in the\nliterature that systematically examines their resilience against environmental\nvariations, noise, and weather changes. This study emphasizes the importance of\nrobustness, alongside accuracy and latency, in evaluating perception systems\nunder practical scenarios. Our work presents an extensive survey of\ncamera-only, LiDAR-only, and multi-modal 3D object detection algorithms,\nthoroughly evaluating their trade-off between accuracy, latency, and\nrobustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair\ncomparisons. Among these, multi-modal 3D detection approaches exhibit superior\nrobustness, and a novel taxonomy is introduced to reorganize the literature for\nenhanced clarity. This survey aims to offer a more practical perspective on the\ncurrent capabilities and the constraints of 3D object detection algorithms in\nreal-world applications, thus steering future research towards\nrobustness-centric advancements.\n","authors":["Ziying Song","Lin Liu","Feiyang Jia","Yadan Luo","Guoxin Zhang","Lei Yang","Li Wang","Caiyan Jia"],"pdf_url":"https://arxiv.org/pdf/2401.06542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08372v3","updated":"2024-08-02T09:56:14Z","published":"2023-12-13T18:59:58Z","title":"SAM-guided Graph Cut for 3D Instance Segmentation","summary":"  This paper addresses the challenge of 3D instance segmentation by\nsimultaneously leveraging 3D geometric and multi-view image information. Many\nprevious works have applied deep learning techniques to 3D point clouds for\ninstance segmentation. However, these methods often failed to generalize to\nvarious types of scenes due to the scarcity and low-diversity of labeled 3D\npoint cloud data. Some recent works have attempted to lift 2D instance\nsegmentations to 3D within a bottom-up framework. The inconsistency in 2D\ninstance segmentations among views can substantially degrade the performance of\n3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to\neffectively exploit 2D segmentation models for 3D instance segmentation.\nSpecifically, we pre-segment the scene into several superpoints in 3D,\nformulating the task into a graph cut problem. The superpoint graph is\nconstructed based on 2D segmentation models, where node features are obtained\nfrom multi-view image features and edge weights are computed based on\nmulti-view segmentation results, enabling the better generalization ability. To\nprocess the graph, we train a graph neural network using pseudo 3D labels from\n2D segmentation models. Experimental results on the ScanNet, ScanNet++ and\nKITTI-360 datasets demonstrate that our method achieves robust segmentation\nperformance and can generalize across different types of scenes. Our project\npage is available at https://zju3dv.github.io/sam_graph.\n","authors":["Haoyu Guo","He Zhu","Sida Peng","Yuang Wang","Yujun Shen","Ruizhen Hu","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.08372v3.pdf","comment":"Project page: https://zju3dv.github.io/sam_graph"},{"id":"http://arxiv.org/abs/2408.01139v1","updated":"2024-08-02T09:35:06Z","published":"2024-08-02T09:35:06Z","title":"Interpreting Global Perturbation Robustness of Image Models using\n  Axiomatic Spectral Importance Decomposition","summary":"  Perturbation robustness evaluates the vulnerabilities of models, arising from\na variety of perturbations, such as data corruptions and adversarial attacks.\nUnderstanding the mechanisms of perturbation robustness is critical for global\ninterpretability. We present a model-agnostic, global mechanistic\ninterpretability method to interpret the perturbation robustness of image\nmodels. This research is motivated by two key aspects. First, previous global\ninterpretability works, in tandem with robustness benchmarks, e.g. mean\ncorruption error (mCE), are not designed to directly interpret the mechanisms\nof perturbation robustness within image models. Second, we notice that the\nspectral signal-to-noise ratios (SNR) of perturbed natural images exponentially\ndecay over the frequency. This power-law-like decay implies that: Low-frequency\nsignals are generally more robust than high-frequency signals -- yet high\nclassification accuracy can not be achieved by low-frequency signals alone. By\napplying Shapley value theory, our method axiomatically quantifies the\npredictive powers of robust features and non-robust features within an\ninformation theory framework. Our method, dubbed as \\textbf{I-ASIDE}\n(\\textbf{I}mage \\textbf{A}xiomatic \\textbf{S}pectral \\textbf{I}mportance\n\\textbf{D}ecomposition \\textbf{E}xplanation), provides a unique insight into\nmodel robustness mechanisms. We conduct extensive experiments over a variety of\nvision models pre-trained on ImageNet to show that \\textbf{I-ASIDE} can not\nonly \\textbf{measure} the perturbation robustness but also \\textbf{provide\ninterpretations} of its mechanisms.\n","authors":["Risn Luo","James McDermott","Colm O'Riordan"],"pdf_url":"https://arxiv.org/pdf/2408.01139v1.pdf","comment":"Accepted by Transactions on Machine Learning Research (TMLR 2024)"},{"id":"http://arxiv.org/abs/2408.01137v1","updated":"2024-08-02T09:31:21Z","published":"2024-08-02T09:31:21Z","title":"PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting\n  Network","summary":"  We present an advanced study on more challenging high-resolution salient\nobject detection (HRSOD) from both dataset and network framework perspectives.\nTo compensate for the lack of HRSOD dataset, we thoughtfully collect a\nlarge-scale high resolution salient object detection dataset, called UHRSD,\ncontaining 5,920 images from real-world complex scenarios at 4K-8K resolutions.\nAll the images are finely annotated in pixel-level, far exceeding previous\nlow-resolution SOD datasets. Aiming at overcoming the contradiction between the\nsampling depth and the receptive field size in the past methods, we propose a\nnovel one-stage framework for HR-SOD task using pyramid grafting mechanism. In\ngeneral, transformer-based and CNN-based backbones are adopted to extract\nfeatures from different resolution images independently and then these features\nare grafted from transformer branch to CNN branch. An attention-based\nCross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine\nbroken detailed information more holistically, guided by different source\nfeature during decoding process. Moreover, we design an Attention Guided Loss\n(AGL) to explicitly supervise the attention matrix generated by CMGM to help\nthe network better interact with the attention from different branches.\nComprehensive experiments on UHRSD and widely-used SOD datasets demonstrate\nthat our method can simultaneously locate salient object and preserve rich\ndetails, outperforming state-of-the-art methods. To verify the generalization\nability of the proposed framework, we apply it to the camouflaged object\ndetection (COD) task. Notably, our method performs superior to most\nstate-of-the-art COD methods without bells and whistles.\n","authors":["Changqun Xia","Chenxi Xie","Zhentao He","Tianshu Yu","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2408.01137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01126v1","updated":"2024-08-02T09:07:31Z","published":"2024-08-02T09:07:31Z","title":"IG-SLAM: Instant Gaussian SLAM","summary":"  3D Gaussian Splatting has recently shown promising results as an alternative\nscene representation in SLAM systems to neural implicit representations.\nHowever, current methods either lack dense depth maps to supervise the mapping\nprocess or detailed training designs that consider the scale of the\nenvironment. To address these drawbacks, we present IG-SLAM, a dense RGB-only\nSLAM system that employs robust Dense-SLAM methods for tracking and combines\nthem with Gaussian Splatting. A 3D map of the environment is constructed using\naccurate pose and dense depth provided by tracking. Additionally, we utilize\ndepth uncertainty in map optimization to improve 3D reconstruction. Our decay\nstrategy in map optimization enhances convergence and allows the system to run\nat 10 fps in a single process. We demonstrate competitive performance with\nstate-of-the-art RGB-only SLAM systems while achieving faster operation speeds.\nWe present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC\ndatasets. The system achieves photo-realistic 3D reconstruction in large-scale\nsequences, particularly in the EuRoC dataset.\n","authors":["Furkan Aykut Sarikamis","Abdullah Aydin Alatan"],"pdf_url":"https://arxiv.org/pdf/2408.01126v1.pdf","comment":"8 pages, 3 page ref, 5 figures, 3DV submission"},{"id":"http://arxiv.org/abs/2408.01120v1","updated":"2024-08-02T09:01:05Z","published":"2024-08-02T09:01:05Z","title":"An Efficient and Effective Transformer Decoder-Based Framework for\n  Multi-Task Visual Grounding","summary":"  Most advanced visual grounding methods rely on Transformers for\nvisual-linguistic feature fusion. However, these Transformer-based approaches\nencounter a significant drawback: the computational costs escalate\nquadratically due to the self-attention mechanism in the Transformer Encoder,\nparticularly when dealing with high-resolution images or long context\nsentences. This quadratic increase in computational burden restricts the\napplicability of visual grounding to more intricate scenes, such as\nconversation-based reasoning segmentation, which involves lengthy language\nexpressions. In this paper, we propose an efficient and effective multi-task\nvisual grounding (EEVG) framework based on Transformer Decoder to address this\nissue, which reduces the cost in both language and visual aspects. In the\nlanguage aspect, we employ the Transformer Decoder to fuse visual and\nlinguistic features, where linguistic features are input as memory and visual\nfeatures as queries. This allows fusion to scale linearly with language\nexpression length. In the visual aspect, we introduce a parameter-free approach\nto reduce computation by eliminating background visual tokens based on\nattention scores. We then design a light mask head to directly predict\nsegmentation masks from the remaining sparse feature maps. Extensive results\nand ablation studies on benchmarks demonstrate the efficiency and effectiveness\nof our approach. Code is available in https://github.com/chenwei746/EEVG.\n","authors":["Wei Chen","Long Chen","Yu Wu"],"pdf_url":"https://arxiv.org/pdf/2408.01120v1.pdf","comment":"21pages, 10 figures, 9 tables. Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.01099v1","updated":"2024-08-02T08:24:05Z","published":"2024-08-02T08:24:05Z","title":"Contribution-based Low-Rank Adaptation with Pre-training Model for Real\n  Image Restoration","summary":"  Recently, pre-trained model and efficient parameter tuning have achieved\nremarkable success in natural language processing and high-level computer\nvision with the aid of masked modeling and prompt tuning. In low-level computer\nvision, however, there have been limited investigations on pre-trained models\nand even efficient fine-tuning strategy has not yet been explored despite its\nimportance and benefit in various real-world tasks such as alleviating memory\ninflation issue when integrating new tasks on AI edge devices. Here, we propose\na novel efficient parameter tuning approach dubbed contribution-based low-rank\nadaptation (CoLoRA) for multiple image restorations along with effective\npre-training method with random order degradations (PROD). Unlike prior arts\nthat tune all network parameters, our CoLoRA effectively fine-tunes small\namount of parameters by leveraging LoRA (low-rank adaptation) for each new\nvision task with our contribution-based method to adaptively determine layer by\nlayer capacity for that task to yield comparable performance to full tuning.\nFurthermore, our PROD strategy allows to extend the capability of pre-trained\nmodels with improved performance as well as robustness to bridge synthetic\npre-training and real-world fine-tuning. Our CoLoRA with PROD has demonstrated\nits superior performance in various image restoration tasks across diverse\ndegradation types on both synthetic and real-world datasets for known and novel\ntasks.\n","authors":["Donwon Park","Hayeon Kim","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2408.01099v1.pdf","comment":"33 pages, 15 figures, for homepage see this url :\n  https://janeyeon.github.io/colora/"},{"id":"http://arxiv.org/abs/2405.04788v3","updated":"2024-08-02T08:22:12Z","published":"2024-05-08T03:43:58Z","title":"SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised\n  Change Detector","summary":"  Change Detection (CD) aims to identify pixels with semantic changes between\nimages. However, annotating massive numbers of pixel-level images is\nlabor-intensive and costly, especially for multi-temporal images, which require\npixel-wise comparisons by human experts. Considering the excellent performance\nof visual language models (VLMs) for zero-shot, open-vocabulary, etc. with\nprompt-based reasoning, it is promising to utilize VLMs to make better CD under\nlimited labeled data. In this paper, we propose a VLM guidance-based\nsemi-supervised CD method, namely SemiCD-VL. The insight of SemiCD-VL is to\nsynthesize free change labels using VLMs to provide additional supervision\nsignals for unlabeled data. However, almost all current VLMs are designed for\nsingle-temporal images and cannot be directly applied to bi- or multi-temporal\nimages. Motivated by this, we first propose a VLM-based mixed change event\ngeneration (CEG) strategy to yield pseudo labels for unlabeled CD data. Since\nthe additional supervised signals provided by these VLM-driven pseudo labels\nmay conflict with the pseudo labels from the consistency regularization\nparadigm (e.g. FixMatch), we propose the dual projection head for de-entangling\ndifferent signal sources. Further, we explicitly decouple the bi-temporal\nimages semantic representation through two auxiliary segmentation decoders,\nwhich are also guided by VLM. Finally, to make the model more adequately\ncapture change representations, we introduce metric-aware supervision by\nfeature-level contrastive loss in auxiliary branches. Extensive experiments\nshow the advantage of SemiCD-VL. For instance, SemiCD-VL improves the FixMatch\nbaseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels. In\naddition, our CEG strategy, in an un-supervised manner, can achieve performance\nfar superior to state-of-the-art un-supervised CD methods.\n","authors":["Kaiyu Li","Xiangyong Cao","Yupeng Deng","Junmin Liu","Deyu Meng","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2405.04788v3.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.01089v1","updated":"2024-08-02T08:08:56Z","published":"2024-08-02T08:08:56Z","title":"Prototypical Partial Optimal Transport for Universal Domain Adaptation","summary":"  Universal domain adaptation (UniDA) aims to transfer knowledge from a labeled\nsource domain to an unlabeled target domain without requiring the same label\nsets of both domains. The existence of domain and category shift makes the task\nchallenging and requires us to distinguish \"known\" samples (i.e., samples whose\nlabels exist in both domains) and \"unknown\" samples (i.e., samples whose labels\nexist in only one domain) in both domains before reducing the domain gap. In\nthis paper, we consider the problem from the point of view of distribution\nmatching which we only need to align two distributions partially. A novel\napproach, dubbed mini-batch Prototypical Partial Optimal Transport (m-PPOT), is\nproposed to conduct partial distribution alignment for UniDA. In training\nphase, besides minimizing m-PPOT, we also leverage the transport plan of m-PPOT\nto reweight source prototypes and target samples, and design reweighted entropy\nloss and reweighted cross-entropy loss to distinguish \"known\" and \"unknown\"\nsamples. Experiments on four benchmarks show that our method outperforms the\nprevious state-of-the-art UniDA methods.\n","authors":["Yucheng Yang","Xiang Gu","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01085v1","updated":"2024-08-02T08:06:12Z","published":"2024-08-02T08:06:12Z","title":"Effect of Fog Particle Size Distribution on 3D Object Detection Under\n  Adverse Weather Conditions","summary":"  LiDAR-based sensors employing optical spectrum signals play a vital role in\nproviding significant information about the target objects in autonomous\ndriving vehicle systems. However, the presence of fog in the atmosphere\nseverely degrades the overall system's performance. This manuscript analyzes\nthe role of fog particle size distributions in 3D object detection under\nadverse weather conditions. We utilise Mie theory and meteorological optical\nrange (MOR) to calculate the attenuation and backscattering coefficient values\nfor point cloud generation and analyze the overall system's accuracy in Car,\nCyclist, and Pedestrian case scenarios under easy, medium and hard detection\ndifficulties. Gamma and Junge (Power-Law) distributions are employed to\nmathematically model the fog particle size distribution under strong and\nmoderate advection fog environments. Subsequently, we modified the KITTI\ndataset based on the backscattering coefficient values and trained it on the\nPV-RCNN++ deep neural network model for Car, Cyclist, and Pedestrian cases\nunder different detection difficulties. The result analysis shows a significant\nvariation in the system's accuracy concerning the changes in target object\ndimensionality, the nature of the fog environment and increasing detection\ndifficulties, with the Car exhibiting the highest accuracy of around 99% and\nthe Pedestrian showing the lowest accuracy of around 73%.\n","authors":["Ajinkya Shinde","Gaurav Sharma","Manisha Pattanaik","Sri Niwas Singh"],"pdf_url":"https://arxiv.org/pdf/2408.01085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01080v1","updated":"2024-08-02T07:57:06Z","published":"2024-08-02T07:57:06Z","title":"FCDFusion: a Fast, Low Color Deviation Method for Fusing Visible and\n  Infrared Image Pairs","summary":"  Visible and infrared image fusion (VIF) aims to combine information from\nvisible and infrared images into a single fused image. Previous VIF methods\nusually employ a color space transformation to keep the hue and saturation from\nthe original visible image. However, for fast VIF methods, this operation\naccounts for the majority of the calculation and is the bottleneck preventing\nfaster processing. In this paper, we propose a fast fusion method, FCDFusion,\nwith little color deviation. It preserves color information without color space\ntransformations, by directly operating in RGB color space. It incorporates\ngamma correction at little extra cost, allowing color and contrast to be\nrapidly improved. We regard the fusion process as a scaling operation on 3D\ncolor vectors, greatly simplifying the calculations. A theoretical analysis and\nexperiments show that our method can achieve satisfactory results in only 7\nFLOPs per pixel. Compared to state-of-the-art fast, color-preserving methods\nusing HSV color space, our method provides higher contrast at only half of the\ncomputational cost. We further propose a new metric, color deviation, to\nmeasure the ability of a VIF method to preserve color. It is specifically\ndesigned for VIF tasks with color visible-light images, and overcomes\ndeficiencies of existing VIF metrics used for this purpose. Our code is\navailable at https://github.com/HeasonLee/FCDFusion.\n","authors":["Hesong Li","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2408.01080v1.pdf","comment":"This article has been accepted by Computational Visual Media"},{"id":"http://arxiv.org/abs/2408.01077v1","updated":"2024-08-02T07:52:28Z","published":"2024-08-02T07:52:28Z","title":"PhysMamba: Leveraging Dual-Stream Cross-Attention SSD for Remote\n  Physiological Measurement","summary":"  Remote Photoplethysmography (rPPG) is a non-contact technique for extracting\nphysiological signals from facial videos, used in applications like emotion\nmonitoring, medical assistance, and anti-face spoofing. Unlike controlled\nlaboratory settings, real-world environments often contain motion artifacts and\nnoise, affecting the performance of existing methods. To address this, we\npropose PhysMamba, a dual-stream time-frequency interactive model based on\nMamba. PhysMamba integrates the state-of-the-art Mamba-2 model and employs a\ndual-stream architecture to learn diverse rPPG features, enhancing robustness\nin noisy conditions. Additionally, we designed the Cross-Attention State Space\nDuality (CASSD) module to improve information exchange and feature\ncomplementarity between the two streams. We validated PhysMamba using PURE,\nUBFC-rPPG and MMPD. Experimental results show that PhysMamba achieves\nstate-of-the-art performance across various scenarios, particularly in complex\nenvironments, demonstrating its potential in practical remote heart rate\nmonitoring applications.\n","authors":["Zhixin Yan","Yan Zhong","Wenjun Zhang","Lin Shu","Hongbin Xu","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2408.01077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01076v1","updated":"2024-08-02T07:51:44Z","published":"2024-08-02T07:51:44Z","title":"Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for\n  Continual Learning","summary":"  Deep neural networks (DNNs) excel on fixed datasets but struggle with\nincremental and shifting data in real-world scenarios. Continual learning\naddresses this challenge by allowing models to learn from new data while\nretaining previously learned knowledge. Existing methods mainly rely on visual\nfeatures, often neglecting the rich semantic information encoded in text. The\nsemantic knowledge available in the label information of the images, offers\nimportant semantic information that can be related with previously acquired\nknowledge of semantic classes. Consequently, effectively leveraging this\ninformation throughout continual learning is expected to be beneficial. To\naddress this, we propose integrating semantic guidance within and across tasks\nby capturing semantic similarity using text embeddings. We start from a\npre-trained CLIP model, employ the \\emph{Semantically-guided Representation\nLearning (SG-RL)} module for a soft-assignment towards all current task\nclasses, and use the Semantically-guided Knowledge Distillation (SG-KD) module\nfor enhanced knowledge transfer. Experimental results demonstrate the\nsuperiority of our method on general and fine-grained datasets. Our code can be\nfound in\nhttps://github.com/aprilsveryown/semantically-guided-continual-learning.\n","authors":["Lu Yu","Zhe Tao","Hantao Yao","Joost Van de Weijer","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2408.01076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01067v1","updated":"2024-08-02T07:40:34Z","published":"2024-08-02T07:40:34Z","title":"Amodal Segmentation for Laparoscopic Surgery Video Instruments","summary":"  Segmentation of surgical instruments is crucial for enhancing surgeon\nperformance and ensuring patient safety. Conventional techniques such as\nbinary, semantic, and instance segmentation share a common drawback: they do\nnot accommodate the parts of instruments obscured by tissues or other\ninstruments. Precisely predicting the full extent of these occluded instruments\ncan significantly improve laparoscopic surgeries by providing critical guidance\nduring operations and assisting in the analysis of potential surgical errors,\nas well as serving educational purposes. In this paper, we introduce Amodal\nSegmentation to the realm of surgical instruments in the medical field. This\ntechnique identifies both the visible and occluded parts of an object. To\nachieve this, we introduce a new Amoal Instruments Segmentation (AIS) dataset,\nwhich was developed by reannotating each instrument with its complete mask,\nutilizing the 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge\ndataset. Additionally, we evaluate several leading amodal segmentation methods\nto establish a benchmark for this new dataset.\n","authors":["Ruohua Shi","Zhaochen Liu","Lingyu Duan","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.01067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02371v2","updated":"2024-08-02T07:14:59Z","published":"2024-07-02T15:40:29Z","title":"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video\n  Generation","summary":"  Text-to-video (T2V) generation has recently garnered significant attention\nthanks to the large multi-modality model Sora. However, T2V generation still\nfaces two important challenges: 1) Lacking a precise open sourced high-quality\ndataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,\nare either with low quality or too large for most research institutions.\nTherefore, it is challenging but crucial to collect a precise high-quality\ntext-video pairs for T2V generation. 2) Ignoring to fully utilize textual\ninformation. Recent T2V methods have focused on vision transformers, using a\nsimple cross attention module for video generation, which falls short of\nthoroughly extracting semantic information from text prompt. To address these\nissues, we introduce OpenVid-1M, a precise high-quality dataset with expressive\ncaptions. This open-scenario dataset contains over 1 million text-video pairs,\nfacilitating research on T2V generation. Furthermore, we curate 433K 1080p\nvideos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition\nvideo generation. Additionally, we propose a novel Multi-modal Video Diffusion\nTransformer (MVDiT) capable of mining both structure information from visual\ntokens and semantic information from text tokens. Extensive experiments and\nablation studies verify the superiority of OpenVid-1M over previous datasets\nand the effectiveness of our MVDiT.\n","authors":["Kepan Nan","Rui Xie","Penghao Zhou","Tiehan Fan","Zhenheng Yang","Zhijie Chen","Xiang Li","Jian Yang","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2407.02371v2.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.19397v2","updated":"2024-08-02T06:53:08Z","published":"2024-07-28T04:46:55Z","title":"Domain Adaptive Lung Nodule Detection in X-ray Image","summary":"  Medical images from different healthcare centers exhibit varied data\ndistributions, posing significant challenges for adapting lung nodule detection\ndue to the domain shift between training and application phases. Traditional\nunsupervised domain adaptive detection methods often struggle with this shift,\nleading to suboptimal outcomes. To overcome these challenges, we introduce a\nnovel domain adaptive approach for lung nodule detection that leverages mean\nteacher self-training and contrastive learning. First, we propose a\nhierarchical contrastive learning strategy to refine nodule representations and\nenhance the distinction between nodules and background. Second, we introduce a\nnodule-level domain-invariant feature learning (NDL) module to capture\ndomain-invariant features through adversarial learning across different\ndomains. Additionally, we propose a new annotated dataset of X-ray images to\naid in advancing lung nodule detection research. Extensive experiments\nconducted on multiple X-ray datasets demonstrate the efficacy of our approach\nin mitigating domain shift impacts.\n","authors":["Haifeng Zhao","Lixiang Jiang","Leilei Ma","Dengdi Sun","Yanping Fu"],"pdf_url":"https://arxiv.org/pdf/2407.19397v2.pdf","comment":"This paper will submit to IEEE SMC 2024"},{"id":"http://arxiv.org/abs/2408.01044v1","updated":"2024-08-02T06:32:45Z","published":"2024-08-02T06:32:45Z","title":"Boosting Gaze Object Prediction via Pixel-level Supervision from Vision\n  Foundation Model","summary":"  Gaze object prediction (GOP) aims to predict the category and location of the\nobject that a human is looking at. Previous methods utilized box-level\nsupervision to identify the object that a person is looking at, but struggled\nwith semantic ambiguity, ie, a single box may contain several items since\nobjects are close together. The Vision foundation model (VFM) has improved in\nobject segmentation using box prompts, which can reduce confusion by more\nprecisely locating objects, offering advantages for fine-grained prediction of\ngaze objects. This paper presents a more challenging gaze object segmentation\n(GOS) task, which involves inferring the pixel-level mask corresponding to the\nobject captured by human gaze behavior. In particular, we propose that the\npixel-level supervision provided by VFM can be integrated into gaze object\nprediction to mitigate semantic ambiguity. This leads to our gaze object\ndetection and segmentation framework that enables accurate pixel-level\npredictions. Different from previous methods that require additional head input\nor ignore head features, we propose to automatically obtain head features from\nscene features to ensure the model's inference efficiency and flexibility in\nthe real world. Moreover, rather than directly fuse features to predict gaze\nheatmap as in existing methods, which may overlook spatial location and subtle\ndetails of the object, we develop a space-to-object gaze regression method to\nfacilitate human-object gaze interaction. Specifically, it first constructs an\ninitial human-object spatial connection, then refines this connection by\ninteracting with semantically clear features in the segmentation branch,\nultimately predicting a gaze heatmap for precise localization. Extensive\nexperiments on GOO-Synth and GOO-Real datasets demonstrate the effectiveness of\nour method.\n","authors":["Yang Jin","Lei Zhang","Shi Yan","Bin Fan","Binglu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01044v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.01040v1","updated":"2024-08-02T06:24:39Z","published":"2024-08-02T06:24:39Z","title":"Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix","summary":"  In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.\n","authors":["Seungeun Oh","Sihun Baek","Jihong Park","Hyelin Nam","Praneeth Vepakomma","Ramesh Raskar","Mehdi Bennis","Seong-Lyun Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01040v1.pdf","comment":"23 pages, 11 figures, 8 tables, to be published in Transactions on\n  Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2408.01037v1","updated":"2024-08-02T06:20:48Z","published":"2024-08-02T06:20:48Z","title":"MambaST: A Plug-and-Play Cross-Spectral Spatial-Temporal Fuser for\n  Efficient Pedestrian Detection","summary":"  This paper proposes MambaST, a plug-and-play cross-spectral spatial-temporal\nfusion pipeline for efficient pedestrian detection. Several challenges exist\nfor pedestrian detection in autonomous driving applications. First, it is\ndifficult to perform accurate detection using RGB cameras under dark or\nlow-light conditions. Cross-spectral systems must be developed to integrate\ncomplementary information from multiple sensor modalities, such as thermal and\nvisible cameras, to improve the robustness of the detections. Second,\npedestrian detection models are latency-sensitive. Efficient and easy-to-scale\ndetection models with fewer parameters are highly desirable for real-time\napplications such as autonomous driving. Third, pedestrian video data provides\nspatial-temporal correlations of pedestrian movement. It is beneficial to\nincorporate temporal as well as spatial information to enhance pedestrian\ndetection. This work leverages recent advances in the state space model (Mamba)\nand proposes a novel Multi-head Hierarchical Patching and Aggregation (MHHPA)\nstructure to extract both fine-grained and coarse-grained information from both\nRGB and thermal imagery. Experimental results show that the proposed MHHPA is\nan effective and efficient alternative to a Transformer model for\ncross-spectral pedestrian detection. Our proposed model also achieves superior\nperformance on small-scale pedestrian detection. The code is available at\nhttps://github.com/XiangboGaoBarry/MambaST}{https://github.com/XiangboGaoBarry/MambaST.\n","authors":["Xiangbo Gao","Asiegbu Miracle Kanu-Asiegbu","Xiaoxiao Du"],"pdf_url":"https://arxiv.org/pdf/2408.01037v1.pdf","comment":"ITSC 2024 Accepted"},{"id":"http://arxiv.org/abs/2408.01035v1","updated":"2024-08-02T06:18:39Z","published":"2024-08-02T06:18:39Z","title":"Structure from Motion-based Motion Estimation and 3D Reconstruction of\n  Unknown Shaped Space Debris","summary":"  With the boost in the number of spacecraft launches in the current decades,\nthe space debris problem is daily becoming significantly crucial. For\nsustainable space utilization, the continuous removal of space debris is the\nmost severe problem for humanity. To maximize the reliability of the debris\ncapture mission in orbit, accurate motion estimation of the target is\nessential. Space debris has lost its attitude and orbit control capabilities,\nand its shape is unknown due to the break. This paper proposes the Structure\nfrom Motion-based algorithm to perform unknown shaped space debris motion\nestimation with limited resources, where only 2D images are required as input.\nThe method then outputs the reconstructed shape of the unknown object and the\nrelative pose trajectory between the target and the camera simultaneously,\nwhich are exploited to estimate the target's motion. The method is\nquantitatively validated with the realistic image dataset generated by the\nmicrogravity experiment in a 2D air-floating testbed and 3D kinematic\nsimulation.\n","authors":["Kentaro Uno","Takehiro Matsuoka","Akiyoshi Uchida","Kazuya Yoshida"],"pdf_url":"https://arxiv.org/pdf/2408.01035v1.pdf","comment":"6 pages, 10 figures. Manuscript accepted at the 2024 IEEE 20th\n  International Conference on Automation Science and Engineerin (CASE 2024)"},{"id":"http://arxiv.org/abs/2408.01031v1","updated":"2024-08-02T06:13:29Z","published":"2024-08-02T06:13:29Z","title":"POA: Pre-training Once for Models of All Sizes","summary":"  Large-scale self-supervised pre-training has paved the way for one foundation\nmodel to handle many different vision tasks. Most pre-training methodologies\ntrain a single model of a certain size at one time. Nevertheless, various\ncomputation or storage constraints in real-world scenarios require substantial\nefforts to develop a series of models with different sizes to deploy. Thus, in\nthis study, we propose a novel tri-branch self-supervised training framework,\ntermed as POA (Pre-training Once for All), to tackle this aforementioned issue.\nOur approach introduces an innovative elastic student branch into a modern\nself-distillation paradigm. At each pre-training step, we randomly sample a\nsub-network from the original student to form the elastic student and train all\nbranches in a self-distilling fashion. Once pre-trained, POA allows the\nextraction of pre-trained models of diverse sizes for downstream tasks.\nRemarkably, the elastic student facilitates the simultaneous pre-training of\nmultiple models with different sizes, which also acts as an additional ensemble\nof models of various sizes to enhance representation learning. Extensive\nexperiments, including k-nearest neighbors, linear probing evaluation and\nassessments on multiple downstream tasks demonstrate the effectiveness and\nadvantages of our POA. It achieves state-of-the-art performance using ViT, Swin\nTransformer and ResNet backbones, producing around a hundred models with\ndifferent sizes through a single pre-training session. The code is available\nat: https://github.com/Qichuzyy/POA.\n","authors":["Yingying Zhang","Xin Guo","Jiangwei Lao","Lei Yu","Lixiang Ru","Jian Wang","Guo Ye","Huimei He","Jingdong Chen","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2408.01031v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.01026v1","updated":"2024-08-02T05:50:49Z","published":"2024-08-02T05:50:49Z","title":"PINNs for Medical Image Analysis: A Survey","summary":"  The incorporation of physical information in machine learning frameworks is\ntransforming medical image analysis (MIA). By integrating fundamental knowledge\nand governing physical laws, these models achieve enhanced robustness and\ninterpretability. In this work, we explore the utility of physics-informed\napproaches for MIA (PIMIA) tasks such as registration, generation,\nclassification, and reconstruction. We present a systematic literature review\nof over 80 papers on physics-informed methods dedicated to MIA. We propose a\nunified taxonomy to investigate what physics knowledge and processes are\nmodelled, how they are represented, and the strategies to incorporate them into\nMIA models. We delve deep into a wide range of image analysis tasks, from\nimaging, generation, prediction, inverse imaging (super-resolution and\nreconstruction), registration, and image analysis (segmentation and\nclassification). For each task, we thoroughly examine and present in a tabular\nformat the central physics-guided operation, the region of interest (with\nrespect to human anatomy), the corresponding imaging modality, the dataset used\nfor model training, the deep network architecture employed, and the primary\nphysical process, equation, or principle utilized. Additionally, we also\nintroduce a novel metric to compare the performance of PIMIA methods across\ndifferent tasks and datasets. Based on this review, we summarize and distil our\nperspectives on the challenges, open research questions, and directions for\nfuture research. We highlight key open challenges in PIMIA, including selecting\nsuitable physics priors and establishing a standardized benchmarking platform.\n","authors":["Chayan Banerjee","Kien Nguyen","Olivier Salvado","Truyen Tran","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2408.01026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03298v2","updated":"2024-08-02T05:26:14Z","published":"2024-06-05T14:08:13Z","title":"L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap\n  Multiview Point Cloud Registration","summary":"  Point cloud registration is a prerequisite for many applications in computer\nvision and robotics. Most existing methods focus on pairwise registration of\ntwo point clouds with high overlap. Although there have been some methods for\nlow overlap cases, they struggle in degraded scenarios. This paper introduces a\nnovel framework dubbed L-PR, designed to register unordered low overlap\nmultiview point clouds leveraging LiDAR fiducial markers. We refer to them as\nLiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco\nmarkers, thin sheets of paper that do not affect the 3D geometry of the\nenvironment. We first propose an improved adaptive threshold marker detection\nmethod to provide robust detection results when the viewpoints among point\nclouds change dramatically. Then, we formulate the unordered multiview point\ncloud registration problem as a maximum a-posteriori (MAP) problem and develop\na framework consisting of two levels of graphs to address it. The first-level\ngraph, constructed as a weighted graph, is designed to efficiently and\noptimally infer initial values of scan poses from the unordered set. The\nsecond-level graph is constructed as a factor graph. By globally optimizing the\nvariables on the graph, including scan poses, marker poses, and marker corner\npositions, we tackle the MAP problem. We conduct both qualitative and\nquantitative experiments to demonstrate that the proposed method surpasses\nprevious state-of-the-art (SOTA) methods and to showcase that L-PR can serve as\na low-cost and efficient tool for 3D asset collection and training data\ncollection. In particular, we collect a new dataset named Livox-3DMatch using\nL-PR and incorporate it into the training of the SOTA learning-based method,\nSGHR, which brings evident improvements for SGHR on various benchmarks.\n","authors":["Yibo Liu","Jinjun Shan","Amaldev Haridevan","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03298v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.01014v1","updated":"2024-08-02T05:17:14Z","published":"2024-08-02T05:17:14Z","title":"EIUP: A Training-Free Approach to Erase Non-Compliant Concepts\n  Conditioned on Implicit Unsafe Prompts","summary":"  Text-to-image diffusion models have shown the ability to learn a diverse\nrange of concepts. However, it is worth noting that they may also generate\nundesirable outputs, consequently giving rise to significant security concerns.\nSpecifically, issues such as Not Safe for Work (NSFW) content and potential\nviolations of style copyright may be encountered. Since image generation is\nconditioned on text, prompt purification serves as a straightforward solution\nfor content safety. Similar to the approach taken by LLM, some efforts have\nbeen made to control the generation of safe outputs by purifying prompts.\nHowever, it is also important to note that even with these efforts, non-toxic\ntext still carries a risk of generating non-compliant images, which is referred\nto as implicit unsafe prompts. Furthermore, some existing works fine-tune the\nmodels to erase undesired concepts from model weights. This type of method\nnecessitates multiple training iterations whenever the concept is updated,\nwhich can be time-consuming and may potentially lead to catastrophic\nforgetting. To address these challenges, we propose a simple yet effective\napproach that incorporates non-compliant concepts into an erasure prompt. This\nerasure prompt proactively participates in the fusion of image spatial features\nand text embeddings. Through attention mechanisms, our method is capable of\nidentifying feature representations of non-compliant concepts in the image\nspace. We re-weight these features to effectively suppress the generation of\nunsafe images conditioned on original implicit unsafe prompts. Our method\nexhibits superior erasure effectiveness while achieving high scores in image\nfidelity compared to the state-of-the-art baselines. WARNING: This paper\ncontains model outputs that may be offensive.\n","authors":["Die Chen","Zhiwen Li","Mingyuan Fan","Cen Chen","Wenmeng Zhou","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2408.01014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00228v3","updated":"2024-08-02T05:17:10Z","published":"2024-03-01T02:19:40Z","title":"DISORF: A Distributed Online 3D Reconstruction Framework for Mobile\n  Robots","summary":"  We present a framework, DISORF, to enable online 3D reconstruction and\nvisualization of scenes captured by resource-constrained mobile robots and edge\ndevices. To address the limited computing capabilities of edge devices and\npotentially limited network availability, we design a framework that\nefficiently distributes computation between the edge device and the remote\nserver. We leverage on-device SLAM systems to generate posed keyframes and\ntransmit them to remote servers that can perform high-quality 3D reconstruction\nand visualization at runtime by leveraging recent advances in neural 3D\nmethods. We identify a key challenge with online training where naive image\nsampling strategies can lead to significant degradation in rendering quality.\nWe propose a novel shifted exponential frame sampling method that addresses\nthis challenge for online training. We demonstrate the effectiveness of our\nframework in enabling high-quality real-time reconstruction and visualization\nof unknown scenes as they are captured and streamed from cameras in mobile\nrobots and edge devices.\n","authors":["Chunlin Li","Hanrui Fan","Xiaorui Huang","Ruofan Liang","Sankeerth Durvasula","Nandita Vijaykumar"],"pdf_url":"https://arxiv.org/pdf/2403.00228v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21450v2","updated":"2024-08-02T05:01:38Z","published":"2024-07-31T08:54:50Z","title":"Forecasting Future Videos from Novel Views via Disentangled 3D Scene\n  Representation","summary":"  Video extrapolation in space and time (VEST) enables viewers to forecast a 3D\nscene into the future and view it from novel viewpoints. Recent methods propose\nto learn an entangled representation, aiming to model layered scene geometry,\nmotion forecasting and novel view synthesis together, while assuming simplified\naffine motion and homography-based warping at each scene layer, leading to\ninaccurate video extrapolation. Instead of entangled scene representation and\nrendering, our approach chooses to disentangle scene geometry from scene\nmotion, via lifting the 2D scene to 3D point clouds, which enables high quality\nrendering of future videos from novel views. To model future 3D scene motion,\nwe propose a disentangled two-stage approach that initially forecasts\nego-motion and subsequently the residual motion of dynamic objects (e.g., cars,\npeople). This approach ensures more precise motion predictions by reducing\ninaccuracies from entanglement of ego-motion with dynamic object motion, where\nbetter ego-motion forecasting could significantly enhance the visual outcomes.\nExtensive experimental analysis on two urban scene datasets demonstrate\nsuperior performance of our proposed method in comparison to strong baselines.\n","authors":["Sudhir Yarram","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2407.21450v2.pdf","comment":"Accepted to ECCV 2024. Project Page:\n  https://skrya.github.io/projects/ffn-dsr/"},{"id":"http://arxiv.org/abs/2408.00998v1","updated":"2024-08-02T04:13:38Z","published":"2024-08-02T04:13:38Z","title":"FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation","summary":"  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nextraordinary image generation based on natural-language text prompts. However,\nthe issue of lacking controllability of such models restricts their practical\napplicability for real-life content creation, for which attention has been\nfocused on leveraging a reference image to control text-to-image synthesis. Due\nto the close correlation between the reference image and the generated image,\nthis problem can also be regarded as the task of manipulating (or editing) the\nreference image as per the text, namely text-driven image-to-image translation.\nThis paper contributes a novel, concise, and efficient approach that adapts the\npre-trained large-scale text-to-image (T2I) diffusion model to the\nimage-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality\nand versatile text-driven I2I translation without any model training, model\nfine-tuning, or online optimization process. To guide T2I generation with a\nreference image, we propose to model diverse guiding factors with\ncorrespondingly different frequency bands of diffusion features in the DCT\nspectral space, and accordingly devise a novel frequency band substitution\nlayer that dynamically substitutes a certain DCT frequency band of the\ndiffusion features with the corresponding counterpart of the reference image\nalong the reverse sampling process. We demonstrate that our method flexibly\nenables highly controllable text-driven I2I translation both in the guiding\nfactor and guiding intensity of the reference image, simply by tuning the type\nand bandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify the superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability.\n","authors":["Xiang Gao","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00998v1.pdf","comment":"Accepted conference paper of ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.12939v3","updated":"2024-08-02T03:33:17Z","published":"2024-07-17T18:10:40Z","title":"GenRC: Generative 3D Room Completion from Sparse Image Collections","summary":"  Sparse RGBD scene completion is a challenging task especially when\nconsidering consistent textures and geometries throughout the entire scene.\nDifferent from existing solutions that rely on human-designed text prompts or\npredefined camera trajectories, we propose GenRC, an automated training-free\npipeline to complete a room-scale 3D mesh with high-fidelity textures. To\nachieve this, we first project the sparse RGBD images to a highly incomplete 3D\nmesh. Instead of iteratively generating novel views to fill in the void, we\nutilized our proposed E-Diffusion to generate a view-consistent panoramic RGBD\nimage which ensures global geometry and appearance consistency. Furthermore, we\nmaintain the input-output scene stylistic consistency through textual inversion\nto replace human-designed text prompts. To bridge the domain gap among\ndatasets, E-Diffusion leverages models trained on large-scale datasets to\ngenerate diverse appearances. GenRC outperforms state-of-the-art methods under\nmost appearance and geometric metrics on ScanNet and ARKitScenes datasets, even\nthough GenRC is not trained on these datasets nor using predefined camera\ntrajectories. Project page: https://minfenli.github.io/GenRC\n","authors":["Ming-Feng Li","Yueh-Feng Ku","Hong-Xuan Yen","Chi Liu","Yu-Lun Liu","Albert Y. C. Chen","Cheng-Hao Kuo","Min Sun"],"pdf_url":"https://arxiv.org/pdf/2407.12939v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2406.08782v2","updated":"2024-08-02T01:56:17Z","published":"2024-06-13T03:27:01Z","title":"Hybrid Spatial-spectral Neural Network for Hyperspectral Image Denoising","summary":"  Hyperspectral image (HSI) denoising is an essential procedure for HSI\napplications. Unfortunately, the existing Transformer-based methods mainly\nfocus on non-local modeling, neglecting the importance of locality in image\ndenoising. Moreover, deep learning methods employ complex spectral learning\nmechanisms, thus introducing large computation costs.\n  To address these problems, we propose a hybrid spatial-spectral denoising\nnetwork (HSSD), in which we design a novel hybrid dual-path network inspired by\nCNN and Transformer characteristics, leading to capturing both local and\nnon-local spatial details while suppressing noise efficiently. Furthermore, to\nreduce computational complexity, we adopt a simple but effective decoupling\nstrategy that disentangles the learning of space and spectral channels, where\nmultilayer perception with few parameters is utilized to learn the global\ncorrelations among spectra. The synthetic and real experiments demonstrate that\nour proposed method outperforms state-of-the-art methods on spatial and\nspectral reconstruction. The code and details are available on\nhttps://github.com/HLImg/HSSD.\n","authors":["Hao Liang"," Chengjie","Kun Li","Xin Tian"],"pdf_url":"https://arxiv.org/pdf/2406.08782v2.pdf","comment":"There are some errors in professional theory"},{"id":"http://arxiv.org/abs/2407.11590v3","updated":"2024-08-02T01:36:59Z","published":"2024-07-16T10:50:10Z","title":"Rethinking Learned Image Compression: Context is All You Need","summary":"  Since LIC has made rapid progress recently compared to traditional methods,\nthis paper attempts to discuss the question about 'Where is the boundary of\nLearned Image Compression(LIC)?'. Thus this paper splits the above problem into\ntwo sub-problems:1)Where is the boundary of rate-distortion performance of\nPSNR? 2)How to further improve the compression gain and achieve the boundary?\nTherefore this paper analyzes the effectiveness of scaling parameters for\nencoder, decoder and context model, which are the three components of LIC. Then\nwe conclude that scaling for LIC is to scale for context model and decoder\nwithin LIC. Extensive experiments demonstrate that overfitting can actually\nserve as an effective context. By optimizing the context, this paper further\nimproves PSNR and achieves state-of-the-art performance, showing a performance\ngain of 14.39% with BD-RATE over VVC.\n","authors":["Jixiang Luo"],"pdf_url":"https://arxiv.org/pdf/2407.11590v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03452v2","updated":"2024-08-02T01:33:25Z","published":"2023-09-07T02:26:55Z","title":"Multimodal Guidance Network for Missing-Modality Inference in Content\n  Moderation","summary":"  Multimodal deep learning, especially vision-language models, have gained\nsignificant traction in recent years, greatly improving performance on many\ndownstream tasks, including content moderation and violence detection. However,\nstandard multimodal approaches often assume consistent modalities between\ntraining and inference, limiting applications in many real-world use cases, as\nsome modalities may not be available during inference. While existing research\nmitigates this problem through reconstructing the missing modalities, they\nunavoidably increase unnecessary computational cost, which could be just as\ncritical, especially for large, deployed infrastructures in industry. To this\nend, we propose a novel guidance network that promotes knowledge sharing during\ntraining, taking advantage of the multimodal representations to train better\nsingle-modality models to be used for inference. Real-world experiments in\nviolence detection shows that our proposed framework trains single-modality\nmodels that significantly outperform traditionally trained counterparts, while\navoiding increases in computational cost for inference.\n","authors":["Zhuokai Zhao","Harish Palani","Tianyi Liu","Lena Evans","Ruth Toner"],"pdf_url":"https://arxiv.org/pdf/2309.03452v2.pdf","comment":"ICME 2024 Camera Ready. Code is available at\n  https://github.com/zhuokaizhao/multimodal-guidance-network"},{"id":"http://arxiv.org/abs/2408.00969v1","updated":"2024-08-02T01:29:43Z","published":"2024-08-02T01:29:43Z","title":"Visible-Thermal Multiple Object Tracking: Large-scale Video Dataset and\n  Progressive Fusion Approach","summary":"  The complementary benefits from visible and thermal infrared data are widely\nutilized in various computer vision task, such as visual tracking, semantic\nsegmentation and object detection, but rarely explored in Multiple Object\nTracking (MOT). In this work, we contribute a large-scale Visible-Thermal video\nbenchmark for MOT, called VT-MOT. VT-MOT has the following main advantages. 1)\nThe data is large scale and high diversity. VT-MOT includes 582 video sequence\npairs, 401k frame pairs from surveillance, drone, and handheld platforms. 2)\nThe cross-modal alignment is highly accurate. We invite several professionals\nto perform both spatial and temporal alignment frame by frame. 3) The\nannotation is dense and high-quality. VT-MOT has 3.99 million annotation boxes\nannotated and double-checked by professionals, including heavy occlusion and\nobject re-acquisition (object disappear and reappear) challenges. To provide a\nstrong baseline, we design a simple yet effective tracking framework, which\neffectively fuses temporal information and complementary information of two\nmodalities in a progressive manner, for robust visible-thermal MOT. A\ncomprehensive experiment are conducted on VT-MOT and the results prove the\nsuperiority and effectiveness of the proposed method compared with\nstate-of-the-art methods. From the evaluation results and analysis, we specify\nseveral potential future directions for visible-thermal MOT. The project is\nreleased in https://github.com/wqw123wqw/PFTrack.\n","authors":["Yabin Zhu","Qianwu Wang","Chenglong Li","Jin Tang","Zhixiang Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00967v1","updated":"2024-08-02T01:24:01Z","published":"2024-08-02T01:24:01Z","title":"Extracting Object Heights From LiDAR & Aerial Imagery","summary":"  This work shows a procedural method for extracting object heights from LiDAR\nand aerial imagery. We discuss how to get heights and the future of LiDAR and\nimagery processing. SOTA object segmentation allows us to take get object\nheights with no deep learning background. Engineers will be keeping track of\nworld data across generations and reprocessing them. They will be using older\nprocedural methods like this paper and newer ones discussed here. SOTA methods\nare going beyond analysis and into generative AI. We cover both a procedural\nmethodology and the newer ones performed with language models. These include\npoint cloud, imagery and text encoding allowing for spatially aware AI.\n","authors":["Jesus Guerrero"],"pdf_url":"https://arxiv.org/pdf/2408.00967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13186v2","updated":"2024-08-02T00:56:39Z","published":"2023-11-22T06:26:24Z","title":"Applications of Spiking Neural Networks in Visual Place Recognition","summary":"  In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for\ntheir largely-unrealized potential energy efficiency and low latency\nparticularly when implemented on neuromorphic hardware. Our paper highlights\nthree advancements for SNNs in Visual Place Recognition (VPR). Firstly, we\npropose Modular SNNs, where each SNN represents a set of non-overlapping\ngeographically distinct places, enabling scalable networks for large\nenvironments. Secondly, we present Ensembles of Modular SNNs, where multiple\nnetworks represent the same place, significantly enhancing accuracy compared to\nsingle-network models. Each of our Modular SNN modules is compact, comprising\nonly 1500 neurons and 474k synapses, making them ideally suited for ensembling\ndue to their small size. Lastly, we investigate the role of sequence matching\nin SNN-based VPR, a technique where consecutive images are used to refine place\nrecognition. We analyze the responsiveness of SNNs to ensembling and sequence\nmatching compared to other VPR techniques. Our contributions highlight the\nviability of SNNs for VPR, offering scalable and robust solutions, and paving\nthe way for their application in various energy-sensitive robotic tasks.\n","authors":["Somayeh Hussaini","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2311.13186v2.pdf","comment":"20 pages, 10 figures, under review"},{"id":"http://arxiv.org/abs/2408.00963v1","updated":"2024-08-02T00:35:18Z","published":"2024-08-02T00:35:18Z","title":"MIS-ME: A Multi-modal Framework for Soil Moisture Estimation","summary":"  Soil moisture estimation is an important task to enable precision agriculture\nin creating optimal plans for irrigation, fertilization, and harvest. It is\ncommon to utilize statistical and machine learning models to estimate soil\nmoisture from traditional data sources such as weather forecasts, soil\nproperties, and crop properties. However, there is a growing interest in\nutilizing aerial and geospatial imagery to estimate soil moisture. Although\nthese images capture high-resolution crop details, they are expensive to curate\nand challenging to interpret. Imagine, an AI-enhanced software tool that\npredicts soil moisture using visual cues captured by smartphones and\nstatistical data given by weather forecasts. This work is a first step towards\nthat goal of developing a multi-modal approach for soil moisture estimation. In\nparticular, we curate a dataset consisting of real-world images taken from\nground stations and their corresponding weather data. We also propose MIS-ME -\nMeteorological & Image based Soil Moisture Estimator, a multi-modal framework\nfor soil moisture estimation. Our extensive analysis shows that MIS-ME achieves\na MAPE of 10.79%, outperforming traditional unimodal approaches with a\nreduction of 2.6% in MAPE for meteorological data and 1.5% in MAPE for image\ndata, highlighting the effectiveness of tailored multi-modal approaches.\n","authors":["Mohammed Rakib","Adil Aman Mohammed","Cole Diggins","Sumit Sharma","Jeff Michael Sadler","Tyson Ochsner","Arun Bagavathi"],"pdf_url":"https://arxiv.org/pdf/2408.00963v1.pdf","comment":"Accepted by DSAA2024"},{"id":"http://arxiv.org/abs/2404.07514v2","updated":"2024-08-02T00:06:55Z","published":"2024-04-11T07:11:43Z","title":"Generalization Gap in Data Augmentation: Insights from Illumination","summary":"  In the field of computer vision, data augmentation is widely used to enrich\nthe feature complexity of training datasets with deep learning techniques.\nHowever, regarding the generalization capabilities of models, the difference in\nartificial features generated by data augmentation and natural visual features\nhas not been fully revealed. This study introduces the concept of \"visual\nrepresentation variables\" to define the possible visual variations in a task as\na joint distribution of these variables. We focus on the visual representation\nvariable \"illumination\", by simulating its distribution degradation and\nexamining how data augmentation techniques enhance model performance on a\nclassification task. Our goal is to investigate the differences in\ngeneralization between models trained with augmented data and those trained\nunder real-world illumination conditions. Results indicate that after applying\nvarious data augmentation methods, model performance has significantly\nimproved. Yet, a noticeable generalization gap still exists after utilizing\nvarious data augmentation methods, emphasizing the critical role of feature\ndiversity in the training set for enhancing model generalization.\n","authors":["Jianqiang Xiao","Weiwen Guo","Junfeng Liu","Mengze Li"],"pdf_url":"https://arxiv.org/pdf/2404.07514v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.00713v2","updated":"2024-08-02T14:40:19Z","published":"2024-08-01T16:58:54Z","title":"Reinforcement Learning applied to Insurance Portfolio Pursuit","summary":"  When faced with a new customer, many factors contribute to an insurance\nfirm's decision of what offer to make to that customer. In addition to the\nexpected cost of providing the insurance, the firm must consider the other\noffers likely to be made to the customer, and how sensitive the customer is to\ndifferences in price. Moreover, firms often target a specific portfolio of\ncustomers that could depend on, e.g., age, location, and occupation. Given such\na target portfolio, firms may choose to modulate an individual customer's offer\nbased on whether the firm desires the customer within their portfolio. We term\nthe problem of modulating offers to achieve a desired target portfolio the\nportfolio pursuit problem. Having formulated the portfolio pursuit problem as a\nsequential decision making problem, we devise a novel reinforcement learning\nalgorithm for its solution. We test our method on a complex synthetic market\nenvironment, and demonstrate that it outperforms a baseline method which mimics\ncurrent industry approaches to portfolio pursuit.\n","authors":["Edward James Young","Alistair Rogers","Elliott Tong","James Jordon"],"pdf_url":"https://arxiv.org/pdf/2408.00713v2.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.00465v2","updated":"2024-08-02T03:56:14Z","published":"2024-08-01T11:09:01Z","title":"Infrequent Resolving Algorithm for Online Linear Programming","summary":"  Online linear programming (OLP) has gained significant attention from both\nresearchers and practitioners due to its extensive applications, such as online\nauction, network revenue management and advertising. Existing OLP algorithms\nfall into two categories: LP-based algorithms and LP-free algorithms. The\nformer one typically guarantees better performance, even offering a constant\nregret, but requires solving a large number of LPs, which could be\ncomputationally expensive. In contrast, LP-free algorithm only requires\nfirst-order computations but induces a worse performance, lacking a constant\nregret bound. In this work, we bridge the gap between these two extremes by\nproposing an algorithm that achieves a constant regret while solving LPs only\n$O(\\log\\log T)$ times over the time horizon $T$. Moreover, when we are allowed\nto solve LPs only $M$ times, we propose an algorithm that can guarantee an\n$O\\left(T^{(1/2+\\epsilon)^{M-1}}\\right)$ regret. Furthermore, when the arrival\nprobabilities are known at the beginning, our algorithm can guarantee a\nconstant regret by solving LPs $O(\\log\\log T)$ times, and an\n$O\\left(T^{(1/2+\\epsilon)^{M}}\\right)$ regret by solving LPs only $M$ times.\nNumerical experiments are conducted to demonstrate the efficiency of the\nproposed algorithms.\n","authors":["Guokai Li","Zizhuo Wang","Jingwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00465v2.pdf","comment":"35 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00374v2","updated":"2024-08-02T13:00:46Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16827v3","updated":"2024-08-02T17:59:31Z","published":"2024-02-26T18:54:35Z","title":"A Survey on Data Selection for Language Models","summary":"  A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.\n","authors":["Alon Albalak","Yanai Elazar","Sang Michael Xie","Shayne Longpre","Nathan Lambert","Xinyi Wang","Niklas Muennighoff","Bairu Hou","Liangming Pan","Haewon Jeong","Colin Raffel","Shiyu Chang","Tatsunori Hashimoto","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.16827v3.pdf","comment":"Paper list available at\n  https://github.com/alon-albalak/data-selection-survey"},{"id":"http://arxiv.org/abs/2408.01420v1","updated":"2024-08-02T17:55:50Z","published":"2024-08-02T17:55:50Z","title":"Mission Impossible: A Statistical Perspective on Jailbreaking LLMs","summary":"  Large language models (LLMs) are trained on a deluge of text data with\nlimited quality control. As a result, LLMs can exhibit unintended or even\nharmful behaviours, such as leaking information, fake news or hate speech.\nCountermeasures, commonly referred to as preference alignment, include\nfine-tuning the pretrained LLMs with carefully crafted text examples of desired\nbehaviour. Even then, empirical evidence shows preference aligned LLMs can be\nenticed to harmful behaviour. This so called jailbreaking of LLMs is typically\nachieved by adversarially modifying the input prompt to the LLM. Our paper\nprovides theoretical insights into the phenomenon of preference alignment and\njailbreaking from a statistical perspective. Under our framework, we first show\nthat pretrained LLMs will mimic harmful behaviour if present in the training\ncorpus. Under that same framework, we then introduce a statistical notion of\nalignment, and lower-bound the jailbreaking probability, showing that it is\nunpreventable under reasonable assumptions. Based on our insights, we propose\nan alteration to the currently prevalent alignment strategy RLHF. Specifically,\nwe introduce a simple modification to the RLHF objective, we call E-RLHF, that\naims to increase the likelihood of safe responses. E-RLHF brings no additional\ntraining cost, and is compatible with other methods. Empirically, we\ndemonstrate that E-RLHF outperforms RLHF on all alignment problems put forward\nby the AdvBench and HarmBench project without sacrificing model performance as\nmeasured by the MT-Bench project.\n","authors":["Jingtong Su","Julia Kempe","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2408.01420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01417v1","updated":"2024-08-02T17:51:57Z","published":"2024-08-02T17:51:57Z","title":"Talk Less, Interact Better: Evaluating In-context Conversational\n  Adaptation in Multimodal LLMs","summary":"  Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.\n","authors":["Yilun Hua","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.01417v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.01416v1","updated":"2024-08-02T17:51:42Z","published":"2024-08-02T17:51:42Z","title":"The Quest for the Right Mediator: A History, Survey, and Theoretical\n  Grounding of Causal Interpretability","summary":"  Interpretability provides a toolset for understanding how and why neural\nnetworks behave in certain ways. However, there is little unity in the field:\nmost studies employ ad-hoc evaluations and do not share theoretical\nfoundations, making it difficult to measure progress and compare the pros and\ncons of different techniques. Furthermore, while mechanistic understanding is\nfrequently discussed, the basic causal units underlying these mechanisms are\noften not explicitly defined. In this paper, we propose a perspective on\ninterpretability research grounded in causal mediation analysis. Specifically,\nwe describe the history and current state of interpretability taxonomized\naccording to the types of causal units (mediators) employed, as well as methods\nused to search over mediators. We discuss the pros and cons of each mediator,\nproviding insights as to when particular kinds of mediators and search methods\nare most appropriate depending on the goals of a given study. We argue that\nthis framing yields a more cohesive narrative of the field, as well as\nactionable insights for future work. Specifically, we recommend a focus on\ndiscovering new mediators with better trade-offs between human-interpretability\nand compute-efficiency, and which can uncover more sophisticated abstractions\nfrom neural networks than the primarily linear mediators employed in current\nwork. We also argue for more standardized evaluations that enable principled\ncomparisons across mediator types, such that we can better understand when\nparticular causal units are better suited to particular use cases.\n","authors":["Aaron Mueller","Jannik Brinkmann","Millicent Li","Samuel Marks","Koyena Pal","Nikhil Prakash","Can Rager","Aruna Sankaranarayanan","Arnab Sen Sharma","Jiuding Sun","Eric Todd","David Bau","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2408.01416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01415v1","updated":"2024-08-02T17:43:34Z","published":"2024-08-02T17:43:34Z","title":"Conditional LoRA Parameter Generation","summary":"  Generative models have achieved remarkable success in image, video, and text\ndomains. Inspired by this, researchers have explored utilizing generative\nmodels to generate neural network parameters. However, these efforts have been\nlimited by the parameter size and the practicality of generating\nhigh-performance parameters. In this paper, we propose COND P-DIFF, a novel\napproach that demonstrates the feasibility of controllable high-performance\nparameter generation, particularly for LoRA (Low-Rank Adaptation) weights,\nduring the fine-tuning process. Specifically, we employ an autoencoder to\nextract efficient latent representations for parameters. We then train a\nconditional latent diffusion model to synthesize high-performing model\nparameters from random noise based on specific task conditions. Experimental\nresults in both computer vision and natural language processing domains\nconsistently demonstrate that COND P-DIFF can generate high-performance\nparameters conditioned on the given task. Moreover, we observe that the\nparameter distribution generated by COND P-DIFF exhibits differences compared\nto the distribution obtained through normal optimization methods, indicating a\ncertain level of generalization capability. Our work paves the way for further\nexploration of condition-driven parameter generation, offering a promising\ndirection for task-specific adaptation of neural networks.\n","authors":["Xiaolong Jin","Kai Wang","Dongwen Tang","Wangbo Zhao","Yukun Zhou","Junshu Tang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2408.01415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01408v1","updated":"2024-08-02T17:33:52Z","published":"2024-08-02T17:33:52Z","title":"Derivation of Back-propagation for Graph Convolutional Networks using\n  Matrix Calculus and its Application to Explainable Artificial Intelligence","summary":"  This paper provides a comprehensive and detailed derivation of the\nbackpropagation algorithm for graph convolutional neural networks using matrix\ncalculus. The derivation is extended to include arbitrary element-wise\nactivation functions and an arbitrary number of layers. The study addresses two\nfundamental problems, namely node classification and link prediction. To\nvalidate our method, we compare it with reverse-mode automatic differentiation.\nThe experimental results demonstrate that the median sum of squared errors of\nthe updated weight matrices, when comparing our method to the approach using\nreverse-mode automatic differentiation, falls within the range of $10^{-18}$ to\n$10^{-14}$. These outcomes are obtained from conducting experiments on a\nfive-layer graph convolutional network, applied to a node classification\nproblem on Zachary's karate club social network and a link prediction problem\non a drug-drug interaction network. Finally, we show how the derived\nclosed-form solution can facilitate the development of explainable AI and\nsensitivity analysis.\n","authors":["Yen-Che Hsiao","Rongting Yue","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2408.01408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.15501v4","updated":"2024-08-02T17:31:24Z","published":"2021-10-29T02:38:54Z","title":"Doubly Robust Interval Estimation for Optimal Policy Evaluation in\n  Online Learning","summary":"  Evaluating the performance of an ongoing policy plays a vital role in many\nareas such as medicine and economics, to provide crucial instructions on the\nearly-stop of the online experiment and timely feedback from the environment.\nPolicy evaluation in online learning thus attracts increasing attention by\ninferring the mean outcome of the optimal policy (i.e., the value) in\nreal-time. Yet, such a problem is particularly challenging due to the dependent\ndata generated in the online environment, the unknown optimal policy, and the\ncomplex exploration and exploitation trade-off in the adaptive experiment. In\nthis paper, we aim to overcome these difficulties in policy evaluation for\nonline learning. We explicitly derive the probability of exploration that\nquantifies the probability of exploring non-optimal actions under commonly used\nbandit algorithms. We use this probability to conduct valid inference on the\nonline conditional mean estimator under each action and develop the doubly\nrobust interval estimation (DREAM) method to infer the value under the\nestimated optimal policy in online learning. The proposed value estimator\nprovides double protection for consistency and is asymptotically normal with a\nWald-type confidence interval provided. Extensive simulation studies and real\ndata applications are conducted to demonstrate the empirical validity of the\nproposed DREAM method.\n","authors":["Ye Shen","Hengrui Cai","Rui Song"],"pdf_url":"https://arxiv.org/pdf/2110.15501v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01402v1","updated":"2024-08-02T17:25:34Z","published":"2024-08-02T17:25:34Z","title":"Pre-trained Language Models Improve the Few-shot Prompt Ability of\n  Decision Transformer","summary":"  Decision Transformer (DT) has emerged as a promising class of algorithms in\noffline reinforcement learning (RL) tasks, leveraging pre-collected datasets\nand Transformer's capability to model long sequences. Recent works have\ndemonstrated that using parts of trajectories from training tasks as prompts in\nDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.\nHowever, collecting data from specific environments can be both costly and\nunsafe in many scenarios, leading to suboptimal performance and limited\nfew-shot prompt abilities due to the data-hungry nature of Transformer-based\nmodels. Additionally, the limited datasets used in pre-training make it\nchallenging for Prompt-DT type of methods to distinguish between various RL\ntasks through prompts alone. To address these challenges, we introduce the\nLanguage model-initialized Prompt Decision Transformer (LPDT), which leverages\npre-trained language models for meta-RL tasks and fine-tunes the model using\nLow-rank Adaptation (LoRA). We further incorporate prompt regularization to\neffectively differentiate between tasks based on prompt feature\nrepresentations. Our approach integrates pre-trained language model and RL\ntasks seamlessly. Extensive empirical studies demonstrate that initializing\nwith a pre-trained language model significantly enhances the performance of\nPrompt-DT on unseen tasks compared to baseline methods.\n","authors":["Yu Yang","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2408.01402v1.pdf","comment":"2 figures, 8 tables. Accepted by the Training Agents with Foundation\n  Models Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2407.19631v2","updated":"2024-08-02T17:10:43Z","published":"2024-07-29T01:22:04Z","title":"\"A Good Bot Always Knows Its Limitations\": Assessing Autonomous System\n  Decision-making Competencies through Factorized Machine Self-confidence","summary":"  How can intelligent machines assess their competencies in completing tasks?\nThis question has come into focus for autonomous systems that algorithmically\nreason and make decisions under uncertainty. It is argued here that machine\nself-confidence - a form of meta-reasoning based on self-assessments of an\nagent's knowledge about the state of the world and itself, as well as its\nability to reason about and execute tasks - leads to many eminently computable\nand useful competency indicators for such agents. This paper presents a\nculmination of work on this concept in the form of a computational framework\ncalled Factorized Machine Self-confidence (FaMSeC), which provides a holistic\nengineering-focused description of factors driving an algorithmic\ndecision-making process, including: outcome assessment, solver quality, model\nquality, alignment quality, and past experience. In FaMSeC, self confidence\nindicators are derived from hierarchical `problem-solving statistics' embedded\nwithin broad classes of probabilistic decision-making algorithms such as Markov\ndecision processes. The problem-solving statistics are obtained by evaluating\nand grading probabilistic exceedance margins with respect to given competency\nstandards, which are specified for each of the various decision-making\ncompetency factors by the informee (e.g. a non-expert user or an expert system\ndesigner). This approach allows `algorithmic goodness of fit' evaluations to be\neasily incorporated into the design of many kinds of autonomous agents in the\nform of human-interpretable competency self-assessment reports. Detailed\ndescriptions and application examples for a Markov decision process agent show\nhow two of the FaMSeC factors (outcome assessment and solver quality) can be\ncomputed and reported for a range of possible tasking contexts through novel\nuse of meta-utility functions, behavior simulations, and surrogate prediction\nmodels.\n","authors":["Brett Israelsen","Nisar R. Ahmed","Matthew Aitken","Eric W. Frew","Dale A. Lawrence","Brian M. Argrow"],"pdf_url":"https://arxiv.org/pdf/2407.19631v2.pdf","comment":"59 pages, 22 figures, draft to be submitted for journal review"},{"id":"http://arxiv.org/abs/2408.01391v1","updated":"2024-08-02T17:01:36Z","published":"2024-08-02T17:01:36Z","title":"FT K-Means: A High-Performance K-Means on GPU with Fault Tolerance","summary":"  K-Means is a widely used algorithm in clustering, however, its efficiency is\nprimarily constrained by the computational cost of distance computing. Existing\nimplementations suffer from suboptimal utilization of computational units and\nlack resilience against soft errors. To address these challenges, we introduce\nFT K-Means, a high-performance GPU-accelerated implementation of K-Means with\nonline fault tolerance. We first present a stepwise optimization strategy that\nachieves competitive performance compared to NVIDIA's cuML library. We further\nimprove FT K-Means with a template-based code generation framework that\nsupports different data types and adapts to different input shapes. A novel\nwarp-level tensor-core error correction scheme is proposed to address the\nfailure of existing fault tolerance methods due to memory asynchronization\nduring copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100\nGPU demonstrate that FT K-Means without fault tolerance outperforms cuML's\nK-Means implementation, showing a performance increase of 10\\%-300\\% in\nscenarios involving irregular data shapes. Moreover, the fault tolerance\nfeature of FT K-Means introduces only an overhead of 11\\%, maintaining robust\nperformance even with tens of errors injected per second.\n","authors":["Shixun Wu","Yitong Ding","Yujia Zhai","Jinyang Liu","Jiajun Huang","Zizhe Jian","Huangliang Dai","Sheng Di","Bryan M. Wong","Zizhong Chen","Franck Cappello"],"pdf_url":"https://arxiv.org/pdf/2408.01391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01387v1","updated":"2024-08-02T16:55:08Z","published":"2024-08-02T16:55:08Z","title":"NeuralBeta: Estimating Beta Using Deep Learning","summary":"  Traditional approaches to estimating beta in finance often involve rigid\nassumptions and fail to adequately capture beta dynamics, limiting their\neffectiveness in use cases like hedging. To address these limitations, we have\ndeveloped a novel method using neural networks called NeuralBeta, which is\ncapable of handling both univariate and multivariate scenarios and tracking the\ndynamic behavior of beta. To address the issue of interpretability, we\nintroduce a new output layer inspired by regularized weighted linear\nregression, which provides transparency into the model's decision-making\nprocess. We conducted extensive experiments on both synthetic and market data,\ndemonstrating NeuralBeta's superior performance compared to benchmark methods\nacross various scenarios, especially instances where beta is highly\ntime-varying, e.g., during regime shifts in the market. This model not only\nrepresents an advancement in the field of beta estimation, but also shows\npotential for applications in other financial contexts that assume linear\nrelationships.\n","authors":["Yuxin Liu","Jimin Lin","Achintya Gopal"],"pdf_url":"https://arxiv.org/pdf/2408.01387v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.20328v2","updated":"2024-08-02T16:51:52Z","published":"2024-03-29T17:59:05Z","title":"Learning Visual Quadrupedal Loco-Manipulation from Demonstrations","summary":"  Quadruped robots are progressively being integrated into human environments.\nDespite the growing locomotion capabilities of quadrupedal robots, their\ninteraction with objects in realistic scenes is still limited. While additional\nrobotic arms on quadrupedal robots enable manipulating objects, they are\nsometimes redundant given that a quadruped robot is essentially a mobile unit\nequipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,\nwe aim to empower a quadruped robot to execute real-world manipulation tasks\nusing only its legs. We decompose the loco-manipulation process into a\nlow-level reinforcement learning (RL)-based controller and a high-level\nBehavior Cloning (BC)-based planner. By parameterizing the manipulation\ntrajectory, we synchronize the efforts of the upper and lower layers, thereby\nleveraging the advantages of both RL and BC. Our approach is validated through\nsimulations and real-world experiments, demonstrating the robot's ability to\nperform tasks that demand mobility and high precision, such as lifting a basket\nfrom the ground while moving, closing a dishwasher, pressing a button, and\npushing a door. Project website: https://zhengmaohe.github.io/leg-manip\n","authors":["Zhengmao He","Kun Lei","Yanjie Ze","Koushil Sreenath","Zhongyu Li","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2403.20328v2.pdf","comment":"Published at IROS 2024. Project website:\n  https://zhengmaohe.github.io/leg-manip"},{"id":"http://arxiv.org/abs/2408.01382v1","updated":"2024-08-02T16:40:58Z","published":"2024-08-02T16:40:58Z","title":"Explaining a probabilistic prediction on the simplex with Shapley\n  compositions","summary":"  Originating in game theory, Shapley values are widely used for explaining a\nmachine learning model's prediction by quantifying the contribution of each\nfeature's value to the prediction. This requires a scalar prediction as in\nbinary classification, whereas a multiclass probabilistic prediction is a\ndiscrete probability distribution, living on a multidimensional simplex. In\nsuch a multiclass setting the Shapley values are typically computed separately\non each class in a one-vs-rest manner, ignoring the compositional nature of the\noutput distribution. In this paper, we introduce Shapley compositions as a\nwell-founded way to properly explain a multiclass probabilistic prediction,\nusing the Aitchison geometry from compositional data analysis. We prove that\nthe Shapley composition is the unique quantity satisfying linearity, symmetry\nand efficiency on the Aitchison simplex, extending the corresponding axiomatic\nproperties of the standard Shapley value. We demonstrate this proper multiclass\ntreatment in a range of scenarios.\n","authors":["Paul-Gauthier No","Miquel Perell-Nieto","Jean-Franois Bonastre","Peter Flach"],"pdf_url":"https://arxiv.org/pdf/2408.01382v1.pdf","comment":"To be published in ECAI2024's proceedings"},{"id":"http://arxiv.org/abs/2408.01379v1","updated":"2024-08-02T16:37:33Z","published":"2024-08-02T16:37:33Z","title":"Resampling and averaging coordinates on data","summary":"  We introduce algorithms for robustly computing intrinsic coordinates on point\nclouds. Our approach relies on generating many candidate coordinates by\nsubsampling the data and varying hyperparameters of the embedding algorithm\n(e.g., manifold learning). We then identify a subset of representative\nembeddings by clustering the collection of candidate coordinates and using\nshape descriptors from topological data analysis. The final output is the\nembedding obtained as an average of the representative embeddings using\ngeneralized Procrustes analysis. We validate our algorithm on both synthetic\ndata and experimental measurements from genomics, demonstrating robustness to\nnoise and outliers.\n","authors":["Andrew J. Blumberg","Mathieu Carriere","Jun Hou Fung","Michael A. Mandell"],"pdf_url":"https://arxiv.org/pdf/2408.01379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01375v1","updated":"2024-08-02T16:32:30Z","published":"2024-08-02T16:32:30Z","title":"Adaptive Recruitment Resource Allocation to Improve Cohort\n  Representativeness in Participatory Biomedical Datasets","summary":"  Large participatory biomedical studies, studies that recruit individuals to\njoin a dataset, are gaining popularity and investment, especially for analysis\nby modern AI methods. Because they purposively recruit participants, these\nstudies are uniquely able to address a lack of historical representation, an\nissue that has affected many biomedical datasets. In this work, we define\nrepresentativeness as the similarity to a target population distribution of a\nset of attributes and our goal is to mirror the U.S. population across\ndistributions of age, gender, race, and ethnicity. Many participatory studies\nrecruit at several institutions, so we introduce a computational approach to\nadaptively allocate recruitment resources among sites to improve\nrepresentativeness. In simulated recruitment of 10,000-participant cohorts from\nmedical centers in the STAR Clinical Research Network, we show that our\napproach yields a more representative cohort than existing baselines. Thus, we\nhighlight the value of computational modeling in guiding recruitment efforts.\n","authors":["Victor Borza","Andrew Estornell","Ellen Wright Clayton","Chien-Ju Ho","Russell Rothman","Yevgeniy Vorobeychik","Bradley Malin"],"pdf_url":"https://arxiv.org/pdf/2408.01375v1.pdf","comment":"Accepted for publication at the American Medical Informatics\n  Association Annual Symposium 2024, 10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.11652v3","updated":"2024-08-02T16:30:55Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v3.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.01374v1","updated":"2024-08-02T16:29:54Z","published":"2024-08-02T16:29:54Z","title":"Hybrid Coordinate Descent for Efficient Neural Network Learning Using\n  Line Search and Gradient Descent","summary":"  This paper presents a novel coordinate descent algorithm leveraging a\ncombination of one-directional line search and gradient information for\nparameter updates for a squared error loss function. Each parameter undergoes\nupdates determined by either the line search or gradient method, contingent\nupon whether the modulus of the gradient of the loss with respect to that\nparameter surpasses a predefined threshold. Notably, a larger threshold value\nenhances algorithmic efficiency. Despite the potentially slower nature of the\nline search method relative to gradient descent, its parallelizability\nfacilitates computational time reduction. Experimental validation conducted on\na 2-layer Rectified Linear Unit network with synthetic data elucidates the\nimpact of hyperparameters on convergence rates and computational efficiency.\n","authors":["Yen-Che Hsiao","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2408.01374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01365v1","updated":"2024-08-02T16:17:59Z","published":"2024-08-02T16:17:59Z","title":"Data Debugging is NP-hard for Classifiers Trained with SGD","summary":"  Data debugging is to find a subset of the training data such that the model\nobtained by retraining on the subset has a better accuracy. A bunch of\nheuristic approaches are proposed, however, none of them are guaranteed to\nsolve this problem effectively. This leaves an open issue whether there exists\nan efficient algorithm to find the subset such that the model obtained by\nretraining on it has a better accuracy. To answer this open question and\nprovide theoretical basis for further study on developing better algorithms for\ndata debugging, we investigate the computational complexity of the problem\nnamed Debuggable. Given a machine learning model $\\mathcal{M}$ obtained by\ntraining on dataset $D$ and a test instance\n$(\\mathbf{x}_\\text{test},y_\\text{test})$ where\n$\\mathcal{M}(\\mathbf{x}_\\text{test})\\neq y_\\text{test}$, Debuggable is to\ndetermine whether there exists a subset $D^\\prime$ of $D$ such that the model\n$\\mathcal{M}^\\prime$ obtained by retraining on $D^\\prime$ satisfies\n$\\mathcal{M}^\\prime(\\mathbf{x}_\\text{test})=y_\\text{test}$. To cover a wide\nrange of commonly used models, we take SGD-trained linear classifier as the\nmodel and derive the following main results. (1) If the loss function and the\ndimension of the model are not fixed, Debuggable is NP-complete regardless of\nthe training order in which all the training samples are processed during SGD.\n(2) For hinge-like loss functions, a comprehensive analysis on the\ncomputational complexity of Debuggable is provided; (3) If the loss function is\na linear function, Debuggable can be solved in linear time, that is, data\ndebugging can be solved easily in this case. These results not only highlight\nthe limitations of current approaches but also offer new insights into data\ndebugging.\n","authors":["Zizheng Guo","Pengyu Chen","Yanzhang Fu","Dongjing Miao"],"pdf_url":"https://arxiv.org/pdf/2408.01365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01362v1","updated":"2024-08-02T16:13:51Z","published":"2024-08-02T16:13:51Z","title":"Autoencoders in Function Space","summary":"  Autoencoders have found widespread application, in both their original\ndeterministic form and in their variational formulation (VAEs). In scientific\napplications it is often of interest to consider data that are comprised of\nfunctions; the same perspective is useful in image processing. In practice,\ndiscretisation (of differential equations arising in the sciences) or\npixellation (of images) renders problems finite dimensional, but conceiving\nfirst of algorithms that operate on functions, and only then discretising or\npixellating, leads to better algorithms that smoothly operate between different\nlevels of discretisation or pixellation. In this paper function-space versions\nof the autoencoder (FAE) and variational autoencoder (FVAE) are introduced,\nanalysed, and deployed. Well-definedness of the objective function governing\nVAEs is a subtle issue, even in finite dimension, and more so on function\nspace. The FVAE objective is well defined whenever the data distribution is\ncompatible with the chosen generative model; this happens, for example, when\nthe data arise from a stochastic differential equation. The FAE objective is\nvalid much more broadly, and can be straightforwardly applied to data governed\nby differential equations. Pairing these objectives with neural operator\narchitectures, which can thus be evaluated on any mesh, enables new\napplications of autoencoders to inpainting, superresolution, and generative\nmodelling of scientific data.\n","authors":["Justin Bunker","Mark Girolami","Hefin Lambley","Andrew M. Stuart","T. J. Sullivan"],"pdf_url":"https://arxiv.org/pdf/2408.01362v1.pdf","comment":"56 pages, 25 figures"},{"id":"http://arxiv.org/abs/2406.04551v2","updated":"2024-08-02T16:09:49Z","published":"2024-06-06T23:35:51Z","title":"Improving Geo-diversity of Generated Images with Contextualized Vendi\n  Score Guidance","summary":"  With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.\n","authors":["Reyhane Askari Hemmat","Melissa Hall","Alicia Sun","Candace Ross","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2406.04551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05737v3","updated":"2024-08-02T16:09:14Z","published":"2023-02-11T16:26:57Z","title":"A Reparameterized Discrete Diffusion Model for Text Generation","summary":"  This work studies discrete diffusion probabilistic models with applications\nto natural language generation. We derive an alternative yet equivalent\nformulation of the sampling from discrete diffusion processes and leverage this\ninsight to develop a family of reparameterized discrete diffusion models. The\nderived generic framework is highly flexible, offers a fresh perspective of the\ngeneration process in discrete diffusion models, and features more effective\ntraining and decoding techniques. We conduct extensive experiments to evaluate\nthe text generation capability of our model, demonstrating significant\nimprovements over existing diffusion models.\n","authors":["Lin Zheng","Jianbo Yuan","Lei Yu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2302.05737v3.pdf","comment":"COLM 2024; Code available at\n  https://github.com/hkunlp/reparam-discrete-diffusion"},{"id":"http://arxiv.org/abs/2408.01349v1","updated":"2024-08-02T15:54:49Z","published":"2024-08-02T15:54:49Z","title":"PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy\n  Correspondence Learning in Cross-Modal Retrieval","summary":"  In the realm of cross-modal retrieval, seamlessly integrating diverse\nmodalities within multimedia remains a formidable challenge, especially given\nthe complexities introduced by noisy correspondence learning (NCL). Such noise\noften stems from mismatched data pairs, which is a significant obstacle\ndistinct from traditional noisy labels. This paper introduces\nPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address\nthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an\nauxiliary \"pseudo-classification\" task that interprets captions as categorical\nlabels, steering the model to learn image-text semantic similarity through a\nnon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,\ncapitalizing on PC$^2$'s pseudo-classification capability, we generate\npseudo-captions to provide more informative and tangible supervision for each\nmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed\nto assistant the correction of correspondence. In addition to technical\ncontributions, we develop a realistic NCL dataset called Noise of Web (NoW),\nwhich could be a new powerful NCL benchmark where noise exists naturally.\nEmpirical evaluations of PC$^2$ showcase marked improvements over existing\nstate-of-the-art robust cross-modal retrieval techniques on both simulated and\nrealistic datasets with various NCL settings. The contributed dataset and\nsource code are released at https://github.com/alipay/PC2-NoiseofWeb.\n","authors":["Yue Duan","Zhangxuan Gu","Zhenzhe Ying","Lei Qi","Changhua Meng","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01349v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2403.13940v2","updated":"2024-08-02T15:54:21Z","published":"2024-03-20T19:25:11Z","title":"A multi-criteria approach for selecting an explanation from the set of\n  counterfactuals produced by an ensemble of explainers","summary":"  Counterfactuals are widely used to explain ML model predictions by providing\nalternative scenarios for obtaining the more desired predictions. They can be\ngenerated by a variety of methods that optimize different, sometimes\nconflicting, quality measures and produce quite different solutions. However,\nchoosing the most appropriate explanation method and one of the generated\ncounterfactuals is not an easy task. Instead of forcing the user to test many\ndifferent explanation methods and analysing conflicting solutions, in this\npaper, we propose to use a multi-stage ensemble approach that will select\nsingle counterfactual based on the multiple-criteria analysis. It offers a\ncompromise solution that scores well on several popular quality measures. This\napproach exploits the dominance relation and the ideal point decision aid\nmethod, which selects one counterfactual from the Pareto front. The conducted\nexperiments demonstrated that the proposed approach generates fully actionable\ncounterfactuals with attractive compromise values of the considered quality\nmeasures.\n","authors":["Ignacy Stpka","Mateusz Lango","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2403.13940v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.01343v1","updated":"2024-08-02T15:41:16Z","published":"2024-08-02T15:41:16Z","title":"StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal\n  Semantic Segmentation","summary":"  Multimodal semantic segmentation shows significant potential for enhancing\nsegmentation accuracy in complex scenes. However, current methods often\nincorporate specialized feature fusion modules tailored to specific modalities,\nthereby restricting input flexibility and increasing the number of training\nparameters. To address these challenges, we propose StitchFusion, a\nstraightforward yet effective modal fusion framework that integrates\nlarge-scale pre-trained models directly as encoders and feature fusers. This\napproach facilitates comprehensive multi-modal and multi-scale feature fusion,\naccommodating any visual modal inputs. Specifically, Our framework achieves\nmodal integration during encoding by sharing multi-modal visual information. To\nenhance information exchange across modalities, we introduce a\nmulti-directional adapter module (MultiAdapter) to enable cross-modal\ninformation transfer during encoding. By leveraging MultiAdapter to propagate\nmulti-scale information across pre-trained encoders during the encoding\nprocess, StitchFusion achieves multi-modal visual information integration\nduring encoding. Extensive comparative experiments demonstrate that our model\nachieves state-of-the-art performance on four multi-modal segmentation datasets\nwith minimal additional parameters. Furthermore, the experimental integration\nof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their\ncomplementary nature. Our code is available at StitchFusion_repo.\n","authors":["Bingyu Li","Da Zhang","Zhiyuan Zhao","Junyu Gao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11891v3","updated":"2024-08-02T15:38:38Z","published":"2023-10-18T11:20:59Z","title":"A Hyperparameter Study for Quantum Kernel Methods","summary":"  Quantum kernel methods are a promising method in quantum machine learning\nthanks to the guarantees connected to them. Their accessibility for analytic\nconsiderations also opens up the possibility of prescreening datasets based on\ntheir potential for a quantum advantage. To do so, earlier works developed the\ngeometric difference, which can be understood as a closeness measure between\ntwo kernel-based machine learning approaches, most importantly between a\nquantum kernel and a classical kernel. This metric links the quantum and\nclassical model complexities, and it was developed to bound generalization\nerror. Therefore, it raises the question of how this metric behaves in an\nempirical setting. In this work, we investigate the effects of hyperparameter\nchoice on the model performance and the generalization gap between classical\nand quantum kernels. The importance of hyperparameters is well known also for\nclassical machine learning. Of special interest are hyperparameters associated\nwith the quantum Hamiltonian evolution feature map, as well as the number of\nqubits to trace out before computing a projected quantum kernel. We conduct a\nthorough investigation of the hyperparameters across 11 datasets and we\nidentify certain aspects that can be exploited. Analyzing the effects of\ncertain hyperparameter settings on the empirical performance, as measured by\ncross validation accuracy, and generalization ability, as measured by geometric\ndifference described above, brings us one step closer to understanding the\npotential of quantum kernel methods on classical datasets.\n","authors":["Sebastian Egginger","Alona Sakhnenko","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2310.11891v3.pdf","comment":"Expanded implications of the paper"},{"id":"http://arxiv.org/abs/2408.01337v1","updated":"2024-08-02T15:34:05Z","published":"2024-08-02T15:34:05Z","title":"MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language\n  Models","summary":"  Multimodal models that jointly process audio and language hold great promise\nin audio understanding and are increasingly being adopted in the music domain.\nBy allowing users to query via text and obtain information about a given audio\ninput, these models have the potential to enable a variety of music\nunderstanding tasks via language-based interfaces. However, their evaluation\nposes considerable challenges, and it remains unclear how to effectively assess\ntheir ability to correctly interpret music-related inputs with current methods.\nMotivated by this, we introduce MuChoMusic, a benchmark for evaluating music\nunderstanding in multimodal language models focused on audio. MuChoMusic\ncomprises 1,187 multiple-choice questions, all validated by human annotators,\non 644 music tracks sourced from two publicly available music datasets, and\ncovering a wide variety of genres. Questions in the benchmark are crafted to\nassess knowledge and reasoning abilities across several dimensions that cover\nfundamental musical concepts and their relation to cultural and functional\ncontexts. Through the holistic analysis afforded by the benchmark, we evaluate\nfive open-source models and identify several pitfalls, including an\nover-reliance on the language modality, pointing to a need for better\nmultimodal integration. Data and code are open-sourced.\n","authors":["Benno Weck","Ilaria Manco","Emmanouil Benetos","Elio Quinton","George Fazekas","Dmitry Bogdanov"],"pdf_url":"https://arxiv.org/pdf/2408.01337v1.pdf","comment":"Accepted at ISMIR 2024. Data: https://doi.org/10.5281/zenodo.12709974\n  Code: https://github.com/mulab-mir/muchomusic Supplementary material:\n  https://mulab-mir.github.io/muchomusic"},{"id":"http://arxiv.org/abs/2408.01336v1","updated":"2024-08-02T15:33:04Z","published":"2024-08-02T15:33:04Z","title":"Sparse Linear Regression when Noises and Covariates are Heavy-Tailed and\n  Contaminated by Outliers","summary":"  We investigate a problem estimating coefficients of linear regression under\nsparsity assumption when covariates and noises are sampled from heavy tailed\ndistributions. Additionally, we consider the situation where not only\ncovariates and noises are sampled from heavy tailed distributions but also\ncontaminated by outliers. Our estimators can be computed efficiently, and\nexhibit sharp error bounds.\n","authors":["Takeyuki Sasai","Hironori Fujisawa"],"pdf_url":"https://arxiv.org/pdf/2408.01336v1.pdf","comment":"This research builds on and improves the results of arxiv:2206.07594.\n  There will be no further update for the earlier manuscript"},{"id":"http://arxiv.org/abs/2404.02476v3","updated":"2024-08-02T15:30:14Z","published":"2024-04-03T05:32:10Z","title":"Deep Reinforcement Learning for Traveling Purchaser Problems","summary":"  The traveling purchaser problem (TPP) is an important combinatorial\noptimization problem with broad applications. Due to the coupling between\nrouting and purchasing, existing works on TPPs commonly address route\nconstruction and purchase planning simultaneously, which, however, leads to\nexact methods with high computational cost and heuristics with sophisticated\ndesign but limited performance. In sharp contrast, we propose a novel approach\nbased on deep reinforcement learning (DRL), which addresses route construction\nand purchase planning separately, while evaluating and optimizing the solution\nfrom a global perspective. The key components of our approach include a\nbipartite graph representation for TPPs to capture the market-product\nrelations, and a policy network that extracts information from the bipartite\ngraph and uses it to sequentially construct the route. One significant benefit\nof our framework is that we can efficiently construct the route using the\npolicy network, and once the route is determined, the associated purchasing\nplan can be easily derived through linear programming, while, leveraging DRL,\nwe can train the policy network to optimize the global solution objective.\nFurthermore, by introducing a meta-learning strategy, the policy network can be\ntrained stably on large-sized TPP instances, and generalize well across\ninstances of varying sizes and distributions, even to much larger instances\nthat are never seen during training. Experiments on various synthetic TPP\ninstances and the TPPLIB benchmark demonstrate that our DRL-based approach can\nsignificantly outperform well-established TPP heuristics, reducing the\noptimality gap by 40%-90%, and also showing an advantage in runtime, especially\non large-sized instances.\n","authors":["Haofeng Yuan","Rongping Zhu","Wanlu Yang","Shiji Song","Keyou You","Yuli Zhang","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2404.02476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01332v1","updated":"2024-08-02T15:29:59Z","published":"2024-08-02T15:29:59Z","title":"HMDN: Hierarchical Multi-Distribution Network for Click-Through Rate\n  Prediction","summary":"  As the recommendation service needs to address increasingly diverse\ndistributions, such as multi-population, multi-scenario, multitarget, and\nmulti-interest, more and more recent works have focused on multi-distribution\nmodeling and achieved great progress. However, most of them only consider\nmodeling in a single multi-distribution manner, ignoring that mixed\nmulti-distributions often coexist and form hierarchical relationships. To\naddress these challenges, we propose a flexible modeling paradigm, named\nHierarchical Multi-Distribution Network (HMDN), which efficiently models these\nhierarchical relationships and can seamlessly integrate with existing\nmulti-distribution methods, such as Mixture of-Experts (MoE) and Dynamic-Weight\n(DW) models. Specifically, we first design a hierarchical multi-distribution\nrepresentation refinement module, employing a multi-level residual quantization\nto obtain fine-grained hierarchical representation. Then, the refined\nhierarchical representation is integrated into the existing single\nmulti-distribution models, seamlessly expanding them into mixed\nmulti-distribution models. Experimental results on both public and industrial\ndatasets validate the effectiveness and flexibility of HMDN.\n","authors":["Xingyu Lou","Yu Yang","Kuiyao Dong","Heyuan Huang","Wenyi Yu","Ping Wang","Xiu Li","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01331v1","updated":"2024-08-02T15:29:39Z","published":"2024-08-02T15:29:39Z","title":"UnifiedNN: Efficient Neural Network Training on the Cloud","summary":"  Nowadays, cloud-based services are widely favored over the traditional\napproach of locally training a Neural Network (NN) model. Oftentimes, a cloud\nservice processes multiple requests from users--thus training multiple NN\nmodels concurrently. However, training NN models concurrently is a challenging\nprocess, which typically requires significant amounts of available computing\nresources and takes a long time to complete. In this paper, we present\nUnifiedNN to effectively train multiple NN models concurrently on the cloud.\nUnifiedNN effectively \"combines\" multiple NN models and features several memory\nand time conservation mechanisms to train multiple NN models simultaneously\nwithout impacting the accuracy of the training process. Specifically, UnifiedNN\nmerges multiple NN models and creates a large singular unified model in order\nto efficiently train all models at once. We have implemented a prototype of\nUnifiedNN in PyTorch and we have compared its performance with relevant\nstate-of-the-art frameworks. Our experimental results demonstrate that\nUnifiedNN can reduce memory consumption by up to 53% and training time by up to\n81% when compared with vanilla PyTorch without impacting the model training and\ntesting accuracy. Finally, our results indicate that UnifiedNN can reduce\nmemory consumption by up to 52% and training time by up to 41% when compared to\nstate-of-the-art frameworks when training multiple models concurrently.\n","authors":["Sifat Ut Taki","Spyridon Mastorakis","Arthi Padmanabhan"],"pdf_url":"https://arxiv.org/pdf/2408.01331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00736v4","updated":"2024-08-02T15:26:37Z","published":"2023-01-02T16:11:05Z","title":"Mixed moving average field guided learning for spatio-temporal data","summary":"  Influenced mixed moving average fields are a versatile modeling class for\nspatio-temporal data. However, their predictive distribution is not generally\nknown. Under this modeling assumption, we define a novel spatio-temporal\nembedding and a theory-guided machine learning approach that employs a\ngeneralized Bayesian algorithm to make ensemble forecasts. We use Lipschitz\npredictors and determine fixed-time and any-time PAC Bayesian bounds in the\nbatch learning setting. Performing causal forecast is a highlight of our\nmethodology as its potential application to data with spatial and temporal\nshort and long-range dependence. We then test the performance of our learning\nmethodology by using linear predictors and data sets simulated from a\nspatio-temporal Ornstein-Uhlenbeck process.\n","authors":["Imma Valentina Curato","Orkun Furat","Lorenzo Proietti","Bennet Stroeh"],"pdf_url":"https://arxiv.org/pdf/2301.00736v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02659v2","updated":"2024-08-02T15:13:26Z","published":"2024-07-02T20:49:21Z","title":"LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model\n  Training Data Through Knowledge Graph Comparison","summary":"  In light of recent legal allegations brought by publishers, newspapers, and\nother creators of copyrighted corpora against large language model developers\nwho use their copyrighted materials for training or fine-tuning purposes, we\npropose a novel system, a variant of a plagiarism detection system, that\nassesses whether a knowledge source has been used in the training or\nfine-tuning of a large language model. Unlike current methods, we utilize an\napproach that uses Resource Description Framework (RDF) triples to create\nknowledge graphs from both a source document and an LLM continuation of that\ndocument. These graphs are then analyzed with respect to content using cosine\nsimilarity and with respect to structure using a normalized version of graph\nedit distance that shows the degree of isomorphism. Unlike traditional\nplagiarism systems that focus on content matching and keyword identification\nbetween a source and a target corpus, our approach enables a broader and more\naccurate evaluation of similarity between a source document and LLM\ncontinuation by focusing on relationships between ideas and their organization\nwith regards to others. Additionally, our approach does not require access to\nLLM metrics like perplexity that may be unavailable in closed large language\nmodel \"black-box\" systems, as well as the training corpus. We thus assess\nwhether an LLM has \"plagiarized\" a corpus in its continuation through\nsimilarity measures. A prototype of our system will be found on a hyperlinked\nGitHub repository.\n","authors":["Devam Mondal","Carlo Lipizzi"],"pdf_url":"https://arxiv.org/pdf/2407.02659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01318v1","updated":"2024-08-02T15:12:52Z","published":"2024-08-02T15:12:52Z","title":"Point Prediction for Streaming Data","summary":"  We present two new approaches for point prediction with streaming data. One\nis based on the Count-Min sketch (CMS) and the other is based on Gaussian\nprocess priors with a random bias. These methods are intended for the most\ngeneral predictive problems where no true model can be usefully formulated for\nthe data stream. In statistical contexts, this is often called the\n$\\mathcal{M}$-open problem class. Under the assumption that the data consists\nof i.i.d samples from a fixed distribution function $F$, we show that the\nCMS-based estimates of the distribution function are consistent.\n  We compare our new methods with two established predictors in terms of\ncumulative $L^1$ error. One is based on the Shtarkov solution (often called the\nnormalized maximum likelihood) in the normal experts setting and the other is\nbased on Dirichlet process priors. These comparisons are for two cases. The\nfirst is one-pass meaning that the updating of the predictors is done using the\nfact that the CMS is a sketch. For predictors that are not one-pass, we use\nstreaming $K$-means to give a representative subset of fixed size that can be\nupdated as data accumulate.\n  Preliminary computational work suggests that the one-pass median version of\nthe CMS method is rarely outperformed by the other methods for sufficiently\ncomplex data. We also find that predictors based on Gaussian process priors\nwith random biases perform well. The Shtarkov predictors we use here did not\nperform as well probably because we were only using the simplest example. The\nother predictors seemed to perform well mainly when the data did not look like\nthey came from an M-open data generator.\n","authors":["Aleena Chanda","N. V. Vinodchandran","Bertrand Clarke"],"pdf_url":"https://arxiv.org/pdf/2408.01318v1.pdf","comment":"42 pages, two figures"},{"id":"http://arxiv.org/abs/2408.01307v1","updated":"2024-08-02T15:00:04Z","published":"2024-08-02T15:00:04Z","title":"Decentralized Smoothing ADMM for Quantile Regression with Non-Convex\n  Sparse Penalties","summary":"  In the rapidly evolving internet-of-things (IoT) ecosystem, effective data\nanalysis techniques are crucial for handling distributed data generated by\nsensors. Addressing the limitations of existing methods, such as the\nsub-gradient approach, which fails to distinguish between active and non-active\ncoefficients effectively, this paper introduces the decentralized smoothing\nalternating direction method of multipliers (DSAD) for penalized quantile\nregression. Our method leverages non-convex sparse penalties like the minimax\nconcave penalty (MCP) and smoothly clipped absolute deviation (SCAD), improving\nthe identification and retention of significant predictors. DSAD incorporates a\ntotal variation norm within a smoothing ADMM framework, achieving consensus\namong distributed nodes and ensuring uniform model performance across disparate\ndata sources. This approach overcomes traditional convergence challenges\nassociated with non-convex penalties in decentralized settings. We present\ntheoretical proofs and extensive simulation results to validate the\neffectiveness of the DSAD, demonstrating its superiority in achieving reliable\nconvergence and enhancing estimation accuracy compared with prior methods.\n","authors":["Reza Mirzaeifard","Diyako Ghaderyan","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2408.01307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13586v2","updated":"2024-08-02T14:59:48Z","published":"2024-05-22T12:30:25Z","title":"Bond Graphs for multi-physics informed Neural Networks for multi-variate\n  time series","summary":"  In the trend of hybrid Artificial Intelligence techniques, Physical-Informed\nMachine Learning has seen a growing interest. It operates mainly by imposing\ndata, learning, or architecture bias with simulation data, Partial Differential\nEquations, or equivariance and invariance properties. While it has shown great\nsuccess on tasks involving one physical domain, such as fluid dynamics,\nexisting methods are not adapted to tasks with complex multi-physical and\nmulti-domain phenomena. In addition, it is mainly formulated as an end-to-end\nlearning scheme. To address these challenges, we propose to leverage Bond\nGraphs, a multi-physics modeling approach, together with Message Passing Graph\nNeural Networks. We propose a Neural Bond graph Encoder (NBgE) producing\nmulti-physics-informed representations that can be fed into any task-specific\nmodel. It provides a unified way to integrate both data and architecture biases\nin deep learning. Our experiments on two challenging multi-domain physical\nsystems - a Direct Current Motor and the Respiratory System - demonstrate the\neffectiveness of our approach on a multivariate time-series forecasting task.\n","authors":["Alexis-Raja Brachet","Pierre-Yves Richard","Cline Hudelot"],"pdf_url":"https://arxiv.org/pdf/2405.13586v2.pdf","comment":"9 pages, 3 figures, paper under review"},{"id":"http://arxiv.org/abs/2407.21043v2","updated":"2024-08-02T14:58:54Z","published":"2024-07-22T04:07:12Z","title":"CP-Prompt: Composition-Based Cross-modal Prompting for\n  Domain-Incremental Continual Learning","summary":"  The key challenge of cross-modal domain-incremental learning (DIL) is to\nenable the learning model to continuously learn from novel data with different\nfeature distributions under the same task without forgetting old ones. However,\nexisting top-performing methods still cause high forgetting rates, by lacking\nintra-domain knowledge extraction and inter-domain common prompting strategy.\nIn this paper, we propose a simple yet effective framework, CP-Prompt, by\ntraining limited parameters to instruct a pre-trained model to learn new\ndomains and avoid forgetting existing feature distributions. CP-Prompt captures\nintra-domain knowledge by compositionally inserting personalized prompts on\nmulti-head self-attention layers and then learns the inter-domain knowledge\nwith a common prompting strategy. CP-Prompt shows superiority compared with\nstate-of-the-art baselines among three widely evaluated DIL tasks. The source\ncode is available at https://github.com/dannis97500/CP_Prompt.\n","authors":["Yu Feng","Zhen Tian","Yifan Zhu","Zongfu Han","Haoran Luo","Guangwei Zhang","Meina Song"],"pdf_url":"https://arxiv.org/pdf/2407.21043v2.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2401.13979v2","updated":"2024-08-02T14:50:05Z","published":"2024-01-25T06:45:32Z","title":"Routoo: Learning to Route to Large Language Models Effectively","summary":"  Developing foundational large language models (LLMs) is becoming increasingly\ncostly and inefficient. Also, closed-source and larger open-source models\ngenerally offer better response quality but come with higher inference costs\nthan smaller models. In this paper, we introduce Routoo, an architecture\ndesigned to optimize the selection of LLMs for specific prompts based on\nperformance, cost, and efficiency. Routoo consists of two key components: a\nperformance predictor and a cost-aware decoding. The performance predictor is a\nlightweight LLM that estimates the performance of various underlying LLMs\nwithout needing to execute and evaluate them. The cost-aware decoding then\nselects the most suitable model based on these predictions and other\nconstraints like cost and latency. We evaluated Routoo using the MMLU benchmark\nacross 57 domains employing open-source models. Our results show that Routoo\nmatches the performance of the Mixtral 8x7b model while reducing inference\ncosts by one-third. Additionally, by allowing increased costs, Routoo surpasses\nMixtral's accuracy by over 5% at equivalent costs, achieving an accuracy of\n75.9%. When integrating GPT4 into our model pool, Routoo nearly matches GPT4's\nperformance at half the cost and exceeds it with a 25% cost reduction. These\noutcomes highlight Routoo's potential to create new SOTA in a cost-effective\nmanner by leveraging the collective knowledge of multiple LLMs.\n","authors":["Alireza Mohammadshahi","Arshad Rafiq Shaikh","Majid Yazdani"],"pdf_url":"https://arxiv.org/pdf/2401.13979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01301v1","updated":"2024-08-02T14:43:45Z","published":"2024-08-02T14:43:45Z","title":"A Decision-driven Methodology for Designing Uncertainty-aware AI\n  Self-Assessment","summary":"  Artificial intelligence (AI) has revolutionized decision-making processes and\nsystems throughout society and, in particular, has emerged as a significant\ntechnology in high-impact scenarios of national interest. Yet, despite AI's\nimpressive predictive capabilities in controlled settings, it still suffers\nfrom a range of practical setbacks preventing its widespread use in various\ncritical scenarios. In particular, it is generally unclear if a given AI\nsystem's predictions can be trusted by decision-makers in downstream\napplications. To address the need for more transparent, robust, and trustworthy\nAI systems, a suite of tools has been developed to quantify the uncertainty of\nAI predictions and, more generally, enable AI to \"self-assess\" the reliability\nof its predictions. In this manuscript, we categorize methods for AI\nself-assessment along several key dimensions and provide guidelines for\nselecting and designing the appropriate method for a practitioner's needs. In\nparticular, we focus on uncertainty estimation techniques that consider the\nimpact of self-assessment on the choices made by downstream decision-makers and\non the resulting costs and benefits of decision outcomes. To demonstrate the\nutility of our methodology for self-assessment design, we illustrate its use\nfor two realistic national-interest scenarios. This manuscript is a practical\nguide for machine learning engineers and AI system users to select the ideal\nself-assessment techniques for each problem.\n","authors":["Gregory Canal","Vladimir Leung","Philip Sage","Eric Heim","I-Jeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01300v1","updated":"2024-08-02T14:41:36Z","published":"2024-08-02T14:41:36Z","title":"Assessing Robustness of Machine Learning Models using Covariate\n  Perturbations","summary":"  As machine learning models become increasingly prevalent in critical\ndecision-making models and systems in fields like finance, healthcare, etc.,\nensuring their robustness against adversarial attacks and changes in the input\ndata is paramount, especially in cases where models potentially overfit. This\npaper proposes a comprehensive framework for assessing the robustness of\nmachine learning models through covariate perturbation techniques. We explore\nvarious perturbation strategies to assess robustness and examine their impact\non model predictions, including separate strategies for numeric and non-numeric\nvariables, summaries of perturbations to assess and compare model robustness\nacross different scenarios, and local robustness diagnosis to identify any\nregions in the data where a model is particularly unstable. Through empirical\nstudies on real world dataset, we demonstrate the effectiveness of our approach\nin comparing robustness across models, identifying the instabilities in the\nmodel, and enhancing model robustness.\n","authors":["Arun Prakash R","Anwesha Bhattacharyya","Joel Vaughan","Vijayan N. Nair"],"pdf_url":"https://arxiv.org/pdf/2408.01300v1.pdf","comment":"31 pages, 11 figures, 14 tables"},{"id":"http://arxiv.org/abs/2408.01297v1","updated":"2024-08-02T14:37:28Z","published":"2024-08-02T14:37:28Z","title":"Optimal Mixed Integer Linear Optimization Trained Multivariate\n  Classification Trees","summary":"  Multivariate decision trees are powerful machine learning tools for\nclassification and regression that attract many researchers and industry\nprofessionals. An optimal binary tree has two types of vertices, (i) branching\nvertices which have exactly two children and where datapoints are assessed on a\nset of discrete features and (ii) leaf vertices at which datapoints are given a\nprediction, and can be obtained by solving a biobjective optimization problem\nthat seeks to (i) maximize the number of correctly classified datapoints and\n(ii) minimize the number of branching vertices. Branching vertices are linear\ncombinations of training features and therefore can be thought of as\nhyperplanes. In this paper, we propose two cut-based mixed integer linear\noptimization (MILO) formulations for designing optimal binary classification\ntrees (leaf vertices assign discrete classes). Our models leverage on-the-fly\nidentification of minimal infeasible subsystems (MISs) from which we derive\ncutting planes that hold the form of packing constraints. We show theoretical\nimprovements on the strongest flow-based MILO formulation currently in the\nliterature and conduct experiments on publicly available datasets to show our\nmodels' ability to scale, strength against traditional branch and bound\napproaches, and robustness in out-of-sample test performance. Our code and data\nare available on GitHub.\n","authors":["Brandon Alston","Illya V. Hicks"],"pdf_url":"https://arxiv.org/pdf/2408.01297v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2206.04857"},{"id":"http://arxiv.org/abs/2405.03389v2","updated":"2024-08-02T14:33:32Z","published":"2024-05-06T11:51:09Z","title":"Don't Waste Your Time: Early Stopping Cross-Validation","summary":"  State-of-the-art automated machine learning systems for tabular data often\nemploy cross-validation; ensuring that measured performances generalize to\nunseen data, or that subsequent ensembling does not overfit. However, using\nk-fold cross-validation instead of holdout validation drastically increases the\ncomputational cost of validating a single configuration. While ensuring better\ngeneralization and, by extension, better performance, the additional cost is\noften prohibitive for effective model selection within a time budget. We aim to\nmake model selection with cross-validation more effective. Therefore, we study\nearly stopping the process of cross-validation during model selection. We\ninvestigate the impact of early stopping on random search for two algorithms,\nMLP and random forest, across 36 classification datasets. We further analyze\nthe impact of the number of folds by considering 3-, 5-, and 10-folds. In\naddition, we investigate the impact of early stopping with Bayesian\noptimization instead of random search and also repeated cross-validation. Our\nexploratory study shows that even a simple-to-understand and easy-to-implement\nmethod consistently allows model selection to converge faster; in ~94% of all\ndatasets, on average by ~214%. Moreover, stopping cross-validation enables\nmodel selection to explore the search space more exhaustively by considering\n+167% configurations on average within one hour, while also obtaining better\noverall performance.\n","authors":["Edward Bergman","Lennart Purucker","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2405.03389v2.pdf","comment":"Accepted at Third International Conference on Automated Machine\n  Learning (AutoML 2024); for code, see\n  https://github.com/automl/DontWasteYourTime-early-stopping"},{"id":"http://arxiv.org/abs/2408.01294v1","updated":"2024-08-02T14:31:37Z","published":"2024-08-02T14:31:37Z","title":"Feature Clock: High-Dimensional Effects in Two-Dimensional Plots","summary":"  Humans struggle to perceive and interpret high-dimensional data. Therefore,\nhigh-dimensional data are often projected into two dimensions for\nvisualization. Many applications benefit from complex nonlinear dimensionality\nreduction techniques, but the effects of individual high-dimensional features\nare hard to explain in the two-dimensional space. Most visualization solutions\nuse multiple two-dimensional plots, each showing the effect of one\nhigh-dimensional feature in two dimensions; this approach creates a need for a\nvisual inspection of k plots for a k-dimensional input space. Our solution,\nFeature Clock, provides a novel approach that eliminates the need to inspect\nthese k plots to grasp the influence of original features on the data structure\ndepicted in two dimensions. Feature Clock enhances the explainability and\ncompactness of visualizations of embedded data and is available in an\nopen-source Python library.\n","authors":["Olga Ovcharenko","Rita Sevastjanova","Valentina Boeva"],"pdf_url":"https://arxiv.org/pdf/2408.01294v1.pdf","comment":"To be published in IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2407.14962v3","updated":"2024-08-02T14:26:55Z","published":"2024-07-20T18:48:35Z","title":"Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives","summary":"  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n","authors":["Desta Haileselassie Hagos","Rick Battle","Danda B. Rawat"],"pdf_url":"https://arxiv.org/pdf/2407.14962v3.pdf","comment":"This version is accepted for publication in the journal of IEEE\n  Transactions on Artificial Intelligence (TAI)"},{"id":"http://arxiv.org/abs/2308.11635v2","updated":"2024-08-02T14:25:40Z","published":"2023-08-13T23:54:40Z","title":"Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive\n  Learning for Cross-Subject EEG-based Emotion Recognition","summary":"  Electroencephalography (EEG) is an objective tool for emotion recognition\nwith promising applications. However, the scarcity of labeled data remains a\nmajor challenge in this field, limiting the widespread use of EEG-based emotion\nrecognition. In this paper, a semi-supervised Dual-stream Self-Attentive\nAdversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed\nto tackle the challenge of limited labeled data in cross-subject EEG-based\nemotion recognition. The DS-AGC framework includes two parallel streams for\nextracting non-structural and structural EEG features. The non-structural\nstream incorporates a semi-supervised multi-domain adaptation method to\nalleviate distribution discrepancy among labeled source domain, unlabeled\nsource domain, and unknown target domain. The structural stream develops a\ngraph contrastive learning method to extract effective graph-based feature\nrepresentation from multiple EEG channels in a semi-supervised manner. Further,\na self-attentive fusion module is developed for feature fusion, sample\nselection, and emotion recognition, which highlights EEG features more relevant\nto emotions and data samples in the labeled source domain that are closer to\nthe target domain. Extensive experiments conducted on two benchmark databases\n(SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-out\ncross-validation evaluation scheme show that the proposed model outperforms\nexisting methods under different incomplete label conditions (with an average\nimprovement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating its\neffectiveness in addressing the label scarcity problem in cross-subject\nEEG-based emotion recognition.\n","authors":["Weishan Ye","Zhiguo Zhang","Fei Teng","Min Zhang","Jianhong Wang","Dong Ni","Fali Li","Peng Xu","Zhen Liang"],"pdf_url":"https://arxiv.org/pdf/2308.11635v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.06496"},{"id":"http://arxiv.org/abs/2408.01283v1","updated":"2024-08-02T14:09:39Z","published":"2024-08-02T14:09:39Z","title":"A Tiny Supervised ODL Core with Auto Data Pruning for Human Activity\n  Recognition","summary":"  In this paper, we introduce a low-cost and low-power tiny supervised\non-device learning (ODL) core that can address the distributional shift of\ninput data for human activity recognition. Although ODL for resource-limited\nedge devices has been studied recently, how exactly to provide the training\nlabels to these devices at runtime remains an open-issue. To address this\nproblem, we propose to combine an automatic data pruning with supervised ODL to\nreduce the number queries needed to acquire predicted labels from a nearby\nteacher device and thus save power consumption during model retraining. The\ndata pruning threshold is automatically tuned, eliminating a manual threshold\ntuning. As a tinyML solution at a few mW for the human activity recognition, we\ndesign a supervised ODL core that supports our automatic data pruning using a\n45nm CMOS process technology. We show that the required memory size for the\ncore is smaller than the same-shaped multilayer perceptron (MLP) and the power\nconsumption is only 3.39mW. Experiments using a human activity recognition\ndataset show that the proposed automatic data pruning reduces the communication\nvolume by 55.7% and power consumption accordingly with only 0.9% accuracy loss.\n","authors":["Hiroki Matsutani","Radu Marculescu"],"pdf_url":"https://arxiv.org/pdf/2408.01283v1.pdf","comment":"IEEE BSN 2024 (accepted)"},{"id":"http://arxiv.org/abs/2310.04561v2","updated":"2024-08-02T14:08:59Z","published":"2023-10-06T19:55:40Z","title":"DragD3D: Realistic Mesh Editing with Rigidity Control Driven by 2D\n  Diffusion Priors","summary":"  Direct mesh editing and deformation are key components in the geometric\nmodeling and animation pipeline. Mesh editing methods are typically framed as\noptimization problems combining user-specified vertex constraints with a\nregularizer that determines the position of the rest of the vertices. The\nchoice of the regularizer is key to the realism and authenticity of the final\nresult. Physics and geometry-based regularizers are not aware of the global\ncontext and semantics of the object, and the more recent deep learning priors\nare limited to a specific class of 3D object deformations. Our main\ncontribution is a vertex-based mesh editing method called DragD3D based on (1)\na novel optimization formulation that decouples the rotation and stretch\ncomponents of the deformation and combines a 3D geometric regularizer with (2)\nthe recently introduced DDS loss which scores the faithfulness of the rendered\n2D image to one from a diffusion model. Thus, our deformation method achieves\nglobally realistic shape deformation which is not restricted to any class of\nobjects. Our new formulation optimizes directly the transformation of the\nneural Jacobian field explicitly separating the rotational and stretching\ncomponents. The objective function of the optimization combines the approximate\ngradients of DDS and the gradients from the geometric loss to satisfy the\nvertex constraints. Additional user control over desired global shape\ndeformation is made possible by allowing explicit per-triangle deformation\ncontrol as well as explicit separation of rotational and stretching components\nof the deformation. We show that our deformations can be controlled to yield\nrealistic shape deformations that are aware of the global context of the\nobjects, and provide better results than just using geometric regularizers.\n","authors":["Tianhao Xie","Eugene Belilovsky","Sudhir Mudur","Tiberiu Popa"],"pdf_url":"https://arxiv.org/pdf/2310.04561v2.pdf","comment":"11 pages, 8 figures, project page:\n  https://tianhaoxie.github.io/project/DragD3D/"},{"id":"http://arxiv.org/abs/2404.09916v2","updated":"2024-08-02T13:59:12Z","published":"2024-04-15T16:43:13Z","title":"Comprehensive Library of Variational LSE Solvers","summary":"  Linear systems of equations can be found in various mathematical domains, as\nwell as in the field of machine learning. By employing noisy intermediate-scale\nquantum devices, variational solvers promise to accelerate finding solutions\nfor large systems. Although there is a wealth of theoretical research on these\nalgorithms, only fragmentary implementations exist. To fill this gap, we have\ndeveloped the variational-lse-solver framework, which realizes existing\napproaches in literature, and introduces several enhancements. The\nuser-friendly interface is designed for researchers that work at the\nabstraction level of identifying and developing end-to-end applications.\n","authors":["Nico Meyer","Martin Rhn","Jakob Murauer","Axel Plinge","Christopher Mutschler","Daniel D. Scherer"],"pdf_url":"https://arxiv.org/pdf/2404.09916v2.pdf","comment":"Accepted to the 2nd International Workshop on Quantum Machine\n  Learning: From Research to Practice (QML@QCE 2024), Montr\\'eal, Qu\\'ebec,\n  Canada. 4 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2408.01273v1","updated":"2024-08-02T13:55:26Z","published":"2024-08-02T13:55:26Z","title":"Certified Robust Invariant Polytope Training in Neural Controlled ODEs","summary":"  We consider a nonlinear control system modeled as an ordinary differential\nequation subject to disturbance, with a state feedback controller parameterized\nas a feedforward neural network. We propose a framework for training\ncontrollers with certified robust forward invariant polytopes, where any\ntrajectory initialized inside the polytope remains within the polytope,\nregardless of the disturbance. First, we parameterize a family of lifted\ncontrol systems in a higher dimensional space, where the original neural\ncontrolled system evolves on an invariant subspace of each lifted system. We\nuse interval analysis and neural network verifiers to further construct a\nfamily of lifted embedding systems, carefully capturing the knowledge of this\ninvariant subspace. If the vector field of any lifted embedding system\nsatisfies a sign constraint at a single point, then a certain convex polytope\nof the original system is robustly forward invariant. Treating the neural\nnetwork controller and the lifted system parameters as variables, we propose an\nalgorithm to train controllers with certified forward invariant polytopes in\nthe closed-loop control system. Through two examples, we demonstrate how the\nsimplicity of the sign constraint allows our approach to scale with system\ndimension to over $50$ states, and outperform state-of-the-art Lyapunov-based\nsampling approaches in runtime.\n","authors":["Akash Harapanahalli","Samuel Coogan"],"pdf_url":"https://arxiv.org/pdf/2408.01273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02604v2","updated":"2024-08-02T13:45:53Z","published":"2024-07-02T18:43:10Z","title":"D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data\n  and eXpert model predictions","summary":"  Large vision language models (VLMs) have progressed incredibly from research\nto applicability for general-purpose use cases. LLaVA-Med, a pioneering large\nlanguage and vision assistant for biomedicine, can perform multi-modal\nbiomedical image and data analysis to provide a natural language interface for\nradiologists. While it is highly generalizable and works with multi-modal data,\nit is currently limited by well-known challenges that exist in the large\nlanguage model space. Hallucinations and imprecision in responses can lead to\nmisdiagnosis which currently hinder the clinical adaptability of VLMs. To\ncreate precise, user-friendly models in healthcare, we propose D-Rax -- a\ndomain-specific, conversational, radiologic assistance tool that can be used to\ngain insights about a particular radiologic image. In this study, we enhance\nthe conversational analysis of chest X-ray (CXR) images to support radiological\nreporting, offering comprehensive insights from medical imaging and aiding in\nthe formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the\nLLaVA-Med architecture on our curated enhanced instruction-following data,\ncomprising of images, instructions, as well as disease diagnosis and\ndemographic predictions derived from MIMIC-CXR imaging data, CXR-related visual\nquestion answer (VQA) pairs, and predictive outcomes from multiple expert AI\nmodels. We observe statistically significant improvement in responses when\nevaluated for both open and close-ended conversations. Leveraging the power of\nstate-of-the-art diagnostic models combined with VLMs, D-Rax empowers\nclinicians to interact with medical images using natural language, which could\npotentially streamline their decision-making process, enhance diagnostic\naccuracy, and conserve their time.\n","authors":["Hareem Nisar","Syed Muhammad Anwar","Zhifan Jiang","Abhijeet Parida","Ramon Sanchez-Jacob","Vishwesh Nath","Holger R. Roth","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2407.02604v2.pdf","comment":"accepted to the MICCAI 2024 Second International Workshop on\n  Foundation Models for General Medical AI"},{"id":"http://arxiv.org/abs/2408.01257v1","updated":"2024-08-02T13:27:56Z","published":"2024-08-02T13:27:56Z","title":"Detection and Characterization of Coordinated Online Behavior: A Survey","summary":"  Coordination is a fundamental aspect of life. The advent of social media has\nmade it integral also to online human interactions, such as those that\ncharacterize thriving online communities and social movements. At the same\ntime, coordination is also core to effective disinformation, manipulation, and\nhate campaigns. This survey collects, categorizes, and critically discusses the\nbody of work produced as a result of the growing interest on coordinated online\nbehavior. We reconcile industry and academic definitions, propose a\ncomprehensive framework to study coordinated online behavior, and review and\ncritically discuss the existing detection and characterization methods. Our\nanalysis identifies open challenges and promising directions of research,\nserving as a guide for scholars, practitioners, and policymakers in\nunderstanding and addressing the complexities inherent to online coordination.\n","authors":["Lorenzo Mannocci","Michele Mazza","Anna Monreale","Maurizio Tesconi","Stefano Cresci"],"pdf_url":"https://arxiv.org/pdf/2408.01257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15327v2","updated":"2024-08-02T13:25:16Z","published":"2024-06-21T17:40:46Z","title":"Fine-grained Attention in Hierarchical Transformers for Tabular\n  Time-series","summary":"  Tabular data is ubiquitous in many real-life systems. In particular,\ntime-dependent tabular data, where rows are chronologically related, is\ntypically used for recording historical events, e.g., financial transactions,\nhealthcare records, or stock history. Recently, hierarchical variants of the\nattention mechanism of transformer architectures have been used to model\ntabular time-series data. At first, rows (or columns) are encoded separately by\ncomputing attention between their fields. Subsequently, encoded rows (or\ncolumns) are attended to one another to model the entire tabular time-series.\nWhile efficient, this approach constrains the attention granularity and limits\nits ability to learn patterns at the field-level across separate rows, or\ncolumns. We take a first step to address this gap by proposing Fieldy, a\nfine-grained hierarchical model that contextualizes fields at both the row and\ncolumn levels. We compare our proposal against state of the art models on\nregression and classification tasks using public tabular time-series datasets.\nOur results show that combining row-wise and column-wise attention improves\nperformance without increasing model size. Code and data are available at\nhttps://github.com/raphaaal/fieldy.\n","authors":["Raphael Azorin","Zied Ben Houidi","Massimo Gallo","Alessandro Finamore","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2406.15327v2.pdf","comment":"9 pages; Camera Ready version"},{"id":"http://arxiv.org/abs/2305.18493v3","updated":"2024-08-02T13:16:48Z","published":"2023-05-29T13:47:51Z","title":"Insights from the Design Space Exploration of Flow-Guided Nanoscale\n  Localization","summary":"  Nanodevices with Terahertz (THz)-based wireless communication capabilities\nare providing a primer for flow-guided localization within the human\nbloodstreams. Such localization is allowing for assigning the locations of\nsensed events with the events themselves, providing benefits along the lines of\nearly and precise diagnostics, and reduced costs and invasiveness. Flow-guided\nlocalization is still in a rudimentary phase, with only a handful of works\ntargeting the problem. Nonetheless, the performance assessments of the proposed\nsolutions are already carried out in a non-standardized way, usually along a\nsingle performance metric, and ignoring various aspects that are relevant at\nsuch a scale (e.g., nanodevices' limited energy) and for such a challenging\nenvironment (e.g., extreme attenuation of in-body THz propagation). As such,\nthese assessments feature low levels of realism and cannot be compared in an\nobjective way. Toward addressing this issue, we account for the environmental\nand scale-related peculiarities of the scenario and assess the performance of\ntwo state-of-the-art flow-guided localization approaches along a set of\nheterogeneous performance metrics such as the accuracy and reliability of\nlocalization.\n","authors":["Filip Lemic","Gerard Calvo Bartra","Arnau Brosa Lpez","Jorge Torres Gmez","Jakob Struye","Falko Dressler","Sergi Abadal","Xavier Costa Perez"],"pdf_url":"https://arxiv.org/pdf/2305.18493v3.pdf","comment":"6 pages, 4 figures, 2 tables, 14 references, accepted at ACM\n  NanoCom'24"},{"id":"http://arxiv.org/abs/2408.01248v1","updated":"2024-08-02T13:10:33Z","published":"2024-08-02T13:10:33Z","title":"Deep progressive reinforcement learning-based flexible resource\n  scheduling framework for IRS and UAV-assisted MEC system","summary":"  The intelligent reflection surface (IRS) and unmanned aerial vehicle\n(UAV)-assisted mobile edge computing (MEC) system is widely used in temporary\nand emergency scenarios. Our goal is to minimize the energy consumption of the\nMEC system by jointly optimizing UAV locations, IRS phase shift, task\noffloading, and resource allocation with a variable number of UAVs. To this\nend, we propose a Flexible REsource Scheduling (FRES) framework by employing a\nnovel deep progressive reinforcement learning which includes the following\ninnovations: Firstly, a novel multi-task agent is presented to deal with the\nmixed integer nonlinear programming (MINLP) problem. The multi-task agent has\ntwo output heads designed for different tasks, in which a classified head is\nemployed to make offloading decisions with integer variables while a fitting\nhead is applied to solve resource allocation with continuous variables.\nSecondly, a progressive scheduler is introduced to adapt the agent to the\nvarying number of UAVs by progressively adjusting a part of neurons in the\nagent. This structure can naturally accumulate experiences and be immune to\ncatastrophic forgetting. Finally, a light taboo search (LTS) is introduced to\nenhance the global search of the FRES. The numerical results demonstrate the\nsuperiority of the FRES framework which can make real-time and optimal resource\nscheduling even in dynamic MEC systems.\n","authors":["Li Dong","Feibo Jiang","Minjie Wang","Yubo Peng","Xiaolong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01248v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.01244v1","updated":"2024-08-02T13:05:33Z","published":"2024-08-02T13:05:33Z","title":"Automated Classification of Dry Bean Varieties Using XGBoost and SVM\n  Models","summary":"  This paper presents a comparative study on the automated classification of\nseven different varieties of dry beans using machine learning models.\nLeveraging a dataset of 12,909 dry bean samples, reduced from an initial 13,611\nthrough outlier removal and feature extraction, we applied Principal Component\nAnalysis (PCA) for dimensionality reduction and trained two multiclass\nclassifiers: XGBoost and Support Vector Machine (SVM). The models were\nevaluated using nested cross-validation to ensure robust performance assessment\nand hyperparameter tuning. The XGBoost and SVM models achieved overall correct\nclassification rates of 94.00% and 94.39%, respectively. The results underscore\nthe efficacy of these machine learning approaches in agricultural applications,\nparticularly in enhancing the uniformity and efficiency of seed classification.\nThis study contributes to the growing body of work on precision agriculture,\ndemonstrating that automated systems can significantly support seed quality\ncontrol and crop yield optimization. Future work will explore incorporating\nmore diverse datasets and advanced algorithms to further improve classification\naccuracy.\n","authors":["Ramtin Ardeshirifar"],"pdf_url":"https://arxiv.org/pdf/2408.01244v1.pdf","comment":"8 pages, 4 figurs"},{"id":"http://arxiv.org/abs/2407.21310v2","updated":"2024-08-02T13:03:00Z","published":"2024-07-31T03:26:14Z","title":"MSMA: Multi-agent Trajectory Prediction in Connected and Autonomous\n  Vehicle Environment with Multi-source Data Integration","summary":"  The prediction of surrounding vehicle trajectories is crucial for\ncollision-free path planning. In this study, we focus on a scenario where a\nconnected and autonomous vehicle (CAV) serves as the central agent, utilizing\nboth sensors and communication technologies to perceive its surrounding\ntraffics consisting of autonomous vehicles (AVs), connected vehicles (CVs), and\nhuman-driven vehicles (HDVs). Our trajectory prediction task is aimed at all\nthe detected surrounding vehicles. To effectively integrate the multi-source\ndata from both sensor and communication technologies, we propose a deep\nlearning framework called MSMA utilizing a cross-attention module for\nmulti-source data fusion. Vector map data is utilized to provide contextual\ninformation. The trajectory dataset is collected in CARLA simulator with\nsynthesized data errors introduced. Numerical experiments demonstrate that in a\nmixed traffic flow scenario, the integration of data from different sources\nenhances our understanding of the environment. This notably improves trajectory\nprediction accuracy, particularly in situations with a high CV market\npenetration rate. The code is available at: https://github.com/xichennn/MSMA.\n","authors":["Xi Chen","Rahul Bhadani","Zhanbo Sun","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2407.21310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09602v2","updated":"2024-08-02T13:00:54Z","published":"2024-07-12T18:00:02Z","title":"Real-time gravitational-wave inference for binary neutron stars using\n  machine learning","summary":"  Mergers of binary neutron stars (BNSs) emit signals in both the\ngravitational-wave (GW) and electromagnetic (EM) spectra. Famously, the 2017\nmulti-messenger observation of GW170817 led to scientific discoveries across\ncosmology, nuclear physics, and gravity. Central to these results were the sky\nlocalization and distance obtained from GW data, which, in the case of\nGW170817, helped to identify the associated EM transient, AT 2017gfo, 11 hours\nafter the GW signal. Fast analysis of GW data is critical for directing\ntime-sensitive EM observations; however, due to challenges arising from the\nlength and complexity of signals, it is often necessary to make approximations\nthat sacrifice accuracy. Here, we present a machine learning framework that\nperforms complete BNS inference in just one second without making any such\napproximations. Our approach enhances multi-messenger observations by providing\n(i) accurate localization even before the merger; (ii) improved localization\nprecision by $\\sim30\\%$ compared to approximate low-latency methods; and (iii)\ndetailed information on luminosity distance, inclination, and masses, which can\nbe used to prioritize expensive telescope time. Additionally, the flexibility\nand reduced cost of our method open new opportunities for equation-of-state\nstudies. Finally, we demonstrate that our method scales to extremely long\nsignals, up to an hour in length, thus serving as a blueprint for data analysis\nfor next-generation ground- and space-based detectors.\n","authors":["Maximilian Dax","Stephen R. Green","Jonathan Gair","Nihar Gupte","Michael Prrer","Vivien Raymond","Jonas Wildberger","Jakob H. Macke","Alessandra Buonanno","Bernhard Schlkopf"],"pdf_url":"https://arxiv.org/pdf/2407.09602v2.pdf","comment":"8+8 pages, 3+7 figures"},{"id":"http://arxiv.org/abs/2408.01239v1","updated":"2024-08-02T12:58:08Z","published":"2024-08-02T12:58:08Z","title":"Tailoring Graph Neural Network-based Flow-guided Localization to\n  Individual Bloodstreams and Activities","summary":"  Flow-guided localization using in-body nanodevices in the bloodstream is\nexpected to be beneficial for early disease detection, continuous monitoring of\nbiological conditions, and targeted treatment. The nanodevices face size and\npower constraints that produce erroneous raw data for localization purposes.\nOn-body anchors receive this data, and use it to derive the locations of\ndiagnostic events of interest. Different Machine Learning (ML) approaches have\nbeen recently proposed for this task, yet they are currently restricted to a\nreference bloodstream of a resting patient. As such, they are unable to deal\nwith the physical diversity of patients' bloodstreams and cannot provide\ncontinuous monitoring due to changes in individual patient's activities. Toward\naddressing these issues for the current State-of-the-Art (SotA) flow-guided\nlocalization approach based on Graph Neural Networks (GNNs), we propose a\npipeline for GNN adaptation based on individual physiological indicators\nincluding height, weight, and heart rate. Our results indicate that the\nproposed adaptions are beneficial in reconciling the individual differences\nbetween bloodstreams and activities.\n","authors":["Pablo Galvn","Filip Lemic","Gerard Calvo Bartra","Sergi Abadal","Xavier Costa Prez"],"pdf_url":"https://arxiv.org/pdf/2408.01239v1.pdf","comment":"7 pages, 9 figures, 2 tables, 16 references, accepted at ACM\n  NanoCom'25"},{"id":"http://arxiv.org/abs/2408.01230v1","updated":"2024-08-02T12:40:01Z","published":"2024-08-02T12:40:01Z","title":"HeteroMorpheus: Universal Control Based on Morphological Heterogeneity\n  Modeling","summary":"  In the field of robotic control, designing individual controllers for each\nrobot leads to high computational costs. Universal control policies, applicable\nacross diverse robot morphologies, promise to mitigate this challenge.\nPredominantly, models based on Graph Neural Networks (GNN) and Transformers are\nemployed, owing to their effectiveness in capturing relational dynamics across\na robot's limbs. However, these models typically employ homogeneous graph\nstructures that overlook the functional diversity of different limbs. To bridge\nthis gap, we introduce HeteroMorpheus, a novel method based on heterogeneous\ngraph Transformer. This method uniquely addresses limb heterogeneity, fostering\nbetter representation of robot dynamics of various morphologies. Through\nextensive experiments we demonstrate the superiority of HeteroMorpheus against\nstate-of-the-art methods in the capability of policy generalization, including\nzero-shot generalization and sample-efficient transfer to unfamiliar robot\nmorphologies.\n","authors":["YiFan Hao","Yang Yang","Junru Song","Wei Peng","Weien Zhou","Tingsong Jiang","Wen Yao"],"pdf_url":"https://arxiv.org/pdf/2408.01230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01215v1","updated":"2024-08-02T12:04:19Z","published":"2024-08-02T12:04:19Z","title":"ZNorm: Z-Score Gradient Normalization for Accelerating Neural Network\n  Training","summary":"  The rapid advancements in deep learning necessitate efficient training\nmethods for deep neural networks (DNNs). As models grow in complexity,\nvanishing and exploding gradients impede convergence and performance. We\npropose Z-Score Normalization for Gradient Descent (ZNorm), an innovative\ntechnique that adjusts only the gradients to enhance training efficiency and\nimprove model performance. ZNorm normalizes the overall gradients, providing\nconsistent gradient scaling across layers, thereby reducing the risks of\nvanishing and exploding gradients. Our extensive experiments on CIFAR-10 and\nmedical datasets demonstrate that ZNorm not only accelerates convergence but\nalso enhances performance metrics. ZNorm consistently outperforms existing\nmethods, achieving superior results using the same computational settings. In\nmedical imaging applications, ZNorm improves tumor prediction and segmentation\nperformances, underscoring its practical utility. These findings highlight\nZNorm's potential as a robust and versatile tool for improving the efficiency\nand effectiveness of deep neural network training across a wide range of\narchitectures and applications.\n","authors":["Juyoung Yun","Hoyoung Kim","Suin Cho","Hangil Kang"],"pdf_url":"https://arxiv.org/pdf/2408.01215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00950v2","updated":"2024-08-02T11:54:57Z","published":"2023-12-13T12:57:55Z","title":"Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G\n  Subnetworks","summary":"  In this paper, we present an unsupervised approach for frequency sub-band\nallocation in wireless networks using graph-based learning. We consider a dense\ndeployment of subnetworks in the factory environment with a limited number of\nsub-bands which must be optimally allocated to coordinate inter-subnetwork\ninterference. We model the subnetwork deployment as a conflict graph and\npropose an unsupervised learning approach inspired by the graph colouring\nheuristic and the Potts model to optimize the sub-band allocation using graph\nneural networks. The numerical evaluation shows that the proposed method\nachieves close performance to the centralized greedy colouring sub-band\nallocation heuristic with lower computational time complexity. In addition, it\nincurs reduced signalling overhead compared to iterative optimization\nheuristics that require all the mutual interfering channel information. We\nfurther demonstrate that the method is robust to different network settings.\n","authors":["Daniel Abode","Ramoni Adeogun","Lou Salan","Renato Abreu","Thomas Jacobsen","Gilberto Berardinelli"],"pdf_url":"https://arxiv.org/pdf/2401.00950v2.pdf","comment":"Accepted in VTC Fall 2024"},{"id":"http://arxiv.org/abs/2408.01200v1","updated":"2024-08-02T11:29:21Z","published":"2024-08-02T11:29:21Z","title":"Certifiably Robust Encoding Schemes","summary":"  Quantum machine learning uses principles from quantum mechanics to process\ndata, offering potential advances in speed and performance. However, previous\nwork has shown that these models are susceptible to attacks that manipulate\ninput data or exploit noise in quantum circuits. Following this, various\nstudies have explored the robustness of these models. These works focus on the\nrobustness certification of manipulations of the quantum states. We extend this\nline of research by investigating the robustness against perturbations in the\nclassical data for a general class of data encoding schemes. We show that for\nsuch schemes, the addition of suitable noise channels is equivalent to\nevaluating the mean value of the noiseless classifier at the smoothed data,\nakin to Randomized Smoothing from classical machine learning. Using our general\nframework, we show that suitable additions of phase-damping noise channels\nimprove empirical and provable robustness for the considered class of encoding\nschemes.\n","authors":["Aman Saxena","Tom Wollschlger","Nicola Franco","Jeanette Miriam Lorenz","Stephan Gnnemann"],"pdf_url":"https://arxiv.org/pdf/2408.01200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01187v1","updated":"2024-08-02T11:14:41Z","published":"2024-08-02T11:14:41Z","title":"Optimizing Variational Quantum Circuits Using Metaheuristic Strategies\n  in Reinforcement Learning","summary":"  Quantum Reinforcement Learning (QRL) offers potential advantages over\nclassical Reinforcement Learning, such as compact state space representation\nand faster convergence in certain scenarios. However, practical benefits\nrequire further validation. QRL faces challenges like flat solution landscapes,\nwhere traditional gradient-based methods are inefficient, necessitating the use\nof gradient-free algorithms. This work explores the integration of\nmetaheuristic algorithms -- Particle Swarm Optimization, Ant Colony\nOptimization, Tabu Search, Genetic Algorithm, Simulated Annealing, and Harmony\nSearch -- into QRL. These algorithms provide flexibility and efficiency in\nparameter optimization. Evaluations in $5\\times5$ MiniGrid Reinforcement\nLearning environments show that, all algorithms yield near-optimal results,\nwith Simulated Annealing and Particle Swarm Optimization performing best. In\nthe Cart Pole environment, Simulated Annealing, Genetic Algorithms, and\nParticle Swarm Optimization achieve optimal results, while the others perform\nslightly better than random action selection. These findings demonstrate the\npotential of Particle Swarm Optimization and Simulated Annealing for efficient\nQRL learning, emphasizing the need for careful algorithm selection and\nadaptation.\n","authors":["Michael Klle","Daniel Seidl","Maximilian Zorn","Philipp Altmann","Jonas Stein","Thomas Gabor"],"pdf_url":"https://arxiv.org/pdf/2408.01187v1.pdf","comment":"Accepted at QCE24 - QCRL24 Workshop"},{"id":"http://arxiv.org/abs/2408.01180v1","updated":"2024-08-02T11:02:38Z","published":"2024-08-02T11:02:38Z","title":"Nested Music Transformer: Sequentially Decoding Compound Tokens in\n  Symbolic Music and Audio Generation","summary":"  Representing symbolic music with compound tokens, where each token consists\nof several different sub-tokens representing a distinct musical feature or\nattribute, offers the advantage of reducing sequence length. While previous\nresearch has validated the efficacy of compound tokens in music sequence\nmodeling, predicting all sub-tokens simultaneously can lead to suboptimal\nresults as it may not fully capture the interdependencies between them. We\nintroduce the Nested Music Transformer (NMT), an architecture tailored for\ndecoding compound tokens autoregressively, similar to processing flattened\ntokens, but with low memory usage. The NMT consists of two transformers: the\nmain decoder that models a sequence of compound tokens and the sub-decoder for\nmodeling sub-tokens of each compound token. The experiment results showed that\napplying the NMT to compound tokens can enhance the performance in terms of\nbetter perplexity in processing various symbolic music datasets and discrete\naudio tokens from the MAESTRO dataset.\n","authors":["Jiwoo Ryu","Hao-Wen Dong","Jongmin Jung","Dasaem Jeong"],"pdf_url":"https://arxiv.org/pdf/2408.01180v1.pdf","comment":"Accepted at 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)"},{"id":"http://arxiv.org/abs/2408.01173v1","updated":"2024-08-02T10:47:10Z","published":"2024-08-02T10:47:10Z","title":"Sustainable Diffusion-based Incentive Mechanism for Generative AI-driven\n  Digital Twins in Industrial Cyber-Physical Systems","summary":"  Industrial Cyber-Physical Systems (ICPSs) are an integral component of modern\nmanufacturing and industries. By digitizing data throughout the product life\ncycle, Digital Twins (DTs) in ICPSs enable a shift from current industrial\ninfrastructures to intelligent and adaptive infrastructures. Thanks to data\nprocess capability, Generative Artificial Intelligence (GAI) can drive the\nconstruction and update of DTs to improve predictive accuracy and prepare for\ndiverse smart manufacturing. However, mechanisms that leverage sensing\nIndustrial Internet of Things (IIoT) devices to share data for the construction\nof DTs are susceptible to adverse selection problems. In this paper, we first\ndevelop a GAI-driven DT architecture for ICPSs. To address the adverse\nselection problem caused by information asymmetry, we propose a contract theory\nmodel and develop the sustainable diffusion-based soft actor-critic algorithm\nto identify the optimal feasible contract. Specifically, we leverage the\ndynamic structured pruning technique to reduce parameter numbers of actor\nnetworks, allowing sustainability and efficient implementation of the proposed\nalgorithm. Finally, numerical results demonstrate the effectiveness of the\nproposed scheme.\n","authors":["Jinbo Wen","Jiawen Kang","Dusit Niyato","Yang Zhang","Shiwen Mao"],"pdf_url":"https://arxiv.org/pdf/2408.01173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01163v1","updated":"2024-08-02T10:25:19Z","published":"2024-08-02T10:25:19Z","title":"Domain Adaptation-Enhanced Searchlight: Enabling brain decoding from\n  visual perception to mental imagery","summary":"  In cognitive neuroscience and brain-computer interface research, accurately\npredicting imagined stimuli is crucial. This study investigates the\neffectiveness of Domain Adaptation (DA) in enhancing imagery prediction using\nprimarily visual data from fMRI scans of 18 subjects. Initially, we train a\nbaseline model on visual stimuli to predict imagined stimuli, utilizing data\nfrom 14 brain regions. We then develop several models to improve imagery\nprediction, comparing different DA methods. Our results demonstrate that DA\nsignificantly enhances imagery prediction, especially with the Regular Transfer\napproach. We then conduct a DA-enhanced searchlight analysis using Regular\nTransfer, followed by permutation-based statistical tests to identify brain\nregions where imagery decoding is consistently above chance across subjects.\nOur DA-enhanced searchlight predicts imagery contents in a highly distributed\nset of brain regions, including the visual cortex and the frontoparietal\ncortex, thereby outperforming standard cross-domain classification methods. The\ncomplete code and data for this paper have been made openly available for the\nuse of the scientific community.\n","authors":["Alexander Olza","David Soto","Roberto Santana"],"pdf_url":"https://arxiv.org/pdf/2408.01163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01156v1","updated":"2024-08-02T10:16:28Z","published":"2024-08-02T10:16:28Z","title":"TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for\n  T-Cell Receptor Repertoires Generation","summary":"  T-cell receptors (TCRs) play a crucial role in the immune system by\nrecognizing and binding to specific antigens presented by infected or cancerous\ncells. Understanding the sequence patterns of TCRs is essential for developing\ntargeted immune therapies and designing effective vaccines. Language models,\nsuch as auto-regressive transformers, offer a powerful solution to this problem\nby learning the probability distributions of TCR repertoires, enabling the\ngeneration of new TCR sequences that inherit the underlying patterns of the\nrepertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only\ntransformer architecture, designed to uncover and replicate sequence patterns\nin TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring\nsequence probability distributions measured by Pearson correlation coefficient.\nFurthermore, by leveraging Reinforcement Learning(RL), we adapted the\ndistribution of TCR sequences to generate TCRs capable of recognizing specific\npeptides, offering significant potential for advancing targeted immune\ntherapies and vaccine development. With the efficacy of RL, fine-tuned\npretrained TCR-GPT models demonstrated the ability to produce TCR repertoires\nlikely to bind specific peptides, illustrating RL's efficiency in enhancing the\nmodel's adaptability to the probability distributions of biologically relevant\nTCR sequences.\n","authors":["Yicheng Lin","Dandan Zhang","Yun Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01144v1","updated":"2024-08-02T09:44:18Z","published":"2024-08-02T09:44:18Z","title":"Enhanced Prediction of Ventilator-Associated Pneumonia in Patients with\n  Traumatic Brain Injury Using Advanced Machine Learning Techniques","summary":"  Background: Ventilator-associated pneumonia (VAP) in traumatic brain injury\n(TBI) patients poses a significant mortality risk and imposes a considerable\nfinancial burden on patients and healthcare systems. Timely detection and\nprognostication of VAP in TBI patients are crucial to improve patient outcomes\nand alleviate the strain on healthcare resources.\n  Methods: We implemented six machine learning models using the MIMIC-III\ndatabase. Our methodology included preprocessing steps, such as feature\nselection with CatBoost and expert opinion, addressing class imbalance with the\nSynthetic Minority Oversampling Technique (SMOTE), and rigorous model tuning\nthrough 5-fold cross-validation to optimize hyperparameters. Key models\nevaluated included SVM, Logistic Regression, Random Forest, XGBoost, ANN, and\nAdaBoost. Additionally, we conducted SHAP analysis to determine feature\nimportance and performed an ablation study to assess feature impacts on model\nperformance.\n  Results: XGBoost outperformed the baseline models and the best existing\nliterature. We used metrics, including AUC, Accuracy, Specificity, Sensitivity,\nF1 Score, PPV, and NPV. XGBoost demonstrated the highest performance with an\nAUC of 0.940 and an Accuracy of 0.875, which are 23.4% and 23.5% higher than\nthe best results in the existing literature, with an AUC of 0.706 and an\nAccuracy of 0.640, respectively. This enhanced performance underscores the\nmodels' effectiveness in clinical settings.\n  Conclusions: This study enhances the predictive modeling of VAP in TBI\npatients, improving early detection and intervention potential. Refined feature\nselection and advanced ensemble techniques significantly boosted model accuracy\nand reliability, offering promising directions for future clinical applications\nand medical diagnostics research.\n","authors":["Negin Ashrafi","Armin Abdollahi","Maryam Pishgar"],"pdf_url":"https://arxiv.org/pdf/2408.01144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01141v1","updated":"2024-08-02T09:37:55Z","published":"2024-08-02T09:37:55Z","title":"Machine learning topological energy braiding of non-Bloch bands","summary":"  Machine learning has been used to identify phase transitions in a variety of\nphysical systems. However, there is still a lack of relevant research on\nnon-Bloch energy braiding in non-Hermitian systems. In this work, we study\nnon-Bloch energy braiding in one-dimensional non-Hermitian systems using\nunsupervised and supervised methods. In unsupervised learning, we use diffusion\nmaps to successfully identify non-Bloch energy braiding without any prior\nknowledge and combine it with k-means to cluster different topological elements\ninto clusters, such as Unlink and Hopf link. In supervised learning, we train a\nConvolutional Neural Network (CNN) based on Bloch energy data to predict not\nonly Bloch energy braiding but also non-Bloch energy braiding with an accuracy\napproaching 100%. By analysing the CNN, we can ascertain that the network has\nsuccessfully acquired the ability to recognise the braiding topology of the\nenergy bands. The present study demonstrates the considerable potential of\nmachine learning in the identification of non-Hermitian topological phases and\nenergy braiding.\n","authors":["Shuwei Shi","Shibing Chu","Yuee Xie","Yuanping Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01129v1","updated":"2024-08-02T09:18:41Z","published":"2024-08-02T09:18:41Z","title":"A Survey of Mamba","summary":"  Deep learning, as a vital technique, has sparked a notable revolution in\nartificial intelligence. As the most representative architecture, Transformers\nhave empowered numerous advanced models, especially the large language models\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space\nmodels, has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering from three main\naspects: the advancements of Mamba-based models, the techniques of adapting\nMamba to diverse data, and the applications where Mamba can excel.\nSpecifically, we first recall the foundational knowledge of various\nrepresentative deep learning models and the details of Mamba as preliminaries.\nThen, to showcase the significance of Mamba, we comprehensively review the\nrelated studies focusing on Mamba models' architecture design, data\nadaptability, and applications. Finally, we present an discussion of current\nlimitations and explore various promising research directions to provide deeper\ninsights for future investigations.\n","authors":["Haohao Qu","Liangbo Ning","Rui An","Wenqi Fan","Tyler Derr","Xin Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.01129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15132v2","updated":"2024-08-02T08:49:14Z","published":"2024-02-23T06:33:51Z","title":"Improving Sentence Embeddings with Automatic Generation of Training Data\n  Using Few-shot Examples","summary":"  Decoder-based large language models (LLMs) have shown high performance on\nmany tasks in natural language processing. This is also true for sentence\nembedding learning, where a decoder-based model, PromptEOL, has achieved the\nbest performance on semantic textual similarity (STS) tasks. However, PromptEOL\nrequires a manually annotated natural language inference (NLI) dataset for\nfine-tuning. We aim to improve sentence embeddings without using large manually\nannotated datasets by automatically generating an NLI dataset with an LLM and\nusing it for fine-tuning of PromptEOL. To achieve this, we explore methods of\ndata generation suitable for sentence embedding learning in this study.\nSpecifically, we will focus on automatic dataset generation through few-shot\nlearning and explore the appropriate methods to leverage few-shot examples.\nExperimental results on the STS tasks demonstrate that our approach outperforms\nexisting models in settings without large manually annotated datasets.\n","authors":["Soma Sato","Hayato Tsukagoshi","Ryohei Sasano","Koichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2402.15132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18819v2","updated":"2024-08-02T08:22:57Z","published":"2024-02-29T03:06:10Z","title":"Dual Operating Modes of In-Context Learning","summary":"  In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,\nacquiring a new skill from in-context samples, and task retrieval, i.e.,\nlocating and activating a relevant pretrained skill. Recent theoretical work\ninvestigates various mathematical models to analyze ICL, but existing models\nexplain only one operating mode at a time. We introduce a probabilistic model,\nwith which one can explain the dual operating modes of ICL simultaneously.\nFocusing on in-context learning of linear functions, we extend existing models\nfor pretraining data by introducing multiple task groups and task-dependent\ninput distributions. We then analyze the behavior of the optimally pretrained\nmodel under the squared loss, i.e., the MMSE estimator of the label given\nin-context examples. Regarding pretraining task distribution as prior and\nin-context examples as the observation, we derive the closed-form expression of\nthe task posterior distribution. With the closed-form expression, we obtain a\nquantitative understanding of the two operating modes of ICL. Furthermore, we\nshed light on an unexplained phenomenon observed in practice: under certain\nsettings, the ICL risk initially increases and then decreases with more\nin-context examples. Our model offers a plausible explanation for this \"early\nascent\" phenomenon: a limited number of in-context samples may lead to the\nretrieval of an incorrect skill, thereby increasing the risk, which will\neventually diminish as task learning takes effect with more in-context samples.\nWe also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,\nwhere in-context examples are assigned random labels. Lastly, we validate our\nfindings and predictions via experiments involving Transformers and large\nlanguage models.\n","authors":["Ziqian Lin","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2402.18819v2.pdf","comment":"54 pages, 23 figures"},{"id":"http://arxiv.org/abs/2408.01094v1","updated":"2024-08-02T08:13:18Z","published":"2024-08-02T08:13:18Z","title":"An Encoding--Searching Separation Perspective on Bi-Encoder Neural\n  Search","summary":"  This paper reviews, analyzes, and proposes a new perspective on the\nbi-encoder architecture for neural search. While the bi-encoder architecture is\nwidely used due to its simplicity and scalability at test time, it has some\nnotable issues such as low performance on seen datasets and weak zero-shot\nperformance on new datasets. In this paper, we analyze these issues and\nsummarize two main critiques: the encoding information bottleneck problem and\nlimitations of the basic assumption of embedding search. We then construct a\nthought experiment to logically analyze the encoding and searching operations\nand challenge the basic assumption of embedding search. Building on these\nobservations, we propose a new perspective on the bi-encoder architecture\ncalled the \\textit{encoding--searching separation} perspective, which\nconceptually and practically separates the encoding and searching operations.\nThis new perspective is applied to explain the root cause of the identified\nissues and discuss ways to mitigate the problems. Finally, we discuss the\nimplications of the ideas underlying the new perspective, the design surface\nthat it exposes and the potential research directions arising from it.\n","authors":["Hung-Nghiep Tran","Akiko Aizawa","Atsuhiro Takasu"],"pdf_url":"https://arxiv.org/pdf/2408.01094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11660v2","updated":"2024-08-02T07:42:37Z","published":"2024-01-22T02:33:38Z","title":"Differentiable Tree Search Network","summary":"  In decision-making problems with limited training data, policy functions\napproximated using deep neural networks often exhibit suboptimal performance.\nAn alternative approach involves learning a world model from the limited data\nand determining actions through online search. However, the performance is\nadversely affected by compounding errors arising from inaccuracies in the\nlearned world model. While methods like TreeQN have attempted to address these\ninaccuracies by incorporating algorithmic inductive biases into the neural\nnetwork architectures, the biases they introduce are often weak and\ninsufficient for complex decision-making tasks. In this work, we introduce\nDifferentiable Tree Search Network (D-TSN), a novel neural network architecture\nthat significantly strengthens the inductive bias by embedding the algorithmic\nstructure of a best-first online search algorithm. D-TSN employs a learned\nworld model to conduct a fully differentiable online search. The world model is\njointly optimized with the search algorithm, enabling the learning of a robust\nworld model and mitigating the effect of prediction inaccuracies. Further, we\nnote that a naive incorporation of best-first search could lead to a\ndiscontinuous loss function in the parameter space. We address this issue by\nadopting a stochastic tree expansion policy, formulating search tree expansion\nas another decision-making task, and introducing an effective variance\nreduction technique for the gradient computation. We evaluate D-TSN in an\noffline-RL setting with a limited training data scenario on Procgen games and\ngrid navigation task, and demonstrate that D-TSN outperforms popular model-free\nand model-based baselines.\n","authors":["Dixant Mittal","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2401.11660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01062v1","updated":"2024-08-02T07:29:49Z","published":"2024-08-02T07:29:49Z","title":"Universality of kernel random matrices and kernel regression in the\n  quadratic regime","summary":"  Kernel ridge regression (KRR) is a popular class of machine learning models\nthat has become an important tool for understanding deep learning. Much of the\nfocus has been on studying the proportional asymptotic regime, $n \\asymp d$,\nwhere $n$ is the number of training samples and $d$ is the dimension of the\ndataset. In this regime, under certain conditions on the data distribution, the\nkernel random matrix involved in KRR exhibits behavior akin to that of a linear\nkernel. In this work, we extend the study of kernel regression to the quadratic\nasymptotic regime, where $n \\asymp d^2$. In this regime, we demonstrate that a\nbroad class of inner-product kernels exhibit behavior similar to a quadratic\nkernel. Specifically, we establish an operator norm approximation bound for the\ndifference between the original kernel random matrix and a quadratic kernel\nrandom matrix with additional correction terms compared to the Taylor expansion\nof the kernel functions. The approximation works for general data distributions\nunder a Gaussian-moment-matching assumption with a covariance structure. This\nnew approximation is utilized to obtain a limiting spectral distribution of the\noriginal kernel matrix and characterize the precise asymptotic training and\ngeneralization errors for KRR in the quadratic regime when $n/d^2$ converges to\na non-zero constant. The generalization errors are obtained for both\ndeterministic and random teacher models. Our proof techniques combine moment\nmethods, Wick's formula, orthogonal polynomials, and resolvent analysis of\nrandom matrices with correlated entries.\n","authors":["Parthe Pandit","Zhichao Wang","Yizhe Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.01062v1.pdf","comment":"75 pages"},{"id":"http://arxiv.org/abs/2408.01050v1","updated":"2024-08-02T06:56:59Z","published":"2024-08-02T06:56:59Z","title":"The Impact of Hyperparameters on Large Language Model Inference\n  Performance: An Evaluation of vLLM and HuggingFace Pipelines","summary":"  The recent surge of open-source large language models (LLMs) enables\ndevelopers to create AI-based solutions while maintaining control over aspects\nsuch as privacy and compliance, thereby providing governance and ownership of\nthe model deployment process. To utilize these LLMs, inference engines are\nneeded. These engines load the model's weights onto available resources, such\nas GPUs, and process queries to generate responses. The speed of inference, or\nperformance, of the LLM, is critical for real-time applications, as it computes\nmillions or billions of floating point operations per inference. Recently,\nadvanced inference engines such as vLLM have emerged, incorporating novel\nmechanisms such as efficient memory management to achieve state-of-the-art\nperformance. In this paper, we analyze the performance, particularly the\nthroughput (tokens generated per unit of time), of 20 LLMs using two inference\nlibraries: vLLM and HuggingFace's pipelines. We investigate how various\nhyperparameters, which developers must configure, influence inference\nperformance. Our results reveal that throughput landscapes are irregular, with\ndistinct peaks, highlighting the importance of hyperparameter optimization to\nachieve maximum performance. We also show that applying hyperparameter\noptimization when upgrading or downgrading the GPU model used for inference can\nimprove throughput from HuggingFace pipelines by an average of 9.16% and 13.7%,\nrespectively.\n","authors":["Matias Martinez"],"pdf_url":"https://arxiv.org/pdf/2408.01050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01040v1","updated":"2024-08-02T06:24:39Z","published":"2024-08-02T06:24:39Z","title":"Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix","summary":"  In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.\n","authors":["Seungeun Oh","Sihun Baek","Jihong Park","Hyelin Nam","Praneeth Vepakomma","Ramesh Raskar","Mehdi Bennis","Seong-Lyun Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01040v1.pdf","comment":"23 pages, 11 figures, 8 tables, to be published in Transactions on\n  Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2408.00023v2","updated":"2024-08-02T06:05:19Z","published":"2024-07-31T05:31:28Z","title":"On the Perturbed States for Transformed Input-robust Reinforcement\n  Learning","summary":"  Reinforcement Learning (RL) agents demonstrating proficiency in a training\nenvironment exhibit vulnerability to adversarial perturbations in input\nobservations during deployment. This underscores the importance of building a\nrobust agent before its real-world deployment. To alleviate the challenging\npoint, prior works focus on developing robust training-based procedures,\nencompassing efforts to fortify the deep neural network component's robustness\nor subject the agent to adversarial training against potent attacks. In this\nwork, we propose a novel method referred to as Transformed Input-robust RL\n(TIRL), which explores another avenue to mitigate the impact of adversaries by\nemploying input transformation-based defenses. Specifically, we introduce two\nprinciples for applying transformation-based defenses in learning robust RL\nagents: (1) autoencoder-styled denoising to reconstruct the original state and\n(2) bounded transformations (bit-depth reduction and vector quantization (VQ))\nto achieve close transformed inputs. The transformations are applied to the\nstate before feeding it into the policy network. Extensive experiments on\nmultiple MuJoCo environments demonstrate that input transformation-based\ndefenses, i.e., VQ, defend against several adversaries in the state\nobservations. The official code is available at\nhttps://github.com/tunglm2203/tirl\n","authors":["Tung M. Luu","Haeyong Kang","Tri Ton","Thanh Nguyen","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2408.00023v2.pdf","comment":"12 pages (Code: https://github.com/tunglm2203/tirl)"},{"id":"http://arxiv.org/abs/2407.16944v3","updated":"2024-08-02T06:05:10Z","published":"2024-07-24T02:23:18Z","title":"An Adaptive Gradient Regularization Method","summary":"  Optimizer plays an important role in neural network training with high\nefficiency and performance. Weight update based on its gradient is the central\npart of the optimizer. It has been shown that normalization and standardization\noperation on weight and gradient can accelerate the training process and\nimprove performance such as Weight Standardization (WS), weight normalization\n(WN) and gradient normalization (GN); there is also gradient centralization\n(GC). In this work, we introduce a new optimization technique based on the\ngradient magnitude in a gradient vector named adaptive gradient regularization\n(AGR), which normalizes the gradient vector in all dimensions as a coefficient\nvector and subtracts the product of the gradient and its coefficient vector by\nthe vanilla gradient. It can be viewed as an adaptive gradient clipping method.\nWe show that the AGR can improve the loss function Lipschitzness with a more\nstable training process and better generalization performance. AGR is very\nsimple to be embedded into vanilla optimizers such as Adan and AdamW with only\nthree lines of code. Our experiments are conducted in image generation, image\nclassification and language representation, which shows that our AGR improves\nthe training result.\n","authors":["Huixiu Jiang","Ling Yang","Yu Bao","Rutong Si"],"pdf_url":"https://arxiv.org/pdf/2407.16944v3.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.01023v1","updated":"2024-08-02T05:48:15Z","published":"2024-08-02T05:48:15Z","title":"Distilling interpretable causal trees from causal forests","summary":"  Machine learning methods for estimating treatment effect heterogeneity\npromise greater flexibility than existing methods that test a few pre-specified\nhypotheses. However, one problem these methods can have is that it can be\nchallenging to extract insights from complicated machine learning models. A\nhigh-dimensional distribution of conditional average treatment effects may give\naccurate, individual-level estimates, but it can be hard to understand the\nunderlying patterns; hard to know what the implications of the analysis are.\nThis paper proposes the Distilled Causal Tree, a method for distilling a\nsingle, interpretable causal tree from a causal forest. This compares well to\nexisting methods of extracting a single tree, particularly in noisy data or\nhigh-dimensional data where there are many correlated features. Here it even\noutperforms the base causal forest in most simulations. Its estimates are\ndoubly robust and asymptotically normal just as those of the causal forest are.\n","authors":["Patrick Rehill"],"pdf_url":"https://arxiv.org/pdf/2408.01023v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.01022v1","updated":"2024-08-02T05:46:17Z","published":"2024-08-02T05:46:17Z","title":"A Family of Distributions of Random Subsets for Controlling Positive and\n  Negative Dependence","summary":"  Positive and negative dependence are fundamental concepts that characterize\nthe attractive and repulsive behavior of random subsets. Although some\nprobabilistic models are known to exhibit positive or negative dependence, it\nis challenging to seamlessly bridge them with a practicable probabilistic\nmodel. In this study, we introduce a new family of distributions, named the\ndiscrete kernel point process (DKPP), which includes determinantal point\nprocesses and parts of Boltzmann machines. We also develop some computational\nmethods for probabilistic operations and inference with DKPPs, such as\ncalculating marginal and conditional probabilities and learning the parameters.\nOur numerical experiments demonstrate the controllability of positive and\nnegative dependence and the effectiveness of the computational methods for\nDKPPs.\n","authors":["Takahiro Kawashima","Hideitsu Hino"],"pdf_url":"https://arxiv.org/pdf/2408.01022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01018v1","updated":"2024-08-02T05:36:14Z","published":"2024-08-02T05:36:14Z","title":"GNN-MolKAN: Harnessing the Power of KAN to Advance Molecular\n  Representation Learning with GNNs","summary":"  Effective molecular representation learning is crucial for molecular property\nprediction and drug design. However, existing approaches struggle with\nlimitations in insufficient annotations and suboptimal architecture design. For\ninstance, Graph Neural Networks (GNNs) suffer from over-squashing, causing the\nloss of important structural details in molecules, thus impairing molecular\nrepresentations. In this work, we propose a new class of GNNs, GNN-MolKAN and\nits augmented variant, GNN-MolKAN+, that integrate the Kolmogorov-Arnold\nNetworks (KAN) architecture from AI + Science into GNNs to address these\nchallenges. Additionally, we introduce Adaptive FastKAN (AdFastKAN), an\nadvanced KAN that offers increased stability and speed, further enhancing the\nperformance of standard GNNs. Notably, our approach holds three key benefits:\n1) Superior Performance: GNN-MolKAN and GNN-MolKAN+ demonstrate superior\nprediction ability, robust generalization to unseen scaffolds, and versatile\ntransferability across different GNN architectures. 2) Efficiency: These models\nrequire less computational time and fewer parameters while matching or\nsurpassing the state-of-the-art (SOTA) self-supervised methods. 3) Few-shot\nLearning Ability: GNN-MolKAN demonstrates great potential in few-shot learning\nscenarios, achieving an average improvement of 6.97% across few-shot\nbenchmarks. Overall, we validate our architecture on 6 classification datasets,\n6 regression datasets, and 4 few-shot learning datasets, consistently achieving\nhighly competitive results across all of them.\n","authors":["Ruifeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.01018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01016v1","updated":"2024-08-02T05:23:19Z","published":"2024-08-02T05:23:19Z","title":"IBB Traffic Graph Data: Benchmarking and Road Traffic Prediction Model","summary":"  Road traffic congestion prediction is a crucial component of intelligent\ntransportation systems, since it enables proactive traffic management, enhances\nsuburban experience, reduces environmental impact, and improves overall safety\nand efficiency. Although there are several public datasets, especially for\nmetropolitan areas, these datasets may not be applicable to practical scenarios\ndue to insufficiency in the scale of data (i.e. number of sensors and road\nlinks) and several external factors like different characteristics of the\ntarget area such as urban, highways and the data collection location. To\naddress this, this paper introduces a novel IBB Traffic graph dataset as an\nalternative benchmark dataset to mitigate these limitations and enrich the\nliterature with new geographical characteristics. IBB Traffic graph dataset\ncovers the sensor data collected at 2451 distinct locations. Moreover, we\npropose a novel Road Traffic Prediction Model that strengthens temporal links\nthrough feature engineering, node embedding with GLEE to represent\ninter-related relationships within the traffic network, and traffic prediction\nwith ExtraTrees. The results indicate that the proposed model consistently\noutperforms the baseline models, demonstrating an average accuracy improvement\nof 4%.\n","authors":["Eren Olug","Kiymet Kaya","Resul Tugay","Sule Gunduz Oguducu"],"pdf_url":"https://arxiv.org/pdf/2408.01016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.13894v2","updated":"2024-08-02T05:05:48Z","published":"2021-04-28T17:26:29Z","title":"Weighed l1 on the simplex: Compressive sensing meets locality","summary":"  Sparse manifold learning algorithms combine techniques in manifold learning\nand sparse optimization to learn features that could be utilized for downstream\ntasks. The standard setting of compressive sensing can not be immediately\napplied to this setup. Due to the intrinsic geometric structure of data,\ndictionary atoms might be redundant and do not satisfy the restricted isometry\nproperty or coherence condition. In addition, manifold learning emphasizes\nlearning local geometry which is not reflected in a standard $\\ell_1$\nminimization problem. We propose weighted $\\ell_0$ and weighted $\\ell_1$\nmetrics that encourage representation via neighborhood atoms suited for\ndictionary based manifold learning. Assuming that the data is generated from\nDelaunay triangulation, we show the equivalence of weighted $\\ell_0$ and\nweighted $\\ell_1$. We discuss an optimization program that learns the\ndictionaries and sparse coefficients and demonstrate the utility of our\nregularization on synthetic and real datasets.\n","authors":["Abiy Tasissa","Pranay Tankala","Demba Ba"],"pdf_url":"https://arxiv.org/pdf/2104.13894v2.pdf","comment":"7 pages, 2 figures. The proof of theorem 1 in v1 does not hold true\n  in general without additional assumptions. This version fixes this problem.\n  For more details, we refer the interested reader to arXiv:2012.02134 which is\n  the journal version of the workshop paper v1"},{"id":"http://arxiv.org/abs/2408.01008v1","updated":"2024-08-02T04:45:58Z","published":"2024-08-02T04:45:58Z","title":"Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with\n  Accelerated LLMs","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across a wide range of natural language processing (NLP) tasks,\nsuch as question-answering, sentiment analysis, text summarization, and machine\ntranslation. However, the ever-growing complexity of LLMs demands immense\ncomputational resources, hindering the broader research and application of\nthese models. To address this, various parameter-efficient fine-tuning\nstrategies, such as Low-Rank Approximation (LoRA) and Adapters, have been\ndeveloped. Despite their potential, these methods often face limitations in\ncompressibility. Specifically, LoRA struggles to scale effectively with the\nincreasing number of trainable parameters in modern large scale LLMs.\nAdditionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which\nutilizes tensor train decomposition, has not yet achieved the level of\ncompression necessary for fine-tuning very large scale models with limited\nresources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA),\na novel parameter-efficient fine-tuning (PEFT) approach that extends LoRETTA\nwith optimized tensor train (TT) decomposition integration. By eliminating\nAdapters and traditional LoRA-based structures, TT-LoRA achieves greater model\ncompression without compromising downstream task performance, along with\nreduced inference latency and computational overhead. We conduct an exhaustive\nparameter search to establish benchmarks that highlight the trade-off between\nmodel compression and performance. Our results demonstrate significant\ncompression of LLMs while maintaining comparable performance to larger models,\nfacilitating their deployment on resource-constraint platforms.\n","authors":["Afia Anjum","Maksim E. Eren","Ismael Boureima","Boian Alexandrov","Manish Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2408.01008v1.pdf","comment":"LA-UR-24-28177"},{"id":"http://arxiv.org/abs/2408.01005v1","updated":"2024-08-02T04:40:15Z","published":"2024-08-02T04:40:15Z","title":"Enhancing Financial Market Predictions: Causality-Driven Feature\n  Selection","summary":"  This paper introduces the FinSen dataset that revolutionizes financial market\nanalysis by integrating economic and financial news articles from 197 countries\nwith stock market data. The dataset's extensive coverage spans 15 years from\n2007 to 2023 with temporal information, offering a rich, global perspective\nwith 160,000 records on financial market news. Our study leverages causally\nvalidated sentiment scores and LSTM models to enhance market forecast accuracy\nand reliability. Utilizing the FinSen dataset, we introduce an innovative Focal\nCalibration Loss, reducing Expected Calibration Error (ECE) to 3.34 percent\nwith the DAN 3 model. This not only improves prediction accuracy but also\naligns probabilistic forecasts closely with real outcomes, crucial for the\nfinancial sector where predicted probability is paramount. Our approach\ndemonstrates the effectiveness of combining sentiment analysis with precise\ncalibration techniques for trustworthy financial forecasting where the cost of\nmisinterpretation can be high. Finsen Data can be found at [this github\nURL](https://github.com/EagleAdelaide/FinSen_Dataset.git).\n","authors":["Wenhao Liang","Zhengyang Li","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01005v1.pdf","comment":"Accepted by The 20th International Conference Advanced Data Mining\n  and Applications 2024 (ADMA 2024)"},{"id":"http://arxiv.org/abs/2408.01000v1","updated":"2024-08-02T04:19:25Z","published":"2024-08-02T04:19:25Z","title":"Adaptive Two-Stage Cloud Resource Scaling via Hierarchical\n  Multi-Indicator Forecasting and Bayesian Decision-Making","summary":"  The surging demand for cloud computing resources, driven by the rapid growth\nof sophisticated large-scale models and data centers, underscores the critical\nimportance of efficient and adaptive resource allocation. As major tech\nenterprises deploy massive infrastructures with thousands of GPUs, existing\ncloud platforms still struggle with low resource utilization due to key\nchallenges: capturing hierarchical indicator structures, modeling non-Gaussian\ndistributions, and decision-making under uncertainty. To address these\nchallenges, we propose HRAMONY, an adaptive Hierarchical Attention-based\nResource Modeling and Decision-Making System. HARMONY combines hierarchical\nmulti-indicator distribution forecasting and uncertainty-aware Bayesian\ndecision-making. It introduces a novel hierarchical attention mechanism that\ncomprehensively models complex inter-indicator dependencies, enabling accurate\npredictions that can adapt to evolving environment states. By transforming\nGaussian projections into adaptive non-Gaussian distributions via Normalizing\nFlows. Crucially, HARMONY leverages the full predictive distributions in an\nadaptive Bayesian process, proactively incorporating uncertainties to optimize\nresource allocation while robustly meeting SLA constraints under varying\nconditions. Extensive evaluations across four large-scale cloud datasets\ndemonstrate HARMONY's state-of-the-art performance, significantly outperforming\nnine established methods. A month-long real-world deployment validated\nHARMONY's substantial practical impact, realizing over 35,000 GPU hours in\nsavings and translating to $100K+ in cost reduction, showcasing its remarkable\neconomic value through adaptive, uncertainty-aware scaling. Our code is\navailable at https://github.com/Floating-LY/HARMONY1.\n","authors":["Yang Luo","Shiyu Wang","Zhemeng Yu","Wei Lu","Xiaofeng Gao","Lintao Ma","Guihai Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00996v1","updated":"2024-08-02T04:09:15Z","published":"2024-08-02T04:09:15Z","title":"IncidentNet: Traffic Incident Detection, Localization and Severity\n  Estimation with Sparse Sensing","summary":"  Prior art in traffic incident detection relies on high sensor coverage and is\nprimarily based on decision-tree and random forest models that have limited\nrepresentation capacity and, as a result, cannot detect incidents with high\naccuracy. This paper presents IncidentNet - a novel approach for classifying,\nlocalizing, and estimating the severity of traffic incidents using deep\nlearning models trained on data captured from sparsely placed sensors in urban\nenvironments. Our model works on microscopic traffic data that can be collected\nusing cameras installed at traffic intersections. Due to the unavailability of\ndatasets that provide microscopic traffic details and traffic incident details\nsimultaneously, we also present a methodology to generate a synthetic\nmicroscopic traffic dataset that matches given macroscopic traffic data.\nIncidentNet achieves a traffic incident detection rate of 98%, with false alarm\nrates of less than 7% in 197 seconds on average in urban environments with\ncameras on less than 20% of the traffic intersections.\n","authors":["Sai Shashank Peddiraju","Kaustubh Harapanahalli","Edward Andert","Aviral Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2408.00996v1.pdf","comment":"6 pages, 6 figures, 2024 IEEE 27th International Conference on\n  Intelligent Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2303.12653v3","updated":"2024-08-02T04:02:26Z","published":"2023-03-09T05:30:53Z","title":"Robust Millimeter Beamforming via Self-Supervised Hybrid Deep Learning","summary":"  Beamforming with large-scale antenna arrays has been widely used in recent\nyears, which is acknowledged as an important part in 5G and incoming 6G. Thus,\nvarious techniques are leveraged to improve its performance, e.g., deep\nlearning, advanced optimization algorithms, etc. Although its performance in\nmany previous research scenarios with deep learning is quite attractive,\nusually it drops rapidly when the environment or dataset is changed. Therefore,\ndesigning effective beamforming network with strong robustness is an open issue\nfor the intelligent wireless communications. In this paper, we propose a robust\nbeamforming self-supervised network, and verify it in two kinds of different\ndatasets with various scenarios. Simulation results show that the proposed\nself-supervised network with hybrid learning performs well in both classic\nDeepMIMO and new WAIR-D dataset with the strong robustness under the various\nenvironments. Also, we present the principle to explain the rationality of this\nkind of hybrid learning, which is instructive to apply with more kinds of\ndatasets.\n","authors":["Fenghao Zhu","Bohao Wang","Zhaohui Yang","Chongwen Huang","Zhaoyang Zhang","George C. Alexandropoulos","Chau Yuen","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2303.12653v3.pdf","comment":"Accept by EUSIPCO 2023"},{"id":"http://arxiv.org/abs/2401.01258v2","updated":"2024-08-02T03:39:18Z","published":"2024-01-02T15:59:00Z","title":"Towards Model-Free LQR Control over Rate-Limited Channels","summary":"  Given the success of model-free methods for control design in many problem\nsettings, it is natural to ask how things will change if realistic\ncommunication channels are utilized for the transmission of gradients or\npolicies. While the resulting problem has analogies with the formulations\nstudied under the rubric of networked control systems, the rich literature in\nthat area has typically assumed that the model of the system is known. As a\nstep towards bridging the fields of model-free control design and networked\ncontrol systems, we ask: \\textit{Is it possible to solve basic control problems\n- such as the linear quadratic regulator (LQR) problem - in a model-free manner\nover a rate-limited channel?} Toward answering this question, we study a\nsetting where a worker agent transmits quantized policy gradients (of the LQR\ncost) to a server over a noiseless channel with a finite bit-rate. We propose a\nnew algorithm titled Adaptively Quantized Gradient Descent (\\texttt{AQGD}), and\nprove that above a certain finite threshold bit-rate, \\texttt{AQGD} guarantees\nexponentially fast convergence to the globally optimal policy, with \\textit{no\ndeterioration of the exponent relative to the unquantized setting}. More\ngenerally, our approach reveals the benefits of adaptive quantization in\npreserving fast linear convergence rates, and, as such, may be of independent\ninterest to the literature on compressed optimization.\n","authors":["Aritra Mitra","Lintao Ye","Vijay Gupta"],"pdf_url":"https://arxiv.org/pdf/2401.01258v2.pdf","comment":"Accepted for an Oral Presentation at the 6th Annual Learning for\n  Dynamics & Control Conference"},{"id":"http://arxiv.org/abs/2407.11046v2","updated":"2024-08-02T03:22:22Z","published":"2024-07-08T12:32:10Z","title":"A Survey on LoRA of Large Language Models","summary":"  Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github page\n(https://github.com/ZJU-LLMs/Awesome-LoRAs.git) for readers to check the\nupdates and initiate discussions on this survey paper.\n","authors":["Yuren Mao","Yuhang Ge","Yijiang Fan","Wenyi Xu","Yu Mi","Zhonghao Hu","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2407.11046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15870v5","updated":"2024-08-02T03:16:07Z","published":"2023-07-29T02:35:37Z","title":"SemiSFL: Split Federated Learning on Unlabeled and Non-IID Data","summary":"  Federated Learning (FL) has emerged to allow multiple clients to\ncollaboratively train machine learning models on their private data at the\nnetwork edge. However, training and deploying large-scale models on\nresource-constrained devices is challenging. Fortunately, Split Federated\nLearning (SFL) offers a feasible solution by alleviating the computation and/or\ncommunication burden on clients. However, existing SFL works often assume\nsufficient labeled data on clients, which is usually impractical. Besides, data\nnon-IIDness poses another challenge to ensure efficient model training. To our\nbest knowledge, the above two issues have not been simultaneously addressed in\nSFL. Herein, we propose a novel Semi-supervised SFL system, termed SemiSFL,\nwhich incorporates clustering regularization to perform SFL with unlabeled and\nnon-IID client data. Moreover, our theoretical and experimental investigations\ninto model convergence reveal that the inconsistent training processes on\nlabeled and unlabeled data have an influence on the effectiveness of clustering\nregularization. To mitigate the training inconsistency, we develop an algorithm\nfor dynamically adjusting the global updating frequency, so as to improve\ntraining performance. Extensive experiments on benchmark models and datasets\nshow that our system provides a 3.8x speed-up in training time, reduces the\ncommunication cost by about 70.3% while reaching the target accuracy, and\nachieves up to 5.8% improvement in accuracy under non-IID scenarios compared to\nthe state-of-the-art baselines.\n","authors":["Yang Xu","Yunming Liao","Hongli Xu","Zhipeng Sun","Liusheng Huang","Chunming Qiao"],"pdf_url":"https://arxiv.org/pdf/2307.15870v5.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2408.00985v1","updated":"2024-08-02T03:02:39Z","published":"2024-08-02T03:02:39Z","title":"Reconstructing Richtmyer-Meshkov instabilities from noisy radiographs\n  using low dimensional features and attention-based neural networks","summary":"  A trained attention-based transformer network can robustly recover the\ncomplex topologies given by the Richtmyer-Meshkoff instability from a sequence\nof hydrodynamic features derived from radiographic images corrupted with blur,\nscatter, and noise. This approach is demonstrated on ICF-like double shell\nhydrodynamic simulations. The key component of this network is a transformer\nencoder that acts on a sequence of features extracted from noisy radiographs.\nThis encoder includes numerous self-attention layers that act to learn temporal\ndependencies in the input sequences and increase the expressiveness of the\nmodel. This approach is demonstrated to exhibit an excellent ability to\naccurately recover the Richtmyer-Meshkov instability growth rates, even despite\nthe gas-metal interface being greatly obscured by radiographic noise.\n","authors":["Daniel A. Serino","Marc L. Klasky","Balasubramanya T. Nadiga","Xiaojian Xu","Trevor Wilcox"],"pdf_url":"https://arxiv.org/pdf/2408.00985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00973v1","updated":"2024-08-02T01:49:29Z","published":"2024-08-02T01:49:29Z","title":"META-ANOVA: Screening interactions for interpretable machine learning","summary":"  There are two things to be considered when we evaluate predictive models. One\nis prediction accuracy,and the other is interpretability. Over the recent\ndecades, many prediction models of high performance, such as ensemble-based\nmodels and deep neural networks, have been developed. However, these models are\noften too complex, making it difficult to intuitively interpret their\npredictions. This complexity in interpretation limits their use in many\nreal-world fields that require accountability, such as medicine, finance, and\ncollege admissions. In this study, we develop a novel method called Meta-ANOVA\nto provide an interpretable model for any given prediction model. The basic\nidea of Meta-ANOVA is to transform a given black-box prediction model to the\nfunctional ANOVA model. A novel technical contribution of Meta-ANOVA is a\nprocedure of screening out unnecessary interaction before transforming a given\nblack-box model to the functional ANOVA model. This screening procedure allows\nthe inclusion of higher order interactions in the transformed functional ANOVA\nmodel without computational difficulties. We prove that the screening procedure\nis asymptotically consistent. Through various experiments with synthetic and\nreal-world datasets, we empirically demonstrate the superiority of Meta-ANOVA\n","authors":["Yongchan Choi","Seokhun Park","Chanmoo Park","Dongha Kim","Yongdai Kim"],"pdf_url":"https://arxiv.org/pdf/2408.00973v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2308.14250v3","updated":"2024-08-02T01:38:16Z","published":"2023-08-28T01:57:38Z","title":"Rule-Based Error Detection and Correction to Operationalize Movement\n  Trajectory Classification","summary":"  Classification of movement trajectories has many applications in\ntransportation and is a key component for large-scale movement trajectory\ngeneration and anomaly detection which has key safety applications in the\naftermath of a disaster or other external shock. However, the current\nstate-of-the-art (SOTA) are based on supervised deep learning - which leads to\nchallenges when the distribution of trajectories changes due to such a shock.\nWe provide a neuro-symbolic rule-based framework to conduct error correction\nand detection of these models to integrate into our movement trajectory\nplatform. We provide a suite of experiments on several recent SOTA models where\nwe show highly accurate error detection, the ability to improve accuracy with a\nchanging test distribution, and accuracy improvement for the base use case in\naddition to a suite of theoretical properties that informed algorithm\ndevelopment. Specifically, we show an F1 scores for predicting errors of up to\n0.984, significant performance increase for out-of distribution accuracy (8.51%\nimprovement over SOTA for zero-shot accuracy), and accuracy improvement over\nthe SOTA model.\n","authors":["Bowen Xi","Kevin Scaria","Divyagna Bavikadi","Paulo Shakarian"],"pdf_url":"https://arxiv.org/pdf/2308.14250v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03452v2","updated":"2024-08-02T01:33:25Z","published":"2023-09-07T02:26:55Z","title":"Multimodal Guidance Network for Missing-Modality Inference in Content\n  Moderation","summary":"  Multimodal deep learning, especially vision-language models, have gained\nsignificant traction in recent years, greatly improving performance on many\ndownstream tasks, including content moderation and violence detection. However,\nstandard multimodal approaches often assume consistent modalities between\ntraining and inference, limiting applications in many real-world use cases, as\nsome modalities may not be available during inference. While existing research\nmitigates this problem through reconstructing the missing modalities, they\nunavoidably increase unnecessary computational cost, which could be just as\ncritical, especially for large, deployed infrastructures in industry. To this\nend, we propose a novel guidance network that promotes knowledge sharing during\ntraining, taking advantage of the multimodal representations to train better\nsingle-modality models to be used for inference. Real-world experiments in\nviolence detection shows that our proposed framework trains single-modality\nmodels that significantly outperform traditionally trained counterparts, while\navoiding increases in computational cost for inference.\n","authors":["Zhuokai Zhao","Harish Palani","Tianyi Liu","Lena Evans","Ruth Toner"],"pdf_url":"https://arxiv.org/pdf/2309.03452v2.pdf","comment":"ICME 2024 Camera Ready. Code is available at\n  https://github.com/zhuokaizhao/multimodal-guidance-network"},{"id":"http://arxiv.org/abs/2408.00963v1","updated":"2024-08-02T00:35:18Z","published":"2024-08-02T00:35:18Z","title":"MIS-ME: A Multi-modal Framework for Soil Moisture Estimation","summary":"  Soil moisture estimation is an important task to enable precision agriculture\nin creating optimal plans for irrigation, fertilization, and harvest. It is\ncommon to utilize statistical and machine learning models to estimate soil\nmoisture from traditional data sources such as weather forecasts, soil\nproperties, and crop properties. However, there is a growing interest in\nutilizing aerial and geospatial imagery to estimate soil moisture. Although\nthese images capture high-resolution crop details, they are expensive to curate\nand challenging to interpret. Imagine, an AI-enhanced software tool that\npredicts soil moisture using visual cues captured by smartphones and\nstatistical data given by weather forecasts. This work is a first step towards\nthat goal of developing a multi-modal approach for soil moisture estimation. In\nparticular, we curate a dataset consisting of real-world images taken from\nground stations and their corresponding weather data. We also propose MIS-ME -\nMeteorological & Image based Soil Moisture Estimator, a multi-modal framework\nfor soil moisture estimation. Our extensive analysis shows that MIS-ME achieves\na MAPE of 10.79%, outperforming traditional unimodal approaches with a\nreduction of 2.6% in MAPE for meteorological data and 1.5% in MAPE for image\ndata, highlighting the effectiveness of tailored multi-modal approaches.\n","authors":["Mohammed Rakib","Adil Aman Mohammed","Cole Diggins","Sumit Sharma","Jeff Michael Sadler","Tyson Ochsner","Arun Bagavathi"],"pdf_url":"https://arxiv.org/pdf/2408.00963v1.pdf","comment":"Accepted by DSAA2024"},{"id":"http://arxiv.org/abs/2405.16522v4","updated":"2024-08-02T00:21:41Z","published":"2024-05-26T11:17:49Z","title":"Multi-State TD Target for Model-Free Reinforcement Learning","summary":"  Temporal difference (TD) learning is a fundamental technique in reinforcement\nlearning that updates value estimates for states or state-action pairs using a\nTD target. This target represents an improved estimate of the true value by\nincorporating both immediate rewards and the estimated value of subsequent\nstates. Traditionally, TD learning relies on the value of a single subsequent\nstate. We propose an enhanced multi-state TD (MSTD) target that utilizes the\nestimated values of multiple subsequent states. Building on this new MSTD\nconcept, we develop complete actor-critic algorithms that include management of\nreplay buffers in two modes, and integrate with deep deterministic policy\noptimization (DDPG) and soft actor-critic (SAC). Experimental results\ndemonstrate that algorithms employing the MSTD target significantly improve\nlearning performance compared to traditional methods.The code is provided on\nGitHub.\n","authors":["Wuhao Wang","Zhiyong Chen","Lepeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.16522v4.pdf","comment":"8 pages, 16 figures"}]},"2024-08-05T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.01355v2","updated":"2024-08-05T02:14:54Z","published":"2024-08-02T16:07:15Z","title":"Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models\n  within Perturbed Inputs","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\nperformance on various visual-language understanding and generation tasks.\nHowever, MLLMs occasionally generate content inconsistent with the given\nimages, which is known as \"hallucination\". Prior works primarily center on\nevaluating hallucination using standard, unperturbed benchmarks, which overlook\nthe prevalent occurrence of perturbed inputs in real-world scenarios-such as\nimage cropping or blurring-that are critical for a comprehensive assessment of\nMLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,\nthe first benchmark designed to evaluate Hallucination in MLLMs within\nPerturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,\ncontaining 1,260 perturbed images from 11 object types. Each image is\naccompanied by detailed annotations, which include fine-grained hallucination\ntypes, such as existence, attribute, and relation. We equip these annotations\nwith a rich set of questions, making Hallu-PI suitable for both discriminative\nand generative tasks. Extensive experiments on 12 mainstream MLLMs, such as\nGPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant\nhallucinations on Hallu-PI, which is not observed in unperturbed scenarios.\nFurthermore, our research reveals a severe bias in MLLMs' ability to handle\ndifferent types of hallucinations. We also design two baselines specifically\nfor perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope\nthat our study will bring researchers' attention to the limitations of MLLMs\nwhen dealing with perturbed inputs, and spur further investigations to address\nthis issue. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/Hallu-PI.\n","authors":["Peng Ding","Jingyu Wu","Jun Kuang","Dan Ma","Xuezhi Cao","Xunliang Cai","Shi Chen","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2408.01355v2.pdf","comment":"Acccepted by ACM MM 2024, 14 pages, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2408.00938v2","updated":"2024-08-05T09:32:30Z","published":"2024-08-01T22:01:42Z","title":"CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting\n  Idiopathic Pulmonary Fibrosis Progression","summary":"  The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly\ncorrelates with higher patient mortality rates. Early detection of IPF\nprogression is critical for initiating timely treatment, which can effectively\nslow down the advancement of the disease. However, the current clinical\ncriteria define disease progression requiring two CT scans with a one-year\ninterval, presenting a dilemma: a disease progression is identified only after\nthe disease has already progressed. To this end, in this paper, we develop a\nnovel diffusion model to accurately predict the progression of IPF by\ngenerating patient's follow-up CT scan from the initial CT scan. Specifically,\nfrom the clinical prior knowledge, we tailor improvements to the traditional\ndiffusion model and propose a Clinically-Informed Residual Diffusion model,\ncalled CIResDiff. The key innovations of CIResDiff include 1) performing the\ntarget region pre-registration to align the lung regions of two CT scans at\ndifferent time points for reducing the generation difficulty, 2) adopting the\nresidual diffusion instead of traditional diffusion to enable the model focus\nmore on differences (i.e., lesions) between the two CT scans rather than the\nlargely identical anatomical content, and 3) designing the clinically-informed\nprocess based on CLIP technology to integrate lung function information which\nis highly relevant to diagnosis into the reverse process for assisting\ngeneration. Extensive experiments on clinical data demonstrate that our\napproach can outperform state-of-the-art methods and effectively predict the\nprogression of IPF.\n","authors":["Caiwen Jiang","Xiaodan Xing","Zaixin Ou","Mianxin Liu","Walsh Simon","Guang Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00380v2","updated":"2024-08-05T02:45:50Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that Stain Normalized Pathology Foundational Model achieves excellent\nperformance relative to the number of WSIs used and the model's parameter\ncount. This suggests that the application of stain normalization has\nsubstantially improved the model's efficiency and generalization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02672v1","updated":"2024-08-05T17:59:51Z","published":"2024-08-05T17:59:51Z","title":"Latent-INR: A Flexible Framework for Implicit Representations of Videos\n  with Discriminative Semantics","summary":"  Implicit Neural Networks (INRs) have emerged as powerful representations to\nencode all forms of data, including images, videos, audios, and scenes. With\nvideo, many INRs for video have been proposed for the compression task, and\nrecent methods feature significant improvements with respect to encoding time,\nstorage, and reconstruction quality. However, these encoded representations\nlack semantic meaning, so they cannot be used for any downstream tasks that\nrequire such properties, such as retrieval. This can act as a barrier for\nadoption of video INRs over traditional codecs as they do not offer any\nsignificant edge apart from compression. To alleviate this, we propose a\nflexible framework that decouples the spatial and temporal aspects of the video\nINR. We accomplish this with a dictionary of per-frame latents that are learned\njointly with a set of video specific hypernetworks, such that given a latent,\nthese hypernetworks can predict the INR weights to reconstruct the given frame.\nThis framework not only retains the compression efficiency, but the learned\nlatents can be aligned with features from large vision models, which grants\nthem discriminative properties. We align these latents with CLIP and show good\nperformance for both compression and video retrieval tasks. By aligning with\nVideoLlama, we are able to perform open-ended chat with our learned latents as\nthe visual inputs. Additionally, the learned latents serve as a proxy for the\nunderlying weights, allowing us perform tasks like video interpolation. These\nsemantic properties and applications, existing simultaneously with ability to\nperform compression, interpolation, and superresolution properties, are a first\nin this field of work.\n","authors":["Shishira R Maiya","Anubhav Gupta","Matthew Gwilliam","Max Ehrlich","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2408.02672v1.pdf","comment":"equal contribution for first two authors; accepted to ECCV2024; 14\n  pages, 4 tables, 10 figures in main paper, supplementary after bibliography"},{"id":"http://arxiv.org/abs/2405.04634v3","updated":"2024-08-05T17:53:28Z","published":"2024-05-07T19:37:22Z","title":"FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic\n  Segmentation of Diverse Landscapes","summary":"  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a\nnew tool to monitor territory and support public policies. Processing ALS data\nat scale requires efficient point classification methods that perform well over\nhighly diverse territories. To evaluate them, researchers need large annotated\nLidar datasets, however, current Lidar benchmark datasets have restricted scope\nand often cover a single urban area. To bridge this data gap, we present the\nFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an\nultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with\nhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is\nbuilt upon France's nationwide open Lidar data. It achieves spatial and\nsemantic diversity via a sampling scheme that explicitly concentrates rare\nclasses and challenging landscapes from five French regions. It should support\nthe development of 3D deep learning approaches for large-scale land monitoring.\nWe describe the nature of the source data, the sampling workflow, the content\nof the resulting dataset, and provide an initial evaluation of segmentation\nperformance using a performant 3D neural architecture.\n","authors":["Charles Gaydon","Michel Daab","Floryne Roche"],"pdf_url":"https://arxiv.org/pdf/2405.04634v3.pdf","comment":"15 pages | 9 figures | 8 tables | Dataset is available at\n  https://huggingface.co/datasets/IGNF/FRACTAL | Trained model is available at\n  https://huggingface.co/IGNF/FRACTAL-LidarHD_7cl_randlanet | Deep learning\n  code repository is on Gihtub at https://github.com/IGNF/myria3d | Data\n  engineering code repository is on Github at https://github.com/IGNF/pacasam"},{"id":"http://arxiv.org/abs/2407.11913v2","updated":"2024-08-05T17:50:03Z","published":"2024-07-16T17:05:20Z","title":"Quantised Global Autoencoder: A Holistic Approach to Representing Visual\n  Data","summary":"  In quantised autoencoders, images are usually split into local patches, each\nencoded by one token. This representation is redundant in the sense that the\nsame number of tokens is spend per region, regardless of the visual information\ncontent in that region. Adaptive discretisation schemes like quadtrees are\napplied to allocate tokens for patches with varying sizes, but this just varies\nthe region of influence for a token which nevertheless remains a local\ndescriptor. Modern architectures add an attention mechanism to the autoencoder\nwhich infuses some degree of global information into the local tokens. Despite\nthe global context, tokens are still associated with a local image region. In\ncontrast, our method is inspired by spectral decompositions which transform an\ninput signal into a superposition of global frequencies. Taking the data-driven\nperspective, we learn custom basis functions corresponding to the codebook\nentries in our VQ-VAE setup. Furthermore, a decoder combines these basis\nfunctions in a non-linear fashion, going beyond the simple linear superposition\nof spectral decompositions. We can achieve this global description with an\nefficient transpose operation between features and channels and demonstrate our\nperformance on compression.\n","authors":["Tim Elsner","Paula Usinger","Victor Czech","Gregor Kobsik","Yanjiang He","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2407.11913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02657v1","updated":"2024-08-05T17:46:53Z","published":"2024-08-05T17:46:53Z","title":"Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation\n  with Multimodal Generative Pretraining","summary":"  We present Lumina-mGPT, a family of multimodal autoregressive models capable\nof various vision and language tasks, particularly excelling in generating\nflexible photorealistic images from text descriptions. Unlike existing\nautoregressive image generation approaches, Lumina-mGPT employs a pretrained\ndecoder-only transformer as a unified framework for modeling multimodal token\nsequences. Our key insight is that a simple decoder-only transformer with\nmultimodal Generative PreTraining (mGPT), utilizing the next-token prediction\nobjective on massive interleaved text-image sequences, can learn broad and\ngeneral multimodal capabilities, thereby illuminating photorealistic\ntext-to-image generation. Building on these pretrained models, we propose\nFlexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text\npairs to fully unlock their potential for high-aesthetic image synthesis at any\nresolution while maintaining their general multimodal capabilities.\nFurthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),\ntransforming Lumina-mGPT into a foundation model that seamlessly achieves\nomnipotent task unification. The resulting model demonstrates versatile\nmultimodal capabilities, including visual generation tasks like flexible\ntext-to-image generation and controllable generation, visual recognition tasks\nlike segmentation and depth estimation, and vision-language tasks like\nmultiturn visual question answering. Additionally, we analyze the differences\nand similarities between diffusion-based and autoregressive methods in a direct\ncomparison.\n","authors":["Dongyang Liu","Shitian Zhao","Le Zhuo","Weifeng Lin","Yu Qiao","Hongsheng Li","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2408.02657v1.pdf","comment":"Code available at: https://github.com/Alpha-VLLM/Lumina-mGPT"},{"id":"http://arxiv.org/abs/2408.02654v1","updated":"2024-08-05T17:33:09Z","published":"2024-08-05T17:33:09Z","title":"On Using Quasirandom Sequences in Machine Learning for Model Weight\n  Initialization","summary":"  The effectiveness of training neural networks directly impacts computational\ncosts, resource allocation, and model development timelines in machine learning\napplications. An optimizer's ability to train the model adequately (in terms of\ntrained model performance) depends on the model's initial weights. Model weight\ninitialization schemes use pseudorandom number generators (PRNGs) as a source\nof randomness.\n  We investigate whether substituting PRNGs for low-discrepancy quasirandom\nnumber generators (QRNGs) -- namely Sobol' sequences -- as a source of\nrandomness for initializers can improve model performance. We examine\nMulti-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long\nShort-Term Memory (LSTM), and Transformer architectures trained on MNIST,\nCIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses\nten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);\nOrthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with\nweights set using PRNG- and QRNG-based initializers are compared pairwise for\neach combination of dataset, architecture, optimizer, and initialization\nscheme.\n  Our findings indicate that QRNG-based neural network initializers either\nreach a higher accuracy or achieve the same accuracy more quickly than\nPRNG-based initializers in 60% of the 120 experiments conducted. Thus, using\nQRNG-based initializers instead of PRNG-based initializers can speed up and\nimprove model training.\n","authors":["Andriy Miranskyy","Adam Sorrenti","Viral Thakar"],"pdf_url":"https://arxiv.org/pdf/2408.02654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02635v1","updated":"2024-08-05T16:58:56Z","published":"2024-08-05T16:58:56Z","title":"Interactive 3D Medical Image Segmentation with SAM 2","summary":"  Interactive medical image segmentation (IMIS) has shown significant potential\nin enhancing segmentation accuracy by integrating iterative feedback from\nmedical professionals. However, the limited availability of enough 3D medical\ndata restricts the generalization and robustness of most IMIS methods. The\nSegment Anything Model (SAM), though effective for 2D images, requires\nexpensive semi-auto slice-by-slice annotations for 3D medical images. In this\npaper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta\nSAM model trained on videos, for 3D medical image segmentation. By treating\nsequential 2D slices of 3D images as video frames, SAM 2 can fully\nautomatically propagate annotations from a single frame to the entire 3D\nvolume. We propose a practical pipeline for using SAM 2 in 3D medical image\nsegmentation and present key findings highlighting its efficiency and potential\nfor further optimization. Concretely, numerical experiments on the BraTS2020\nand the medical segmentation decathlon datasets demonstrate that SAM 2 still\nhas a gap with supervised methods but can narrow the gap in specific settings\nand organ types, significantly reducing the annotation burden on medical\nprofessionals. Our code will be open-sourced and available at\nhttps://github.com/Chuyun-Shen/SAM_2_Medical_3D.\n","authors":["Chuyun Shen","Wenhao Li","Yuhang Shi","Xiangfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02629v1","updated":"2024-08-05T16:53:23Z","published":"2024-08-05T16:53:23Z","title":"VidGen-1M: A Large-Scale Dataset for Text-to-video Generation","summary":"  The quality of video-text pairs fundamentally determines the upper bound of\ntext-to-video models. Currently, the datasets used for training these models\nsuffer from significant shortcomings, including low temporal consistency,\npoor-quality captions, substandard video quality, and imbalanced data\ndistribution. The prevailing video curation process, which depends on image\nmodels for tagging and manual rule-based curation, leads to a high\ncomputational load and leaves behind unclean data. As a result, there is a lack\nof appropriate training datasets for text-to-video models. To address this\nproblem, we present VidGen-1M, a superior training dataset for text-to-video\nmodels. Produced through a coarse-to-fine curation strategy, this dataset\nguarantees high-quality videos and detailed captions with excellent temporal\nconsistency. When used to train the video generation model, this dataset has\nled to experimental results that surpass those obtained with other models.\n","authors":["Zhiyu Tan","Xiaomeng Yang","Luozheng Qin","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2408.02629v1.pdf","comment":"project page: https://sais-fuxi.github.io/projects/vidgen-1m"},{"id":"http://arxiv.org/abs/2312.02396v3","updated":"2024-08-05T16:49:51Z","published":"2023-12-04T23:26:12Z","title":"Unsupervised Change Detection for Space Habitats Using 3D Point Clouds","summary":"  This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.\n","authors":["Jamie Santos","Holly Dinkel","Julia Di","Paulo V. K. Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2312.02396v3.pdf","comment":"15 pages, 7 figures, Manuscript was presented at the AIAA SciTech\n  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:\n  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:\n  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]"},{"id":"http://arxiv.org/abs/2408.02623v1","updated":"2024-08-05T16:48:03Z","published":"2024-08-05T16:48:03Z","title":"YOWOv3: An Efficient and Generalized Framework for Human Action\n  Detection and Recognition","summary":"  In this paper, we propose a new framework called YOWOv3, which is an improved\nversion of YOWOv2, designed specifically for the task of Human Action Detection\nand Recognition. This framework is designed to facilitate extensive\nexperimentation with different configurations and supports easy customization\nof various components within the model, reducing efforts required for\nunderstanding and modifying the code. YOWOv3 demonstrates its superior\nperformance compared to YOWOv2 on two widely used datasets for Human Action\nDetection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessor\nmodel YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2,\nrespectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model -\nYOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33%\nand 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate that\nYOWOv3 significantly reduces the number of parameters and GFLOPs while still\nachieving comparable performance.\n","authors":["Duc Manh Nguyen Dang","Viet Hang Duong","Jia Ching Wang","Nhan Bui Duc"],"pdf_url":"https://arxiv.org/pdf/2408.02623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02615v1","updated":"2024-08-05T16:39:39Z","published":"2024-08-05T16:39:39Z","title":"LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local\n  Attention and Mamba","summary":"  Recent Transformer-based diffusion models have shown remarkable performance,\nlargely attributed to the ability of the self-attention mechanism to accurately\ncapture both global and local contexts by computing all-pair interactions among\ninput tokens. However, their quadratic complexity poses significant\ncomputational challenges for long-sequence inputs. Conversely, a recent state\nspace model called Mamba offers linear complexity by compressing a filtered\nglobal context into a hidden state. Despite its efficiency, compression\ninevitably leads to information loss of fine-grained local dependencies among\ntokens, which are crucial for effective visual generative modeling. Motivated\nby these observations, we introduce Local Attentional Mamba (LaMamba) blocks\nthat combine the strengths of self-attention and Mamba, capturing both global\ncontexts and local details with linear complexity. Leveraging the efficient\nU-Net architecture, our model exhibits exceptional scalability and surpasses\nthe performance of DiT across various model scales on ImageNet at 256x256\nresolution, all while utilizing substantially fewer GFLOPs and a comparable\nnumber of parameters. Compared to state-of-the-art diffusion models on ImageNet\n256x256 and 512x512, our largest model presents notable advantages, such as a\nreduction of up to 62\\% GFLOPs compared to DiT-XL/2, while achieving superior\nperformance with comparable or fewer parameters.\n","authors":["Yunxiang Fu","Chaoqi Chen","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11515v2","updated":"2024-08-05T16:39:15Z","published":"2024-03-18T07:01:21Z","title":"SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption\n  of Monocular Depth Estimation in Autonomous Navigation Applications","summary":"  Monocular depth estimation (MDE) has advanced significantly, primarily\nthrough the integration of convolutional neural networks (CNNs) and more\nrecently, Transformers. However, concerns about their susceptibility to\nadversarial attacks have emerged, especially in safety-critical domains like\nautonomous driving and robotic navigation. Existing approaches for assessing\nCNN-based depth prediction methods have fallen short in inducing comprehensive\ndisruptions to the vision system, often limited to specific local areas. In\nthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel\napproach designed to comprehensively disrupt monocular depth estimation (MDE)\nin autonomous navigation applications. Our patch is crafted to selectively\nundermine MDE in two distinct ways: by distorting estimated distances or by\ncreating the illusion of an object disappearing from the system's perspective.\nNotably, our patch is shape-sensitive, meaning it considers the specific shape\nand scale of the target object, thereby extending its influence beyond\nimmediate proximity. Furthermore, our patch is trained to effectively address\ndifferent scales and distances from the camera. Experimental results\ndemonstrate that our approach induces a mean depth estimation error surpassing\n0.5, impacting up to 99% of the targeted region for CNN-based MDE models.\nAdditionally, we investigate the vulnerability of Transformer-based MDE models\nto patch-based attacks, revealing that SSAP yields a significant error of 0.59\nand exerts substantial influence over 99% of the target region on these models.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Bassem Ouni","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2403.11515v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.01351"},{"id":"http://arxiv.org/abs/2303.01351v3","updated":"2024-08-05T16:37:37Z","published":"2023-03-02T15:31:53Z","title":"APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth\n  Estimation for Autonomous Navigation","summary":"  In recent times, monocular depth estimation (MDE) has experienced significant\nadvancements in performance, largely attributed to the integration of\ninnovative architectures, i.e., convolutional neural networks (CNNs) and\nTransformers. Nevertheless, the susceptibility of these models to adversarial\nattacks has emerged as a noteworthy concern, especially in domains where safety\nand security are paramount. This concern holds particular weight for MDE due to\nits critical role in applications like autonomous driving and robotic\nnavigation, where accurate scene understanding is pivotal. To assess the\nvulnerability of CNN-based depth prediction methods, recent work tries to\ndesign adversarial patches against MDE. However, the existing approaches fall\nshort of inducing a comprehensive and substantially disruptive impact on the\nvision system. Instead, their influence is partial and confined to specific\nlocal areas. These methods lead to erroneous depth predictions only within the\noverlapping region with the input image, without considering the\ncharacteristics of the target object, such as its size, shape, and position. In\nthis paper, we introduce a novel adversarial patch named APARATE. This patch\npossesses the ability to selectively undermine MDE in two distinct ways: by\ndistorting the estimated distances or by creating the illusion of an object\ndisappearing from the perspective of the autonomous system. Notably, APARATE is\ndesigned to be sensitive to the shape and scale of the target object, and its\ninfluence extends beyond immediate proximity. APARATE, results in a mean depth\nestimation error surpassing $0.5$, significantly impacting as much as $99\\%$ of\nthe targeted region when applied to CNN-based MDE models. Furthermore, it\nyields a significant error of $0.34$ and exerts substantial influence over\n$94\\%$ of the target region in the context of Transformer-based MDE.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.01351v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07338v2","updated":"2024-08-05T16:34:43Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred for CIL. In this work, we revisit\nCIL with PTMs and argue that the core factors in CIL are adaptivity for model\nupdating and generalizability for knowledge transferring. 1) We first reveal\nthat frozen PTM can already provide generalizable embeddings for CIL.\nSurprisingly, a simple baseline (SimpleCIL) which continually sets the\nclassifiers of PTM to prototype features can beat state-of-the-art even without\ntraining on the downstream task. 2) Due to the distribution gap between\npre-trained and downstream datasets, PTM can be further cultivated with\nadaptivity via model adaptation. We propose AdaPt and mERge (APER), which\naggregates the embeddings of PTM and adapted models for classifier\nconstruction. APER is a general framework that can be orthogonally combined\nwith any parameter-efficient tuning method, which holds the advantages of PTM's\ngeneralizability and adapted model's adaptivity. 3) Additionally, considering\nprevious ImageNet-based benchmarks are unsuitable in the era of PTM due to data\noverlapping, we propose four new benchmarks for assessment, namely ImageNet-A,\nObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the\neffectiveness of APER with a unified and concise framework. Code is available\nat https://github.com/zhoudw-zdw/RevisitingCIL\n","authors":["Da-Wei Zhou","Zi-Wen Cai","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v2.pdf","comment":"Accepted to IJCV. Code is available at:\n  https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2408.02595v1","updated":"2024-08-05T16:07:31Z","published":"2024-08-05T16:07:31Z","title":"Modelling Visual Semantics via Image Captioning to extract Enhanced\n  Multi-Level Cross-Modal Semantic Incongruity Representation with Attention\n  for Multimodal Sarcasm Detection","summary":"  Sarcasm is a type of irony, characterized by an inherent mismatch between the\nliteral interpretation and the intended connotation. Though sarcasm detection\nin text has been extensively studied, there are situations in which textual\ninput alone might be insufficient to perceive sarcasm. The inclusion of\nadditional contextual cues, such as images, is essential to recognize sarcasm\nin social media data effectively. This study presents a novel framework for\nmultimodal sarcasm detection that can process input triplets. Two components of\nthese triplets comprise the input text and its associated image, as provided in\nthe datasets. Additionally, a supplementary modality is introduced in the form\nof descriptive image captions. The motivation behind incorporating this visual\nsemantic representation is to more accurately capture the discrepancies between\nthe textual and visual content, which are fundamental to the sarcasm detection\ntask. The primary contributions of this study are: (1) a robust textual feature\nextraction branch that utilizes a cross-lingual language model; (2) a visual\nfeature extraction branch that incorporates a self-regulated residual ConvNet\nintegrated with a lightweight spatially aware attention module; (3) an\nadditional modality in the form of image captions generated using an\nencoder-decoder architecture capable of reading text embedded in images; (4)\ndistinct attention modules to effectively identify the incongruities between\nthe text and two levels of image representations; (5) multi-level cross-domain\nsemantic incongruity representation achieved through feature fusion. Compared\nwith cutting-edge baselines, the proposed model achieves the best accuracy of\n92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and\nMultiBully datasets.\n","authors":["Sajal Aggarwal","Ananya Pandey","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.02595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02571v1","updated":"2024-08-05T15:45:59Z","published":"2024-08-05T15:45:59Z","title":"Contrastive Learning-based Multi Modal Architecture for Emoticon\n  Prediction by Employing Image-Text Pairs","summary":"  The emoticons are symbolic representations that generally accompany the\ntextual content to visually enhance or summarize the true intention of a\nwritten message. Although widely utilized in the realm of social media, the\ncore semantics of these emoticons have not been extensively explored based on\nmultiple modalities. Incorporating textual and visual information within a\nsingle message develops an advanced way of conveying information. Hence, this\nresearch aims to analyze the relationship among sentences, visuals, and\nemoticons. For an orderly exposition, this paper initially provides a detailed\nexamination of the various techniques for extracting multimodal features,\nemphasizing the pros and cons of each method. Through conducting a\ncomprehensive examination of several multimodal algorithms, with specific\nemphasis on the fusion approaches, we have proposed a novel contrastive\nlearning based multimodal architecture. The proposed model employs the joint\ntraining of dual-branch encoder along with the contrastive learning to\naccurately map text and images into a common latent space. Our key finding is\nthat by integrating the principle of contrastive learning with that of the\nother two branches yields superior results. The experimental results\ndemonstrate that our suggested methodology surpasses existing multimodal\napproaches in terms of accuracy and robustness. The proposed model attained an\naccuracy of 91% and an MCC-score of 90% while assessing emoticons using the\nMultimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidence\nthat deep features acquired by contrastive learning are more efficient,\nsuggesting that the proposed fusion technique also possesses strong\ngeneralisation capabilities for recognising emoticons across several modes.\n","authors":["Ananya Pandey","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.02571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02568v1","updated":"2024-08-05T15:43:56Z","published":"2024-08-05T15:43:56Z","title":"Cross-Modality Clustering-based Self-Labeling for Multimodal Data\n  Classification","summary":"  Technological advances facilitate the ability to acquire multimodal data,\nposing a challenge for recognition systems while also providing an opportunity\nto use the heterogeneous nature of the information to increase the\ngeneralization capability of models. An often overlooked issue is the cost of\nthe labeling process, which is typically high due to the need for a significant\ninvestment in time and money associated with human experts. Existing\nsemi-supervised learning methods often focus on operating in the feature space\ncreated by the fusion of available modalities, neglecting the potential for\ncross-utilizing complementary information available in each modality. To\naddress this problem, we propose Cross-Modality Clustering-based Self-Labeling\n(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances\nbelonging to each modality in the deep feature space and then propagates known\nlabels within the resulting clusters. Next, information about the instances'\nclass membership in each modality is exchanged based on the Euclidean distance\nto ensure more accurate labeling. Experimental evaluation conducted on 20\ndatasets derived from the MM-IMDb dataset indicates that cross-propagation of\nlabels between modalities -- especially when the number of pre-labeled\ninstances is small -- can allow for more reliable labeling and thus increase\nthe classification performance in each modality.\n","authors":["Pawe Zyblewski","Leandro L. Minku"],"pdf_url":"https://arxiv.org/pdf/2408.02568v1.pdf","comment":"10 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2301.07088v3","updated":"2024-08-05T15:38:05Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Many self-supervised learning methods are pre-trained on the well-curated\nImageNet-1K dataset. In this work, given the excellent scalability of web data,\nwe consider self-supervised pre-training on noisy web sourced image-text paired\ndata. First, we conduct a benchmark study of representative self-supervised\npre-training methods on large-scale web data in a like-for-like setting. We\ncompare a range of methods, including single-modal ones that use masked\ntraining objectives and multi-modal ones that use image-text constrastive\ntraining. We observe that existing multi-modal methods do not outperform their\nsingle-modal counterparts on vision transfer learning tasks. We derive an\ninformation-theoretical view to explain these benchmark results, which provides\ninsight into how to design a novel vision learner. Inspired by this insight, we\npresent a new visual representation pre-training method, MUlti-modal\nGenerator~(MUG), that learns from scalable web sourced image-text data. MUG\nachieves state-of-the-art transfer performance on a variety of tasks and\ndemonstrates promising scaling properties. Pre-trained models and code will be\nmade public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v3.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2408.02561v1","updated":"2024-08-05T15:37:18Z","published":"2024-08-05T15:37:18Z","title":"HQOD: Harmonious Quantization for Object Detection","summary":"  Task inharmony problem commonly occurs in modern object detectors, leading to\ninconsistent qualities between classification and regression tasks. The\npredicted boxes with high classification scores but poor localization positions\nor low classification scores but accurate localization positions will worsen\nthe performance of detectors after Non-Maximum Suppression. Furthermore, when\nobject detectors collaborate with Quantization-Aware Training (QAT), we observe\nthat the task inharmony problem will be further exacerbated, which is\nconsidered one of the main causes of the performance degradation of quantized\ndetectors. To tackle this issue, we propose the Harmonious Quantization for\nObject Detection (HQOD) framework, which consists of two components. Firstly,\nwe propose a task-correlated loss to encourage detectors to focus on improving\nsamples with lower task harmony quality during QAT. Secondly, a harmonious\nIntersection over Union (IoU) loss is incorporated to balance the optimization\nof the regression branch across different IoU levels. The proposed HQOD can be\neasily integrated into different QAT algorithms and detectors. Remarkably, on\nthe MS COCO dataset, our 4-bit ATSS with ResNet-50 backbone achieves a\nstate-of-the-art mAP of 39.6%, even surpassing the full-precision one.\n","authors":["Long Huang","Zhiwei Dong","Song-Lu Chen","Ruiyao Zhang","Shutong Ti","Feng Chen","Xu-Cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2408.02561v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo (ICME),\n  July 15 - July 19, 2024, Niagra Falls, Ontario, Canada"},{"id":"http://arxiv.org/abs/2408.02555v1","updated":"2024-08-05T15:33:45Z","published":"2024-08-05T15:33:45Z","title":"MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh\n  Tokenization","summary":"  We introduce MeshAnything V2, an autoregressive transformer that generates\nArtist-Created Meshes (AM) aligned to given shapes. It can be integrated with\nvarious 3D asset production pipelines to achieve high-quality, highly\ncontrollable AM generation. MeshAnything V2 surpasses previous methods in both\nefficiency and performance using models of the same size. These improvements\nare due to our newly proposed mesh tokenization method: Adjacent Mesh\nTokenization (AMT). Different from previous methods that represent each face\nwith three vertices, AMT uses a single vertex whenever possible. Compared to\nprevious methods, AMT requires about half the token sequence length to\nrepresent the same mesh in average. Furthermore, the token sequences from AMT\nare more compact and well-structured, fundamentally benefiting AM generation.\nOur extensive experiments show that AMT significantly improves the efficiency\nand performance of AM generation. Project Page:\nhttps://buaacyw.github.io/meshanything-v2/\n","authors":["Yiwen Chen","Yikai Wang","Yihao Luo","Zhengyi Wang","Zilong Chen","Jun Zhu","Chi Zhang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2408.02555v1.pdf","comment":"Project Page: https://buaacyw.github.io/meshanything-v2/ Github:\n  https://github.com/buaacyw/MeshAnythingV2"},{"id":"http://arxiv.org/abs/2408.02507v1","updated":"2024-08-05T14:31:09Z","published":"2024-08-05T14:31:09Z","title":"Estimating Pore Location of PBF-LB/M Processes with Segmentation Models","summary":"  Reliably manufacturing defect free products is still an open challenge for\nLaser Powder Bed Fusion processes. Particularly, pores that occur frequently\nhave a negative impact on mechanical properties like fatigue performance.\nTherefore, an accurate localisation of pores is mandatory for quality\nassurance, but requires time-consuming post-processing steps like computer\ntomography scans. Although existing solutions using in-situ monitoring data can\ndetect pore occurrence within a layer, they are limited in their localisation\nprecision. Therefore, we propose a pore localisation approach that estimates\ntheir position within a single layer using a Gaussian kernel density\nestimation. This allows segmentation models to learn the correlation between\nin-situ monitoring data and the derived probability distribution of pore\noccurrence. Within our experiments, we compare the prediction performance of\ndifferent segmentation models depending on machine parameter configuration and\ngeometry features. From our results, we conclude that our approach allows a\nprecise localisation of pores that requires minimal data preprocessing. Our\nresearch extends the literature by providing a foundation for more precise pore\ndetection systems.\n","authors":["Hans Aoyang Zhou","Jan Theunissen","Marco Kemmerling","Anas Abdelrazeq","Johannes Henrich Schleifenbaum","Robert H. Schmitt"],"pdf_url":"https://arxiv.org/pdf/2408.02507v1.pdf","comment":"20 pages, 7 figures, This work has been submitted to the Journal\n  Progress in Additive Manufacturing"},{"id":"http://arxiv.org/abs/2408.02496v1","updated":"2024-08-05T14:19:03Z","published":"2024-08-05T14:19:03Z","title":"Automatic rating of incomplete hippocampal inversions evaluated across\n  multiple cohorts","summary":"  Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal\nmalrotation, is an atypical anatomical pattern of the hippocampus found in\nabout 20% of the general population. IHI can be visually assessed on coronal\nslices of T1 weighted MR images, using a composite score that combines four\nanatomical criteria. IHI has been associated with several brain disorders\n(epilepsy, schizophrenia). However, these studies were based on small samples.\nFurthermore, the factors (genetic or environmental) that contribute to the\ngenesis of IHI are largely unknown. Large-scale studies are thus needed to\nfurther understand IHI and their potential relationships to neurological and\npsychiatric disorders. However, visual evaluation is long and tedious,\njustifying the need for an automatic method. In this paper, we propose, for the\nfirst time, to automatically rate IHI. We proceed by predicting four anatomical\ncriteria, which are then summed up to form the IHI score, providing the\nadvantage of an interpretable score. We provided an extensive experimental\ninvestigation of different machine learning methods and training strategies. We\nperformed automatic rating using a variety of deep learning models (conv5-FC3,\nResNet and SECNN) as well as a ridge regression. We studied the generalization\nof our models using different cohorts and performed multi-cohort learning. We\nrelied on a large population of 2,008 participants from the IMAGEN study, 993\nand 403 participants from the QTIM/QTAB studies as well as 985 subjects from\nthe UKBiobank. We showed that deep learning models outperformed a ridge\nregression. We demonstrated that the performances of the conv5-FC3 network were\nat least as good as more complex networks while maintaining a low complexity\nand computation time. We showed that training on a single cohort may lack in\nvariability while training on several cohorts improves generalization.\n","authors":["Lisa Hemforth","Baptiste Couvy-Duchesne","Kevin De Matos","Camille Brianceau","Matthieu Joulot","Tobias Banaschewski","Arun L. W. Bokde","Sylvane Desrivires","Herta Flor","Antoine Grigis","Hugh Garavan","Penny Gowland","Andreas Heinz","Rdiger Brhl","Jean-Luc Martinot","Marie-Laure Paillre Martinot","Eric Artiges","Dimitri Papadopoulos","Herve Lemaitre","Tomas Paus","Luise Poustka","Sarah Hohmann","Nathalie Holz","Juliane H. Frhner","Michael N. Smolka","Nilakshi Vaidya","Henrik Walter","Robert Whelan","Gunter Schumann","Christian Bchel","JB Poline","Bernd Itterman","Vincent Frouin","Alexandre Martin","IMAGEN study group","Claire Cury","Olivier Colliot"],"pdf_url":"https://arxiv.org/pdf/2408.02496v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:016"},{"id":"http://arxiv.org/abs/2408.02494v1","updated":"2024-08-05T14:18:29Z","published":"2024-08-05T14:18:29Z","title":"HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions","summary":"  Traditional deep learning models rely on methods such as softmax\ncross-entropy and ArcFace loss for tasks like classification and face\nrecognition. These methods mainly explore angular features in a hyperspherical\nspace, often resulting in entangled inter-class features due to dense angular\ndata across many classes. In this paper, a new field of feature exploration is\nproposed known as HyperSpaceX which enhances class discrimination by exploring\nboth angular and radial dimensions in multi-hyperspherical spaces, facilitated\nby a novel DistArc loss. The proposed DistArc loss encompasses three feature\narrangement components: two angular and one radial, enforcing intra-class\nbinding and inter-class separation in multi-radial arrangement, improving\nfeature discriminability. Evaluation of HyperSpaceX framework for the novel\nrepresentation utilizes a proposed predictive measure that accounts for both\nangular and radial elements, providing a more comprehensive assessment of model\naccuracy beyond standard metrics. Experiments across seven object\nclassification and six face recognition datasets demonstrate state-of-the-art\n(SoTA) results obtained from HyperSpaceX, achieving up to a 20% performance\nimprovement on large-scale object datasets in lower dimensions and up to 6%\ngain in higher dimensions.\n","authors":["Chiranjeev Chiranjeev","Muskan Dosi","Kartik Thakral","Mayank Vatsa","Richa Singh"],"pdf_url":"https://arxiv.org/pdf/2408.02494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10107v2","updated":"2024-08-05T14:06:35Z","published":"2024-06-14T15:08:04Z","title":"Annotation Cost-Efficient Active Learning for Deep Metric Learning\n  Driven Remote Sensing Image Retrieval","summary":"  Deep metric learning (DML) has shown to be effective for content-based image\nretrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on a\nhigh number of annotated images to accurately learn model parameters of deep\nneural networks (DNNs). However, gathering such data is time-consuming and\ncostly. To address this, we propose an annotation cost-efficient active\nlearning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims to\ncreate a small but informative training set made up of similar and dissimilar\nimage pairs to be utilized for accurately learning a metric space. The\ninformativeness of image pairs is evaluated by combining uncertainty and\ndiversity criteria. To assess the uncertainty of image pairs, we introduce two\nalgorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binary\nclassifier guided uncertainty estimation (BCGUE). MGUE algorithm automatically\nestimates a threshold value that acts as a boundary between similar and\ndissimilar image pairs based on the distances in the metric space. The closer\nthe similarity between image pairs is to the estimated threshold value the\nhigher their uncertainty. BCGUE algorithm estimates the uncertainty of the\nimage pairs based on the confidence of the classifier in assigning correct\nsimilarity labels. The diversity criterion is assessed through a\nclustering-based strategy. ANNEAL combines either MGUE or BCGUE algorithm with\nthe clustering-based strategy to select the most informative image pairs, which\nare then labelled by expert annotators as similar or dissimilar. This way of\nannotating images significantly reduces the annotation cost compared to\nannotating images with land-use land-cover class labels. Experimental results\non two RS benchmark datasets demonstrate the effectiveness of our method. The\ncode of this work is publicly available at\n\\url{https://git.tu-berlin.de/rsim/anneal_tgrs}.\n","authors":["Genc Hoxha","Gencer Sumbul","Julia Henkel","Lars Mllenbrok","Begm Demir"],"pdf_url":"https://arxiv.org/pdf/2406.10107v2.pdf","comment":"Accepted for publication in the IEEE Transactions on Geoscience and\n  Remote Sensing (TGRS)"},{"id":"http://arxiv.org/abs/2408.02484v1","updated":"2024-08-05T14:05:25Z","published":"2024-08-05T14:05:25Z","title":"Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection","summary":"  Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier\ntopic due to its capability to detect HOIs beyond a predefined set of\ncategories. This task entails not only identifying the interactiveness of\nhuman-object pairs and localizing them but also recognizing both seen and\nunseen interaction categories. In this paper, we introduce a novel framework\nfor zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP.\nThis approach enhances the generalization of large foundation models, such as\nCLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learning\nmethods, we propose learning decoupled vision and language prompts for\ninteractiveness-aware visual feature extraction and generalizable interaction\nclassification, respectively. Specifically, we integrate prior knowledge of\ndifferent granularity into conditional vision prompts, including an\ninput-conditioned instance prior and a global spatial pattern prior. The former\nencourages the image encoder to treat instances belonging to seen or\npotentially unseen HOI concepts equally while the latter provides\nrepresentative plausible spatial configuration of the human and object under\ninteraction. Besides, we employ language-aware prompt learning with a\nconsistency constraint to preserve the knowledge of the large foundation model\nto enable better generalization in the text branch. Extensive experiments\ndemonstrate the efficacy of our detector with conditional multi-modal prompts,\noutperforming previous state-of-the-art on unseen classes of various zero-shot\nsettings. The code and models are available at\n\\url{https://github.com/ltttpku/CMMP}.\n","authors":["Ting Lei","Shaofeng Yin","Yuxin Peng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02875v2","updated":"2024-08-05T14:01:26Z","published":"2024-03-05T11:38:48Z","title":"Enhancing Conceptual Understanding in Multimodal Contrastive Learning\n  through Hard Negative Samples","summary":"  Current multimodal models leveraging contrastive learning often face\nlimitations in developing fine-grained conceptual understanding. This is due to\nrandom negative samples during pretraining, causing almost exclusively very\ndissimilar concepts to be compared in the loss function. Consequently, the\nmodels struggle with fine-grained semantic differences. To address this\nproblem, we introduce a novel pretraining method incorporating synthetic hard\nnegative text examples. The hard negatives permute terms corresponding to\nvisual concepts, leading to a more fine-grained visual and textual concept\nalignment. Further, we introduce InpaintCOCO, a new challenging dataset for\nassessing the fine-grained alignment of colors, objects, and sizes in\nvision-language models. We created the dataset using generative inpainting from\nCOCO images by changing the visual concepts so that the images no longer match\ntheir original captions. Our results show significant improvements in\nfine-grained concept understanding across a wide range of vision-language\ndatasets, including our InpaintCOCO dataset.\n","authors":["Philipp J. Rsch","Norbert Oswald","Michaela Geierhos","Jindich Libovick"],"pdf_url":"https://arxiv.org/pdf/2403.02875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02464v1","updated":"2024-08-05T13:44:22Z","published":"2024-08-05T13:44:22Z","title":"Fairness and Bias Mitigation in Computer Vision: A Survey","summary":"  Computer vision systems have witnessed rapid progress over the past two\ndecades due to multiple advances in the field. As these systems are\nincreasingly being deployed in high-stakes real-world applications, there is a\ndire need to ensure that they do not propagate or amplify any discriminatory\ntendencies in historical or human-curated data or inadvertently learn biases\nfrom spurious correlations. This paper presents a comprehensive survey on\nfairness that summarizes and sheds light on ongoing trends and successes in the\ncontext of computer vision. The topics we discuss include 1) The origin and\ntechnical definitions of fairness drawn from the wider fair machine learning\nliterature and adjacent disciplines. 2) Work that sought to discover and\nanalyze biases in computer vision systems. 3) A summary of methods proposed to\nmitigate bias in computer vision systems in recent years. 4) A comprehensive\nsummary of resources and datasets produced by researchers to measure, analyze,\nand mitigate bias and enhance fairness. 5) Discussion of the field's success,\ncontinuing trends in the context of multimodal foundation and generative\nmodels, and gaps that still need to be addressed. The presented\ncharacterization should help researchers understand the importance of\nidentifying and mitigating bias in computer vision and the state of the field\nand identify potential directions for future research.\n","authors":["Sepehr Dehdashtian","Ruozhen He","Yi Li","Guha Balakrishnan","Nuno Vasconcelos","Vicente Ordonez","Vishnu Naresh Boddeti"],"pdf_url":"https://arxiv.org/pdf/2408.02464v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02462v1","updated":"2024-08-05T13:40:33Z","published":"2024-08-05T13:40:33Z","title":"An investigation into the causes of race bias in AI-based cine CMR\n  segmentation","summary":"  Artificial intelligence (AI) methods are being used increasingly for the\nautomated segmentation of cine cardiac magnetic resonance (CMR) imaging.\nHowever, these methods have been shown to be subject to race bias, i.e. they\nexhibit different levels of performance for different races depending on the\n(im)balance of the data used to train the AI model. In this paper we\ninvestigate the source of this bias, seeking to understand its root cause(s) so\nthat it can be effectively mitigated. We perform a series of classification and\nsegmentation experiments on short-axis cine CMR images acquired from Black and\nWhite subjects from the UK Biobank and apply AI interpretability methods to\nunderstand the results. In the classification experiments, we found that race\ncan be predicted with high accuracy from the images alone, but less accurately\nfrom ground truth segmentations, suggesting that the distributional shift\nbetween races, which is often the cause of AI bias, is mostly image-based\nrather than segmentation-based. The interpretability methods showed that most\nattention in the classification models was focused on non-heart regions, such\nas subcutaneous fat. Cropping the images tightly around the heart reduced\nclassification accuracy to around chance level. Similarly, race can be\npredicted from the latent representations of a biased segmentation model,\nsuggesting that race information is encoded in the model. Cropping images\ntightly around the heart reduced but did not eliminate segmentation bias. We\nalso investigate the influence of possible confounders on the bias observed.\n","authors":["Tiarna Lee","Esther Puyol-Anton","Bram Ruijsink","Sebastien Roujol","Theodore Barfoot","Shaheim Ogbomo-Harmitt","Miaojing Shi","Andrew P. King"],"pdf_url":"https://arxiv.org/pdf/2408.02462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15269v2","updated":"2024-08-05T13:23:17Z","published":"2024-06-21T16:04:14Z","title":"You Only Acquire Sparse-channel (YOAS): A Unified Framework for\n  Dense-channel EEG Generation","summary":"  High-precision acquisition of dense-channel electroencephalogram (EEG)\nsignals is often impeded by the costliness and lack of portability of\nequipment. In contrast, generating dense-channel EEG signals effectively from\nsparse channels shows promise and economic viability. However, sparse-channel\nEEG poses challenges such as reduced spatial resolution, information loss,\nsignal mixing, and heightened susceptibility to noise and interference. To\naddress these challenges, we first theoretically formulate the dense-channel\nEEG generation problem as by optimizing a set of cross-channel EEG signal\ngeneration problems. Then, we propose the YOAS framework for generating\ndense-channel data from sparse-channel EEG signals. The YOAS totally consists\nof four sequential stages: Data Preparation, Data Preprocessing, Biased-EEG\nGeneration, and Synthetic EEG Generation. Data Preparation and Preprocessing\ncarefully consider the distribution of EEG electrodes and low signal-to-noise\nratio problem of EEG signals. Biased-EEG Generation includes sub-modules of\nBiasEEGGanFormer and BiasEEGDiffFormer, which facilitate long-term feature\nextraction with attention and generate signals by combining electrode position\nalignment with diffusion model, respectively. Synthetic EEG Generation\nsynthesizes the final signals, employing a deduction paradigm for multi-channel\nEEG generation. Extensive experiments confirmed YOAS's feasibility, efficiency,\nand theoretical validity, even remarkably enhancing data discernibility. This\nbreakthrough in dense-channel EEG signal generation from sparse-channel data\nopens new avenues for exploration in EEG signal processing and application.\n","authors":["Hongyu Chen","Weiming Zeng","Luhui Cai","Lei Wang","Jia Lu","Yueyang Li","Hongjie Yan","Wai Ting Siok","Nizhuan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.15269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12890v3","updated":"2024-08-05T13:10:02Z","published":"2023-11-21T06:24:09Z","title":"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback","summary":"  Visual programming, a modular and generalizable paradigm, integrates\ndifferent modules and Python operators to solve various vision-language tasks.\nUnlike end-to-end models that need task-specific data, it advances in\nperforming visual processing and reasoning in an unsupervised manner. Current\nvisual programming methods generate programs in a single pass for each task\nwhere the ability to evaluate and optimize based on feedback, unfortunately, is\nlacking, which consequentially limits their effectiveness for complex,\nmulti-step problems. Drawing inspiration from benders decomposition, we\nintroduce De-fine, a training-free framework that automatically decomposes\ncomplex tasks into simpler subtasks and refines programs through auto-feedback.\nThis model-agnostic approach can improve logical reasoning performance by\nintegrating the strengths of multiple models. Our experiments across various\nvisual tasks show that De-fine creates more robust programs. Moreover, viewing\neach feedback module as an independent agent will yield fresh prospects for the\nfield of agent research.\n","authors":["Minghe Gao","Juncheng Li","Hao Fei","Liang Pang","Wei Ji","Guoming Wang","Zheqi Lv","Wenqiao Zhang","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.12890v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07990v2","updated":"2024-08-05T12:55:47Z","published":"2024-04-11T17:59:56Z","title":"OpenBias: Open-set Bias Detection in Text-to-Image Generative Models","summary":"  Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.\n","authors":["Moreno D'Inc","Elia Peruzzo","Massimiliano Mancini","Dejia Xu","Vidit Goel","Xingqian Xu","Zhangyang Wang","Humphrey Shi","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2404.07990v2.pdf","comment":"CVPR 2024 Highlight - Code:\n  https://github.com/Picsart-AI-Research/OpenBias"},{"id":"http://arxiv.org/abs/2303.10571v2","updated":"2024-08-05T12:44:04Z","published":"2023-03-19T05:20:52Z","title":"Reinforcement Learning Friendly Vision-Language Model for Minecraft","summary":"  One of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https://github.com/PKU-RL/CLIP4MC.\n","authors":["Haobin Jiang","Junpeng Yue","Hao Luo","Ziluo Ding","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2303.10571v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2404.11129v2","updated":"2024-08-05T12:39:06Z","published":"2024-04-17T07:20:56Z","title":"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales","summary":"  The remarkable performance of Multimodal Large Language Models (MLLMs) has\nunequivocally demonstrated their proficient understanding capabilities in\nhandling a wide array of visual tasks. Nevertheless, the opaque nature of their\nblack-box reasoning processes persists as an enigma, rendering them\nuninterpretable and struggling with hallucination. Their ability to execute\nintricate compositional reasoning tasks is also constrained, culminating in a\nstagnation of learning progression for these models. In this work, we introduce\nFact, a novel paradigm designed to generate multimodal rationales that are\nfaithful, concise, and transferable for teaching MLLMs. This paradigm utilizes\nverifiable visual programming to generate executable code guaranteeing\nfaithfulness and precision. Subsequently, through a series of operations\nincluding pruning, merging, and bridging, the rationale enhances its\nconciseness. Furthermore, we filter rationales that can be transferred to\nend-to-end paradigms from programming paradigms to guarantee transferability.\nEmpirical evidence from experiments demonstrates the superiority of our method\nacross models of varying parameter sizes, significantly enhancing their\ncompositional reasoning and generalization ability. Our approach also reduces\nhallucinations owing to its high correlation between images and text.\n","authors":["Minghe Gao","Shuang Chen","Liang Pang","Yuan Yao","Jisheng Dang","Wenqiao Zhang","Juncheng Li","Siliang Tang","Yueting Zhuang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2404.11129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02427v1","updated":"2024-08-05T12:34:49Z","published":"2024-08-05T12:34:49Z","title":"Attenuation-adjusted deep learning of pore defects in 2D radiographs of\n  additive manufacturing powders","summary":"  The presence of gas pores in metal feedstock powder for additive\nmanufacturing greatly affects the final AM product. Since current porosity\nanalysis often involves lengthy X-ray computed tomography (XCT) scans with a\nfull rotation around the sample, motivation exists to explore methods that\nallow for high throughput -- possibly enabling in-line porosity analysis during\nmanufacturing. Through labelling pore pixels on single 2D radiographs of\npowders, this work seeks to simulate such future efficient setups. High\nsegmentation accuracy is achieved by combining a model of X-ray attenuation\nthrough particles with a variant of the widely applied UNet architecture;\nnotably, F1-score increases by $11.4\\%$ compared to the baseline UNet. The\nproposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)\nmaking tight particle cutouts, and 3) subtracting an ideal particle without\npores generated from a distance map inspired by Lambert-Beers law. This paper\nexplores four image processing methods, where the fastest (yet still\nunoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,\nand the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalable\nnature, these strategies can be involved in making high throughput porosity\nanalysis of metal feedstock powder for additive manufacturing.\n","authors":["Andreas Bjerregaard","David Schumacher","Jon Sporring"],"pdf_url":"https://arxiv.org/pdf/2408.02427v1.pdf","comment":"Implementation on https://github.com/yhsure/porosity"},{"id":"http://arxiv.org/abs/2408.02426v1","updated":"2024-08-05T12:33:07Z","published":"2024-08-05T12:33:07Z","title":"FPT+: A Parameter and Memory Efficient Transfer Learning Method for\n  High-resolution Medical Image Classification","summary":"  The success of large-scale pre-trained models has established fine-tuning as\na standard method for achieving significant improvements in downstream tasks.\nHowever, fine-tuning the entire parameter set of a pre-trained model is costly.\nParameter-efficient transfer learning (PETL) has recently emerged as a\ncost-effective alternative for adapting pre-trained models to downstream tasks.\nDespite its advantages, the increasing model size and input resolution present\nchallenges for PETL, as the training memory consumption is not reduced as\neffectively as the parameter usage. In this paper, we introduce Fine-grained\nPrompt Tuning plus (FPT+), a PETL method designed for high-resolution medical\nimage classification, which significantly reduces memory consumption compared\nto other PETL methods. FPT+ performs transfer learning by training a\nlightweight side network and accessing pre-trained knowledge from a large\npre-trained model (LPM) through fine-grained prompts and fusion modules.\nSpecifically, we freeze the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM processes high-resolution images to extract\nfine-grained features, while the side network employs the corresponding\ndown-sampled low-resolution images to minimize the memory usage. To enable the\nside network to leverage pre-trained knowledge, we propose fine-grained prompts\nand fusion modules, which collaborate to summarize information through the\nLPM's intermediate activations. We evaluate FPT+ on eight medical image\ndatasets of varying sizes, modalities, and complexities. Experimental results\ndemonstrate that FPT+ outperforms other PETL methods, using only 1.03% of the\nlearnable parameters and 3.18% of the memory required for fine-tuning an entire\nViT-B model. Our code is available at https://github.com/YijinHuang/FPT.\n","authors":["Yijin Huang","Pujin Cheng","Roger Tam","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2408.02426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19719v2","updated":"2024-08-05T12:29:47Z","published":"2024-07-29T06:03:13Z","title":"Revolutionizing Urban Safety Perception Assessments: Integrating\n  Multimodal Large Language Models with Street View Images","summary":"  Measuring urban safety perception is an important and complex task that\ntraditionally relies heavily on human resources. This process often involves\nextensive field surveys, manual data collection, and subjective assessments,\nwhich can be time-consuming, costly, and sometimes inconsistent. Street View\nImages (SVIs), along with deep learning methods, provide a way to realize\nlarge-scale urban safety detection. However, achieving this goal often requires\nextensive human annotation to train safety ranking models, and the\narchitectural differences between cities hinder the transferability of these\nmodels. Thus, a fully automated method for conducting safety evaluations is\nessential. Recent advances in multimodal large language models (MLLMs) have\ndemonstrated powerful reasoning and analytical capabilities. Cutting-edge\nmodels, e.g., GPT-4 have shown surprising performance in many tasks. We\nemployed these models for urban safety ranking on a human-annotated anchor set\nand validated that the results from MLLMs align closely with human perceptions.\nAdditionally, we proposed a method based on the pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) feature and K-Nearest Neighbors (K-NN)\nretrieval to quickly assess the safety index of the entire city. Experimental\nresults show that our method outperforms existing training needed deep learning\napproaches, achieving efficient and accurate urban safety evaluations. The\nproposed automation for urban safety perception assessment is a valuable tool\nfor city planners, policymakers, and researchers aiming to improve urban\nenvironments.\n","authors":["Jiaxin Zhang","Yunqin Li","Tomohiro Fukuda","Bowen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19719v2.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02421v1","updated":"2024-08-05T12:27:28Z","published":"2024-08-05T12:27:28Z","title":"FE-Adapter: Adapting Image-based Emotion Classifiers to Videos","summary":"  Utilizing large pre-trained models for specific tasks has yielded impressive\nresults. However, fully fine-tuning these increasingly large models is becoming\nprohibitively resource-intensive. This has led to a focus on more\nparameter-efficient transfer learning, primarily within the same modality. But\nthis approach has limitations, particularly in video understanding where\nsuitable pre-trained models are less common. Addressing this, our study\nintroduces a novel cross-modality transfer learning approach from images to\nvideos, which we call parameter-efficient image-to-video transfer learning. We\npresent the Facial-Emotion Adapter (FE-Adapter), designed for efficient\nfine-tuning in video tasks. This adapter allows pre-trained image models, which\ntraditionally lack temporal processing capabilities, to analyze dynamic video\ncontent efficiently. Notably, it uses about 15 times fewer parameters than\nprevious methods, while improving accuracy. Our experiments in video emotion\nrecognition demonstrate that the FE-Adapter can match or even surpass existing\nfine-tuning and video emotion models in both performance and efficiency. This\nbreakthrough highlights the potential for cross-modality approaches in\nenhancing the capabilities of AI models, particularly in fields like video\nemotion analysis where the demand for efficiency and accuracy is constantly\nrising.\n","authors":["Shreyank N Gowda","Boyan Gao","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2408.02421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12198v2","updated":"2024-08-05T12:20:49Z","published":"2024-02-19T15:03:04Z","title":"Zero shot VLMs for hate meme detection: Are we there yet?","summary":"  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n","authors":["Naquee Rizwan","Paramananda Bhaskar","Mithun Das","Swadhin Satyaprakash Majhi","Punyajoy Saha","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.12198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19905v2","updated":"2024-08-05T12:12:48Z","published":"2024-06-28T13:20:17Z","title":"Solving Token Gradient Conflict in Mixture-of-Experts for Large\n  Vision-Language Model","summary":"  The Mixture-of-Experts (MoE) has gained increasing attention in studying\nLarge Vision-Language Models (LVLMs). It uses a sparse model to replace the\ndense model, achieving comparable performance while activating fewer parameters\nduring inference, thus significantly reducing the inference cost. Existing MoE\nmethods in LVLMs encourage different experts to handle different tokens, and\nthey usually employ a router to predict the routing of each token. However, the\npredictions are based solely on sample features and do not truly reveal the\noptimization directions of tokens. This may lead to severe optimization\ninterference between different tokens assigned to an expert. To address this\nproblem, this paper proposes a novel method based on token-level gradient\nanalysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first\nuse token-level gradients to identify conflicting tokens in experts. After\nthat, we add a specialized loss tailored to eliminate conflicts among tokens\nwithin each expert. Our method can serve as a plug-in for diverse Large\nVision-Language Models, and extensive experimental results demonstrate its\neffectiveness. The code will be publicly available at\nhttps://github.com/longrongyang/STGC.\n","authors":["Longrong Yang","Dong Shen","Chaoxiang Cai","Fan Yang","Size Li","Di Zhang","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02408v1","updated":"2024-08-05T12:09:38Z","published":"2024-08-05T12:09:38Z","title":"Multi-weather Cross-view Geo-localization Using Denoising Diffusion\n  Models","summary":"  Cross-view geo-localization in GNSS-denied environments aims to determine an\nunknown location by matching drone-view images with the correct geo-tagged\nsatellite-view images from a large gallery. Recent research shows that learning\ndiscriminative image representations under specific weather conditions can\nsignificantly enhance performance. However, the frequent occurrence of unseen\nextreme weather conditions hinders progress. This paper introduces MCGF, a\nMulti-weather Cross-view Geo-localization Framework designed to dynamically\nadapt to unseen weather conditions. MCGF establishes a joint optimization\nbetween image restoration and geo-localization using denoising diffusion\nmodels. For image restoration, MCGF incorporates a shared encoder and a\nlightweight restoration module to help the backbone eliminate weather-specific\ninformation. For geo-localization, MCGF uses EVA-02 as a backbone for feature\nextraction, with cross-entropy loss for training and cosine distance for\ntesting. Extensive experiments on University160k-WX demonstrate that MCGF\nachieves competitive results for geo-localization in varying weather\nconditions.\n","authors":["Tongtong Feng","Qing Li","Xin Wang","Mingzi Wang","Guangyao Li","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.02408v1.pdf","comment":"Accepted by ACM MM24 workshop"},{"id":"http://arxiv.org/abs/2408.02398v1","updated":"2024-08-05T11:42:41Z","published":"2024-08-05T11:42:41Z","title":"Tensorial template matching for fast cross-correlation with rotations\n  and its application for tomography","summary":"  Object detection is a main task in computer vision. Template matching is the\nreference method for detecting objects with arbitrary templates. However,\ntemplate matching computational complexity depends on the rotation accuracy,\nbeing a limiting factor for large 3D images (tomograms). Here, we implement a\nnew algorithm called tensorial template matching, based on a mathematical\nframework that represents all rotations of a template with a tensor field.\nContrary to standard template matching, the computational complexity of the\npresented algorithm is independent of the rotation accuracy. Using both,\nsynthetic and real data from tomography, we demonstrate that tensorial template\nmatching is much faster than template matching and has the potential to improve\nits accuracy\n","authors":["Antonio Martinez-Sanchez","Ulrike Homberg","Jos Mara Almira","Harold Phelippeau"],"pdf_url":"https://arxiv.org/pdf/2408.02398v1.pdf","comment":"Accepted in The 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02394v1","updated":"2024-08-05T11:40:59Z","published":"2024-08-05T11:40:59Z","title":"CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point\n  Cloud Registration","summary":"  Image-to-point cloud registration aims to determine the relative camera pose\nof an RGB image with respect to a point cloud. It plays an important role in\ncamera localization within pre-built LiDAR maps. Despite the modality gaps,\nmost learning-based methods establish 2D-3D point correspondences in feature\nspace without any feedback mechanism for iterative optimization, resulting in\npoor accuracy and interpretability. In this paper, we propose to reformulate\nthe registration procedure as an iterative Markov decision process, allowing\nfor incremental adjustments to the camera pose based on each intermediate\nstate. To achieve this, we employ reinforcement learning to develop a\ncross-modal registration agent (CMR-Agent), and use imitation learning to\ninitialize its registration policy for stability and quick-start of the\ntraining. According to the cross-modal observations, we propose a 2D-3D hybrid\nstate representation that fully exploits the fine-grained features of RGB\nimages while reducing the useless neutral states caused by the spatial\ntruncation of camera frustum. Additionally, the overall framework is\nwell-designed to efficiently reuse one-shot cross-modal embeddings, avoiding\nrepetitive and time-consuming feature extraction. Extensive experiments on the\nKITTI-Odometry and NuScenes datasets demonstrate that CMR-Agent achieves\ncompetitive accuracy and efficiency in registration. Once the one-shot\nembeddings are completed, each iteration only takes a few milliseconds.\n","authors":["Gongxin Yao","Yixin Xuan","Xinyang Li","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02394v1.pdf","comment":"Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2408.02392v1","updated":"2024-08-05T11:39:22Z","published":"2024-08-05T11:39:22Z","title":"MaFreeI2P: A Matching-Free Image-to-Point Cloud Registration Paradigm\n  with Active Camera Pose Retrieval","summary":"  Image-to-point cloud registration seeks to estimate their relative camera\npose, which remains an open question due to the data modality gaps. The recent\nmatching-based methods tend to tackle this by building 2D-3D correspondences.\nIn this paper, we reveal the information loss inherent in these methods and\npropose a matching-free paradigm, named MaFreeI2P. Our key insight is to\nactively retrieve the camera pose in SE(3) space by contrasting the geometric\nfeatures between the point cloud and the query image. To achieve this, we first\nsample a set of candidate camera poses and construct their cost volume using\nthe cross-modal features. Superior to matching, cost volume can preserve more\ninformation and its feature similarity implicitly reflects the confidence level\nof the sampled poses. Afterwards, we employ a convolutional network to\nadaptively formulate a similarity assessment function, where the input cost\nvolume is further improved by filtering and pose-based weighting. Finally, we\nupdate the camera pose based on the similarity scores, and adopt a heuristic\nstrategy to iteratively shrink the pose sampling space for convergence. Our\nMaFreeI2P achieves a very competitive registration accuracy and recall on the\nKITTI-Odometry and Apollo-DaoxiangLake datasets.\n","authors":["Gongxin Yao","Xinyang Li","Yixin Xuan","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02392v1.pdf","comment":"Accepted to IEEE Conference on Multimedia Expo 2024"},{"id":"http://arxiv.org/abs/2309.01446v4","updated":"2024-08-05T11:34:10Z","published":"2023-09-04T08:54:20Z","title":"Open Sesame! Universal Black Box Jailbreaking of Large Language Models","summary":"  Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.\n","authors":["Raz Lapid","Ron Langberg","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2309.01446v4.pdf","comment":"Accepted at SeT-LLM @ ICLR 2024"},{"id":"http://arxiv.org/abs/2408.02382v1","updated":"2024-08-05T11:14:23Z","published":"2024-08-05T11:14:23Z","title":"Cross Psuedo Supervision Framework for Sparsely Labelled Geo-spatial\n  Images","summary":"  Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning and is one of the key elements in developing smart and sustainable\ncities. This study introduces a semi-supervised segmentation model for LULC\nprediction using high-resolution satellite images with a huge diversity in data\ndistributions in different areas from the country of India. Our approach\nensures a robust generalization across different types of buildings, roads,\ntrees, and water bodies within these distinct areas. We propose a modified\nCross Pseudo Supervision framework to train image segmentation models on\nsparsely labelled data. The proposed framework addresses the limitations of the\npopular \"Cross Pseudo Supervision\" technique for semi-supervised learning.\nSpecifically, it tackles the challenges of training segmentation models on\nnoisy satellite image data with sparse and inaccurate labels. This\ncomprehensive approach enhances the accuracy and utility of LULC mapping for\nvarious urban planning applications.\n","authors":["Yash Dixit","Naman Srivastava","Joel D Joy","Rohan Olikara","Swarup E","Rakshit Ramesh"],"pdf_url":"https://arxiv.org/pdf/2408.02382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02369v1","updated":"2024-08-05T10:38:50Z","published":"2024-08-05T10:38:50Z","title":"The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC\n  2024","summary":"  This paper delineates the visual speech recognition (VSR) system introduced\nby the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC 2024), engaging in all four tracks, including the\nfixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In\nterms of data processing, we leverage the lip motion extractor from the\nbaseline1 to produce multiscale video data. Besides, various augmentation\ntechniques are applied during training, encompassing speed perturbation, random\nrotation, horizontal flipping, and color transformation. The VSR model adopts\nan end-to-end architecture with joint CTC/attention loss, introducing Enhanced\nResNet3D visual frontend, E-Branchformer encoder, and Bi-directional\nTransformer decoder. Our approach yields a 30.47% CER for the Single-Speaker\nTask and 34.30% CER for the Multi-Speaker Task, securing second place in the\nopen track of the Single-Speaker Task and first place in the other three\ntracks.\n","authors":["He Wang","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2408.02369v1.pdf","comment":"2 pages, 2 figures, CNVSRC 2024 System Report"},{"id":"http://arxiv.org/abs/2408.02367v1","updated":"2024-08-05T10:32:06Z","published":"2024-08-05T10:32:06Z","title":"StoDIP: Efficient 3D MRF image reconstruction with deep image priors and\n  stochastic iterations","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI for multiparametric tissue mapping. The reconstruction of\nquantitative maps requires tailored algorithms for removing aliasing artefacts\nfrom the compressed sampled MRF acquisitions. Within approaches found in the\nliterature, many focus solely on two-dimensional (2D) image reconstruction,\nneglecting the extension to volumetric (3D) scans despite their higher\nrelevance and clinical value. A reason for this is that transitioning to 3D\nimaging without appropriate mitigations presents significant challenges,\nincluding increased computational cost and storage requirements, and the need\nfor large amount of ground-truth (artefact-free) data for training. To address\nthese issues, we introduce StoDIP, a new algorithm that extends the\nground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging.\nStoDIP employs memory-efficient stochastic updates across the multicoil MRF\ndata, a carefully selected neural network architecture, as well as faster\nnonuniform FFT (NUFFT) transformations. This enables a faster convergence\ncompared against a conventional DIP implementation without these features.\nTested on a dataset of whole-brain scans from healthy volunteers, StoDIP\ndemonstrated superior performance over the ground-truth-free reconstruction\nbaselines, both quantitatively and qualitatively.\n","authors":["Perla Mayo","Matteo Cencini","Carolin M. Pirkl","Marion I. Menzel","Michela Tosetti","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2408.02367v1.pdf","comment":"10 pages, 2 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2407.08583v2","updated":"2024-08-05T10:31:24Z","published":"2024-07-11T15:08:11Z","title":"The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective","summary":"  The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs; on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stages of MLLMs specific\ndata-centric approaches can be employed to enhance certain MLLM capabilities,\nand 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal\ndata in specific roles. To promote the data-model co-development for MLLM\ncommunity, we systematically review existing works related to MLLMs from the\ndata-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.\n","authors":["Zhen Qin","Daoyuan Chen","Wenhao Zhang","Liuyi Yao","Yilun Huang","Bolin Ding","Yaliang Li","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.08583v2.pdf","comment":"Ongoing work. 21 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md"},{"id":"http://arxiv.org/abs/2405.10802v2","updated":"2024-08-05T10:20:11Z","published":"2024-05-17T14:16:40Z","title":"Reduced storage direct tensor ring decomposition for convolutional\n  neural networks compression","summary":"  Convolutional neural networks (CNNs) are among the most widely used machine\nlearning models for computer vision tasks, such as image classification. To\nimprove the efficiency of CNNs, many CNNs compressing approaches have been\ndeveloped. Low-rank methods approximate the original convolutional kernel with\na sequence of smaller convolutional kernels, which leads to reduced storage and\ntime complexities. In this study, we propose a novel low-rank CNNs compression\nmethod that is based on reduced storage direct tensor ring decomposition\n(RSDTR). The proposed method offers a higher circular mode permutation\nflexibility, and it is characterized by large parameter and FLOPS compression\nrates, while preserving a good classification accuracy of the compressed\nnetwork. The experiments, performed on the CIFAR-10 and ImageNet datasets,\nclearly demonstrate the efficiency of RSDTR in comparison to other\nstate-of-the-art CNNs compression approaches.\n","authors":["Mateusz Gabor","Rafa Zdunek"],"pdf_url":"https://arxiv.org/pdf/2405.10802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02348v1","updated":"2024-08-05T09:50:16Z","published":"2024-08-05T09:50:16Z","title":"Earth System Data Cubes: Avenues for advancing Earth system research","summary":"  Recent advancements in Earth system science have been marked by the\nexponential increase in the availability of diverse, multivariate datasets\ncharacterised by moderate to high spatio-temporal resolutions. Earth System\nData Cubes (ESDCs) have emerged as one suitable solution for transforming this\nflood of data into a simple yet robust data structure. ESDCs achieve this by\norganising data into an analysis-ready format aligned with a spatio-temporal\ngrid, facilitating user-friendly analysis and diminishing the need for\nextensive technical data processing knowledge. Despite these significant\nbenefits, the completion of the entire ESDC life cycle remains a challenging\ntask. Obstacles are not only of a technical nature but also relate to\ndomain-specific problems in Earth system research. There exist barriers to\nrealising the full potential of data collections in light of novel cloud-based\ntechnologies, particularly in curating data tailored for specific application\ndomains. These include transforming data to conform to a spatio-temporal grid\nwith minimum distortions and managing complexities such as spatio-temporal\nautocorrelation issues. Addressing these challenges is pivotal for the\neffective application of Artificial Intelligence (AI) approaches. Furthermore,\nadhering to open science principles for data dissemination, reproducibility,\nvisualisation, and reuse is crucial for fostering sustainable research.\nOvercoming these challenges offers a substantial opportunity to advance\ndata-driven Earth system research, unlocking the full potential of an\nintegrated, multidimensional view of Earth system processes. This is\nparticularly true when such research is coupled with innovative research\nparadigms and technological progress.\n","authors":["David Montero","Guido Kraemer","Anca Anghelea","Csar Aybar","Gunnar Brandt","Gustau Camps-Valls","Felix Cremer","Ida Flik","Fabian Gans","Sarah Habershon","Chaonan Ji","Teja Kattenborn","Laura Martnez-Ferrer","Francesco Martinuzzi","Martin Reinhardt","Maximilian Schting","Khalil Teber","Miguel D. Mahecha"],"pdf_url":"https://arxiv.org/pdf/2408.02348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16248v3","updated":"2024-08-05T09:05:59Z","published":"2024-07-23T07:36:54Z","title":"Spatiotemporal Graph Guided Multi-modal Network for Livestreaming\n  Product Retrieval","summary":"  With the rapid expansion of e-commerce, more consumers have become accustomed\nto making purchases via livestreaming. Accurately identifying the products\nbeing sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a\nfundamental and daunting challenge. The LPR task encompasses three primary\ndilemmas in real-world scenarios: 1) the recognition of intended products from\ndistractor products present in the background; 2) the video-image heterogeneity\nthat the appearance of products showcased in live streams often deviates\nsubstantially from standardized product images in stores; 3) there are numerous\nconfusing products with subtle visual nuances in the shop. To tackle these\nchallenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).\nFirst, we employ a text-guided attention mechanism that leverages the spoken\ncontent of salespeople to guide the model to focus toward intended products,\nemphasizing their salience over cluttered background products. Second, a\nlong-range spatiotemporal graph network is further designed to achieve both\ninstance-level interaction and frame-level matching, solving the misalignment\ncaused by video-image heterogeneity. Third, we propose a multi-modal hard\nexample mining, assisting the model in distinguishing highly similar products\nwith fine-grained features across the video-image-text domain. Through\nextensive quantitative and qualitative experiments, we demonstrate the superior\nperformance of our proposed SGMN model, surpassing the state-of-the-art methods\nby a substantial margin. The code is available at\nhttps://github.com/Huxiaowan/SGMN.\n","authors":["Xiaowan Hu","Yiyi Chen","Yan Li","Minquan Wang","Haoqian Wang","Quan Chen","Han Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16248v3.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.01090v2","updated":"2024-08-05T08:47:19Z","published":"2023-11-02T08:55:11Z","title":"Infusion: internal diffusion for inpainting of dynamic textures and\n  complex motion","summary":"  Video inpainting is the task of filling a region in a video in a visually\nconvincing manner. It is very challenging due to the high dimensionality of the\ndata and the temporal consistency required for obtaining convincing results.\nRecently, diffusion models have shown impressive results in modeling complex\ndata distributions, including images and videos. Such models remain nonetheless\nvery expensive to train and to perform inference with, which strongly reduce\ntheir applicability to videos, and yields unreasonable computational loads. We\nshow that in the case of video inpainting, thanks to the highly auto-similar\nnature of videos, the training data of a diffusion model can be restricted to\nthe input video and still produce very satisfying results. This leads us to\nadopt an internal learning approach, which also allows us to greatly reduce the\nneural network size by about three orders of magnitude less than current\ndiffusion models used for image inpainting. We also introduce a new method for\nefficient training and inference of diffusion models in the context of internal\nlearning, by splitting the diffusion process into different learning intervals\ncorresponding to different noise levels of the diffusion process. To the best\nof our knowledge, this is the first video inpainting method based purely on\ndiffusion. Other methods require additional components such as optical flow\nestimation, which limits their performance in the case of dynamic textures and\ncomplex motions. We show qualitative and quantitative results, demonstrating\nthat our method reaches state of the art performance in the case of dynamic\ntextures and complex dynamic backgrounds.\n","authors":["Nicolas Cherel","Andrs Almansa","Yann Gousseau","Alasdair Newson"],"pdf_url":"https://arxiv.org/pdf/2311.01090v2.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.02307v1","updated":"2024-08-05T08:36:13Z","published":"2024-08-05T08:36:13Z","title":"Low-Cost Self-Ensembles Based on Multi-Branch Transformation and Grouped\n  Convolution","summary":"  Recent advancements in low-cost ensemble learning have demonstrated improved\nefficiency for image classification. However, the existing low-cost ensemble\nmethods show relatively lower accuracy compared to conventional ensemble\nlearning. In this paper, we propose a new low-cost ensemble learning, which can\nsimultaneously achieve high efficiency and classification performance. A CNN is\ntransformed into a multi-branch structure without introduction of additional\ncomponents, which maintains the computational complexity as that of the\noriginal single model and also enhances diversity among the branches' outputs\nvia sufficient separation between different pathways of the branches. In\naddition, we propose a new strategy that applies grouped convolution in the\nbranches with different numbers of groups in different branches, which boosts\nthe diversity of the branches' outputs. For training, we employ knowledge\ndistillation using the ensemble of the outputs as the teacher signal. The high\ndiversity among the outputs enables to form a powerful teacher, enhancing the\nindividual branch's classification performance and consequently the overall\nensemble performance. Experimental results show that our method achieves\nstate-of-the-art classification accuracy and higher uncertainty estimation\nperformance compared to previous low-cost ensemble methods. The code is\navailable at https://github.com/hjdw2/SEMBG.\n","authors":["Hojung Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02306v1","updated":"2024-08-05T08:35:59Z","published":"2024-08-05T08:35:59Z","title":"Mixture-of-Noises Enhanced Forgery-Aware Predictor for Multi-Face\n  Manipulation Detection and Localization","summary":"  With the advancement of face manipulation technology, forgery images in\nmulti-face scenarios are gradually becoming a more complex and realistic\nchallenge. Despite this, detection and localization methods for such multi-face\nmanipulations remain underdeveloped. Traditional manipulation localization\nmethods either indirectly derive detection results from localization masks,\nresulting in limited detection performance, or employ a naive two-branch\nstructure to simultaneously obtain detection and localization results, which\ncannot effectively benefit the localization capability due to limited\ninteraction between two tasks. This paper proposes a new framework, namely\nMoNFAP, specifically tailored for multi-face manipulation detection and\nlocalization. The MoNFAP primarily introduces two novel modules: the\nForgery-aware Unified Predictor (FUP) Module and the Mixture-of-Noises Module\n(MNM). The FUP integrates detection and localization tasks using a token\nlearning strategy and multiple forgery-aware transformers, which facilitates\nthe use of classification information to enhance localization capability.\nBesides, motivated by the crucial role of noise information in forgery\ndetection, the MNM leverages multiple noise extractors based on the concept of\nthe mixture of experts to enhance the general RGB features, further boosting\nthe performance of our framework. Finally, we establish a comprehensive\nbenchmark for multi-face detection and localization and the proposed\n\\textit{MoNFAP} achieves significant performance. The codes will be made\navailable.\n","authors":["Changtao Miao","Qi Chu","Tao Gong","Zhentao Tan","Zhenchao Jin","Wanyi Zhuang","Man Luo","Honggang Hu","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15362v2","updated":"2024-08-05T08:26:24Z","published":"2024-07-22T04:09:27Z","title":"A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model","summary":"  Remarkable strides in computational pathology have been made in the\ntask-agnostic foundation model that advances the performance of a wide array of\ndownstream clinical tasks. Despite the promising performance, there are still\nseveral challenges. First, prior works have resorted to either vision-only or\nvision-captions data, disregarding invaluable pathology reports and gene\nexpression profiles which respectively offer distinct knowledge for versatile\nclinical applications. Second, the current progress in pathology FMs\npredominantly concentrates on the patch level, where the restricted context of\npatch-level pretraining fails to capture whole-slide patterns. Here we curated\nthe largest multimodal dataset consisting of H\\&E diagnostic whole slide images\nand their associated pathology reports and RNA-Seq data, resulting in 26,169\nslide-level modality pairs from 10,275 patients across 32 cancer types. To\nleverage these data for CPath, we propose a novel whole-slide pretraining\nparadigm which injects multimodal knowledge at the whole-slide context into the\npathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed\nparadigm revolutionizes the workflow of pretraining for CPath, which enables\nthe pathology FM to acquire the whole-slide context. To our knowledge, this is\nthe first attempt to incorporate multimodal knowledge at the slide level for\nenhancing pathology FMs, expanding the modelling context from unimodal to\nmultimodal knowledge and from patch-level to slide-level. To systematically\nevaluate the capabilities of mSTAR, extensive experiments including slide-level\nunimodal and multimodal applications, are conducted across 7 diverse types of\ntasks on 43 subtasks, resulting in the largest spectrum of downstream tasks.\nThe average performance in various slide-level applications consistently\ndemonstrates significant performance enhancements for mSTAR compared to SOTA\nFMs.\n","authors":["Yingxue Xu","Yihui Wang","Fengtao Zhou","Jiabo Ma","Shu Yang","Huangjing Lin","Xin Wang","Jiguang Wang","Li Liang","Anjia Han","Ronald Cheong Kin Chan","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15362v2.pdf","comment":"45 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02301v1","updated":"2024-08-05T08:23:59Z","published":"2024-08-05T08:23:59Z","title":"Network Fission Ensembles for Low-Cost Self-Ensembles","summary":"  Recent ensemble learning methods for image classification have been shown to\nimprove classification accuracy with low extra cost. However, they still\nrequire multiple trained models for ensemble inference, which eventually\nbecomes a significant burden when the model size increases. In this paper, we\npropose a low-cost ensemble learning and inference, called Network Fission\nEnsembles (NFE), by converting a conventional network itself into a multi-exit\nstructure. Starting from a given initial network, we first prune some of the\nweights to reduce the training burden. We then group the remaining weights into\nseveral sets and create multiple auxiliary paths using each set to construct\nmulti-exits. We call this process Network Fission. Through this, multiple\noutputs can be obtained from a single network, which enables ensemble learning.\nSince this process simply changes the existing network structure to multi-exits\nwithout using additional networks, there is no extra computational burden for\nensemble learning and inference. Moreover, by learning from multiple losses of\nall exits, the multi-exits improve performance via regularization, and high\nperformance can be achieved even with increased network sparsity. With our\nsimple yet effective method, we achieve significant improvement compared to\nexisting ensemble methods. The code is available at\nhttps://github.com/hjdw2/NFE.\n","authors":["Hojung Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02297v1","updated":"2024-08-05T08:14:28Z","published":"2024-08-05T08:14:28Z","title":"Perception Matters: Enhancing Embodied AI with Uncertainty-Aware\n  Semantic Segmentation","summary":"  Embodied AI has made significant progress acting in unexplored environments.\nHowever, tasks such as object search have largely focused on efficient policy\nlearning. In this work, we identify several gaps in current search methods:\nThey largely focus on dated perception models, neglect temporal aggregation,\nand transfer from ground truth directly to noisy perception at test time,\nwithout accounting for the resulting overconfidence in the perceived state. We\naddress the identified problems through calibrated perception probabilities and\nuncertainty across aggregation and found decisions, thereby adapting the models\nfor sequential tasks. The resulting methods can be directly integrated with\npretrained models across a wide family of existing search approaches at no\nadditional training cost. We perform extensive evaluations of aggregation\nmethods across both different semantic perception models and policies,\nconfirming the importance of calibrated uncertainties in both the aggregation\nand found decisions. We make the code and trained models available at\nhttp://semantic-search.cs.uni-freiburg.de.\n","authors":["Sai Prasanna","Daniel Honerkamp","Kshitij Sirohi","Tim Welschehold","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2408.02297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02291v1","updated":"2024-08-05T08:00:30Z","published":"2024-08-05T08:00:30Z","title":"SelfGeo: Self-supervised and Geodesic-consistent Estimation of Keypoints\n  on Deformable Shapes","summary":"  Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex\ntask, even more challenging when an object shape is deforming. As keypoints\nshould be semantically and geometrically consistent across all the 3D frames -\neach keypoint should be anchored to a specific part of the deforming shape\nirrespective of intrinsic and extrinsic motion. This paper presents, \"SelfGeo\",\na self-supervised method that computes persistent 3D keypoints of non-rigid\nobjects from arbitrary PCDs without the need of human annotations. The gist of\nSelfGeo is to estimate keypoints between frames that respect invariant\nproperties of deforming bodies. Our main contribution is to enforce that\nkeypoints deform along with the shape while keeping constant geodesic distances\namong them. This principle is then propagated to the design of a set of losses\nwhich minimization let emerge repeatable keypoints in specific semantic\nlocations of the non-rigid shape. We show experimentally that the use of\ngeodesic has a clear advantage in challenging dynamic scenes and with different\nclasses of deforming shapes (humans and animals). Code and data are available\nat: https://github.com/IIT-PAVIS/SelfGeo\n","authors":["Mohammad Zohaib","Luca Cosmo","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2408.02291v1.pdf","comment":"This paper has been accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07720v4","updated":"2024-08-05T07:56:29Z","published":"2024-07-10T14:53:37Z","title":"Exploiting Scale-Variant Attention for Segmenting Small Medical Objects","summary":"  Early detection and accurate diagnosis can predict the risk of malignant\ndisease transformation, thereby increasing the probability of effective\ntreatment. Identifying mild syndrome with small pathological regions serves as\nan ominous warning and is fundamental in the early diagnosis of diseases. While\ndeep learning algorithms, particularly convolutional neural networks (CNNs),\nhave shown promise in segmenting medical objects, analyzing small areas in\nmedical images remains challenging. This difficulty arises due to information\nlosses and compression defects from convolution and pooling operations in CNNs,\nwhich become more pronounced as the network deepens, especially for small\nmedical objects. To address these challenges, we propose a novel scale-variant\nattention-based network (SvANet) for accurately segmenting small-scale objects\nin medical images. The SvANet consists of scale-variant attention, cross-scale\nguidance, Monte Carlo attention, and vision transformer, which incorporates\ncross-scale features and alleviates compression artifacts for enhancing the\ndiscrimination of small medical objects. Quantitative experimental results\ndemonstrate the superior performance of SvANet, achieving 96.12%, 96.11%,\n89.79%, 84.15%, 80.25%, 73.05%, and 72.58% in mean Dice coefficient for\nsegmenting kidney tumors, skin lesions, hepatic tumors, polyps, surgical\nexcision cells, retinal vasculatures, and sperms, which occupy less than 1% of\nthe image areas in KiTS23, ISIC 2018, ATLAS, PolypGen, TissueNet, FIVES, and\nSpermHealth datasets, respectively.\n","authors":["Wei Dai","Rui Liu","Zixuan Wu","Tianyi Wu","Min Wang","Junxian Zhou","Yixuan Yuan","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07720v4.pdf","comment":"14 pages, 9 figures, under review"},{"id":"http://arxiv.org/abs/2408.02285v1","updated":"2024-08-05T07:37:55Z","published":"2024-08-05T07:37:55Z","title":"Joint-Motion Mutual Learning for Pose Estimation in Videos","summary":"  Human pose estimation in videos has long been a compelling yet challenging\ntask within the realm of computer vision. Nevertheless, this task remains\ndifficult because of the complex video scenes, such as video defocus and\nself-occlusion. Recent methods strive to integrate multi-frame visual features\ngenerated by a backbone network for pose estimation. However, they often ignore\nthe useful joint information encoded in the initial heatmap, which is a\nby-product of the backbone generation. Comparatively, methods that attempt to\nrefine the initial heatmap fail to consider any spatio-temporal motion\nfeatures. As a result, the performance of existing methods for pose estimation\nfalls short due to the lack of ability to leverage both local joint (heatmap)\ninformation and global motion (feature) dynamics.\n  To address this problem, we propose a novel joint-motion mutual learning\nframework for pose estimation, which effectively concentrates on both local\njoint dependency and global pixel-level motion dynamics. Specifically, we\nintroduce a context-aware joint learner that adaptively leverages initial\nheatmaps and motion flow to retrieve robust local joint feature. Given that\nlocal joint feature and global motion flow are complementary, we further\npropose a progressive joint-motion mutual learning that synergistically\nexchanges information and interactively learns between joint feature and motion\nflow to improve the capability of the model. More importantly, to capture more\ndiverse joint and motion cues, we theoretically analyze and propose an\ninformation orthogonality objective to avoid learning redundant information\nfrom multi-cues. Empirical experiments show our method outperforms prior arts\non three challenging benchmarks.\n","authors":["Sifan Wu","Haipeng Chen","Yifang Yin","Sihao Hu","Runyang Feng","Yingying Jiao","Ziqi Yang","Zhenguang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02285v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.18534v2","updated":"2024-08-05T07:37:06Z","published":"2024-07-26T06:29:09Z","title":"Boosting Cross-Domain Point Classification via Distilling Relational\n  Priors from 2D Transformers","summary":"  Semantic pattern of an object point cloud is determined by its topological\nconfiguration of local geometries. Learning discriminative representations can\nbe challenging due to large shape variations of point sets in local regions and\nincomplete surface in a global perspective, which can be made even more severe\nin the context of unsupervised domain adaptation (UDA). In specific,\ntraditional 3D networks mainly focus on local geometric details and ignore the\ntopological structure between local geometries, which greatly limits their\ncross-domain generalization. Recently, the transformer-based models have\nachieved impressive performance gain in a range of image-based tasks,\nbenefiting from its strong generalization capability and scalability stemming\nfrom capturing long range correlation across local patches. Inspired by such\nsuccesses of visual transformers, we propose a novel Relational Priors\nDistillation (RPD) method to extract relational priors from the well-trained\ntransformers on massive images, which can significantly empower cross-domain\nrepresentations with consistent topological priors of objects. To this end, we\nestablish a parameter-frozen pre-trained transformer module shared between 2D\nteacher and 3D student models, complemented by an online knowledge distillation\nstrategy for semantically regularizing the 3D student model. Furthermore, we\nintroduce a novel self-supervised task centered on reconstructing masked point\ncloud patches using corresponding masked multi-view image features, thereby\nempowering the model with incorporating 3D geometric information. Experiments\non the PointDA-10 and the Sim-to-Real datasets verify that the proposed method\nconsistently achieves the state-of-the-art performance of UDA for point cloud\nclassification. The source code of this work is available at\nhttps://github.com/zou-longkun/RPD.git.\n","authors":["Longkun Zou","Wanru Zhu","Ke Chen","Lihua Guo","Kailing Guo","Kui Jia","Yaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2407.18534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02284v1","updated":"2024-08-05T07:34:44Z","published":"2024-08-05T07:34:44Z","title":"Cascading Refinement Video Denoising with Uncertainty Adaptivity","summary":"  Accurate alignment is crucial for video denoising. However, estimating\nalignment in noisy environments is challenging. This paper introduces a\ncascading refinement video denoising method that can refine alignment and\nrestore images simultaneously. Better alignment enables restoration of more\ndetailed information in each frame. Furthermore, better image quality leads to\nbetter alignment. This method has achieved SOTA performance by a large margin\non the CRVD dataset. Simultaneously, aiming to deal with multi-level noise, an\nuncertainty map was created after each iteration. Because of this, redundant\ncomputation on the easily restored videos was avoided. By applying this method,\nthe entire computation was reduced by 25% on average.\n","authors":["Xinyuan Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02275v1","updated":"2024-08-05T07:10:40Z","published":"2024-08-05T07:10:40Z","title":"Geometric Algebra Meets Large Language Models: Instruction-Based\n  Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes","summary":"  This paper introduces a novel integration of Large Language Models (LLMs)\nwith Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene\nediting, particularly for object repositioning tasks, which traditionally\nrequires intricate manual processes and specialized expertise. These\nconventional methods typically suffer from reliance on large training datasets\nor lack a formalized language for precise edits. Utilizing CGA as a robust\nformal language, our system, shenlong, precisely models spatial transformations\nnecessary for accurate object repositioning. Leveraging the zero-shot learning\ncapabilities of pre-trained LLMs, shenlong translates natural language\ninstructions into CGA operations which are then applied to the scene,\nfacilitating exact spatial transformations within 3D scenes without the need\nfor specialized pre-training. Implemented in a realistic simulation\nenvironment, shenlong ensures compatibility with existing graphics pipelines.\nTo accurately assess the impact of CGA, we benchmark against robust Euclidean\nSpace baselines, evaluating both latency and accuracy. Comparative performance\nevaluations indicate that shenlong significantly reduces LLM response times by\n16% and boosts success rates by 9.6% on average compared to the traditional\nmethods. Notably, shenlong achieves a 100% perfect success rate in common\npractical queries, a benchmark where other systems fall short. These\nadvancements underscore shenlong's potential to democratize 3D scene editing,\nenhancing accessibility and fostering innovation across sectors such as\neducation, digital entertainment, and virtual reality.\n","authors":["Dimitris Angelis","Prodromos Kolyvakis","Manos Kamarianakis","George Papagiannakis"],"pdf_url":"https://arxiv.org/pdf/2408.02275v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02272v1","updated":"2024-08-05T07:00:10Z","published":"2024-08-05T07:00:10Z","title":"COM Kitchens: An Unedited Overhead-view Video Dataset as a\n  Vision-Language Benchmark","summary":"  Procedural video understanding is gaining attention in the vision and\nlanguage community. Deep learning-based video analysis requires extensive data.\nConsequently, existing works often use web videos as training resources, making\nit challenging to query instructional contents from raw video observations. To\naddress this issue, we propose a new dataset, COM Kitchens. The dataset\nconsists of unedited overhead-view videos captured by smartphones, in which\nparticipants performed food preparation based on given recipes. Fixed-viewpoint\nvideo datasets often lack environmental diversity due to high camera setup\ncosts. We used modern wide-angle smartphone lenses to cover cooking counters\nfrom sink to cooktop in an overhead view, capturing activity without in-person\nassistance. With this setup, we collected a diverse dataset by distributing\nsmartphones to participants. With this dataset, we propose the novel\nvideo-to-text retrieval task Online Recipe Retrieval (OnRR) and new video\ncaptioning domain Dense Video Captioning on unedited Overhead-View videos\n(DVC-OV). Our experiments verified the capabilities and limitations of current\nweb-video-based SOTA methods in handling these tasks.\n","authors":["Koki Maeda","Tosho Hirasawa","Atsushi Hashimoto","Jun Harashima","Leszek Rybicki","Yusuke Fukasawa","Yoshitaka Ushiku"],"pdf_url":"https://arxiv.org/pdf/2408.02272v1.pdf","comment":"ECCV2024 accepted"},{"id":"http://arxiv.org/abs/2407.18289v2","updated":"2024-08-05T06:53:43Z","published":"2024-07-25T15:18:28Z","title":"MARINE: A Computer Vision Model for Detecting Rare Predator-Prey\n  Interactions in Animal Videos","summary":"  Encounters between predator and prey play an essential role in ecosystems,\nbut their rarity makes them difficult to detect in video recordings. Although\nadvances in action recognition (AR) and temporal action detection (AD),\nespecially transformer-based models and vision foundation models, have achieved\nhigh performance on human action datasets, animal videos remain relatively\nunder-researched. This thesis addresses this gap by proposing the model MARINE,\nwhich utilizes motion-based frame selection designed for fast animal actions\nand DINOv2 feature extraction with a trainable classification head for action\nrecognition. MARINE outperforms VideoMAE in identifying predator attacks in\nvideos of fish, both on a small and specific coral reef dataset (81.53\\%\nagainst 52.64\\% accuracy), and on a subset of the more extensive Animal Kingdom\ndataset (94.86\\% against 83.14\\% accuracy). In a multi-label setting on a\nrepresentative sample of Animal Kingdom, MARINE achieves 23.79\\% mAP,\npositioning it mid-field among existing benchmarks. Furthermore, in an AD task\non the coral reef dataset, MARINE achieves 80.78\\% AP (against VideoMAE's\n34.89\\%) although at a lowered t-IoU threshold of 25\\%. Therefore, despite room\nfor improvement, MARINE offers an effective starter framework to apply to AR\nand AD tasks on animal recordings and thus contribute to the study of natural\necosystems.\n","authors":["Zsfia Katona","Seyed Sahand Mohammadi Ziabari","Fatemeh Karimi Nejadasl"],"pdf_url":"https://arxiv.org/pdf/2407.18289v2.pdf","comment":"This is an MSc thesis by Zsofia Katona, supervised by the two other\n  authors"},{"id":"http://arxiv.org/abs/2407.18288v2","updated":"2024-08-05T06:50:44Z","published":"2024-07-25T14:21:35Z","title":"Leveraging Foundation Models via Knowledge Distillation in Multi-Object\n  Tracking: Distilling DINOv2 Features to FairMOT","summary":"  Multiple Object Tracking (MOT) is a computer vision task that has been\nemployed in a variety of sectors. Some common limitations in MOT are varying\nobject appearances, occlusions, or crowded scenes. To address these challenges,\nmachine learning methods have been extensively deployed, leveraging large\ndatasets, sophisticated models, and substantial computational resources. Due to\npractical limitations, access to the above is not always an option. However,\nwith the recent release of foundation models by prominent AI companies,\npretrained models have been trained on vast datasets and resources using\nstate-of-the-art methods. This work tries to leverage one such foundation\nmodel, called DINOv2, through using knowledge distillation. The proposed method\nuses a teacher-student architecture, where DINOv2 is the teacher and the\nFairMOT backbone HRNetv2 W18 is the student. The results imply that although\nthe proposed method shows improvements in certain scenarios, it does not\nconsistently outperform the original FairMOT model. These findings highlight\nthe potential and limitations of applying foundation models in knowledge\n","authors":["Niels G. Faber","Seyed Sahand Mohammadi Ziabari","Fatemeh Karimi Nejadasl"],"pdf_url":"https://arxiv.org/pdf/2407.18288v2.pdf","comment":"This is an MSc thesis by Niels Faber, supervised by the two other\n  authors"},{"id":"http://arxiv.org/abs/2408.02265v1","updated":"2024-08-05T06:42:00Z","published":"2024-08-05T06:42:00Z","title":"Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary\n  Concepts","summary":"  The concept bottleneck model (CBM) is an interpretable-by-design framework\nthat makes decisions by first predicting a set of interpretable concepts, and\nthen predicting the class label based on the given concepts. Existing CBMs are\ntrained with a fixed set of concepts (concepts are either annotated by the\ndataset or queried from language models). However, this closed-world assumption\nis unrealistic in practice, as users may wonder about the role of any desired\nconcept in decision-making after the model is deployed. Inspired by the large\nsuccess of recent vision-language pre-trained models such as CLIP in zero-shot\nclassification, we propose \"OpenCBM\" to equip the CBM with open vocabulary\nconcepts via: (1) Aligning the feature space of a trainable image feature\nextractor with that of a CLIP's image encoder via a prototype based feature\nalignment; (2) Simultaneously training an image classifier on the downstream\ndataset; (3) Reconstructing the trained classification head via any set of\nuser-desired textual concepts encoded by CLIP's text encoder. To reveal\npotentially missing concepts from users, we further propose to iteratively find\nthe closest concept embedding to the residual parameters during the\nreconstruction until the residual is small enough. To the best of our\nknowledge, our \"OpenCBM\" is the first CBM with concepts of open vocabularies,\nproviding users the unique benefit such as removing, adding, or replacing any\ndesired concept to explain the model's prediction even after a model is\ntrained. Moreover, our model significantly outperforms the previous\nstate-of-the-art CBM by 9% in the classification accuracy on the benchmark\ndataset CUB-200-2011.\n","authors":["Andong Tan","Fengtao Zhou","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02265v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2305.15213v3","updated":"2024-08-05T06:40:52Z","published":"2023-05-24T14:51:18Z","title":"GTNet: Graph Transformer Network for 3D Point Cloud Classification and\n  Semantic Segmentation","summary":"  Recently, graph-based and Transformer-based deep learning networks have\ndemonstrated excellent performances on various point cloud tasks. Most of the\nexisting graph methods are based on static graph, which take a fixed input to\nestablish graph relations. Moreover, many graph methods apply maximization and\naveraging to aggregate neighboring features, so that only a single neighboring\npoint affects the feature of centroid or different neighboring points have the\nsame influence on the centroid's feature, which ignoring the correlation and\ndifference between points. Most Transformer-based methods extract point cloud\nfeatures based on global attention and lack the feature learning on local\nneighbors. To solve the problems of these two types of models, we propose a new\nfeature extraction block named Graph Transformer and construct a 3D point point\ncloud learning network called GTNet to learn features of point clouds on local\nand global patterns. Graph Transformer integrates the advantages of graph-based\nand Transformer-based methods, and consists of Local Transformer and Global\nTransformer modules. Local Transformer uses a dynamic graph to calculate all\nneighboring point weights by intra-domain cross-attention with dynamically\nupdated graph relations, so that every neighboring point could affect the\nfeatures of centroid with different weights; Global Transformer enlarges the\nreceptive field of Local Transformer by a global self-attention. In addition,\nto avoid the disappearance of the gradient caused by the increasing depth of\nnetwork, we conduct residual connection for centroid features in GTNet; we also\nadopt the features of centroid and neighbors to generate the local geometric\ndescriptors in Local Transformer to strengthen the local information learning\ncapability of the model. Finally, we use GTNet for shape classification, part\nsegmentation and semantic segmentation tasks in this paper.\n","authors":["Wei Zhou","Qian Wang","Weiwei Jin","Xinzhe Shi","Ying He"],"pdf_url":"https://arxiv.org/pdf/2305.15213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02263v1","updated":"2024-08-05T06:38:43Z","published":"2024-08-05T06:38:43Z","title":"VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object\n  Tracking","summary":"  Current LiDAR point cloud-based 3D single object tracking (SOT) methods\ntypically rely on point-based representation network. Despite demonstrated\nsuccess, such networks suffer from some fundamental problems: 1) It contains\npooling operation to cope with inherently disordered point clouds, hindering\nthe capture of 3D spatial information that is useful for tracking, a regression\ntask. 2) The adopted set abstraction operation hardly handles\ndensity-inconsistent point clouds, also preventing 3D spatial information from\nbeing modeled. To solve these problems, we introduce a novel tracking\nframework, termed VoxelTrack. By voxelizing inherently disordered point clouds\ninto 3D voxels and extracting their features via sparse convolution blocks,\nVoxelTrack effectively models precise and robust 3D spatial information,\nthereby guiding accurate position prediction for tracked objects. Moreover,\nVoxelTrack incorporates a dual-stream encoder with cross-iterative feature\nfusion module to further explore fine-grained 3D spatial information for\ntracking. Benefiting from accurate 3D spatial information being modeled, our\nVoxelTrack simplifies tracking pipeline with a single regression loss.\nExtensive experiments are conducted on three widely-adopted datasets including\nKITTI, NuScenes and Waymo Open Dataset. The experimental results confirm that\nVoxelTrack achieves state-of-the-art performance (88.3%, 71.4% and 63.6% mean\nprecision on the three datasets, respectively), and outperforms the existing\ntrackers with a real-time speed of 36 Fps on a single TITAN RTX GPU. The source\ncode and model will be released.\n","authors":["Yuxuan Lu","Jiahao Nie","Zhiwei He","Hongjie Gu","Xudong Lv"],"pdf_url":"https://arxiv.org/pdf/2408.02263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08651v4","updated":"2024-08-05T06:37:48Z","published":"2024-03-13T16:06:07Z","title":"HAIFIT: Fashion Image Translation for Human-to-AI Style Learning and\n  Generation","summary":"  In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications. In\naddition, our method also has obvious advantages in model training and\ninference speed, contributing to reducing designers' time costs and improving\ndesign efficiency.\n","authors":["Jianan Jiang","Xinglin Li","Weiren Yu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08651v4.pdf","comment":"10 pages,8 figures"},{"id":"http://arxiv.org/abs/2408.02261v1","updated":"2024-08-05T06:32:20Z","published":"2024-08-05T06:32:20Z","title":"Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs","summary":"  The challenge of semantic segmentation in Unsupervised Domain Adaptation\n(UDA) emerges not only from domain shifts between source and target images but\nalso from discrepancies in class taxonomies across domains. Traditional UDA\nresearch assumes consistent taxonomy between the source and target domains,\nthereby limiting their ability to recognize and adapt to the taxonomy of the\ntarget domain. This paper introduces a novel approach, Cross-Domain Semantic\nSegmentation on Inconsistent Taxonomy using Vision Language Models (CSI), which\neffectively performs domain-adaptive semantic segmentation even in situations\nof source-target class mismatches. CSI leverages the semantic generalization\npotential of Visual Language Models (VLMs) to create synergy with previous UDA\nmethods. It leverages segment reasoning obtained through traditional UDA\nmethods, combined with the rich semantic knowledge embedded in VLMs, to relabel\nnew classes in the target domain. This approach allows for effective adaptation\nto extended taxonomies without requiring any ground truth label for the target\ndomain. Our method has shown to be effective across various benchmarks in\nsituations of inconsistent taxonomy settings (coarse-to-fine taxonomy and open\ntaxonomy) and demonstrates consistent synergy effects when integrated with\nprevious state-of-the-art UDA methods. The implementation is available at\nhttp://github.com/jkee58/CSI.\n","authors":["Jeongkee Lim","Yusung Kim"],"pdf_url":"https://arxiv.org/pdf/2408.02261v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2312.16477v3","updated":"2024-08-05T05:51:21Z","published":"2023-12-27T08:52:41Z","title":"Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding","summary":"  In recent years, the results of view-based 3D shape recognition methods have\nsaturated, and models with excellent performance cannot be deployed on\nmemory-limited devices due to their huge size of parameters. To address this\nproblem, we introduce a compression method based on knowledge distillation for\nthis field, which largely reduces the number of parameters while preserving\nmodel performance as much as possible. Specifically, to enhance the\ncapabilities of smaller models, we design a high-performing large model called\nGroup Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first\nestablishes relationships between view-level features. Additionally, to capture\ndeeper features, we employ the grouping module to enhance view-level features\ninto group-level features. Finally, the group-level ViT aggregates group-level\nfeatures into complete, well-formed 3D shape descriptors. Notably, in both\nViTs, we introduce spatial encoding of camera coordinates as innovative\nposition embeddings. Furthermore, we propose two compressed versions based on\nGMViT, namely GMViT-simple and GMViT-mini. To enhance the training\neffectiveness of the small models, we introduce a knowledge distillation method\nthroughout the GMViT process, where the key outputs of each GMViT component\nserve as distillation targets. Extensive experiments demonstrate the efficacy\nof the proposed method. The large model GMViT achieves excellent 3D\nclassification and retrieval results on the benchmark datasets ModelNet,\nShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,\nreduce the parameter size by 8 and 17.6 times, respectively, and improve shape\nrecognition speed by 1.5 times on average, while preserving at least 90% of the\nclassification and retrieval performance. The code is available at\nhttps://github.com/bigdata-graph/GMViT.\n","authors":["Lixiang Xu","Qingzhe Cui","Richang Hong","Wei Xu","Enhong Chen","Xin Yuan","Chenglong Li","Yuanyan Tang"],"pdf_url":"https://arxiv.org/pdf/2312.16477v3.pdf","comment":"13pages, 8 figuers"},{"id":"http://arxiv.org/abs/2408.02250v1","updated":"2024-08-05T05:48:45Z","published":"2024-08-05T05:48:45Z","title":"Hierarchical Clustering using Reversible Binary Cellular Automata for\n  High-Dimensional Data","summary":"  This work proposes a hierarchical clustering algorithm for high-dimensional\ndatasets using the cyclic space of reversible finite cellular automata. In\ncellular automaton (CA) based clustering, if two objects belong to the same\ncycle, they are closely related and considered as part of the same cluster.\nHowever, if a high-dimensional dataset is clustered using the cycles of one CA,\nclosely related objects may belong to different cycles. This paper identifies\nthe relationship between objects in two different cycles based on the median of\nall elements in each cycle so that they can be grouped in the next stage.\nFurther, to minimize the number of intermediate clusters which in turn reduces\nthe computational cost, a rule selection strategy is taken to find the best\nrules based on information propagation and cycle structure. After encoding the\ndataset using frequency-based encoding such that the consecutive data elements\nmaintain a minimum hamming distance in encoded form, our proposed clustering\nalgorithm iterates over three stages to finally cluster the data elements into\nthe desired number of clusters given by user. This algorithm can be applied to\nvarious fields, including healthcare, sports, chemical research, agriculture,\netc. When verified over standard benchmark datasets with various performance\nmetrics, our algorithm is at par with the existing algorithms with quadratic\ntime complexity.\n","authors":["Baby C. J.","Kamalika Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2408.02250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02245v1","updated":"2024-08-05T05:33:59Z","published":"2024-08-05T05:33:59Z","title":"Curriculum learning based pre-training using Multi-Modal Contrastive\n  Masked Autoencoders","summary":"  In this paper, we propose a new pre-training method for image understanding\ntasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method\nutilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques.\nRecent approaches either use masked autoencoding (e.g., MultiMAE) or\ncontrastive learning(e.g., Pri3D, or combine them in a single contrastive\nmasked autoencoder architecture such as CMAE and CAV-MAE. However, none of the\nsingle contrastive masked autoencoder is applicable to RGB-D datasets. To\nimprove the performance and efficacy of such methods, we propose a new\npre-training strategy based on CL. Specifically, in the first stage, we\npre-train the model using contrastive learning to learn cross-modal\nrepresentations. In the second stage, we initialize the modality-specific\nencoders using the weights from the first stage and then pre-train the model\nusing masked autoencoding and denoising/noise prediction used in diffusion\nmodels. Masked autoencoding focuses on reconstructing the missing patches in\nthe input modality using local spatial correlations, while denoising learns\nhigh frequency components of the input data. Our approach is scalable, robust\nand suitable for pre-training with limited RGB-D datasets. Extensive\nexperiments on multiple datasets such as ScanNet, NYUv2 and SUN RGB-D show the\nefficacy and superior performance of our approach. Specifically, we show an\nimprovement of +1.0% mIoU against Mask3D on ScanNet semantic segmentation. We\nfurther demonstrate the effectiveness of our approach in low-data regime by\nevaluating it for semantic segmentation task against the state-of-the-art\nmethods.\n","authors":["Muhammad Abdullah Jamal","Omid Mohareri"],"pdf_url":"https://arxiv.org/pdf/2408.02245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02244v1","updated":"2024-08-05T05:30:36Z","published":"2024-08-05T05:30:36Z","title":"Evaluating Vision-Language Models for Zero-Shot Detection,\n  Classification, and Association of Motorcycles, Passengers, and Helmets","summary":"  Motorcycle accidents pose significant risks, particularly when riders and\npassengers do not wear helmets. This study evaluates the efficacy of an\nadvanced vision-language foundation model, OWLv2, in detecting and classifying\nvarious helmet-wearing statuses of motorcycle occupants using video data. We\nextend the dataset provided by the CVPR AI City Challenge and employ a cascaded\nmodel approach for detection and classification tasks, integrating OWLv2 and\nCNN models. The results highlight the potential of zero-shot learning to\naddress challenges arising from incomplete and biased training datasets,\ndemonstrating the usage of such models in detecting motorcycles, helmet usage,\nand occupant positions under varied conditions. We have achieved an average\nprecision of 0.5324 for helmet detection and provided precision-recall curves\ndetailing the detection and classification performance. Despite limitations\nsuch as low-resolution data and poor visibility, our research shows promising\nadvancements in automated vehicle safety and traffic safety enforcement\nsystems.\n","authors":["Lucas Choi","Ross Greer"],"pdf_url":"https://arxiv.org/pdf/2408.02244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11070v2","updated":"2024-08-05T05:13:06Z","published":"2024-04-17T04:59:36Z","title":"Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based\n  sky-segmentation in urban canyon","summary":"  Accurate, continuous, and reliable positioning is a critical component of\nachieving autonomous driving. However, in complex urban canyon environments,\nthe vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused\nby high buildings, trees, and elevated structures seriously affect positioning\nresults. To address these challenges, a sky-view images segmentation algorithm\nbased on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection.\nBuilding upon this, a novel NLOS detection and mitigation algorithm (named\nS-NDM) is extended to the tightly coupled Global Navigation Satellite Systems\n(GNSS), Inertial Measurement Units (IMU), and visual feature system which is\ncalled Sky-GVIO, with the aim of achieving continuous and accurate positioning\nin urban canyon environments. Furthermore, the system harmonizes Single Point\nPositioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its\noperational versatility and resilience. In urban canyon environments, the\npositioning performance of S-NDM algorithm proposed in this paper is evaluated\nunder different tightly coupled SPP-related and RTK-related models. The results\nexhibit that Sky-GVIO system achieves meter-level accuracy under SPP mode and\nsub-decimeter precision with RTK, surpassing the performance of GNSS/INS/Vision\nframeworks devoid of S-NDM. Additionally, the sky-view image dataset, inclusive\nof training and evaluation subsets, has been made publicly accessible for\nscholarly exploration at https://github.com/whuwangjr/sky-view-images .\n","authors":["Jingrong Wang","Bo Xu","Ronghe Jin","Shoujian Zhang","Kefu Gao","Jingnan Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02231v1","updated":"2024-08-05T04:51:46Z","published":"2024-08-05T04:51:46Z","title":"REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language\n  Models","summary":"  Text-to-Image (T2I) and multimodal large language models (MLLMs) have been\nadopted in solutions for several computer vision and multimodal learning tasks.\nHowever, it has been found that such vision-language models lack the ability to\ncorrectly reason over spatial relationships. To tackle this shortcoming, we\ndevelop the REVISION framework which improves spatial fidelity in\nvision-language models. REVISION is a 3D rendering based pipeline that\ngenerates spatially accurate synthetic images, given a textual prompt. REVISION\nis an extendable framework, which currently supports 100+ 3D assets, 11 spatial\nrelationships, all with diverse camera perspectives and backgrounds. Leveraging\nimages from REVISION as additional guidance in a training-free manner\nconsistently improves the spatial consistency of T2I models across all spatial\nrelationships, achieving competitive performance on the VISOR and T2I-CompBench\nbenchmarks. We also design RevQA, a question-answering benchmark to evaluate\nthe spatial reasoning abilities of MLLMs, and find that state-of-the-art models\nare not robust to complex spatial reasoning under adversarial settings. Our\nresults and findings indicate that utilizing rendering-based frameworks is an\neffective approach for developing spatially-aware generative models.\n","authors":["Agneet Chatterjee","Yiran Luo","Tejas Gokhale","Yezhou Yang","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2408.02231v1.pdf","comment":"Accepted to ECCV 2024. Project Page :\n  https://agneetchatterjee.com/revision/"},{"id":"http://arxiv.org/abs/2408.02226v1","updated":"2024-08-05T04:10:52Z","published":"2024-08-05T04:10:52Z","title":"ProCreate, Dont Reproduce! Propulsive Energy Diffusion for Creative\n  Generation","summary":"  In this paper, we propose ProCreate, a simple and easy-to-implement method to\nimprove sample diversity and creativity of diffusion-based image generative\nmodels and to prevent training data reproduction. ProCreate operates on a set\nof reference images and actively propels the generated image embedding away\nfrom the reference embeddings during the generation process. We propose FSCG-8\n(Few-Shot Creative Generation 8), a few-shot creative generation dataset on\neight different categories -- encompassing different concepts, styles, and\nsettings -- in which ProCreate achieves the highest sample diversity and\nfidelity. Furthermore, we show that ProCreate is effective at preventing\nreplicating training data in a large-scale evaluation using training text\nprompts. Code and FSCG-8 are available at\nhttps://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The\nproject page is available at https://procreate-diffusion.github.io.\n","authors":["Jack Lu","Ryan Teehan","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2408.02226v1.pdf","comment":"Accepted for ECCV 2024. Project page:\n  https://procreate-diffusion.github.io"},{"id":"http://arxiv.org/abs/2408.02222v1","updated":"2024-08-05T03:54:40Z","published":"2024-08-05T03:54:40Z","title":"Cross-modulated Attention Transformer for RGBT Tracking","summary":"  Existing Transformer-based RGBT trackers achieve remarkable performance\nbenefits by leveraging self-attention to extract uni-modal features and\ncross-attention to enhance multi-modal feature interaction and template-search\ncorrelation computation. Nevertheless, the independent search-template\ncorrelation calculations ignore the consistency between branches, which can\nresult in ambiguous and inappropriate correlation weights. It not only limits\nthe intra-modal feature representation, but also harms the robustness of\ncross-attention for multi-modal feature interaction and search-template\ncorrelation computation. To address these issues, we propose a novel approach\ncalled Cross-modulated Attention Transformer (CAFormer), which performs\nintra-modality self-correlation, inter-modality feature interaction, and\nsearch-template correlation computation in a unified attention model, for RGBT\ntracking. In particular, we first independently generate correlation maps for\neach modality and feed them into the designed Correlation Modulated Enhancement\nmodule, modulating inaccurate correlation weights by seeking the consensus\nbetween modalities. Such kind of design unifies self-attention and\ncross-attention schemes, which not only alleviates inaccurate attention weight\ncomputation in self-attention but also eliminates redundant computation\nintroduced by extra cross-attention scheme. In addition, we propose a\ncollaborative token elimination strategy to further improve tracking inference\nefficiency and accuracy. Extensive experiments on five public RGBT tracking\nbenchmarks show the outstanding performance of the proposed CAFormer against\nstate-of-the-art methods.\n","authors":["Yun Xiao","Jiacong Zhao","Andong Lu","Chenglong Li","Yin Lin","Bing Yin","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02222v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.02214v1","updated":"2024-08-05T03:33:00Z","published":"2024-08-05T03:33:00Z","title":"More Than Positive and Negative: Communicating Fine Granularity in\n  Medical Diagnosis","summary":"  With the advance of deep learning, much progress has been made in building\npowerful artificial intelligence (AI) systems for automatic Chest X-ray (CXR)\nanalysis. Most existing AI models are trained to be a binary classifier with\nthe aim of distinguishing positive and negative cases. However, a large gap\nexists between the simple binary setting and complicated real-world medical\nscenarios. In this work, we reinvestigate the problem of automatic radiology\ndiagnosis. We first observe that there is considerable diversity among cases\nwithin the positive class, which means simply classifying them as positive\nloses many important details. This motivates us to build AI models that can\ncommunicate fine-grained knowledge from medical images like human experts. To\nthis end, we first propose a new benchmark on fine granularity learning from\nmedical images. Specifically, we devise a division rule based on medical\nknowledge to divide positive cases into two subcategories, namely atypical\npositive and typical positive. Then, we propose a new metric termed\nAUC$^\\text{FG}$ on the two subcategories for evaluation of the ability to\nseparate them apart. With the proposed benchmark, we encourage the community to\ndevelop AI diagnosis systems that could better learn fine granularity from\nmedical images. Last, we propose a simple risk modulation approach to this\nproblem by only using coarse labels in training. Empirical results show that\ndespite its simplicity, the proposed method achieves superior performance and\nthus serves as a strong baseline.\n","authors":["Xiangyu Peng","Kai Wang","Jianfei Yang","Yingying Zhu","Yang You"],"pdf_url":"https://arxiv.org/pdf/2408.02214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02210v1","updated":"2024-08-05T03:22:10Z","published":"2024-08-05T03:22:10Z","title":"ExoViP: Step-by-step Verification and Exploration with Exoskeleton\n  Modules for Compositional Visual Reasoning","summary":"  Compositional visual reasoning methods, which translate a complex query into\na structured composition of feasible visual tasks, have exhibited a strong\npotential in complicated multi-modal tasks. Empowered by recent advances in\nlarge language models (LLMs), this multi-modal challenge has been brought to a\nnew stage by treating LLMs as few-shot/zero-shot planners, i.e.,\nvision-language (VL) programming. Such methods, despite their numerous merits,\nsuffer from challenges due to LLM planning mistakes or inaccuracy of visual\nexecution modules, lagging behind the non-compositional models. In this work,\nwe devise a \"plug-and-play\" method, ExoViP, to correct errors in both the\nplanning and execution stages through introspective verification. We employ\nverification modules as \"exoskeletons\" to enhance current VL programming\nschemes. Specifically, our proposed verification module utilizes a mixture of\nthree sub-verifiers to validate predictions after each reasoning step,\nsubsequently calibrating the visual module predictions and refining the\nreasoning trace planned by LLMs. Experimental results on two representative VL\nprogramming methods showcase consistent improvements on five compositional\nreasoning tasks on standard benchmarks. In light of this, we believe that\nExoViP can foster better performance and generalization on open-domain\nmulti-modal challenges.\n","authors":["Yuxuan Wang","Alan Yuille","Zhuowan Li","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.02210v1.pdf","comment":"To Appear at COLM 2024"},{"id":"http://arxiv.org/abs/2402.17521v3","updated":"2024-08-05T03:16:03Z","published":"2024-02-27T14:05:05Z","title":"AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Scene\n  Understanding","summary":"  The recent advancements in point cloud learning have enabled intelligent\nvehicles and robots to comprehend 3D environments better. However, processing\nlarge-scale 3D scenes remains a challenging problem, such that efficient\ndownsampling methods play a crucial role in point cloud learning. Existing\ndownsampling methods either require a huge computational burden or sacrifice\nfine-grained geometric information. For such purpose, this paper presents an\nadvanced sampler that achieves both high accuracy and efficiency. The proposed\nmethod utilizes voxel centroid sampling as a foundation but effectively\naddresses the challenges regarding voxel size determination and the\npreservation of critical geometric cues. Specifically, we propose a Voxel\nAdaptation Module that adaptively adjusts voxel sizes with the reference of\npoint-based downsampling ratio. This ensures that the sampling results exhibit\na favorable distribution for comprehending various 3D objects or scenes.\nMeanwhile, we introduce a network compatible with arbitrary voxel sizes for\nsampling and feature extraction while maintaining high efficiency. The proposed\napproach is demonstrated with 3D object detection and 3D semantic segmentation.\nCompared to existing state-of-the-art methods, our approach achieves better\naccuracy on outdoor and indoor large-scale datasets, e.g. Waymo and ScanNet,\nwith promising efficiency.\n","authors":["Hongcheng Yang","Dingkang Liang","Dingyuan Zhang","Zhe Liu","Zhikang Zou","Xingyu Jiang","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.17521v3.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2305.03713v3","updated":"2024-08-05T02:38:33Z","published":"2023-05-05T17:54:34Z","title":"Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head\n  Videos","summary":"  Modern avatar generators allow anyone to synthesize photorealistic real-time\ntalking avatars, ushering in a new era of avatar-based human communication,\nsuch as with immersive AR/VR interactions or videoconferencing with limited\nbandwidths. Their safe adoption, however, requires a mechanism to verify if the\nrendered avatar is trustworthy: does it use the appearance of an individual\nwithout their consent? We term this task avatar fingerprinting. To tackle it,\nwe first introduce a large-scale dataset of real and synthetic videos of people\ninteracting on a video call, where the synthetic videos are generated using the\nfacial appearance of one person and the expressions of another. We verify the\nidentity driving the expressions in a synthetic video, by learning motion\nsignatures that are independent of the facial appearance shown. Our solution,\nthe first in this space, achieves an average AUC of 0.85. Critical to its\npractical use, it also generalizes to new generators never seen in training\n(average AUC of 0.83). The proposed dataset and other resources can be found\nat: https://research.nvidia.com/labs/nxp/avatar-fingerprinting/.\n","authors":["Ekta Prashnani","Koki Nagano","Shalini De Mello","David Luebke","Orazio Gallo"],"pdf_url":"https://arxiv.org/pdf/2305.03713v3.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02192v1","updated":"2024-08-05T02:37:59Z","published":"2024-08-05T02:37:59Z","title":"Unsupervised Domain Adaption Harnessing Vision-Language Pre-training","summary":"  This paper addresses two vital challenges in Unsupervised Domain Adaptation\n(UDA) with a focus on harnessing the power of Vision-Language Pre-training\n(VLP) models. Firstly, UDA has primarily relied on ImageNet pre-trained models.\nHowever, the potential of VLP models in UDA remains largely unexplored. The\nrich representation of VLP models holds significant promise for enhancing UDA\ntasks. To address this, we propose a novel method called Cross-Modal Knowledge\nDistillation (CMKD), leveraging VLP models as teacher models to guide the\nlearning process in the target domain, resulting in state-of-the-art\nperformance. Secondly, current UDA paradigms involve training separate models\nfor each task, leading to significant storage overhead and impractical model\ndeployment as the number of transfer tasks grows. To overcome this challenge,\nwe introduce Residual Sparse Training (RST) exploiting the benefits conferred\nby VLP's extensive pre-training, a technique that requires minimal adjustment\n(approximately 0.1\\%$\\sim$0.5\\%) of VLP model parameters to achieve performance\ncomparable to fine-tuning. Combining CMKD and RST, we present a comprehensive\nsolution that effectively leverages VLP models for UDA tasks while reducing\nstorage overhead for model deployment. Furthermore, CMKD can serve as a\nbaseline in conjunction with other methods like FixMatch, enhancing the\nperformance of UDA. Our proposed method outperforms existing techniques on\nstandard benchmarks. Our code will be available at:\nhttps://github.com/Wenlve-Zhou/VLP-UDA.\n","authors":["Wenlve Zhou","Zhiheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02191v1","updated":"2024-08-05T02:35:13Z","published":"2024-08-05T02:35:13Z","title":"Dense Feature Interaction Network for Image Inpainting Localization","summary":"  Image inpainting, which is the task of filling in missing areas in an image,\nis a common image editing technique. Inpainting can be used to conceal or alter\nimage contents in malicious manipulation of images, driving the need for\nresearch in image inpainting detection. Existing methods mostly rely on a basic\nencoder-decoder structure, which often results in a high number of false\npositives or misses the inpainted regions, especially when dealing with targets\nof varying semantics and scales. Additionally, the absence of an effective\napproach to capture boundary artifacts leads to less accurate edge\nlocalization. In this paper, we describe a new method for inpainting detection\nbased on a Dense Feature Interaction Network (DeFI-Net). DeFI-Net uses a novel\nfeature pyramid architecture to capture and amplify multi-scale representations\nacross various stages, thereby improving the detection of image inpainting by\nbetter revealing feature-level interactions. Additionally, the network can\nadaptively direct the lower-level features, which carry edge and shape\ninformation, to refine the localization of manipulated regions while\nintegrating the higher-level semantic features. Using DeFI-Net, we develop a\nmethod combining complementary representations to accurately identify inpainted\nareas. Evaluation on five image inpainting datasets demonstrate the\neffectiveness of our approach, which achieves state-of-the-art performance in\ndetecting inpainting across diverse models.\n","authors":["Ye Yao","Tingfeng Han","Shan Jia","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2408.02191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10993v3","updated":"2024-08-05T01:58:58Z","published":"2023-12-18T07:44:40Z","title":"Realistic Human Motion Generation with Cross-Diffusion Models","summary":"  We introduce the Cross Human Motion Diffusion Model (CrossDiff), a novel\napproach for generating high-quality human motion based on textual\ndescriptions. Our method integrates 3D and 2D information using a shared\ntransformer network within the training of the diffusion model, unifying motion\nnoise into a single feature space. This enables cross-decoding of features into\nboth 3D and 2D motion representations, regardless of their original dimension.\nThe primary advantage of CrossDiff is its cross-diffusion mechanism, which\nallows the model to reverse either 2D or 3D noise into clean motion during\ntraining. This capability leverages the complementary information in both\nmotion representations, capturing intricate human movement details often missed\nby models relying solely on 3D information. Consequently, CrossDiff effectively\ncombines the strengths of both representations to generate more realistic\nmotion sequences. In our experiments, our model demonstrates competitive\nstate-of-the-art performance on text-to-motion benchmarks. Moreover, our method\nconsistently provides enhanced motion generation quality, capturing complex\nfull-body movement intricacies. Additionally, with a pretrained model,our\napproach accommodates using in the wild 2D motion data without 3D motion ground\ntruth during training to generate 3D motion, highlighting its potential for\nbroader applications and efficient use of available data resources. Project\npage: https://wonderno.github.io/CrossDiff-webpage/.\n","authors":["Zeping Ren","Shaoli Huang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2312.10993v3.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.02181v1","updated":"2024-08-05T01:50:09Z","published":"2024-08-05T01:50:09Z","title":"AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing\n  Pipelines","summary":"  Anomaly detection in manufacturing pipelines remains a critical challenge,\nintensified by the complexity and variability of industrial environments. This\npaper introduces AssemAI, an interpretable image-based anomaly detection system\ntailored for smart manufacturing pipelines. Our primary contributions include\nthe creation of a tailored image dataset and the development of a custom object\ndetection model, YOLO-FF, designed explicitly for anomaly detection in\nmanufacturing assembly environments. Utilizing the preprocessed image dataset\nderived from an industry-focused rocket assembly pipeline, we address the\nchallenge of imbalanced image data and demonstrate the importance of\nimage-based methods in anomaly detection. The proposed approach leverages\ndomain knowledge in data preparation, model development and reasoning. We\ncompare our method against several baselines, including simple CNN and custom\nVisual Transformer (ViT) models, showcasing the effectiveness of our custom\ndata preparation and pretrained CNN integration. Additionally, we incorporate\nexplainability techniques at both user and model levels, utilizing ontology for\nuser-friendly explanations and SCORE-CAM for in-depth feature and model\nanalysis. Finally, the model was also deployed in a real-time setting. Our\nresults include ablation studies on the baselines, providing a comprehensive\nevaluation of the proposed system. This work highlights the broader impact of\nadvanced image-based anomaly detection in enhancing the reliability and\nefficiency of smart manufacturing processes.\n","authors":["Renjith Prasad","Chathurangi Shyalika","Ramtin Zand","Fadi El Kalach","Revathy Venkataramanan","Ramy Harik","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2408.02181v1.pdf","comment":"8 Pages, 6 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2407.20937v2","updated":"2024-08-05T01:27:39Z","published":"2024-07-30T16:19:14Z","title":"EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from\n  bi-planar X-ray images","summary":"  X-ray images ease the diagnosis and treatment process due to their rapid\nimaging speed and high resolution. However, due to the projection process of\nX-ray imaging, much spatial information has been lost. To accurately provide\nefficient spinal morphological and structural information, reconstructing the\n3-D structures of the spine from the 2-D X-ray images is essential. It is\nchallenging for current reconstruction methods to preserve the edge information\nand local shapes of the asymmetrical vertebrae structures. In this study, we\npropose a new Edge-Aware Reconstruction network (EAR) to focus on the\nperformance improvement of the edge information and vertebrae shapes. In our\nnetwork, by using the auto-encoder architecture as the backbone, the edge\nattention module and frequency enhancement module are proposed to strengthen\nthe perception of the edge reconstruction. Meanwhile, we also combine four loss\nterms, including reconstruction loss, edge loss, frequency loss and projection\nloss. The proposed method is evaluated using three publicly accessible datasets\nand compared with four state-of-the-art models. The proposed method is superior\nto other methods and achieves 25.32%, 15.32%, 86.44%, 80.13%, 23.7612 and\n0.3014 with regard to MSE, MAE, Dice, SSIM, PSNR and frequency distance. Due to\nthe end-to-end and accurate reconstruction process, EAR can provide sufficient\n3-D spatial information and precise preoperative surgical planning guidance.\n","authors":["Lixing Tan","Shuang Song","Yaofeng He","Kangneng Zhou","Tong Lu","Ruoxiu Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.20937v2.pdf","comment":"13 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2311.03572v2","updated":"2024-08-05T01:05:54Z","published":"2023-11-06T22:17:18Z","title":"Unsupervised Region-Growing Network for Object Segmentation in\n  Atmospheric Turbulence","summary":"  Moving object segmentation in the presence of atmospheric turbulence is\nhighly challenging due to turbulence-induced irregular and time-varying\ndistortions. In this paper, we present an unsupervised approach for segmenting\nmoving objects in videos downgraded by atmospheric turbulence. Our key approach\nis a detect-then-grow scheme: we first identify a small set of moving object\npixels with high confidence, then gradually grow a foreground mask from those\nseeds to segment all moving objects. This method leverages rigid geometric\nconsistency among video frames to disentangle different types of motions, and\nthen uses the Sampson distance to initialize the seedling pixels. After growing\nper-frame foreground masks, we use spatial grouping loss and temporal\nconsistency loss to further refine the masks in order to ensure their\nspatio-temporal consistency. Our method is unsupervised and does not require\ntraining on labeled data. For validation, we collect and release the first\nreal-captured long-range turbulent video dataset with ground truth masks for\nmoving objects. Results show that our method achieves good accuracy in\nsegmenting moving objects and is robust for long-range videos with various\nturbulence strengths.\n","authors":["Dehao Qin","Ripon Saha","Suren Jayasuriya","Jinwei Ye","Nianyi Li"],"pdf_url":"https://arxiv.org/pdf/2311.03572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18302v2","updated":"2024-08-05T23:48:12Z","published":"2024-02-28T12:50:16Z","title":"EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous\n  Driving","summary":"  This paper introduces the task of Auditory Referring Multi-Object Tracking\n(AR-MOT), which dynamically tracks specific objects in a video sequence based\non audio expressions and appears as a challenging problem in autonomous\ndriving. Due to the lack of semantic modeling capacity in audio and video,\nexisting works have mainly focused on text-based multi-object tracking, which\noften comes at the cost of tracking quality, interaction efficiency, and even\nthe safety of assistance systems, limiting the application of such methods in\nautonomous driving. In this paper, we delve into the problem of AR-MOT from the\nperspective of audio-video fusion and audio-video tracking. We put forward\nEchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers.\nThe dual streams are intertwined with our Bidirectional Frequency-domain\nCross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and\nvideo features from both frequency- and spatiotemporal domains. Moreover, we\npropose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract\nhomogeneous semantic features between expressions and visual objects by\nlearning homogeneous features between different audio and video objects\neffectively. Aside from the architectural design, we establish the first set of\nlarge-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD.\nExtensive experiments on the established benchmarks demonstrate the\neffectiveness of the proposed EchoTrack and its components. The source code and\ndatasets are available at https://github.com/lab206/EchoTrack.\n","authors":["Jiacheng Lin","Jiajun Chen","Kunyu Peng","Xuan He","Zhiyong Li","Rainer Stiefelhagen","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2402.18302v2.pdf","comment":"Accepted to IEEE Transactions on Intelligent Transportation Systems\n  (T-ITS). The source code and datasets are available at\n  https://github.com/lab206/EchoTrack"},{"id":"http://arxiv.org/abs/2408.02865v1","updated":"2024-08-05T23:31:07Z","published":"2024-08-05T23:31:07Z","title":"VisionUnite: A Vision-Language Foundation Model for Ophthalmology\n  Enhanced with Clinical Knowledge","summary":"  The need for improved diagnostic methods in ophthalmology is acute,\nespecially in the less developed regions with limited access to specialists and\nadvanced equipment. Therefore, we introduce VisionUnite, a novel\nvision-language foundation model for ophthalmology enhanced with clinical\nknowledge. VisionUnite has been pretrained on an extensive dataset comprising\n1.24 million image-text pairs, and further refined using our proposed MMFundus\ndataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate\nthat VisionUnite outperforms existing generative foundation models such as\nGPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable\nto junior ophthalmologists. VisionUnite performs well in various clinical\nscenarios including open-ended multi-disease diagnosis, clinical explanation,\nand patient interaction, making it a highly versatile tool for initial\nophthalmic disease screening. VisionUnite can also serve as an educational aid\nfor junior ophthalmologists, accelerating their acquisition of knowledge\nregarding both common and rare ophthalmic conditions. VisionUnite represents a\nsignificant advancement in ophthalmology, with broad implications for\ndiagnostics, medical education, and understanding of disease mechanisms.\n","authors":["Zihan Li","Diping Song","Zefeng Yang","Deming Wang","Fei Li","Xiulan Zhang","Paul E. Kinahan","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.02865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02859v1","updated":"2024-08-05T22:59:50Z","published":"2024-08-05T22:59:50Z","title":"Multistain Pretraining for Slide Representation Learning in Pathology","summary":"  Developing self-supervised learning (SSL) models that can learn universal and\ntransferable representations of H&E gigapixel whole-slide images (WSIs) is\nbecoming increasingly valuable in computational pathology. These models hold\nthe potential to advance critical tasks such as few-shot classification, slide\nretrieval, and patient stratification. Existing approaches for slide\nrepresentation learning extend the principles of SSL from small images (e.g.,\n224 x 224 patches) to entire slides, usually by aligning two different\naugmentations (or views) of the slide. Yet the resulting representation remains\nconstrained by the limited clinical and biological diversity of the views.\nInstead, we postulate that slides stained with multiple markers, such as\nimmunohistochemistry, can be used as different views to form a rich\ntask-agnostic training signal. To this end, we introduce Madeleine, a\nmultimodal pretraining strategy for slide representation learning. Madeleine is\ntrained with a dual global-local cross-stain alignment objective on large\ncohorts of breast cancer samples (N=4,211 WSIs across five stains) and kidney\ntransplant samples (N=12,070 WSIs across four stains). We demonstrate the\nquality of slide representations learned by Madeleine on various downstream\nevaluations, ranging from morphological and molecular classification to\nprognostic prediction, comprising 21 tasks using 7,299 WSIs from multiple\nmedical centers. Code is available at https://github.com/mahmoodlab/MADELEINE.\n","authors":["Guillaume Jaume","Anurag Vaidya","Andrew Zhang","Andrew H. Song","Richard J. Chen","Sharifa Sahai","Dandan Mo","Emilio Madrigal","Long Phi Le","Faisal Mahmood"],"pdf_url":"https://arxiv.org/pdf/2408.02859v1.pdf","comment":"ECCV'24"},{"id":"http://arxiv.org/abs/2408.02855v1","updated":"2024-08-05T22:49:20Z","published":"2024-08-05T22:49:20Z","title":"Analyzing Data Efficiency and Performance of Machine Learning Algorithms\n  for Assessing Low Back Pain Physical Rehabilitation Exercises","summary":"  Analyzing human motion is an active research area, with various applications.\nIn this work, we focus on human motion analysis in the context of physical\nrehabilitation using a robot coach system. Computer-aided assessment of\nphysical rehabilitation entails evaluation of patient performance in completing\nprescribed rehabilitation exercises, based on processing movement data captured\nwith a sensory system, such as RGB and RGB-D cameras. As 2D and 3D human pose\nestimation from RGB images had made impressive improvements, we aim to compare\nthe assessment of physical rehabilitation exercises using movement data\nobtained from both RGB-D camera (Microsoft Kinect) and estimation from RGB\nvideos (OpenPose and BlazePose algorithms). A Gaussian Mixture Model (GMM) is\nemployed from position (and orientation) features, with performance metrics\ndefined based on the log-likelihood values from GMM. The evaluation is\nperformed on a medical database of clinical patients carrying out low back-pain\nrehabilitation exercises, previously coached by robot Poppy.\n","authors":["Aleksa Marusic","Louis Annabi","Sao Msi Nguyen","Adriana Tapus"],"pdf_url":"https://arxiv.org/pdf/2408.02855v1.pdf","comment":"European Conference on Mobile Robots (2023)"},{"id":"http://arxiv.org/abs/2408.02840v1","updated":"2024-08-05T21:29:33Z","published":"2024-08-05T21:29:33Z","title":"GAReT: Cross-view Video Geolocalization with Adapters and\n  Auto-Regressive Transformers","summary":"  Cross-view video geo-localization (CVGL) aims to derive GPS trajectories from\nstreet-view videos by aligning them with aerial-view images. Despite their\npromising performance, current CVGL methods face significant challenges. These\nmethods use camera and odometry data, typically absent in real-world scenarios.\nThey utilize multiple adjacent frames and various encoders for feature\nextraction, resulting in high computational costs. Moreover, these approaches\nindependently predict each street-view frame's location, resulting in\ntemporally inconsistent GPS trajectories. To address these challenges, in this\nwork, we propose GAReT, a fully transformer-based method for CVGL that does not\nrequire camera and odometry data. We introduce GeoAdapter, a\ntransformer-adapter module designed to efficiently aggregate image-level\nrepresentations and adapt them for video inputs. Specifically, we train a\ntransformer encoder on video frames and aerial images, then freeze the encoder\nto optimize the GeoAdapter module to obtain video-level representation. To\naddress temporally inconsistent trajectories, we introduce TransRetriever, an\nencoder-decoder transformer model that predicts GPS locations of street-view\nframes by encoding top-k nearest neighbor predictions per frame and\nauto-regressively decoding the best neighbor based on the previous frame's\npredictions. Our method's effectiveness is validated through extensive\nexperiments, demonstrating state-of-the-art performance on benchmark datasets.\nOur code is available at https://github.com/manupillai308/GAReT.\n","authors":["Manu S Pillai","Mamshad Nayeem Rizve","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2408.02840v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02834v1","updated":"2024-08-05T21:11:34Z","published":"2024-08-05T21:11:34Z","title":"DaCapo: a modular deep learning framework for scalable 3D image\n  segmentation","summary":"  DaCapo is a specialized deep learning library tailored to expedite the\ntraining and application of existing machine learning approaches on large,\nnear-isotropic image data. In this correspondence, we introduce DaCapo's unique\nfeatures optimized for this specific domain, highlighting its modular\nstructure, efficient experiment management tools, and scalable deployment\ncapabilities. We discuss its potential to improve access to large-scale,\nisotropic image segmentation and invite the community to explore and contribute\nto this open-source initiative.\n","authors":["William Patton","Jeff L. Rhoades","Marwan Zouinkhi","David G. Ackerman","Caroline Malin-Mayor","Diane Adjavon","Larissa Heinrich","Davis Bennett","Yurii Zubov","CellMap Project Team","Aubrey V. Weigel","Jan Funke"],"pdf_url":"https://arxiv.org/pdf/2408.02834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19082v2","updated":"2024-08-05T21:09:50Z","published":"2024-07-26T21:02:11Z","title":"Regularized Multi-Decoder Ensemble for an Error-Aware Scene\n  Representation Network","summary":"  Feature grid Scene Representation Networks (SRNs) have been applied to\nscientific data as compact functional surrogates for analysis and\nvisualization. As SRNs are black-box lossy data representations, assessing the\nprediction quality is critical for scientific visualization applications to\nensure that scientists can trust the information being visualized. Currently,\nexisting architectures do not support inference time reconstruction quality\nassessment, as coordinate-level errors cannot be evaluated in the absence of\nground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN)\nensemble architecture consisting of a shared feature grid with multiple\nlightweight multi-layer perceptron decoders. MDSRN can generate a set of\nplausible predictions for a given input coordinate to compute the mean as the\nprediction of the multi-decoder ensemble and the variance as a confidence\nscore. The coordinate-level variance can be rendered along with the data to\ninform the reconstruction quality, or be integrated into uncertainty-aware\nvolume visualization algorithms. To prevent the misalignment between the\nquantified variance and the prediction quality, we propose a novel variance\nregularization loss for ensemble learning that promotes the Regularized\nmulti-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates\nclosely to the true model error. We comprehensively evaluate the quality of\nvariance quantification and data reconstruction of Monte Carlo Dropout, Mean\nField Variational Inference, Deep Ensemble, and Predicting Variance compared to\nthe proposed MDSRN and RMDSRN across diverse scalar field datasets. We\ndemonstrate that RMDSRN attains the most accurate data reconstruction and\ncompetitive variance-error correlation among uncertain SRNs under the same\nneural network parameter budgets.\n","authors":["Tianyu Xiong","Skylar W. Wurster","Hanqi Guo","Tom Peterka","Han-Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2407.19082v2.pdf","comment":"To be published in Proc. IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.02813v1","updated":"2024-08-05T20:27:45Z","published":"2024-08-05T20:27:45Z","title":"Mitigating Malicious Attacks in Federated Learning via Confidence-aware\n  Defense","summary":"  Federated Learning (FL) is an emerging distributed machine learning paradigm\nthat allows multiple clients to collaboratively train a global model without\nsharing private local data. However, FL systems are vulnerable to attacks from\nmalicious clients, who can degrade the global model performance through data\npoisoning and model poisoning. Existing defense methods typically focus on a\nsingle type of attack, such as Byzantine attacks or backdoor attacks, and are\noften ineffective against potential data poisoning attacks like label flipping\nand label shuffling. Additionally, these methods often lack accuracy and\nrobustness in detecting and handling malicious updates. To address these\nissues, we propose a novel method based on model confidence scores, which\nevaluates the uncertainty of client model updates to detect and defend against\nmalicious clients. Our approach is comprehensively effective for both model\npoisoning and data poisoning attacks and is capable of accurately identifying\nand mitigating potential malicious updates from being aggregated. Experimental\nresults demonstrate that our method significantly improves the robustness of FL\nsystems against various types of attacks, also achieving higher model accuracy\nand stability across various scenarios.\n","authors":["Qilei Li","Ahmed M. Abdelmoniem"],"pdf_url":"https://arxiv.org/pdf/2408.02813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02803v1","updated":"2024-08-05T19:46:59Z","published":"2024-08-05T19:46:59Z","title":"SiCo: A Size-Controllable Virtual Try-On Approach for Informed\n  Decision-Making","summary":"  Virtual try-on (VTO) applications aim to improve the online shopping\nexperience by allowing users to preview garments, before making purchase\ndecisions. However, many VTO tools fail to consider the crucial relationship\nbetween a garment's size and the user's body size, often employing a\none-size-fits-all approach when visualizing a clothing item. This results in\npoor size recommendations and purchase decisions leading to increased return\nrates. To address this limitation, we introduce SiCo, an online VTO system,\nwhere users can upload images of themselves and visualize how different sizes\nof clothing would look on their body to help make better-informed purchase\ndecisions. Our user study shows SiCo's superiority over baseline VTO. The\nresults indicate that our approach significantly enhances user ability to gauge\nthe appearance of outfits on their bodies and boosts their confidence in\nselecting clothing sizes that match desired goals. Based on our evaluation, we\nbelieve our VTO design has the potential to reduce return rates and enhance the\nonline clothes shopping experience. Our code is available at\nhttps://github.com/SherryXTChen/SiCo.\n","authors":["Sherry X. Chen","Alex Christopher Lim","Yimeng Liu","Pradeep Sen","Misha Sra"],"pdf_url":"https://arxiv.org/pdf/2408.02803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02796v1","updated":"2024-08-05T19:23:45Z","published":"2024-08-05T19:23:45Z","title":"Gaussian Mixture based Evidential Learning for Stereo Matching","summary":"  In this paper, we introduce a novel Gaussian mixture based evidential\nlearning solution for robust stereo matching. Diverging from previous\nevidential deep learning approaches that rely on a single Gaussian\ndistribution, our framework posits that individual image data adheres to a\nmixture-of-Gaussian distribution in stereo matching. This assumption yields\nmore precise pixel-level predictions and more accurately mirrors the real-world\nimage distribution. By further employing the inverse-Gamma distribution as an\nintermediary prior for each mixture component, our probabilistic model achieves\nimproved depth estimation compared to its counterpart with the single Gaussian\nand effectively captures the model uncertainty, which enables a strong\ncross-domain generation ability. We evaluated our method for stereo matching by\ntraining the model using the Scene Flow dataset and testing it on KITTI 2015\nand Middlebury 2014. The experiment results consistently show that our method\nbrings improvements over the baseline methods in a trustworthy manner. Notably,\nour approach achieved new state-of-the-art results on both the in-domain\nvalidated data and the cross-domain datasets, demonstrating its effectiveness\nand robustness in stereo matching tasks.\n","authors":["Weide Liu","Xingxing Wang","Lu Wang","Jun Cheng","Fayao Liu","Xulei Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11751v2","updated":"2024-08-05T19:22:13Z","published":"2024-03-18T12:59:35Z","title":"Relational Representation Learning Network for Cross-Spectral Image\n  Patch Matching","summary":"  Recently, feature relation learning has drawn widespread attention in\ncross-spectral image patch matching. However, existing related research focuses\non extracting diverse relations between image patch features and ignores\nsufficient intrinsic feature representations of individual image patches.\nTherefore, we propose an innovative relational representation learning idea\nthat simultaneously focuses on sufficiently mining the intrinsic features of\nindividual image patches and the relations between image patch features. Based\non this, we construct a Relational Representation Learning Network (RRL-Net).\nSpecifically, we innovatively construct an autoencoder to fully characterize\nthe individual intrinsic features, and introduce a feature interaction learning\n(FIL) module to extract deep-level feature relations. To further fully mine\nindividual intrinsic features, a lightweight multi-dimensional global-to-local\nattention (MGLA) module is constructed to enhance the global feature extraction\nof individual image patches and capture local dependencies within global\nfeatures. By combining the MGLA module, we further explore the feature\nextraction network and construct an attention-based lightweight feature\nextraction (ALFE) network. In addition, we propose a multi-loss post-pruning\n(MLPP) optimization strategy, which greatly promotes network optimization while\navoiding increases in parameters and inference time. Extensive experiments\ndemonstrate that our RRL-Net achieves state-of-the-art (SOTA) performance on\nmultiple public datasets. Our code will be made public later.\n","authors":["Chuang Yu","Yunpeng Liu","Jinmiao Zhao","Dou Quan","Zelin Shi"],"pdf_url":"https://arxiv.org/pdf/2403.11751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09894v2","updated":"2024-08-05T19:21:16Z","published":"2023-12-15T15:45:52Z","title":"PathoDuet: Foundation Models for Pathological Slide Analysis of H&E and\n  IHC Stains","summary":"  Large amounts of digitized histopathological data display a promising future\nfor developing pathological foundation models via self-supervised learning\nmethods. Foundation models pretrained with these methods serve as a good basis\nfor downstream tasks. However, the gap between natural and histopathological\nimages hinders the direct application of existing methods. In this work, we\npresent PathoDuet, a series of pretrained models on histopathological images,\nand a new self-supervised learning framework in histopathology. The framework\nis featured by a newly-introduced pretext token and later task raisers to\nexplicitly utilize certain relations between images, like multiple\nmagnifications and multiple stains. Based on this, two pretext tasks,\ncross-scale positioning and cross-stain transferring, are designed to pretrain\nthe model on Hematoxylin and Eosin (H&E) images and transfer the model to\nimmunohistochemistry (IHC) images, respectively. To validate the efficacy of\nour models, we evaluate the performance over a wide variety of downstream\ntasks, including patch-level colorectal cancer subtyping and whole slide image\n(WSI)-level classification in H&E field, together with expression level\nprediction of IHC marker, tumor identification and slide-level qualitative\nanalysis in IHC field. The experimental results show the superiority of our\nmodels over most tasks and the efficacy of proposed pretext tasks. The codes\nand models are available at https://github.com/openmedlab/PathoDuet.\n","authors":["Shengyi Hua","Fang Yan","Tianle Shen","Lei Ma","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09894v2.pdf","comment":"Accepted for Medical Image Analysis"},{"id":"http://arxiv.org/abs/2408.02792v1","updated":"2024-08-05T19:19:29Z","published":"2024-08-05T19:19:29Z","title":"Lesion Elevation Prediction from Skin Images Improves Diagnosis","summary":"  While deep learning-based computer-aided diagnosis for skin lesion image\nanalysis is approaching dermatologists' performance levels, there are several\nworks showing that incorporating additional features such as shape priors,\ntexture, color constancy, and illumination further improves the lesion\ndiagnosis performance. In this work, we look at another clinically useful\nfeature, skin lesion elevation, and investigate the feasibility of predicting\nand leveraging skin lesion elevation labels. Specifically, we use a deep\nlearning model to predict image-level lesion elevation labels from 2D skin\nlesion images. We test the elevation prediction accuracy on the derm7pt\ndataset, and use the elevation prediction model to estimate elevation labels\nfor images from five other datasets: ISIC 2016, 2017, and 2018 Challenge\ndatasets, MSK, and DermoFit. We evaluate cross-domain generalization by using\nthese estimated elevation labels as auxiliary inputs to diagnosis models, and\nshow that these improve the classification performance, with AUROC improvements\nof up to 6.29% and 2.69% for dermoscopic and clinical images, respectively. The\ncode is publicly available at https://github.com/sfu-mial/LesionElevation.\n","authors":["Kumar Abhishek","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2408.02792v1.pdf","comment":"Medical Image Computing and Computer-Assisted Intervention (MICCAI)\n  ISIC Skin Image Analysis Workshop (MICCAI ISIC) 2024; 12 pages, 2 tables, 4\n  figures"},{"id":"http://arxiv.org/abs/2408.02788v1","updated":"2024-08-05T19:11:46Z","published":"2024-08-05T19:11:46Z","title":"GazeXplain: Learning to Predict Natural Language Explanations of Visual\n  Scanpaths","summary":"  While exploring visual scenes, humans' scanpaths are driven by their\nunderlying attention processes. Understanding visual scanpaths is essential for\nvarious applications. Traditional scanpath models predict the where and when of\ngaze shifts without providing explanations, creating a gap in understanding the\nrationale behind fixations. To bridge this gap, we introduce GazeXplain, a\nnovel study of visual scanpath prediction and explanation. This involves\nannotating natural-language explanations for fixations across eye-tracking\ndatasets and proposing a general model with an attention-language decoder that\njointly predicts scanpaths and generates explanations. It integrates a unique\nsemantic alignment mechanism to enhance the consistency between fixations and\nexplanations, alongside a cross-dataset co-training approach for\ngeneralization. These novelties present a comprehensive and adaptable solution\nfor explainable human visual scanpath prediction. Extensive experiments on\ndiverse eye-tracking datasets demonstrate the effectiveness of GazeXplain in\nboth scanpath prediction and explanation, offering valuable insights into human\nvisual attention and cognitive processes.\n","authors":["Xianyu Chen","Ming Jiang","Qi Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.02788v1.pdf","comment":"To appear in ECCV2024"},{"id":"http://arxiv.org/abs/2408.02787v1","updated":"2024-08-05T19:11:05Z","published":"2024-08-05T19:11:05Z","title":"Segmentation Style Discovery: Application to Skin Lesion Images","summary":"  Variability in medical image segmentation, arising from annotator\npreferences, expertise, and their choice of tools, has been well documented.\nWhile the majority of multi-annotator segmentation approaches focus on modeling\nannotator-specific preferences, they require annotator-segmentation\ncorrespondence. In this work, we introduce the problem of segmentation style\ndiscovery, and propose StyleSeg, a segmentation method that learns plausible,\ndiverse, and semantically consistent segmentation styles from a corpus of\nimage-mask pairs without any knowledge of annotator correspondence. StyleSeg\nconsistently outperforms competing methods on four publicly available skin\nlesion segmentation (SLS) datasets. We also curate ISIC-MultiAnnot, the largest\nmulti-annotator SLS dataset with annotator correspondence, and our results show\na strong alignment, using our newly proposed measure AS2, between the predicted\nstyles and annotator preferences. The code and the dataset are available at\nhttps://github.com/sfu-mial/StyleSeg.\n","authors":["Kumar Abhishek","Jeremy Kawahara","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2408.02787v1.pdf","comment":"Medical Image Computing and Computer-Assisted Intervention (MICCAI)\n  ISIC Skin Image Analysis Workshop (MICCAI ISIC) 2024; 13 pages, 2 tables, 3\n  figures"},{"id":"http://arxiv.org/abs/2309.05490v2","updated":"2024-08-05T18:57:42Z","published":"2023-09-11T14:32:04Z","title":"Learning Semantic Segmentation with Query Points Supervision on Aerial\n  Images","summary":"  Semantic segmentation is crucial in remote sensing, where high-resolution\nsatellite images are segmented into meaningful regions. Recent advancements in\ndeep learning have significantly improved satellite image segmentation.\nHowever, most of these methods are typically trained in fully supervised\nsettings that require high-quality pixel-level annotations, which are expensive\nand time-consuming to obtain. In this work, we present a weakly supervised\nlearning algorithm to train semantic segmentation algorithms that only rely on\nquery point annotations instead of full mask labels. Our proposed approach\nperforms accurate semantic segmentation and improves efficiency by\nsignificantly reducing the cost and time required for manual annotation.\nSpecifically, we generate superpixels and extend the query point labels into\nthose superpixels that group similar meaningful semantics. Then, we train\nsemantic segmentation models supervised with images partially labeled with the\nsuperpixel pseudo-labels. We benchmark our weakly supervised training approach\non an aerial image dataset and different semantic segmentation architectures,\nshowing that we can reach competitive performance compared to fully supervised\ntraining while reducing the annotation effort. The code of our proposed\napproach is publicly available at: https://github.com/santiago2205/LSSQPS.\n","authors":["Santiago Rivier","Carlos Hinojosa","Silvio Giancola","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2309.05490v2.pdf","comment":"Paper Accepted at ICIP 2024 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2408.02780v1","updated":"2024-08-05T18:57:33Z","published":"2024-08-05T18:57:33Z","title":"LR-Net: A Lightweight and Robust Network for Infrared Small Target\n  Detection","summary":"  Limited by equipment limitations and the lack of target intrinsic features,\nexisting infrared small target detection methods have difficulty meeting actual\ncomprehensive performance requirements. Therefore, we propose an innovative\nlightweight and robust network (LR-Net), which abandons the complex structure\nand achieves an effective balance between detection accuracy and resource\nconsumption. Specifically, to ensure the lightweight and robustness, on the one\nhand, we construct a lightweight feature extraction attention (LFEA) module,\nwhich can fully extract target features and strengthen information interaction\nacross channels. On the other hand, we construct a simple refined feature\ntransfer (RFT) module. Compared with direct cross-layer connections, the RFT\nmodule can improve the network's feature refinement extraction capability with\nlittle resource consumption. Meanwhile, to solve the problem of small target\nloss in high-level feature maps, on the one hand, we propose a low-level\nfeature distribution (LFD) strategy to use low-level features to supplement the\ninformation of high-level features. On the other hand, we introduce an\nefficient simplified bilinear interpolation attention module (SBAM) to promote\nthe guidance constraints of low-level features on high-level features and the\nfusion of the two. In addition, We abandon the traditional resizing method and\nadopt a new training and inference cropping strategy, which is more robust to\ndatasets with multi-scale samples. Extensive experimental results show that our\nLR-Net achieves state-of-the-art (SOTA) performance. Notably, on the basis of\nthe proposed LR-Net, we achieve 3rd place in the \"ICPR 2024 Resource-Limited\nInfrared Small Target Detection Challenge Track 2: Lightweight Infrared Small\nTarget Detection\".\n","authors":["Chuang Yu","Yunpeng Liu","Jinmiao Zhao","Zelin Shi"],"pdf_url":"https://arxiv.org/pdf/2408.02780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02773v1","updated":"2024-08-05T18:49:58Z","published":"2024-08-05T18:49:58Z","title":"Refined Infrared Small Target Detection Scheme with Single-Point\n  Supervision","summary":"  Recently, infrared small target detection with single-point supervision has\nattracted extensive attention. However, the detection accuracy of existing\nmethods has difficulty meeting actual needs. Therefore, we propose an\ninnovative refined infrared small target detection scheme with single-point\nsupervision, which has excellent segmentation accuracy and detection rate.\nSpecifically, we introduce label evolution with single point supervision\n(LESPS) framework and explore the performance of various excellent infrared\nsmall target detection networks based on this framework. Meanwhile, to improve\nthe comprehensive performance, we construct a complete post-processing\nstrategy. On the one hand, to improve the segmentation accuracy, we use a\ncombination of test-time augmentation (TTA) and conditional random field (CRF)\nfor post-processing. On the other hand, to improve the detection rate, we\nintroduce an adjustable sensitivity (AS) strategy for post-processing, which\nfully considers the advantages of multiple detection results and reasonably\nadds some areas with low confidence to the fine segmentation image in the form\nof centroid points. In addition, to further improve the performance and explore\nthe characteristics of this task, on the one hand, we construct and find that a\nmulti-stage loss is helpful for fine-grained detection. On the other hand, we\nfind that a reasonable sliding window cropping strategy for test samples has\nbetter performance for actual multi-size samples. Extensive experimental\nresults show that the proposed scheme achieves state-of-the-art (SOTA)\nperformance. Notably, the proposed scheme won the third place in the \"ICPR 2024\nResource-Limited Infrared Small Target Detection Challenge Track 1: Weakly\nSupervised Infrared Small Target Detection\".\n","authors":["Jinmiao Zhao","Zelin Shi","Chuang Yu","Yunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00761v4","updated":"2024-08-05T18:40:07Z","published":"2023-12-01T18:29:08Z","title":"Deep Unlearning: Fast and Efficient Gradient-free Approach to Class\n  Forgetting","summary":"  Machine unlearning is a prominent and challenging field, driven by regulatory\ndemands for user data deletion and heightened privacy awareness. Existing\napproaches involve retraining model or multiple finetuning steps for each\ndeletion request, often constrained by computational limits and restricted data\naccess. In this work, we introduce a novel class unlearning algorithm designed\nto strategically eliminate specific classes from the learned model. Our\nalgorithm first estimates the Retain and the Forget Spaces using Singular Value\nDecomposition on the layerwise activations for a small subset of samples from\nthe retain and unlearn classes, respectively. We then compute the shared\ninformation between these spaces and remove it from the forget space to isolate\nclass-discriminatory feature space. Finally, we obtain the unlearned model by\nupdating the weights to suppress the class discriminatory features from the\nactivation spaces. We demonstrate our algorithm's efficacy on ImageNet using a\nVision Transformer with only $\\sim 1.5\\%$ drop in retain accuracy compared to\nthe original model while maintaining under $1\\%$ accuracy on the unlearned\nclass samples. Furthermore, our algorithm exhibits competitive unlearning\nperformance and resilience against Membership Inference Attacks (MIA). Compared\nto baselines, it achieves an average accuracy improvement of $1.38\\%$ on the\nImageNet dataset while requiring up to $10 \\times$ fewer samples for\nunlearning. Additionally, under stronger MIA attacks on the CIFAR-100 dataset\nusing a ResNet18 architecture, our approach outperforms the best baseline by\n$1.8\\%$. Our code is available at\nhttps://github.com/sangamesh-kodge/class_forgetting.\n","authors":["Sangamesh Kodge","Gobinda Saha","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2312.00761v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02769v1","updated":"2024-08-05T18:38:29Z","published":"2024-08-05T18:38:29Z","title":"From Recognition to Prediction: Leveraging Sequence Reasoning for Action\n  Anticipation","summary":"  The action anticipation task refers to predicting what action will happen\nbased on observed videos, which requires the model to have a strong ability to\nsummarize the present and then reason about the future. Experience and common\nsense suggest that there is a significant correlation between different\nactions, which provides valuable prior knowledge for the action anticipation\ntask. However, previous methods have not effectively modeled this underlying\nstatistical relationship. To address this issue, we propose a novel end-to-end\nvideo modeling architecture that utilizes attention mechanisms, named\nAnticipation via Recognition and Reasoning (ARR). ARR decomposes the action\nanticipation task into action recognition and sequence reasoning tasks, and\neffectively learns the statistical relationship between actions by next action\nprediction (NAP). In comparison to existing temporal aggregation strategies,\nARR is able to extract more effective features from observable videos to make\nmore reasonable predictions. In addition, to address the challenge of\nrelationship modeling that requires extensive training data, we propose an\ninnovative approach for the unsupervised pre-training of the decoder, which\nleverages the inherent temporal dynamics of video to enhance the reasoning\ncapabilities of the network. Extensive experiments on the Epic-kitchen-100,\nEGTEA Gaze+, and 50salads datasets demonstrate the efficacy of the proposed\nmethods. The code is available at https://github.com/linuxsino/ARR.\n","authors":["Xin Liu","Chao Hao","Zitong Yu","Huanjing Yue","Jingyu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02769v1.pdf","comment":"Accepted by ACM TOMM"},{"id":"http://arxiv.org/abs/2408.02766v1","updated":"2024-08-05T18:34:15Z","published":"2024-08-05T18:34:15Z","title":"ConDL: Detector-Free Dense Image Matching","summary":"  In this work, we introduce a deep-learning framework designed for estimating\ndense image correspondences. Our fully convolutional model generates dense\nfeature maps for images, where each pixel is associated with a descriptor that\ncan be matched across multiple images. Unlike previous methods, our model is\ntrained on synthetic data that includes significant distortions, such as\nperspective changes, illumination variations, shadows, and specular highlights.\nUtilizing contrastive learning, our feature maps achieve greater invariance to\nthese distortions, enabling robust matching. Notably, our method eliminates the\nneed for a keypoint detector, setting it apart from many existing\nimage-matching techniques.\n","authors":["Monika Kwiatkowski","Simon Matern","Olaf Hellwich"],"pdf_url":"https://arxiv.org/pdf/2408.02766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02761v1","updated":"2024-08-05T18:24:48Z","published":"2024-08-05T18:24:48Z","title":"Dimensionality Reduction and Nearest Neighbors for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation","summary":"  Clinically deployed deep learning-based segmentation models are known to fail\non data outside of their training distributions. While clinicians review the\nsegmentations, these models tend to perform well in most instances, which could\nexacerbate automation bias. Therefore, detecting out-of-distribution images at\ninference is critical to warn the clinicians that the model likely failed. This\nwork applied the Mahalanobis distance (MD) post hoc to the bottleneck features\nof four Swin UNETR and nnU-net models that segmented the liver on T1-weighted\nmagnetic resonance imaging and computed tomography. By reducing the dimensions\nof the bottleneck features with either principal component analysis or uniform\nmanifold approximation and projection, images the models failed on were\ndetected with high performance and minimal computational load. In addition,\nthis work explored a non-parametric alternative to the MD, a k-th nearest\nneighbors distance (KNN). KNN drastically improved scalability and performance\nover MD when both were applied to raw and average-pooled bottleneck features.\n","authors":["McKell Woodland","Nihil Patel","Austin Castelo","Mais Al Taie","Mohamed Eltaher","Joshua P. Yung","Tucker J. Netherton","Tiffany L. Calderone","Jessica I. Sanchez","Darrel W. Cleere","Ahmed Elsaiey","Nakul Gupta","David Victor","Laura Beretta","Ankit B. Patel Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2408.02761v1.pdf","comment":"Expansion of \"Dimensionality Reduction for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation\" arXiv:2308.03723\n  . Submitted to the Journal for Machine Learning in Biomedical Imaging. Code\n  available at https://github.com/mckellwoodland/dimen_reduce_mahal"},{"id":"http://arxiv.org/abs/2405.01531v2","updated":"2024-08-05T18:20:39Z","published":"2024-05-02T17:59:01Z","title":"Improving Intervention Efficacy via Concept Realignment in Concept\n  Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) ground image classification on\nhuman-understandable concepts to allow for interpretable model decisions.\nCrucially, the CBM design inherently allows for human interventions, in which\nexpert users are given the ability to modify potentially misaligned concept\nchoices to influence the decision behavior of the model in an interpretable\nfashion. However, existing approaches often require numerous human\ninterventions per image to achieve strong performances, posing practical\nchallenges in scenarios where obtaining human feedback is expensive. In this\npaper, we find that this is noticeably driven by an independent treatment of\nconcepts during intervention, wherein a change of one concept does not\ninfluence the use of other ones in the model's final decision. To address this\nissue, we introduce a trainable concept intervention realignment module, which\nleverages concept relations to realign concept assignments post-intervention.\nAcross standard, real-world benchmarks, we find that concept realignment can\nsignificantly improve intervention efficacy; significantly reducing the number\nof interventions needed to reach a target classification performance or concept\nprediction accuracy. In addition, it easily integrates into existing\nconcept-based architectures without requiring changes to the models themselves.\nThis reduced cost of human-model collaboration is crucial to enhancing the\nfeasibility of CBMs in resource-constrained environments. Our code is available\nat: https://github.com/ExplainableML/concept_realignment.\n","authors":["Nishad Singhi","Jae Myung Kim","Karsten Roth","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2405.01531v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02750v1","updated":"2024-08-05T18:09:02Z","published":"2024-08-05T18:09:02Z","title":"Privacy-Safe Iris Presentation Attack Detection","summary":"  This paper proposes a framework for a privacy-safe iris presentation attack\ndetection (PAD) method, designed solely with synthetically-generated,\nidentity-leakage-free iris images. Once trained, the method is evaluated in a\nclassical way using state-of-the-art iris PAD benchmarks. We designed two\ngenerative models for the synthesis of ISO/IEC 19794-6-compliant iris images.\nThe first model synthesizes bona fide-looking samples. To avoid ``identity\nleakage,'' the generated samples that accidentally matched those used in the\nmodel's training were excluded. The second model synthesizes images of irises\nwith textured contact lenses and is conditioned by a given contact lens brand\nto have better control over textured contact lens appearance when forming the\ntraining set. Our experiments demonstrate that models trained solely on\nsynthetic data achieve a lower but still reasonable performance when compared\nto solutions trained with iris images collected from human subjects. This is\nthe first-of-its-kind attempt to use solely synthetic data to train a\nfully-functional iris PAD solution, and despite the performance gap between\nregular and the proposed methods, this study demonstrates that with the\nincreasing fidelity of generative models, creating such privacy-safe iris PAD\nmethods may be possible. The source codes and generative models trained for\nthis work are offered along with the paper.\n","authors":["Mahsa Mitcheff","Patrick Tinsley","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2408.02750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02718v1","updated":"2024-08-05T17:56:41Z","published":"2024-08-05T17:56:41Z","title":"MMIU: Multimodal Multi-image Understanding for Evaluating Large\n  Vision-Language Models","summary":"  The capability to process multiple images is crucial for Large\nVision-Language Models (LVLMs) to develop a more thorough and nuanced\nunderstanding of a scene. Recent multi-image LVLMs have begun to address this\nneed. However, their evaluation has not kept pace with their development. To\nfill this gap, we introduce the Multimodal Multi-image Understanding (MMIU)\nbenchmark, a comprehensive evaluation suite designed to assess LVLMs across a\nwide range of multi-image tasks. MMIU encompasses 7 types of multi-image\nrelationships, 52 tasks, 77K images, and 11K meticulously curated\nmultiple-choice questions, making it the most extensive benchmark of its kind.\nOur evaluation of 24 popular LVLMs, including both open-source and proprietary\nmodels, reveals significant challenges in multi-image comprehension,\nparticularly in tasks involving spatial understanding. Even the most advanced\nmodels, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Through\nmulti-faceted analytical experiments, we identify key performance gaps and\nlimitations, providing valuable insights for future model and data\nimprovements. We aim for MMIU to advance the frontier of LVLM research and\ndevelopment, moving us toward achieving sophisticated multimodal multi-image\nuser interactions.\n","authors":["Fanqing Meng","Jin Wang","Chuanhao Li","Quanfeng Lu","Hao Tian","Jiaqi Liao","Xizhou Zhu","Jifeng Dai","Yu Qiao","Ping Luo","Kaipeng Zhang","Wenqi Shao"],"pdf_url":"https://arxiv.org/pdf/2408.02718v1.pdf","comment":"Project Page: https://mmiu-bench.github.io/"},{"id":"http://arxiv.org/abs/2311.13297v2","updated":"2024-08-05T17:49:58Z","published":"2023-11-22T10:27:19Z","title":"Retargeting Visual Data with Deformation Fields","summary":"  Seam carving is an image editing method that enable content-aware resizing,\nincluding operations like removing objects. However, the seam-finding strategy\nbased on dynamic programming or graph-cut limits its applications to broader\nvisual data formats and degrees of freedom for editing. Our observation is that\ndescribing the editing and retargeting of images more generally by a\ndisplacement field yields a generalisation of content-aware deformations. We\npropose to learn a deformation with a neural network that keeps the output\nplausible while trying to deform it only in places with low information\ncontent. This technique applies to different kinds of visual data, including\nimages, 3D scenes given as neural radiance fields, or even polygon meshes.\nExperiments conducted on different visual data show that our method achieves\nbetter content-aware retargeting compared to previous methods.\n","authors":["Tim Elsner","Julia Berger","Tong Wu","Victor Czech","Lin Gao","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2311.13297v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02710v1","updated":"2024-08-05T13:12:57Z","published":"2024-08-05T13:12:57Z","title":"RCDM: Enabling Robustness for Conditional Diffusion Model","summary":"  The conditional diffusion model (CDM) enhances the standard diffusion model\nby providing more control, improving the quality and relevance of the outputs,\nand making the model adaptable to a wider range of complex tasks. However,\ninaccurate conditional inputs in the inverse process of CDM can easily lead to\ngenerating fixed errors in the neural network, which diminishes the\nadaptability of a well-trained model. The existing methods like data\naugmentation, adversarial training, robust optimization can improve the\nrobustness, while they often face challenges such as high computational\ncomplexity, limited applicability to unknown perturbations, and increased\ntraining difficulty. In this paper, we propose a lightweight solution, the\nRobust Conditional Diffusion Model (RCDM), based on control theory to\ndynamically reduce the impact of noise and significantly enhance the model's\nrobustness. RCDM leverages the collaborative interaction between two neural\nnetworks, along with optimal control strategies derived from control theory, to\noptimize the weights of two networks during the sampling process. Unlike\nconventional techniques, RCDM establishes a mathematical relationship between\nfixed errors and the weights of the two neural networks without incurring\nadditional computational overhead. Extensive experiments were conducted on\nMNIST and CIFAR-10 datasets, and the results demonstrate the effectiveness and\nadaptability of our proposed model.\n","authors":["Weifeng Xu","Xiang Zhu","Xiaoyong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02708v1","updated":"2024-08-05T12:33:07Z","published":"2024-08-05T12:33:07Z","title":"Scribble-Based Interactive Segmentation of Medical Hyperspectral Images","summary":"  Hyperspectral imaging (HSI) is an advanced medical imaging modality that\ncaptures optical data across a broad spectral range, providing novel insights\ninto the biochemical composition of tissues. HSI may enable precise\ndifferentiation between various tissue types and pathologies, making it\nparticularly valuable for tumour detection, tissue classification, and disease\ndiagnosis.\n  Deep learning-based segmentation methods have shown considerable\nadvancements, offering automated and accurate results. However, these methods\nface challenges with HSI datasets due to limited annotated data and\ndiscrepancies from hardware and acquisition\ntechniques~\\cite{clancy2020surgical,studier2023heiporspectral}. Variability in\nclinical protocols also leads to different definitions of structure boundaries.\nInteractive segmentation methods, utilizing user knowledge and clinical\ninsights, can overcome these issues and achieve precise segmentation results\n\\cite{zhao2013overview}.\n  This work introduces a scribble-based interactive segmentation framework for\nmedical hyperspectral images. The proposed method utilizes deep learning for\nfeature extraction and a geodesic distance map generated from user-provided\nscribbles to obtain the segmentation results. The experiment results show that\nutilising the geodesic distance maps based on deep learning-extracted features\nachieved better segmentation results than geodesic distance maps directly\ngenerated from hyperspectral images, reconstructed RGB images, or Euclidean\ndistance maps.\n","authors":["Zhonghao Wang","Junwen Wang","Charlie Budd","Oscar MacCormac","Jonathan Shapey","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2408.02708v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.17900v4","updated":"2024-08-05T07:06:14Z","published":"2024-07-25T09:42:24Z","title":"The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer","summary":"  Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.\n","authors":["Danqing Hu","Bing Liu","Xiaofeng Zhu","Nan Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17900v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00992v2","updated":"2024-08-05T02:09:58Z","published":"2024-08-02T03:44:14Z","title":"Fairness in Large Language Models in Three Hour","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains but often lack fairness considerations, potentially leading to\ndiscriminatory outcomes against marginalized populations. Unlike fairness in\ntraditional machine learning, fairness in LLMs involves unique backgrounds,\ntaxonomies, and fulfillment techniques. This tutorial provides a systematic\noverview of recent advances in the literature concerning fair LLMs, beginning\nwith real-world case studies to introduce LLMs, followed by an analysis of bias\ncauses therein. The concept of fairness in LLMs is then explored, summarizing\nthe strategies for evaluating bias and the algorithms designed to promote\nfairness. Additionally, resources for assessing bias in LLMs, including\ntoolkits and datasets, are compiled, and current research challenges and open\nquestions in the field are discussed. The repository is available at\n\\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.\n","authors":["Thang Doan Viet","Zichong Wang","Minh Nhat Nguyen","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16020v4","updated":"2024-08-05T02:01:12Z","published":"2024-07-22T19:55:44Z","title":"Sparks of Quantum Advantage and Rapid Retraining in Machine Learning","summary":"  The advent of quantum computing holds the potential to revolutionize various\nfields by solving complex problems more efficiently than classical computers.\nDespite this promise, practical quantum advantage is hindered by current\nhardware limitations, notably the small number of qubits and high noise levels.\nIn this study, we leverage adiabatic quantum computers to optimize\nKolmogorov-Arnold Networks, a powerful neural network architecture for\nrepresenting complex functions with minimal parameters. By modifying the\nnetwork to use Bezier curves as the basis functions and formulating the\noptimization problem into a Quadratic Unconstrained Binary Optimization\nproblem, we create a fixed-sized solution space, independent of the number of\ntraining samples. This strategy allows for the optimization of an entire neural\nnetwork in a single training iteration in which, due to order of operations, a\nmajority of the processing is done using a collapsed version of the training\ndataset. This inherently creates extremely fast training speeds, which are\nvalidated experimentally, compared to classical optimizers including Adam,\nStochastic Gradient Descent, Adaptive Gradient, and simulated annealing.\nAdditionally, we introduce a novel rapid retraining capability, enabling the\nnetwork to be retrained with new data without reprocessing old samples, thus\nenhancing learning efficiency in dynamic environments. Experiments on\nretraining demonstrate a hundred times speed up using adiabatic quantum\ncomputing based optimization compared to that of the gradient descent based\noptimizers, with theoretical models allowing this speed up to be much larger!\nOur findings suggest that with further advancements in quantum hardware and\nalgorithm optimization, quantum-optimized machine learning models could have\nbroad applications across various domains, with initial focus on rapid\nretraining.\n","authors":["William Troy"],"pdf_url":"https://arxiv.org/pdf/2407.16020v4.pdf","comment":"Major updates to the paper for timings and explanations of\n  optimization strategies used. Further optimized the code and updated the\n  figures to reflect the faster timings for v3"},{"id":"http://arxiv.org/abs/2405.04634v3","updated":"2024-08-05T17:53:28Z","published":"2024-05-07T19:37:22Z","title":"FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic\n  Segmentation of Diverse Landscapes","summary":"  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a\nnew tool to monitor territory and support public policies. Processing ALS data\nat scale requires efficient point classification methods that perform well over\nhighly diverse territories. To evaluate them, researchers need large annotated\nLidar datasets, however, current Lidar benchmark datasets have restricted scope\nand often cover a single urban area. To bridge this data gap, we present the\nFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an\nultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with\nhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is\nbuilt upon France's nationwide open Lidar data. It achieves spatial and\nsemantic diversity via a sampling scheme that explicitly concentrates rare\nclasses and challenging landscapes from five French regions. It should support\nthe development of 3D deep learning approaches for large-scale land monitoring.\nWe describe the nature of the source data, the sampling workflow, the content\nof the resulting dataset, and provide an initial evaluation of segmentation\nperformance using a performant 3D neural architecture.\n","authors":["Charles Gaydon","Michel Daab","Floryne Roche"],"pdf_url":"https://arxiv.org/pdf/2405.04634v3.pdf","comment":"15 pages | 9 figures | 8 tables | Dataset is available at\n  https://huggingface.co/datasets/IGNF/FRACTAL | Trained model is available at\n  https://huggingface.co/IGNF/FRACTAL-LidarHD_7cl_randlanet | Deep learning\n  code repository is on Gihtub at https://github.com/IGNF/myria3d | Data\n  engineering code repository is on Github at https://github.com/IGNF/pacasam"},{"id":"http://arxiv.org/abs/2407.11913v2","updated":"2024-08-05T17:50:03Z","published":"2024-07-16T17:05:20Z","title":"Quantised Global Autoencoder: A Holistic Approach to Representing Visual\n  Data","summary":"  In quantised autoencoders, images are usually split into local patches, each\nencoded by one token. This representation is redundant in the sense that the\nsame number of tokens is spend per region, regardless of the visual information\ncontent in that region. Adaptive discretisation schemes like quadtrees are\napplied to allocate tokens for patches with varying sizes, but this just varies\nthe region of influence for a token which nevertheless remains a local\ndescriptor. Modern architectures add an attention mechanism to the autoencoder\nwhich infuses some degree of global information into the local tokens. Despite\nthe global context, tokens are still associated with a local image region. In\ncontrast, our method is inspired by spectral decompositions which transform an\ninput signal into a superposition of global frequencies. Taking the data-driven\nperspective, we learn custom basis functions corresponding to the codebook\nentries in our VQ-VAE setup. Furthermore, a decoder combines these basis\nfunctions in a non-linear fashion, going beyond the simple linear superposition\nof spectral decompositions. We can achieve this global description with an\nefficient transpose operation between features and channels and demonstrate our\nperformance on compression.\n","authors":["Tim Elsner","Paula Usinger","Victor Czech","Gregor Kobsik","Yanjiang He","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2407.11913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02654v1","updated":"2024-08-05T17:33:09Z","published":"2024-08-05T17:33:09Z","title":"On Using Quasirandom Sequences in Machine Learning for Model Weight\n  Initialization","summary":"  The effectiveness of training neural networks directly impacts computational\ncosts, resource allocation, and model development timelines in machine learning\napplications. An optimizer's ability to train the model adequately (in terms of\ntrained model performance) depends on the model's initial weights. Model weight\ninitialization schemes use pseudorandom number generators (PRNGs) as a source\nof randomness.\n  We investigate whether substituting PRNGs for low-discrepancy quasirandom\nnumber generators (QRNGs) -- namely Sobol' sequences -- as a source of\nrandomness for initializers can improve model performance. We examine\nMulti-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long\nShort-Term Memory (LSTM), and Transformer architectures trained on MNIST,\nCIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses\nten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);\nOrthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with\nweights set using PRNG- and QRNG-based initializers are compared pairwise for\neach combination of dataset, architecture, optimizer, and initialization\nscheme.\n  Our findings indicate that QRNG-based neural network initializers either\nreach a higher accuracy or achieve the same accuracy more quickly than\nPRNG-based initializers in 60% of the 120 experiments conducted. Thus, using\nQRNG-based initializers instead of PRNG-based initializers can speed up and\nimprove model training.\n","authors":["Andriy Miranskyy","Adam Sorrenti","Viral Thakar"],"pdf_url":"https://arxiv.org/pdf/2408.02654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02641v1","updated":"2024-08-05T17:14:35Z","published":"2024-08-05T17:14:35Z","title":"Detection of Compromised Functions in a Serverless Cloud Environment","summary":"  Serverless computing is an emerging cloud paradigm with serverless functions\nat its core. While serverless environments enable software developers to focus\non developing applications without the need to actively manage the underlying\nruntime infrastructure, they open the door to a wide variety of security\nthreats that can be challenging to mitigate with existing methods. Existing\nsecurity solutions do not apply to all serverless architectures, since they\nrequire significant modifications to the serverless infrastructure or rely on\nthird-party services for the collection of more detailed data. In this paper,\nwe present an extendable serverless security threat detection model that\nleverages cloud providers' native monitoring tools to detect anomalous behavior\nin serverless applications. Our model aims to detect compromised serverless\nfunctions by identifying post-exploitation abnormal behavior related to\ndifferent types of attacks on serverless functions, and therefore, it is a last\nline of defense. Our approach is not tied to any specific serverless\napplication, is agnostic to the type of threats, and is adaptable through model\nadjustments. To evaluate our model's performance, we developed a serverless\ncybersecurity testbed in an AWS cloud environment, which includes two different\nserverless applications and simulates a variety of attack scenarios that cover\nthe main security threats faced by serverless functions. Our evaluation\ndemonstrates our model's ability to detect all implemented attacks while\nmaintaining a negligible false alarm rate.\n","authors":["Danielle Lavi","Oleg Brodt","Dudu Mimran","Yuval Elovici","Asaf Shabtai"],"pdf_url":"https://arxiv.org/pdf/2408.02641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02637v1","updated":"2024-08-05T17:01:33Z","published":"2024-08-05T17:01:33Z","title":"Command-line Obfuscation Detection using Small Language Models","summary":"  To avoid detection, adversaries often use command-line obfuscation. There are\nnumerous techniques of the command-line obfuscation, all designed to alter the\ncommand-line syntax without affecting its original functionality. This\nvariability forces most security solutions to create an exhaustive enumeration\nof signatures for even a single pattern. In contrast to using signatures, we\nhave implemented a scalable NLP-based detection method that leverages a\ncustom-trained, small transformer language model that can be applied to any\nsource of execution logs. The evaluation on top of real-world telemetry\ndemonstrates that our approach yields high-precision detections even on\nhigh-volume telemetry from a diverse set of environments spanning from\nuniversities and businesses to healthcare or finance. The practical value is\ndemonstrated in a case study of real-world samples detected by our model. We\nshow the model's superiority to signatures on established malware known to\nemploy obfuscation and showcase previously unseen obfuscated samples detected\nby our model.\n","authors":["Vojtech Outrata","Michael Adam Polak","Martin Kopp"],"pdf_url":"https://arxiv.org/pdf/2408.02637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02396v3","updated":"2024-08-05T16:49:51Z","published":"2023-12-04T23:26:12Z","title":"Unsupervised Change Detection for Space Habitats Using 3D Point Clouds","summary":"  This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.\n","authors":["Jamie Santos","Holly Dinkel","Julia Di","Paulo V. K. Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2312.02396v3.pdf","comment":"15 pages, 7 figures, Manuscript was presented at the AIAA SciTech\n  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:\n  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:\n  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]"},{"id":"http://arxiv.org/abs/2303.07338v2","updated":"2024-08-05T16:34:43Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred for CIL. In this work, we revisit\nCIL with PTMs and argue that the core factors in CIL are adaptivity for model\nupdating and generalizability for knowledge transferring. 1) We first reveal\nthat frozen PTM can already provide generalizable embeddings for CIL.\nSurprisingly, a simple baseline (SimpleCIL) which continually sets the\nclassifiers of PTM to prototype features can beat state-of-the-art even without\ntraining on the downstream task. 2) Due to the distribution gap between\npre-trained and downstream datasets, PTM can be further cultivated with\nadaptivity via model adaptation. We propose AdaPt and mERge (APER), which\naggregates the embeddings of PTM and adapted models for classifier\nconstruction. APER is a general framework that can be orthogonally combined\nwith any parameter-efficient tuning method, which holds the advantages of PTM's\ngeneralizability and adapted model's adaptivity. 3) Additionally, considering\nprevious ImageNet-based benchmarks are unsuitable in the era of PTM due to data\noverlapping, we propose four new benchmarks for assessment, namely ImageNet-A,\nObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the\neffectiveness of APER with a unified and concise framework. Code is available\nat https://github.com/zhoudw-zdw/RevisitingCIL\n","authors":["Da-Wei Zhou","Zi-Wen Cai","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v2.pdf","comment":"Accepted to IJCV. Code is available at:\n  https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2408.02604v1","updated":"2024-08-05T16:27:38Z","published":"2024-08-05T16:27:38Z","title":"Learning rheological parameters of non-Newtonian fluids from velocimetry\n  data","summary":"  We solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates\nvelocimetry data in order to jointly reconstruct the flow field and learn the\nunknown N-S parameters. By incorporating a Carreau shear-thinning viscosity\nmodel into the N-S problem, we devise an algorithm that learns the most likely\nCarreau parameters of a shear-thinning fluid, and estimates their\nuncertainties, from velocimetry data alone. We then conduct a flow-MRI\nexperiment to obtain velocimetry data of an axisymmetric laminar jet through an\nidealised medical device (FDA nozzle) for a blood analogue fluid. We show that\nthe algorithm can successfully reconstruct the flow field by learning the most\nlikely Carreau parameters, and that the learned parameters are in very good\nagreement with rheometry measurements. The algorithm accepts any algebraic\neffective viscosity model, as long as the model is differentiable, and it can\nbe extended to more complicated non-Newtonian fluids (e.g. Oldroyd-B fluid) if\na viscoelastic model is incorporated into the N-S problem.\n","authors":["Alexandros Kontogiannis","Richard Hodgkinson","Emily L. Manchester"],"pdf_url":"https://arxiv.org/pdf/2408.02604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02598v1","updated":"2024-08-05T16:15:31Z","published":"2024-08-05T16:15:31Z","title":"AI-Driven Strategies for Reducing Student Withdrawal -- A Study of EMU\n  Student Stopout","summary":"  Not everyone who enrolls in college will leave with a certificate or degree,\nbut the number of people who drop out or take a break is much higher than\nexperts previously believed. In December 2013, there were 29 million people\nwith some college education but no degree. That number jumped to 36 million by\nDecember of 2018, according to a new report from the National Student\nClearinghouse Research Center[1]. It is imperative to understand the underlying\nfactors contributing to student withdrawal and to assist decision-makers to\nidentify effective strategies to prevent it. By analyzing the characteristics\nand educational pathways of the stopout student population, our aim is to\nprovide actionable insights that can benefit institutions facing similar\nchallenges. Eastern Michigan University (EMU) faces significant challenges in\nstudent retention, with approximately 55% of its undergraduate students not\ncompleting their degrees within six years. As an institution committed to\nstudent success, EMU conducted a comprehensive study of student withdrawals to\nunderstand the influencing factors. And the paper revealed a high correlation\nbetween certain factors and withdrawals, even in the early stages of university\nattendance. Based on these findings, we developed a predictive model that\nemploys artificial intelligence techniques to assess the potential risk that\nstudents abandon their studies. These models enable universities to implement\nearly intervention strategies, support at-risk students, and improve overall\nhigher education success.\n","authors":["Yan Zhao","Amy Otteson"],"pdf_url":"https://arxiv.org/pdf/2408.02598v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.16517v2","updated":"2024-08-05T16:02:51Z","published":"2024-02-26T11:58:02Z","title":"Discovering Artificial Viscosity Models for Discontinuous Galerkin\n  Approximation of Conservation Laws using Physics-Informed Machine Learning","summary":"  Finite element-based high-order solvers of conservation laws offer large\naccuracy but face challenges near discontinuities due to the Gibbs phenomenon.\nArtificial viscosity is a popular and effective solution to this problem based\non physical insight. In this work, we present a physics-informed machine\nlearning algorithm to automate the discovery of artificial viscosity models in\na non-supervised paradigm. The algorithm is inspired by reinforcement learning\nand trains a neural network acting cell-by-cell (the viscosity model) by\nminimizing a loss defined as the difference with respect to a reference\nsolution thanks to automatic differentiation. This enables a dataset-free\ntraining procedure. We prove that the algorithm is effective by integrating it\ninto a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase\nseveral numerical tests on scalar and vectorial problems, such as Burgers' and\nEuler's equations in one and two dimensions. Results demonstrate that the\nproposed approach trains a model that is able to outperform classical viscosity\nmodels. Moreover, we show that the learnt artificial viscosity model is able to\ngeneralize across different problems and parameters.\n","authors":["Matteo Caldana","Paola F. Antonietti","Luca Dede'"],"pdf_url":"https://arxiv.org/pdf/2402.16517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02581v1","updated":"2024-08-05T15:59:36Z","published":"2024-08-05T15:59:36Z","title":"Operational range bounding of spectroscopy models with anomaly detection","summary":"  Safe operation of machine learning models requires architectures that\nexplicitly delimit their operational ranges. We evaluate the ability of anomaly\ndetection algorithms to provide indicators correlated with degraded model\nperformance. By placing acceptance thresholds over such indicators, hard\nboundaries are formed that define the model's coverage. As a use case, we\nconsider the extraction of exoplanetary spectra from transit light curves,\nspecifically within the context of ESA's upcoming Ariel mission. Isolation\nForests are shown to effectively identify contexts where prediction models are\nlikely to fail. Coverage/error trade-offs are evaluated under conditions of\ndata and concept drift. The best performance is seen when Isolation Forests\nmodel projections of the prediction model's explainability SHAP values.\n","authors":["Lus F. Simes","Pierluigi Casale","Marlia Felismino","Kai Hou Yip","Ingo P. Waldmann","Giovanna Tinetti","Theresa Lueftinger"],"pdf_url":"https://arxiv.org/pdf/2408.02581v1.pdf","comment":"To appear in \"Proceedings of SPAICE 2024: 1st ESA/IAA conference on\n  AI in and for Space\". Conference page at https://spaice.esa.int/"},{"id":"http://arxiv.org/abs/2405.06093v2","updated":"2024-08-05T15:51:50Z","published":"2024-05-09T20:45:58Z","title":"Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human\n  Annotation: A Case Study Using Schedule-of-Event Table Detection","summary":"  Large Language Models (LLMs) have demonstrated their efficacy across a broad\nspectrum of tasks in healthcare applications. However, often LLMs need to be\nfine-tuned on task-specific expert annotated data to achieve optimal\nperformance, which can be expensive and time consuming. In this study, we\nfine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels\nobtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)\ntables, which specify care plan in clinical trial protocols. We introduce a\nfiltering mechanism to select high-confidence labels for this table\nclassification task, thereby reducing the noise in the auto-generated labels.\nWe show that fine-tuned PaLM-2 with those labels achieves performance that\nexceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is\nclose to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our\nresults show that leveraging LLM-generated labels through powerful models like\ngemini-pro can potentially serve as a viable strategy for improving LLM\nperformance through fine-tuning in specialized tasks, particularly in domains\nwhere expert annotations are scarce, expensive, or time-consuming to obtain.\n","authors":["Bhawesh Kumar","Jonathan Amar","Eric Yang","Nan Li","Yugang Jia"],"pdf_url":"https://arxiv.org/pdf/2405.06093v2.pdf","comment":"23 pages. Published in MLHC 2024"},{"id":"http://arxiv.org/abs/2407.01281v2","updated":"2024-08-05T15:50:32Z","published":"2024-07-01T13:35:53Z","title":"Bridging Smoothness and Approximation: Theoretical Insights into\n  Over-Smoothing in Graph Neural Networks","summary":"  In this paper, we explore the approximation theory of functions defined on\ngraphs. Our study builds upon the approximation results derived from the\n$K$-functional. We establish a theoretical framework to assess the lower bounds\nof approximation for target functions using Graph Convolutional Networks (GCNs)\nand examine the over-smoothing phenomenon commonly observed in these networks.\nInitially, we introduce the concept of a $K$-functional on graphs, establishing\nits equivalence to the modulus of smoothness. We then analyze a typical type of\nGCN to demonstrate how the high-frequency energy of the output decays, an\nindicator of over-smoothing. This analysis provides theoretical insights into\nthe nature of over-smoothing within GCNs. Furthermore, we establish a lower\nbound for the approximation of target functions by GCNs, which is governed by\nthe modulus of smoothness of these functions. This finding offers a new\nperspective on the approximation capabilities of GCNs. In our numerical\nexperiments, we analyze several widely applied GCNs and observe the phenomenon\nof energy decay. These observations corroborate our theoretical results on\nexponential decay order.\n","authors":["Guangrui Yang","Jianfei Li","Ming Li","Han Feng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.01281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02575v1","updated":"2024-08-05T15:48:51Z","published":"2024-08-05T15:48:51Z","title":"Artificial Intelligence for Public Health Surveillance in Africa:\n  Applications and Opportunities","summary":"  Artificial Intelligence (AI) is revolutionizing various fields, including\npublic health surveillance. In Africa, where health systems frequently\nencounter challenges such as limited resources, inadequate infrastructure,\nfailed health information systems and a shortage of skilled health\nprofessionals, AI offers a transformative opportunity. This paper investigates\nthe applications of AI in public health surveillance across the continent,\npresenting successful case studies and examining the benefits, opportunities,\nand challenges of implementing AI technologies in African healthcare settings.\nOur paper highlights AI's potential to enhance disease monitoring and health\noutcomes, and support effective public health interventions. The findings\npresented in the paper demonstrate that AI can significantly improve the\naccuracy and timeliness of disease detection and prediction, optimize resource\nallocation, and facilitate targeted public health strategies. Additionally, our\npaper identified key barriers to the widespread adoption of AI in African\npublic health systems and proposed actionable recommendations to overcome these\nchallenges.\n","authors":["Jean Marie Tshimula","Mitterrand Kalengayi","Dieumerci Makenga","Dorcas Lilonge","Marius Asumani","Dborah Madiya","lie Nkuba Kalonji","Hugues Kanda","Ren Manass Galekwa","Josias Kumbu","Hardy Mikese","Grace Tshimula","Jean Tshibangu Muabila","Christian N. Mayemba","D'Jeff K. Nkashama","Kalonji Kalala","Steve Ataky","Tighana Wenge Basele","Mbuyi Mukendi Didier","Selain K. Kasereka","Maximilien V. Dialufuma","Godwill Ilunga Wa Kumwita","Lionel Muyuku","Jean-Paul Kimpesa","Dominique Muteba","Aaron Aruna Abedi","Lambert Mukendi Ntobo","Gloria M. Bundutidi","Dsir Kulimba Mashinda","Emmanuel Kabengele Mpinga","Nathanal M. Kasoro"],"pdf_url":"https://arxiv.org/pdf/2408.02575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02568v1","updated":"2024-08-05T15:43:56Z","published":"2024-08-05T15:43:56Z","title":"Cross-Modality Clustering-based Self-Labeling for Multimodal Data\n  Classification","summary":"  Technological advances facilitate the ability to acquire multimodal data,\nposing a challenge for recognition systems while also providing an opportunity\nto use the heterogeneous nature of the information to increase the\ngeneralization capability of models. An often overlooked issue is the cost of\nthe labeling process, which is typically high due to the need for a significant\ninvestment in time and money associated with human experts. Existing\nsemi-supervised learning methods often focus on operating in the feature space\ncreated by the fusion of available modalities, neglecting the potential for\ncross-utilizing complementary information available in each modality. To\naddress this problem, we propose Cross-Modality Clustering-based Self-Labeling\n(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances\nbelonging to each modality in the deep feature space and then propagates known\nlabels within the resulting clusters. Next, information about the instances'\nclass membership in each modality is exchanged based on the Euclidean distance\nto ensure more accurate labeling. Experimental evaluation conducted on 20\ndatasets derived from the MM-IMDb dataset indicates that cross-propagation of\nlabels between modalities -- especially when the number of pre-labeled\ninstances is small -- can allow for more reliable labeling and thus increase\nthe classification performance in each modality.\n","authors":["Pawe Zyblewski","Leandro L. Minku"],"pdf_url":"https://arxiv.org/pdf/2408.02568v1.pdf","comment":"10 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2301.07088v3","updated":"2024-08-05T15:38:05Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Many self-supervised learning methods are pre-trained on the well-curated\nImageNet-1K dataset. In this work, given the excellent scalability of web data,\nwe consider self-supervised pre-training on noisy web sourced image-text paired\ndata. First, we conduct a benchmark study of representative self-supervised\npre-training methods on large-scale web data in a like-for-like setting. We\ncompare a range of methods, including single-modal ones that use masked\ntraining objectives and multi-modal ones that use image-text constrastive\ntraining. We observe that existing multi-modal methods do not outperform their\nsingle-modal counterparts on vision transfer learning tasks. We derive an\ninformation-theoretical view to explain these benchmark results, which provides\ninsight into how to design a novel vision learner. Inspired by this insight, we\npresent a new visual representation pre-training method, MUlti-modal\nGenerator~(MUG), that learns from scalable web sourced image-text data. MUG\nachieves state-of-the-art transfer performance on a variety of tasks and\ndemonstrates promising scaling properties. Pre-trained models and code will be\nmade public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v3.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2408.02551v1","updated":"2024-08-05T15:26:39Z","published":"2024-08-05T15:26:39Z","title":"Process-constrained batch Bayesian approaches for yield optimization in\n  multi-reactor systems","summary":"  The optimization of yields in multi-reactor systems, which are advanced tools\nin heterogeneous catalysis research, presents a significant challenge due to\nhierarchical technical constraints. To this respect, this work introduces a\nnovel approach called process-constrained batch Bayesian optimization via\nThompson sampling (pc-BO-TS) and its generalized hierarchical extension\n(hpc-BO-TS). This method, tailored for the efficiency demands in multi-reactor\nsystems, integrates experimental constraints and balances between exploration\nand exploitation in a sequential batch optimization strategy. It offers an\nimprovement over other Bayesian optimization methods. The performance of\npc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in a\nrealistic scenario based on data obtained from high-throughput experiments done\non a multi-reactor system available in the REALCAT platform. The proposed\nmethods often outperform other sequential Bayesian optimizations and existing\nprocess-constrained batch Bayesian optimization methods. This work proposes a\nnovel approach to optimize the yield of a reaction in a multi-reactor system,\nmarking a significant step forward in digital catalysis and generally in\noptimization methods for chemical engineering.\n","authors":["Markus Grimm","Sbastien Paul","Pierre Chainais"],"pdf_url":"https://arxiv.org/pdf/2408.02551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02547v1","updated":"2024-08-05T15:17:34Z","published":"2024-08-05T15:17:34Z","title":"The Role of Functional Muscle Networks in Improving Hand Gesture\n  Perception for Human-Machine Interfaces","summary":"  Developing accurate hand gesture perception models is critical for various\nrobotic applications, enabling effective communication between humans and\nmachines and directly impacting neurorobotics and interactive robots. Recently,\nsurface electromyography (sEMG) has been explored for its rich informational\ncontext and accessibility when combined with advanced machine learning\napproaches and wearable systems. The literature presents numerous approaches to\nboost performance while ensuring robustness for neurorobots using sEMG, often\nresulting in models requiring high processing power, large datasets, and less\nscalable solutions. This paper addresses this challenge by proposing the\ndecoding of muscle synchronization rather than individual muscle activation. We\nstudy coherence-based functional muscle networks as the core of our perception\nmodel, proposing that functional synchronization between muscles and the\ngraph-based network of muscle connectivity encode contextual information about\nintended hand gestures. This can be decoded using shallow machine learning\napproaches without the need for deep temporal networks. Our technique could\nimpact myoelectric control of neurorobots by reducing computational burdens and\nenhancing efficiency. The approach is benchmarked on the Ninapro database,\nwhich contains 12 EMG signals from 40 subjects performing 17 hand gestures. It\nachieves an accuracy of 85.1%, demonstrating improved performance compared to\nexisting methods while requiring much less computational power. The results\nsupport the hypothesis that a coherence-based functional muscle network encodes\ncritical information related to gesture execution, significantly enhancing hand\ngesture perception with potential applications for neurorobotic systems and\ninteractive machines.\n","authors":["Costanza Armanini","Tuka Alhanai","Farah E. Shamout","S. Farokh Atashzar"],"pdf_url":"https://arxiv.org/pdf/2408.02547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02545v1","updated":"2024-08-05T15:16:24Z","published":"2024-08-05T15:16:24Z","title":"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation","summary":"  Implementing Retrieval-Augmented Generation (RAG) systems is inherently\ncomplex, requiring deep understanding of data, use cases, and intricate design\ndecisions. Additionally, evaluating these systems presents significant\nchallenges, necessitating assessment of both retrieval accuracy and generative\nquality through a multi-faceted approach. We introduce RAG Foundry, an\nopen-source framework for augmenting large language models for RAG use cases.\nRAG Foundry integrates data creation, training, inference and evaluation into a\nsingle workflow, facilitating the creation of data-augmented datasets for\ntraining and evaluating large language models in RAG settings. This integration\nenables rapid prototyping and experimentation with various RAG techniques,\nallowing users to easily generate datasets and train RAG models using internal\nor specialized knowledge sources. We demonstrate the framework effectiveness by\naugmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG\nconfigurations, showcasing consistent improvements across three\nknowledge-intensive datasets. Code is released as open-source in\nhttps://github.com/IntelLabs/RAGFoundry.\n","authors":["Daniel Fleischer","Moshe Berchansky","Moshe Wasserblat","Peter Izsak"],"pdf_url":"https://arxiv.org/pdf/2408.02545v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2406.04216v3","updated":"2024-08-05T15:08:02Z","published":"2024-06-06T16:15:34Z","title":"What Do Language Models Learn in Context? The Structured Task Hypothesis","summary":"  Large language models (LLMs) exhibit an intriguing ability to learn a novel\ntask from in-context examples presented in a demonstration, termed in-context\nlearning (ICL). Understandably, a swath of research has been dedicated to\nuncovering the theories underpinning ICL. One popular hypothesis explains ICL\nby task selection. LLMs identify the task based on the demonstration and\ngeneralize it to the prompt. Another popular hypothesis is that ICL is a form\nof meta-learning, i.e., the models learn a learning algorithm at pre-training\ntime and apply it to the demonstration. Finally, a third hypothesis argues that\nLLMs use the demonstration to select a composition of tasks learned during\npre-training to perform ICL. In this paper, we empirically explore these three\nhypotheses that explain LLMs' ability to learn in context with a suite of\nexperiments derived from common text classification tasks. We invalidate the\nfirst two hypotheses with counterexamples and provide evidence in support of\nthe last hypothesis. Our results suggest an LLM could learn a novel task in\ncontext via composing tasks learned during pre-training.\n","authors":["Jiaoda Li","Yifan Hou","Mrinmaya Sachan","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.04216v3.pdf","comment":"This work is published in ACL 2024"},{"id":"http://arxiv.org/abs/2310.02812v2","updated":"2024-08-05T15:06:24Z","published":"2023-10-04T13:37:34Z","title":"Time-Series Classification in Smart Manufacturing Systems: An\n  Experimental Evaluation of State-of-the-Art Machine Learning Algorithms","summary":"  Manufacturing is gathering extensive amounts of diverse data, thanks to the\ngrowing number of sensors and rapid advances in sensing technologies. Among the\nvarious data types available in SMS settings, time-series data plays a pivotal\nrole. Hence, TSC emerges is crucial in this domain. The objective of this study\nis to fill this gap by providing a rigorous experimental evaluation of the SoTA\nML and DL algorithms for TSC tasks in manufacturing and industrial settings. We\nfirst explored and compiled a comprehensive list of more than 92 SoTA\nalgorithms from both TSC and manufacturing literature. Following, we selected\nthe 36 most representative algorithms from this list. To evaluate their\nperformance across various manufacturing classification tasks, we curated a set\nof 22 manufacturing datasets, representative of different characteristics that\ncover diverse manufacturing problems. Subsequently, we implemented and\nevaluated the algorithms on the manufacturing benchmark datasets, and analyzed\nthe results for each dataset. Based on the results, ResNet, DrCIF,\nInceptionTime, and ARSENAL are the top-performing algorithms, boasting an\naverage accuracy of over 96.6% across all 22 manufacturing TSC datasets. These\nfindings underscore the robustness, efficiency, scalability, and effectiveness\nof convolutional kernels in capturing temporal features in time-series data, as\nthree out of the top four performing algorithms leverage these kernels for\nfeature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve\nrecognition for their effectiveness in capturing features within time-series\ndata using RNN-based structures.\n","authors":["Mojtaba A. Farahani","M. R. McCormick","Ramy Harik","Thorsten Wuest"],"pdf_url":"https://arxiv.org/pdf/2310.02812v2.pdf","comment":"Published in Robotics and Computer-Integrated Manufacturing journal"},{"id":"http://arxiv.org/abs/2408.02533v1","updated":"2024-08-05T15:03:19Z","published":"2024-08-05T15:03:19Z","title":"LMEMs for post-hoc analysis of HPO Benchmarking","summary":"  The importance of tuning hyperparameters in Machine Learning (ML) and Deep\nLearning (DL) is established through empirical research and applications,\nevident from the increase in new hyperparameter optimization (HPO) algorithms\nand benchmarks steadily added by the community. However, current benchmarking\npractices using averaged performance across many datasets may obscure key\ndifferences between HPO methods, especially for pairwise comparisons. In this\nwork, we apply Linear Mixed-Effect Models-based (LMEMs) significance testing\nfor post-hoc analysis of HPO benchmarking runs. LMEMs allow flexible and\nexpressive modeling on the entire experiment data, including information such\nas benchmark meta-features, offering deeper insights than current analysis\npractices. We demonstrate this through a case study on the PriorBand paper's\nexperiment data to find insights not reported in the original work.\n","authors":["Anton Geburek","Neeratyoy Mallik","Danny Stoll","Xavier Bouthillier","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2408.02533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14294v2","updated":"2024-08-05T14:59:27Z","published":"2024-02-22T05:16:04Z","title":"High-arity PAC learning via exchangeability","summary":"  We develop a theory of high-arity PAC learning, which is statistical learning\nin the presence of \"structured correlation\". In this theory, hypotheses are\neither graphs, hypergraphs or, more generally, structures in finite relational\nlanguages, and i.i.d. sampling is replaced by sampling an induced substructure,\nproducing an exchangeable distribution. Our main theorems establish a\nhigh-arity (agnostic) version of the fundamental theorem of statistical\nlearning.\n","authors":["Leonardo N. Coregliano","Maryanthe Malliaris"],"pdf_url":"https://arxiv.org/pdf/2402.14294v2.pdf","comment":"150 pages, 1 figure. (This version makes expository changes to\n  Sections 1 and 2 and adds Appendix B on Bayes predictors.)"},{"id":"http://arxiv.org/abs/2408.02525v1","updated":"2024-08-05T14:46:04Z","published":"2024-08-05T14:46:04Z","title":"Single-tap Latency Reduction with Single- or Double- tap Prediction","summary":"  Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops\n(touchpad), and single and double taps are the most basic and common operations\non them. The detection of single or double taps causes the single-tap latency\nproblem, which creates a bottleneck in terms of the sensitivity of touch\ninputs. To reduce the single-tap latency, we propose a novel\nmachine-learning-based tap prediction method called PredicTaps. Our method\npredicts whether a detected tap is a single tap or the first contact of a\ndouble tap without having to wait for the hundreds of milliseconds\nconventionally required. We present three evaluations and one user evaluation\nthat demonstrate its broad applicability and usability for various tap\nsituations on two form factors (touchpad and smartphone). The results showed\nPredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops\nand to 17.6 ms on smartphones without reducing usability.\n","authors":["Naoto Nishida","Kaori Ikematsu","Junichi Sato","Shota Yamanaka","Kota Tsubouchi"],"pdf_url":"https://arxiv.org/pdf/2408.02525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02514v1","updated":"2024-08-05T14:34:40Z","published":"2024-08-05T14:34:40Z","title":"Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem\n  Compatibility Estimation","summary":"  This paper explores the automated process of determining stem compatibility\nby identifying audio recordings of single instruments that blend well with a\ngiven musical context. To tackle this challenge, we present Stem-JEPA, a novel\nJoint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset\nusing a self-supervised learning approach.\n  Our model comprises two networks: an encoder and a predictor, which are\njointly trained to predict the embeddings of compatible stems from the\nembeddings of a given context, typically a mix of several instruments. Training\na model in this manner allows its use in estimating stem compatibility -\nretrieving, aligning, or generating a stem to match a given mix - or for\ndownstream tasks such as genre or key estimation, as the training paradigm\nrequires the model to learn information related to timbre, harmony, and rhythm.\n  We evaluate our model's performance on a retrieval task on the MUSDB18\ndataset, testing its ability to find the missing stem from a mix and through a\nsubjective user study. We also show that the learned embeddings capture\ntemporal alignment information and, finally, evaluate the representations\nlearned by our model on several downstream tasks, highlighting that they\neffectively capture meaningful musical features.\n","authors":["Alain Riou","Stefan Lattner","Gatan Hadjeres","Michael Anslow","Geoffroy Peeters"],"pdf_url":"https://arxiv.org/pdf/2408.02514v1.pdf","comment":"Proceedings of the 25th International Society for Music Information\n  Retrieval Conference, ISMIR 2024"},{"id":"http://arxiv.org/abs/2408.02509v1","updated":"2024-08-05T14:31:26Z","published":"2024-08-05T14:31:26Z","title":"Practical Attacks against Black-box Code Completion Engines","summary":"  Modern code completion engines, powered by large language models, have\ndemonstrated impressive capabilities to generate functionally correct code\nbased on surrounding context. As these tools are extensively used by millions\nof developers, it is crucial to investigate their security implications. In\nthis work, we present INSEC, a novel attack that directs code completion\nengines towards generating vulnerable code. In line with most commercial\ncompletion engines, such as GitHub Copilot, INSEC assumes only black-box query\naccess to the targeted engine, without requiring any knowledge of the engine's\ninternals. Our attack works by inserting a malicious attack string as a short\ncomment in the completion input. To derive the attack string, we design a\nseries of specialized initialization schemes and an optimization procedure for\nfurther refinement. We demonstrate the strength of INSEC not only on\nstate-of-the-art open-source models but also on black-box commercial services\nsuch as the OpenAI API and GitHub Copilot. On a comprehensive set of\nsecurity-critical test cases covering 16 CWEs across 5 programming languages,\nINSEC significantly increases the likelihood of the considered completion\nengines in generating unsafe code by >50% in absolute, while maintaining the\nability in producing functionally correct code. At the same time, our attack\nhas low resource requirements, and can be developed for a cost of well under\nten USD on commodity hardware.\n","authors":["Slobodan Jenko","Jingxuan He","Niels Mndler","Mark Vero","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2408.02509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02496v1","updated":"2024-08-05T14:19:03Z","published":"2024-08-05T14:19:03Z","title":"Automatic rating of incomplete hippocampal inversions evaluated across\n  multiple cohorts","summary":"  Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal\nmalrotation, is an atypical anatomical pattern of the hippocampus found in\nabout 20% of the general population. IHI can be visually assessed on coronal\nslices of T1 weighted MR images, using a composite score that combines four\nanatomical criteria. IHI has been associated with several brain disorders\n(epilepsy, schizophrenia). However, these studies were based on small samples.\nFurthermore, the factors (genetic or environmental) that contribute to the\ngenesis of IHI are largely unknown. Large-scale studies are thus needed to\nfurther understand IHI and their potential relationships to neurological and\npsychiatric disorders. However, visual evaluation is long and tedious,\njustifying the need for an automatic method. In this paper, we propose, for the\nfirst time, to automatically rate IHI. We proceed by predicting four anatomical\ncriteria, which are then summed up to form the IHI score, providing the\nadvantage of an interpretable score. We provided an extensive experimental\ninvestigation of different machine learning methods and training strategies. We\nperformed automatic rating using a variety of deep learning models (conv5-FC3,\nResNet and SECNN) as well as a ridge regression. We studied the generalization\nof our models using different cohorts and performed multi-cohort learning. We\nrelied on a large population of 2,008 participants from the IMAGEN study, 993\nand 403 participants from the QTIM/QTAB studies as well as 985 subjects from\nthe UKBiobank. We showed that deep learning models outperformed a ridge\nregression. We demonstrated that the performances of the conv5-FC3 network were\nat least as good as more complex networks while maintaining a low complexity\nand computation time. We showed that training on a single cohort may lack in\nvariability while training on several cohorts improves generalization.\n","authors":["Lisa Hemforth","Baptiste Couvy-Duchesne","Kevin De Matos","Camille Brianceau","Matthieu Joulot","Tobias Banaschewski","Arun L. W. Bokde","Sylvane Desrivires","Herta Flor","Antoine Grigis","Hugh Garavan","Penny Gowland","Andreas Heinz","Rdiger Brhl","Jean-Luc Martinot","Marie-Laure Paillre Martinot","Eric Artiges","Dimitri Papadopoulos","Herve Lemaitre","Tomas Paus","Luise Poustka","Sarah Hohmann","Nathalie Holz","Juliane H. Frhner","Michael N. Smolka","Nilakshi Vaidya","Henrik Walter","Robert Whelan","Gunter Schumann","Christian Bchel","JB Poline","Bernd Itterman","Vincent Frouin","Alexandre Martin","IMAGEN study group","Claire Cury","Olivier Colliot"],"pdf_url":"https://arxiv.org/pdf/2408.02496v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:016"},{"id":"http://arxiv.org/abs/2408.02487v1","updated":"2024-08-05T14:09:30Z","published":"2024-08-05T14:09:30Z","title":"A First Look at License Compliance Capability of LLMs in Code Generation","summary":"  Recent advances in Large Language Models (LLMs) have revolutionized code\ngeneration, leading to widespread adoption of AI coding tools by developers.\nHowever, LLMs can generate license-protected code without providing the\nnecessary license information, leading to potential intellectual property\nviolations during software production. This paper addresses the critical, yet\nunderexplored, issue of license compliance in LLM-generated code by\nestablishing a benchmark to evaluate the ability of LLMs to provide accurate\nlicense information for their generated code. To establish this benchmark, we\nconduct an empirical study to identify a reasonable standard for \"striking\nsimilarity\" that excludes the possibility of independent creation, indicating a\ncopy relationship between the LLM output and certain open-source code. Based on\nthis standard, we propose an evaluation benchmark LiCoEval, to evaluate the\nlicense compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popular\nLLMs, finding that even top-performing LLMs produce a non-negligible proportion\n(0.88% to 2.01%) of code strikingly similar to existing open-source\nimplementations. Notably, most LLMs fail to provide accurate license\ninformation, particularly for code under copyleft licenses. These findings\nunderscore the urgent need to enhance LLM compliance capabilities in code\ngeneration tasks. Our study provides a foundation for future research and\ndevelopment to improve license compliance in AI-assisted software development,\ncontributing to both the protection of open-source software copyrights and the\nmitigation of legal risks for LLM users.\n","authors":["Weiwei Xu","Kai Gao","Hao He","Minghui Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02473v1","updated":"2024-08-05T13:57:32Z","published":"2024-08-05T13:57:32Z","title":"Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture\n  and Automated Deployment Flow","summary":"  One of the challenges for Tiny Machine Learning (tinyML) is keeping up with\nthe evolution of Machine Learning models from Convolutional Neural Networks to\nTransformers. We address this by leveraging a heterogeneous architectural\ntemplate coupling RISC-V processors with hardwired accelerators supported by an\nautomated deployment flow. We demonstrate an Attention-based model in a tinyML\npower envelope with an octa-core cluster coupled with an accelerator for\nquantized Attention. Our deployment flow enables an end-to-end 8-bit\nMobileBERT, achieving leading-edge energy efficiency and throughput of 2960\nGOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOI\ntechnology).\n","authors":["Philip Wiese","Gamze slamolu","Moritz Scherer","Luka Macan","Victor J. B. Jung","Alessio Burrello","Francesco Conti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2408.02473v1.pdf","comment":"Pre-print manuscript submitted for review to the IEEE Design and Test\n  Special Issue on tinyML"},{"id":"http://arxiv.org/abs/2408.02456v1","updated":"2024-08-05T13:28:51Z","published":"2024-08-05T13:28:51Z","title":"Enhancing Heterogeneous Knowledge Graph Completion with a Novel\n  GAT-based Approach","summary":"  Knowledge graphs (KGs) play a vital role in enhancing search results and\nrecommendation systems. With the rapid increase in the size of the KGs, they\nare becoming inaccuracy and incomplete. This problem can be solved by the\nknowledge graph completion methods, of which graph attention network\n(GAT)-based methods stand out since their superior performance. However,\nexisting GAT-based knowledge graph completion methods often suffer from\noverfitting issues when dealing with heterogeneous knowledge graphs, primarily\ndue to the unbalanced number of samples. Additionally, these methods\ndemonstrate poor performance in predicting the tail (head) entity that shares\nthe same relation and head (tail) entity with others. To solve these problems,\nwe propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH\nincorporates two separate attention network modules that work synergistically\nto predict the missing entities. We also introduce novel encoding and feature\ntransformation approaches, enabling the robust performance of GATH in scenarios\nwith imbalanced samples. Comprehensive experiments are conducted to evaluate\nthe GATH's performance. Compared with the existing SOTA GAT-based model on\nHits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the\nFB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.\n","authors":["Wanxu Wei","Yitong Song","Bin Yao"],"pdf_url":"https://arxiv.org/pdf/2408.02456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14244v2","updated":"2024-08-05T12:59:32Z","published":"2024-05-23T07:23:33Z","title":"Tell me why: Training preferences-based RL with human preferences and\n  step-level explanations","summary":"  Human-in-the-loop reinforcement learning allows the training of agents\nthrough various interfaces, even for non-expert humans. Recently,\npreference-based methods (PbRL), where the human has to give his preference\nover two trajectories, increased in popularity since they allow training in\ndomains where more direct feedback is hard to formulate. However, the current\nPBRL methods have limitations and do not provide humans with an expressive\ninterface for giving feedback. With this work, we propose a new\npreference-based learning method that provides humans with a more expressive\ninterface to provide their preference over trajectories and a factual\nexplanation (or annotation of why they have this preference). These\nexplanations allow the human to explain what parts of the trajectory are most\nrelevant for the preference. We allow the expression of the explanations over\nindividual trajectory steps. We evaluate our method in various simulations\nusing a simulated human oracle (with realistic restrictions), and our results\nshow that our extended feedback can improve the speed of learning.\n","authors":["Jakob Karalus"],"pdf_url":"https://arxiv.org/pdf/2405.14244v2.pdf","comment":"Workshop on Reinforcement Learning Beyond Rewards @ Reinforcement\n  Learning Conference (2024)"},{"id":"http://arxiv.org/abs/2202.04309v2","updated":"2024-08-05T12:58:37Z","published":"2022-02-09T06:56:41Z","title":"Vertical Federated Learning: Challenges, Methodologies and Experiments","summary":"  Recently, federated learning (FL) has emerged as a promising distributed\nmachine learning (ML) technology, owing to the advancing computational and\nsensing capacities of end-user devices, however with the increasing concerns on\nusers' privacy. As a special architecture in FL, vertical FL (VFL) is capable\nof constructing a hyper ML model by embracing sub-models from different\nclients. These sub-models are trained locally by vertically partitioned data\nwith distinct attributes. Therefore, the design of VFL is fundamentally\ndifferent from that of conventional FL, raising new and unique research issues.\nIn this paper, we aim to discuss key challenges in VFL with effective\nsolutions, and conduct experiments on real-life datasets to shed light on these\nissues. Specifically, we first propose a general framework on VFL, and\nhighlight the key differences between VFL and conventional FL. Then, we discuss\nresearch challenges rooted in VFL systems under four aspects, i.e., security\nand privacy risks, expensive computation and communication costs, possible\nstructural damage caused by model splitting, and system heterogeneity.\nAfterwards, we develop solutions to addressing the aforementioned challenges,\nand conduct extensive experiments to showcase the effectiveness of our proposed\nsolutions.\n","authors":["Kang Wei","Jun Li","Chuan Ma","Ming Ding","Sha Wei","Fan Wu","Guihai Chen","Thilina Ranbaduge"],"pdf_url":"https://arxiv.org/pdf/2202.04309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02433v1","updated":"2024-08-05T12:46:21Z","published":"2024-08-05T12:46:21Z","title":"On Probabilistic Embeddings in Optimal Dimension Reduction","summary":"  Dimension reduction algorithms are a crucial part of many data science\npipelines, including data exploration, feature creation and selection, and\ndenoising. Despite their wide utilization, many non-linear dimension reduction\nalgorithms are poorly understood from a theoretical perspective. In this work\nwe consider a generalized version of multidimensional scaling, which is posed\nas an optimization problem in which a mapping from a high-dimensional feature\nspace to a lower-dimensional embedding space seeks to preserve either inner\nproducts or norms of the distribution in feature space, and which encompasses\nmany commonly used dimension reduction algorithms. We analytically investigate\nthe variational properties of this problem, leading to the following insights:\n1) Solutions found using standard particle descent methods may lead to\nnon-deterministic embeddings, 2) A relaxed or probabilistic formulation of the\nproblem admits solutions with easily interpretable necessary conditions, 3) The\nglobally optimal solutions to the relaxed problem actually must give a\ndeterministic embedding. This progression of results mirrors the classical\ndevelopment of optimal transportation, and in a case relating to the\nGromov-Wasserstein distance actually gives explicit insight into the structure\nof the optimal embeddings, which are parametrically determined and\ndiscontinuous. Finally, we illustrate that a standard computational\nimplementation of this task does not learn deterministic embeddings, which\nmeans that it learns sub-optimal mappings, and that the embeddings learned in\nthat context have highly misleading clustering structure, underscoring the\ndelicate nature of solving this problem computationally.\n","authors":["Ryan Murray","Adam Pickarski"],"pdf_url":"https://arxiv.org/pdf/2408.02433v1.pdf","comment":"26 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2303.10571v2","updated":"2024-08-05T12:44:04Z","published":"2023-03-19T05:20:52Z","title":"Reinforcement Learning Friendly Vision-Language Model for Minecraft","summary":"  One of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https://github.com/PKU-RL/CLIP4MC.\n","authors":["Haobin Jiang","Junpeng Yue","Hao Luo","Ziluo Ding","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2303.10571v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.19858v2","updated":"2024-08-05T12:42:38Z","published":"2024-07-29T10:26:52Z","title":"AI-Powered Energy Algorithmic Trading: Integrating Hidden Markov Models\n  with Neural Networks","summary":"  In quantitative finance, machine learning methods are essential for alpha\ngeneration. This study introduces a new approach that combines Hidden Markov\nModels (HMM) and neural networks, integrated with Black-Litterman portfolio\noptimization. During the COVID period (2019-2022), this dual-model approach\nachieved a 97% return with a Sharpe ratio of 0.992. It incorporates two risk\nmodels to enhance risk management, showing efficiency during volatile periods.\nThe methodology was implemented on the QuantConnect platform, which was chosen\nfor its robust framework and experimental reproducibility. The system, which\npredicts future price movements, includes a three-year warm-up to ensure proper\nalgorithm function. It targets highly liquid, large-cap energy stocks to ensure\nstable and predictable performance while also considering broker payments. The\ndual-model alpha system utilizes log returns to select the optimal state based\non the historical performance. It combines state predictions with neural\nnetwork outputs, which are based on historical data, to generate trading\nsignals. This study examined the architecture of the trading system, data\npre-processing, training, and performance. The full code and backtesting data\nare available under the MIT license.\n","authors":["Tiago Monteiro"],"pdf_url":"https://arxiv.org/pdf/2407.19858v2.pdf","comment":"14 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.02427v1","updated":"2024-08-05T12:34:49Z","published":"2024-08-05T12:34:49Z","title":"Attenuation-adjusted deep learning of pore defects in 2D radiographs of\n  additive manufacturing powders","summary":"  The presence of gas pores in metal feedstock powder for additive\nmanufacturing greatly affects the final AM product. Since current porosity\nanalysis often involves lengthy X-ray computed tomography (XCT) scans with a\nfull rotation around the sample, motivation exists to explore methods that\nallow for high throughput -- possibly enabling in-line porosity analysis during\nmanufacturing. Through labelling pore pixels on single 2D radiographs of\npowders, this work seeks to simulate such future efficient setups. High\nsegmentation accuracy is achieved by combining a model of X-ray attenuation\nthrough particles with a variant of the widely applied UNet architecture;\nnotably, F1-score increases by $11.4\\%$ compared to the baseline UNet. The\nproposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)\nmaking tight particle cutouts, and 3) subtracting an ideal particle without\npores generated from a distance map inspired by Lambert-Beers law. This paper\nexplores four image processing methods, where the fastest (yet still\nunoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,\nand the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalable\nnature, these strategies can be involved in making high throughput porosity\nanalysis of metal feedstock powder for additive manufacturing.\n","authors":["Andreas Bjerregaard","David Schumacher","Jon Sporring"],"pdf_url":"https://arxiv.org/pdf/2408.02427v1.pdf","comment":"Implementation on https://github.com/yhsure/porosity"},{"id":"http://arxiv.org/abs/2402.12198v2","updated":"2024-08-05T12:20:49Z","published":"2024-02-19T15:03:04Z","title":"Zero shot VLMs for hate meme detection: Are we there yet?","summary":"  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n","authors":["Naquee Rizwan","Paramananda Bhaskar","Mithun Das","Swadhin Satyaprakash Majhi","Punyajoy Saha","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.12198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02412v1","updated":"2024-08-05T12:11:09Z","published":"2024-08-05T12:11:09Z","title":"PENDRAM: Enabling High-Performance and Energy-Efficient Processing of\n  Deep Neural Networks through a Generalized DRAM Data Mapping Policy","summary":"  Convolutional Neural Networks (CNNs), a prominent type of Deep Neural\nNetworks (DNNs), have emerged as a state-of-the-art solution for solving\nmachine learning tasks. To improve the performance and energy efficiency of CNN\ninference, the employment of specialized hardware accelerators is prevalent.\nHowever, CNN accelerators still face performance- and energy-efficiency\nchallenges due to high off-chip memory (DRAM) access latency and energy, which\nare especially crucial for latency- and energy-constrained embedded\napplications. Moreover, different DRAM architectures have different profiles of\naccess latency and energy, thus making it challenging to optimize them for high\nperformance and energy-efficient CNN accelerators. To address this, we present\nPENDRAM, a novel design space exploration methodology that enables\nhigh-performance and energy-efficient CNN acceleration through a generalized\nDRAM data mapping policy. Specifically, it explores the impact of different\nDRAM data mapping policies and DRAM architectures across different CNN\npartitioning and scheduling schemes on the DRAM access latency and energy, then\nidentifies the pareto-optimal design choices. The experimental results show\nthat our DRAM data mapping policy improves the energy-delay-product of DRAM\naccesses in the CNN accelerator over other mapping policies by up to 96%. In\nthis manner, our PENDRAM methodology offers high-performance and\nenergy-efficient CNN acceleration under any given DRAM architectures for\ndiverse embedded AI applications.\n","authors":["Rachmad Vidya Wicaksana Putra","Muhammad Abdullah Hanif","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2408.02412v1.pdf","comment":"11 pages, 15 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2004.10341"},{"id":"http://arxiv.org/abs/2408.02407v1","updated":"2024-08-05T12:01:42Z","published":"2024-08-05T12:01:42Z","title":"Terracorder: Sense Long and Prosper","summary":"  In-situ sensing devices need to be deployed in remote environments for long\nperiods of time; minimizing their power consumption is vital for maximising\nboth their operational lifetime and coverage. We introduce Terracorder -- a\nversatile multi-sensor device -- and showcase its exceptionally low power\nconsumption using an on-device reinforcement learning scheduler. We prototype a\nunique device setup for biodiversity monitoring and compare its battery life\nusing our scheduler against a number of fixed schedules; the scheduler captures\nmore than 80% of events at less than 50% of the number of activations of the\nbest-performing fixed schedule. We then explore how a collaborative scheduler\ncan maximise the useful operation of a network of devices, improving overall\nnetwork power consumption and robustness.\n","authors":["Josh Millar","Sarab Sethi","Hamed Haddadi","Anil Madhavapeddy"],"pdf_url":"https://arxiv.org/pdf/2408.02407v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.05996v3","updated":"2024-08-05T11:55:19Z","published":"2024-03-09T19:56:40Z","title":"Dissecting Deep RL with High Update Ratios: Combatting Value Divergence","summary":"  We show that deep reinforcement learning algorithms can retain their ability\nto learn without resetting network parameters in settings where the number of\ngradient updates greatly exceeds the number of environment samples by\ncombatting value function divergence. Under large update-to-data ratios, a\nrecent study by Nikishin et al. (2022) suggested the emergence of a primacy\nbias, in which agents overfit early interactions and downplay later experience,\nimpairing their ability to learn. In this work, we investigate the phenomena\nleading to the primacy bias. We inspect the early stages of training that were\nconjectured to cause the failure to learn and find that one fundamental\nchallenge is a long-standing acquaintance: value function divergence.\nOverinflated Q-values are found not only on out-of-distribution but also\nin-distribution data and can be linked to overestimation on unseen action\nprediction propelled by optimizer momentum. We employ a simple unit-ball\nnormalization that enables learning under large update ratios, show its\nefficacy on the widely used dm_control suite, and obtain strong performance on\nthe challenging dog tasks, competitive with model-based approaches. Our results\nquestion, in parts, the prior explanation for sub-optimal learning due to\noverfitting early data.\n","authors":["Marcel Hussing","Claas Voelcker","Igor Gilitschenski","Amir-massoud Farahmand","Eric Eaton"],"pdf_url":"https://arxiv.org/pdf/2403.05996v3.pdf","comment":"Accepted as a conference paper at the First Reinforcement Learning\n  Conference (RLC)"},{"id":"http://arxiv.org/abs/2407.19707v3","updated":"2024-08-05T11:22:34Z","published":"2024-07-29T05:05:13Z","title":"Neural networks for bifurcation and linear stability analysis of steady\n  states in partial differential equations","summary":"  This research introduces an extended application of neural networks for\nsolving nonlinear partial differential equations (PDEs). A neural network,\ncombined with a pseudo-arclength continuation, is proposed to construct\nbifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neural\nnetwork approach is also presented for solving eigenvalue problems to analyze\nsolution linear stability, focusing on identifying the largest eigenvalue. The\neffectiveness of the proposed neural network is examined through experiments on\nthe Bratu equation and the Burgers equation. Results from a finite difference\nmethod are also presented as comparison. Varying numbers of grid points are\nemployed in each case to assess the behavior and accuracy of both the neural\nnetwork and the finite difference method. The experimental results demonstrate\nthat the proposed neural network produces better solutions, generates more\naccurate bifurcation diagrams, has reasonable computational times, and proves\neffective for linear stability analysis.\n","authors":["Muhammad Luthfi Shahab","Hadi Susanto"],"pdf_url":"https://arxiv.org/pdf/2407.19707v3.pdf","comment":"Accepted for publication in Applied Mathematics and Computation"},{"id":"http://arxiv.org/abs/2309.14857v2","updated":"2024-08-05T11:20:33Z","published":"2023-09-26T11:35:25Z","title":"Cluster Exploration using Informative Manifold Projections","summary":"  Dimensionality reduction (DR) is one of the key tools for the visual\nexploration of high-dimensional data and uncovering its cluster structure in\ntwo- or three-dimensional spaces. The vast majority of DR methods in the\nliterature do not take into account any prior knowledge a practitioner may have\nregarding the dataset under consideration. We propose a novel method to\ngenerate informative embeddings which not only factor out the structure\nassociated with different kinds of prior knowledge but also aim to reveal any\nremaining underlying structure. To achieve this, we employ a linear combination\nof two objectives: firstly, contrastive PCA that discounts the structure\nassociated with the prior information, and secondly, kurtosis projection\npursuit which ensures meaningful data separation in the obtained embeddings. We\nformulate this task as a manifold optimization problem and validate it\nempirically across a variety of datasets considering three distinct types of\nprior knowledge. Lastly, we provide an automated framework to perform iterative\nvisual exploration of high-dimensional data.\n","authors":["Stavros Gerolymatos","Xenophon Evangelopoulos","Vladimir Gusev","John Y. Goulermas"],"pdf_url":"https://arxiv.org/pdf/2309.14857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02384v1","updated":"2024-08-05T11:16:26Z","published":"2024-08-05T11:16:26Z","title":"Strategic Federated Learning: Application to Smart Meter Data Clustering","summary":"  Federated learning (FL) involves several clients that share with a fusion\ncenter (FC), the model each client has trained with its own data. Conventional\nFL, which can be interpreted as an estimation or distortion-based approach,\nignores the final use of model information (MI) by the FC and the other\nclients. In this paper, we introduce a novel FL framework in which the FC uses\nan aggregate version of the MI to make decisions that affect the client's\nutility functions. Clients cannot choose the decisions and can only use the MI\nreported to the FC to maximize their utility. Depending on the alignment\nbetween the client and FC utilities, the client may have an individual interest\nin adding strategic noise to the model. This general framework is stated and\nspecialized to the case of clustering, in which noisy cluster representative\ninformation is reported. This is applied to the problem of power consumption\nscheduling. In this context, utility non-alignment occurs, for instance, when\nthe client wants to consume when the price of electricity is low, whereas the\nFC wants the consumption to occur when the total power is the lowest. This is\nillustrated with aggregated real data from Ausgrid \\cite{ausgrid}. Our\nnumerical analysis clearly shows that the client can increase his utility by\nadding noise to the model reported to the FC. Corresponding results and source\ncodes can be downloaded from \\cite{source-code}.\n","authors":["Hassan Mohamad","Chao Zhang","Samson Lasaulce","Vineeth S Varma","Mrouane Debbah","Mounir Ghogho"],"pdf_url":"https://arxiv.org/pdf/2408.02384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18103v2","updated":"2024-08-05T11:13:57Z","published":"2024-07-25T15:07:35Z","title":"Fine-Tuning Large Language Models for Stock Return Prediction Using\n  Newsflow","summary":"  Large language models (LLMs) and their fine-tuning techniques have\ndemonstrated superior performance in various language understanding and\ngeneration tasks. This paper explores fine-tuning LLMs for stock return\nforecasting with financial newsflow. In quantitative investing, return\nforecasting is fundamental for subsequent tasks like stock picking, portfolio\noptimization, etc. We formulate the model to include text representation and\nforecasting modules. We propose to compare the encoder-only and decoder-only\nLLMs, considering they generate text representations in distinct ways. The\nimpact of these different representations on forecasting performance remains an\nopen question. Meanwhile, we compare two simple methods of integrating LLMs'\ntoken-level representations into the forecasting module. The experiments on\nreal news and investment universes reveal that: (1) aggregated representations\nfrom LLMs' token-level embeddings generally produce return predictions that\nenhance the performance of long-only and long-short portfolios; (2) in the\nrelatively large investment universe, the decoder LLMs-based prediction model\nleads to stronger portfolios, whereas in the small universes, there are no\nconsistent winners. Among the three LLMs studied (DeBERTa, Mistral, Llama),\nMistral performs more robustly across different universes; (3) return\npredictions derived from LLMs' text representations are a strong signal for\nportfolio construction, outperforming conventional sentiment scores.\n","authors":["Tian Guo","Emmanuel Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2407.18103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02367v1","updated":"2024-08-05T10:32:06Z","published":"2024-08-05T10:32:06Z","title":"StoDIP: Efficient 3D MRF image reconstruction with deep image priors and\n  stochastic iterations","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI for multiparametric tissue mapping. The reconstruction of\nquantitative maps requires tailored algorithms for removing aliasing artefacts\nfrom the compressed sampled MRF acquisitions. Within approaches found in the\nliterature, many focus solely on two-dimensional (2D) image reconstruction,\nneglecting the extension to volumetric (3D) scans despite their higher\nrelevance and clinical value. A reason for this is that transitioning to 3D\nimaging without appropriate mitigations presents significant challenges,\nincluding increased computational cost and storage requirements, and the need\nfor large amount of ground-truth (artefact-free) data for training. To address\nthese issues, we introduce StoDIP, a new algorithm that extends the\nground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging.\nStoDIP employs memory-efficient stochastic updates across the multicoil MRF\ndata, a carefully selected neural network architecture, as well as faster\nnonuniform FFT (NUFFT) transformations. This enables a faster convergence\ncompared against a conventional DIP implementation without these features.\nTested on a dataset of whole-brain scans from healthy volunteers, StoDIP\ndemonstrated superior performance over the ground-truth-free reconstruction\nbaselines, both quantitatively and qualitatively.\n","authors":["Perla Mayo","Matteo Cencini","Carolin M. Pirkl","Marion I. Menzel","Michela Tosetti","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2408.02367v1.pdf","comment":"10 pages, 2 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2407.08583v2","updated":"2024-08-05T10:31:24Z","published":"2024-07-11T15:08:11Z","title":"The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective","summary":"  The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs; on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stages of MLLMs specific\ndata-centric approaches can be employed to enhance certain MLLM capabilities,\nand 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal\ndata in specific roles. To promote the data-model co-development for MLLM\ncommunity, we systematically review existing works related to MLLMs from the\ndata-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.\n","authors":["Zhen Qin","Daoyuan Chen","Wenhao Zhang","Liuyi Yao","Yilun Huang","Bolin Ding","Yaliang Li","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.08583v2.pdf","comment":"Ongoing work. 21 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md"},{"id":"http://arxiv.org/abs/2405.10802v2","updated":"2024-08-05T10:20:11Z","published":"2024-05-17T14:16:40Z","title":"Reduced storage direct tensor ring decomposition for convolutional\n  neural networks compression","summary":"  Convolutional neural networks (CNNs) are among the most widely used machine\nlearning models for computer vision tasks, such as image classification. To\nimprove the efficiency of CNNs, many CNNs compressing approaches have been\ndeveloped. Low-rank methods approximate the original convolutional kernel with\na sequence of smaller convolutional kernels, which leads to reduced storage and\ntime complexities. In this study, we propose a novel low-rank CNNs compression\nmethod that is based on reduced storage direct tensor ring decomposition\n(RSDTR). The proposed method offers a higher circular mode permutation\nflexibility, and it is characterized by large parameter and FLOPS compression\nrates, while preserving a good classification accuracy of the compressed\nnetwork. The experiments, performed on the CIFAR-10 and ImageNet datasets,\nclearly demonstrate the efficiency of RSDTR in comparison to other\nstate-of-the-art CNNs compression approaches.\n","authors":["Mateusz Gabor","Rafa Zdunek"],"pdf_url":"https://arxiv.org/pdf/2405.10802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02361v1","updated":"2024-08-05T10:10:01Z","published":"2024-08-05T10:10:01Z","title":"Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought\n  Decoding","summary":"  State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models.\n","authors":["Renato Vukovic","David Arps","Carel van Niekerk","Benjamin Matthias Ruppik","Hsien-Chin Lin","Michael Heck","Milica Gai"],"pdf_url":"https://arxiv.org/pdf/2408.02361v1.pdf","comment":"Accepted to appear at SIGDIAL 2024. 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02357v1","updated":"2024-08-05T10:06:53Z","published":"2024-08-05T10:06:53Z","title":"On the consistent reasoning paradox of intelligence and optimal trust in\n  AI: The power of 'I don't know'","summary":"  We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning,\nwhich lies at the core of human intelligence, is the ability to handle tasks\nthat are equivalent, yet described by different sentences ('Tell me the time!'\nand 'What is the time?'). The CRP asserts that consistent reasoning implies\nfallibility -- in particular, human-like intelligence in AI necessarily comes\nwith human-like fallibility. Specifically, it states that there are problems,\ne.g. in basic arithmetic, where any AI that always answers and strives to mimic\nhuman intelligence by reasoning consistently will hallucinate (produce wrong,\nyet plausible answers) infinitely often. The paradox is that there exists a\nnon-consistently reasoning AI (which therefore cannot be on the level of human\nintelligence) that will be correct on the same set of problems. The CRP also\nshows that detecting these hallucinations, even in a probabilistic sense, is\nstrictly harder than solving the original problems, and that there are problems\nthat an AI may answer correctly, but it cannot provide a correct logical\nexplanation for how it arrived at the answer. Therefore, the CRP implies that\nany trustworthy AI (i.e., an AI that never answers incorrectly) that also\nreasons consistently must be able to say 'I don't know'. Moreover, this can\nonly be done by implicitly computing a new concept that we introduce, termed\nthe 'I don't know' function -- something currently lacking in modern AI. In\nview of these insights, the CRP also provides a glimpse into the behaviour of\nArtificial General Intelligence (AGI). An AGI cannot be 'almost sure', nor can\nit always explain itself, and therefore to be trustworthy it must be able to\nsay 'I don't know'.\n","authors":["Alexander Bastounis","Paolo Campodonico","Mihaela van der Schaar","Ben Adcock","Anders C. Hansen"],"pdf_url":"https://arxiv.org/pdf/2408.02357v1.pdf","comment":"12 pages and 50 pages of supplementary material, 7 figures"},{"id":"http://arxiv.org/abs/2408.02355v1","updated":"2024-08-05T10:02:33Z","published":"2024-08-05T10:02:33Z","title":"Quantile Regression using Random Forest Proximities","summary":"  Due to the dynamic nature of financial markets, maintaining models that\nproduce precise predictions over time is difficult. Often the goal isn't just\npoint prediction but determining uncertainty. Quantifying uncertainty,\nespecially the aleatoric uncertainty due to the unpredictable nature of market\ndrivers, helps investors understand varying risk levels. Recently, quantile\nregression forests (QRF) have emerged as a promising solution: Unlike most\nbasic quantile regression methods that need separate models for each quantile,\nquantile regression forests estimate the entire conditional distribution of the\ntarget variable with a single model, while retaining all the salient features\nof a typical random forest. We introduce a novel approach to compute quantile\nregressions from random forests that leverages the proximity (i.e., distance\nmetric) learned by the model and infers the conditional distribution of the\ntarget variable. We evaluate the proposed methodology using publicly available\ndatasets and then apply it towards the problem of forecasting the average daily\nvolume of corporate bonds. We show that using quantile regression using Random\nForest proximities demonstrates superior performance in approximating\nconditional target distributions and prediction intervals to the original\nversion of QRF. We also demonstrate that the proposed framework is\nsignificantly more computationally efficient than traditional approaches to\nquantile regressions.\n","authors":["Mingshu Li","Bhaskarjit Sarmah","Dhruv Desai","Joshua Rosaler","Snigdha Bhagat","Philip Sommer","Dhagash Mehta"],"pdf_url":"https://arxiv.org/pdf/2408.02355v1.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.13185v2","updated":"2024-08-05T10:01:48Z","published":"2024-01-24T02:16:03Z","title":"Fast Partition-Based Cross-Validation With Centering and Scaling for\n  $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$","summary":"  We present algorithms that substantially accelerate partition-based\ncross-validation for machine learning models that require matrix products\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. Our\nalgorithms have applications in model selection for, e.g., principal component\nanalysis (PCA), principal component regression (PCR), ridge regression (RR),\nordinary least squares (OLS), and partial least squares (PLS). Our algorithms\nsupport all combinations of column-wise centering and scaling of $\\mathbf{X}$\nand $\\mathbf{Y}$, and we demonstrate in our accompanying implementation that\nthis adds only a manageable, practical constant over efficient variants without\npreprocessing. We prove the correctness of our algorithms under a fold-based\npartitioning scheme and show that the running time is independent of the number\nof folds; that is, they have the same time complexity as that of computing\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ and\nspace complexity equivalent to storing $\\mathbf{X}$, $\\mathbf{Y}$,\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$, and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$.\nImportantly, unlike alternatives found in the literature, we avoid data leakage\ndue to preprocessing. We achieve these results by eliminating redundant\ncomputations in the overlap between training partitions. Concretely, we show\nhow to manipulate $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and\n$\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ using only samples from the validation\npartition to obtain the preprocessed training partition-wise\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. To our\nknowledge, we are the first to derive correct and efficient cross-validation\nalgorithms for any of the $16$ combinations of column-wise centering and\nscaling, for which we also prove only $12$ give distinct matrix products.\n","authors":["Ole-Christian Galbo Engstrm","Martin Holm Jensen"],"pdf_url":"https://arxiv.org/pdf/2401.13185v2.pdf","comment":"31 pages, 2 tables, 1 figure, 7 algorithms"},{"id":"http://arxiv.org/abs/2408.02349v1","updated":"2024-08-05T09:54:08Z","published":"2024-08-05T09:54:08Z","title":"Active Sensing of Knee Osteoarthritis Progression with Reinforcement\n  Learning","summary":"  Osteoarthritis (OA) is the most common musculoskeletal disease, which has no\ncure. Knee OA (KOA) is one of the highest causes of disability worldwide, and\nit costs billions of United States dollars to the global community. Prediction\nof KOA progression has been of high interest to the community for years, as it\ncan advance treatment development through more efficient clinical trials and\nimprove patient outcomes through more efficient healthcare utilization.\nExisting approaches for predicting KOA, however, are predominantly static, i.e.\nconsider data from a single time point to predict progression many years into\nthe future, and knee level, i.e. consider progression in a single joint only.\nDue to these and related reasons, these methods fail to deliver the level of\npredictive performance, which is sufficient to result in cost savings and\nbetter patient outcomes. Collecting extensive data from all patients on a\nregular basis could address the issue, but it is limited by the high cost at a\npopulation level. In this work, we propose to go beyond static prediction\nmodels in OA, and bring a novel Active Sensing (AS) approach, designed to\ndynamically follow up patients with the objective of maximizing the number of\ninformative data acquisitions, while minimizing their total cost over a period\nof time. Our approach is based on Reinforcement Learning (RL), and it leverages\na novel reward function designed specifically for AS of disease progression in\nmore than one part of a human body. Our method is end-to-end, relies on\nmulti-modal Deep Learning, and requires no human input at inference time.\nThroughout an exhaustive experimental evaluation, we show that using RL can\nprovide a higher monetary benefit when compared to state-of-the-art baselines.\n","authors":["Khanh Nguyen","Huy Hoang Nguyen","Egor Panfilov","Aleksei Tiulpin"],"pdf_url":"https://arxiv.org/pdf/2408.02349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02346v1","updated":"2024-08-05T09:45:31Z","published":"2024-08-05T09:45:31Z","title":"Exploiting Hankel-Toeplitz Structures for Fast Computation of Kernel\n  Precision Matrices","summary":"  The Hilbert-space Gaussian Process (HGP) approach offers a\nhyperparameter-independent basis function approximation for speeding up\nGaussian Process (GP) inference by projecting the GP onto M basis functions.\nThese properties result in a favorable data-independent $\\mathcal{O}(M^3)$\ncomputational complexity during hyperparameter optimization but require a\ndominating one-time precomputation of the precision matrix costing\n$\\mathcal{O}(NM^2)$ operations. In this paper, we lower this dominating\ncomputational complexity to $\\mathcal{O}(NM)$ with no additional\napproximations. We can do this because we realize that the precision matrix can\nbe split into a sum of Hankel-Toeplitz matrices, each having $\\mathcal{O}(M)$\nunique entries. Based on this realization we propose computing only these\nunique entries at $\\mathcal{O}(NM)$ costs. Further, we develop two theorems\nthat prescribe sufficient conditions for the complexity reduction to hold\ngenerally for a wide range of other approximate GP models, such as the\nVariational Fourier Feature (VFF) approach. The two theorems do this with no\nassumptions on the data and no additional approximations of the GP models\nthemselves. Thus, our contribution provides a pure speed-up of several\nexisting, widely used, GP approximations, without further approximations.\n","authors":["Frida Viset","Anton Kullberg","Frederiek Wesel","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2408.02346v1.pdf","comment":"Published in Transactions on Machine Learning (TMLR) July 2024"},{"id":"http://arxiv.org/abs/2408.02344v1","updated":"2024-08-05T09:41:34Z","published":"2024-08-05T09:41:34Z","title":"Machine Learning Applications in Medical Prognostics: A Comprehensive\n  Review","summary":"  Machine learning (ML) has revolutionized medical prognostics by integrating\nadvanced algorithms with clinical data to enhance disease prediction, risk\nassessment, and patient outcome forecasting. This comprehensive review\ncritically examines the application of various ML techniques in medical\nprognostics, focusing on their efficacy, challenges, and future directions. The\nmethodologies discussed include Random Forest (RF) for sepsis prediction,\nlogistic regression for cardiovascular risk assessment, Convolutional Neural\nNetworks (CNNs) for cancer detection, and Long Short-Term Memory (LSTM)\nnetworks for predicting clinical deterioration. RF models demonstrate robust\nperformance in handling high-dimensional data and capturing non-linear\nrelationships, making them particularly effective for sepsis prediction.\nLogistic regression remains valuable for its interpretability and ease of use\nin cardiovascular risk assessment. CNNs have shown exceptional accuracy in\ncancer detection, leveraging their ability to learn complex visual patterns\nfrom medical imaging. LSTM networks excel in analyzing temporal data, providing\naccurate predictions of clinical deterioration. The review highlights the\nstrengths and limitations of each technique, the importance of model\ninterpretability, and the challenges of data quality and privacy. Future\nresearch directions include the integration of multi-modal data sources, the\napplication of transfer learning, and the development of continuous learning\nsystems. These advancements aim to enhance the predictive power and clinical\napplicability of ML models, ultimately improving patient outcomes in healthcare\nsettings.\n","authors":["Michael Fascia"],"pdf_url":"https://arxiv.org/pdf/2408.02344v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2408.02337v1","updated":"2024-08-05T09:23:49Z","published":"2024-08-05T09:23:49Z","title":"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR\n  Dataset Construction","summary":"  Advancements in AI and natural language processing have revolutionized\nmachine-human language interactions, with question answering (QA) systems\nplaying a pivotal role. The knowledge base question answering (KBQA) task,\nutilizing structured knowledge graphs (KG), allows for handling extensive\nknowledge-intensive questions. However, a significant gap exists in KBQA\ndatasets, especially for low-resource languages. Many existing construction\npipelines for these datasets are outdated and inefficient in human labor, and\nmodern assisting tools like Large Language Models (LLM) are not utilized to\nreduce the workload. To address this, we have designed and implemented a\nmodern, semi-automated approach for creating datasets, encompassing tasks such\nas KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),\ntailored explicitly for low-resource environments. We executed this pipeline\nand introduced the PUGG dataset, the first Polish KBQA dataset, and novel\ndatasets for MRC and IR. Additionally, we provide a comprehensive\nimplementation, insightful findings, detailed statistics, and evaluation of\nbaseline models.\n","authors":["Albert Sawczyn","Katsiaryna Viarenich","Konrad Wojtasik","Aleksandra Domogaa","Marcin Oleksy","Maciej Piasecki","Tomasz Kajdanowicz"],"pdf_url":"https://arxiv.org/pdf/2408.02337v1.pdf","comment":"Accepted for ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2408.02320v1","updated":"2024-08-05T09:02:24Z","published":"2024-08-05T09:02:24Z","title":"A Sharp Convergence Theory for The Probability Flow ODEs of Diffusion\n  Models","summary":"  Diffusion models, which convert noise into new data instances by learning to\nreverse a diffusion process, have become a cornerstone in contemporary\ngenerative modeling. In this work, we develop non-asymptotic convergence theory\nfor a popular diffusion-based sampler (i.e., the probability flow ODE sampler)\nin discrete time, assuming access to $\\ell_2$-accurate estimates of the (Stein)\nscore functions. For distributions in $\\mathbb{R}^d$, we prove that\n$d/\\varepsilon$ iterations -- modulo some logarithmic and lower-order terms --\nare sufficient to approximate the target distribution to within $\\varepsilon$\ntotal-variation distance. This is the first result establishing nearly linear\ndimension-dependency (in $d$) for the probability flow ODE sampler. Imposing\nonly minimal assumptions on the target data distribution (e.g., no smoothness\nassumption is imposed), our results also characterize how $\\ell_2$ score\nestimation errors affect the quality of the data generation processes. In\ncontrast to prior works, our theory is developed based on an elementary yet\nversatile non-asymptotic approach without the need of resorting to SDE and ODE\ntoolboxes.\n","authors":["Gen Li","Yuting Wei","Yuejie Chi","Yuxin Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02320v1.pdf","comment":"This manuscript presents improved theory for probability flow ODEs\n  compared to its earlier version arXiv:2306.09251"},{"id":"http://arxiv.org/abs/2308.09605v2","updated":"2024-08-05T08:53:12Z","published":"2023-08-18T14:58:23Z","title":"Solving PDEs on Spheres with Physics-Informed Convolutional Neural\n  Networks","summary":"  Physics-informed neural networks (PINNs) have been demonstrated to be\nefficient in solving partial differential equations (PDEs) from a variety of\nexperimental perspectives. Some recent studies have also proposed PINN\nalgorithms for PDEs on surfaces, including spheres. However, theoretical\nunderstanding of the numerical performance of PINNs, especially PINNs on\nsurfaces or manifolds, is still lacking. In this paper, we establish rigorous\nanalysis of the physics-informed convolutional neural network (PICNN) for\nsolving PDEs on the sphere. By using and improving the latest approximation\nresults of deep convolutional neural networks and spherical harmonic analysis,\nwe prove an upper bound for the approximation error with respect to the Sobolev\nnorm. Subsequently, we integrate this with innovative localization complexity\nanalysis to establish fast convergence rates for PICNN. Our theoretical results\nare also confirmed and supplemented by our experiments. In light of these\nfindings, we explore potential strategies for circumventing the curse of\ndimensionality that arises when solving high-dimensional PDEs.\n","authors":["Guanhang Lei","Zhen Lei","Lei Shi","Chenyu Zeng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2308.09605v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02313v1","updated":"2024-08-05T08:46:46Z","published":"2024-08-05T08:46:46Z","title":"A Lean Transformer Model for Dynamic Malware Analysis and Detection","summary":"  Malware is a fast-growing threat to the modern computing world and existing\nlines of defense are not efficient enough to address this issue. This is mainly\ndue to the fact that many prevention solutions rely on signature-based\ndetection methods that can easily be circumvented by hackers. Therefore, there\nis a recurrent need for behavior-based analysis where a suspicious file is ran\nin a secured environment and its traces are collected to reports for analysis.\nPrevious works have shown some success leveraging Neural Networks and API calls\nsequences extracted from these execution reports.\n  Recently, Large Language Models and Generative AI have demonstrated\nimpressive capabilities mainly in Natural Language Processing tasks and\npromising applications in the cybersecurity field for both attackers and\ndefenders.\n  In this paper, we design an Encoder-Only model, based on the Transformers\narchitecture, to detect malicious files, digesting their API call sequences\ncollected by an execution emulation solution. We are also limiting the size of\nthe model architecture and the number of its parameters since it is often\nconsidered that Large Language Models may be overkill for specific tasks such\nas the one we are dealing with hereafter. In addition to achieving decent\ndetection results, this approach has the advantage of reducing our carbon\nfootprint by limiting training and inference times and facilitating technical\noperations with less hardware requirements.\n  We also carry out some analysis of our results and highlight the limits and\npossible improvements when using Transformers to analyze malicious files.\n","authors":["Tony Quertier","Benjamin Marais","Grgoire Barru","Stphane Morucci","Svan Az","Sbastien Salladin"],"pdf_url":"https://arxiv.org/pdf/2408.02313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02312v1","updated":"2024-08-05T08:45:50Z","published":"2024-08-05T08:45:50Z","title":"Optimization of Iterative Blind Detection based on Expectation\n  Maximization and Belief Propagation","summary":"  We study iterative blind symbol detection for block-fading linear\ninter-symbol interference channels. Based on the factor graph framework, we\ndesign a joint channel estimation and detection scheme that combines the\nexpectation maximization (EM) algorithm and the ubiquitous belief propagation\n(BP) algorithm. Interweaving the iterations of both schemes significantly\nreduces the EM algorithm's computational burden while retaining its excellent\nperformance. To this end, we apply simple yet effective model-based learning\nmethods to find a suitable parameter update schedule by introducing momentum in\nboth the EM parameter updates as well as in the BP message passing. Numerical\nsimulations verify that the proposed method can learn efficient schedules that\ngeneralize well and even outperform coherent BP detection in high\nsignal-to-noise scenarios.\n","authors":["Luca Schmid","Tomer Raviv","Nir Shlezinger","Laurent Schmalen"],"pdf_url":"https://arxiv.org/pdf/2408.02312v1.pdf","comment":"Accepted for presentation at Asilomar Conference on Signals, Systems,\n  and Computers 2024"},{"id":"http://arxiv.org/abs/2408.02310v1","updated":"2024-08-05T08:41:07Z","published":"2024-08-05T08:41:07Z","title":"On the Robustness of Malware Detectors to Adversarial Samples","summary":"  Adversarial examples add imperceptible alterations to inputs with the\nobjective to induce misclassification in machine learning models. They have\nbeen demonstrated to pose significant challenges in domains like image\nclassification, with results showing that an adversarially perturbed image to\nevade detection against one classifier is most likely transferable to other\nclassifiers. Adversarial examples have also been studied in malware analysis.\nUnlike images, program binaries cannot be arbitrarily perturbed without\nrendering them non-functional. Due to the difficulty of crafting adversarial\nprogram binaries, there is no consensus on the transferability of adversarially\nperturbed programs to different detectors. In this work, we explore the\nrobustness of malware detectors against adversarially perturbed malware. We\ninvestigate the transferability of adversarial attacks developed against one\ndetector, against other machine learning-based malware detectors, and code\nsimilarity techniques, specifically, locality sensitive hashing-based\ndetectors. Our analysis reveals that adversarial program binaries crafted for\none detector are generally less effective against others. We also evaluate an\nensemble of detectors and show that they can potentially mitigate the impact of\nadversarial program binaries. Finally, we demonstrate that substantial program\nchanges made to evade detection may result in the transformation technique\nbeing identified, implying that the adversary must make minimal changes to the\nprogram binary.\n","authors":["Muhammad Salman","Benjamin Zi Hao Zhao","Hassan Jameel Asghar","Muhammad Ikram","Sidharth Kaushik","Mohamed Ali Kaafar"],"pdf_url":"https://arxiv.org/pdf/2408.02310v1.pdf","comment":"This is the full version of the paper with the same title to appear\n  in the proceedings of the 2024 Workshop on Security and Artificial\n  Intelligence (SECAI 2024)"},{"id":"http://arxiv.org/abs/2408.02301v1","updated":"2024-08-05T08:23:59Z","published":"2024-08-05T08:23:59Z","title":"Network Fission Ensembles for Low-Cost Self-Ensembles","summary":"  Recent ensemble learning methods for image classification have been shown to\nimprove classification accuracy with low extra cost. However, they still\nrequire multiple trained models for ensemble inference, which eventually\nbecomes a significant burden when the model size increases. In this paper, we\npropose a low-cost ensemble learning and inference, called Network Fission\nEnsembles (NFE), by converting a conventional network itself into a multi-exit\nstructure. Starting from a given initial network, we first prune some of the\nweights to reduce the training burden. We then group the remaining weights into\nseveral sets and create multiple auxiliary paths using each set to construct\nmulti-exits. We call this process Network Fission. Through this, multiple\noutputs can be obtained from a single network, which enables ensemble learning.\nSince this process simply changes the existing network structure to multi-exits\nwithout using additional networks, there is no extra computational burden for\nensemble learning and inference. Moreover, by learning from multiple losses of\nall exits, the multi-exits improve performance via regularization, and high\nperformance can be achieved even with increased network sparsity. With our\nsimple yet effective method, we achieve significant improvement compared to\nexisting ensemble methods. The code is available at\nhttps://github.com/hjdw2/NFE.\n","authors":["Hojung Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10504v2","updated":"2024-08-05T08:22:56Z","published":"2024-02-16T08:27:55Z","title":"On the resilience of the quadratic Littlewood-Offord problem","summary":"  We study the statistical resilience of the anti-concentration properties of\nRademacher polynomials in face of adversarial deterministic noise taking the\nform of sign-flips. Given a multilinear polynomial $f:\\mathbb{R}^n \\to\n\\mathbb{R}$ and a Rademacher vector $\\boldsymbol{\\xi} \\in \\{\\pm 1\\}^n$ (with\nindependent entries), our results provide probabilistic lower bound estimations\non the number of sign-flips that $\\boldsymbol{\\xi}$ can sustain without\n``inflating\" the atom probability $\\sup_{x \\in \\mathbb{R} }\n\\mathbb{P}\\{f(\\boldsymbol{\\xi}) = x\\}$ otherwise resulting in an adversarially\nbiased distribution. Special emphasis is put on bilinear and quadratic forms,\nfor which strengthened estimates are attained. From a computational\nperspective, our results in this venue are instance-bound in such a way that\nallows for an efficient computation of the statistical resilience guarantees\nfrom the quadratic polynomial itself directly. All of our probabilistic lower\nbound resilience guarantees are asymptotically tight.\n  On route, we provide a short proof for a new small-ball probability estimate\nfitting Rademacher multilinear polynomials $f: \\mathbb{R}^n \\to \\mathbb{R}$\nremoveing a polylog-factor from the classical Meka-Nguyen-Vu bound provided the\ncoefficients are independent of $n$ (dimension-free, hereafter). This removal\nwas conjectured to be possible by Meka-Nguyen-Vu regardless of our assumption.\nBilinear Rademacher forms with dimension-free coefficients arise naturally in\nCombinatorics and specifically in the dense case of the edge-statistics\nconjecture posed by Alon, Hefetz, Krivelevich, and Tyomkyn. This case of the\nconjecture was resolved by Kwan and Sauermann. Replacing the appeal to the\nMeka-Nguyen-Vu classical bound in the work of Kwan, Sudakov, and Tran with our\nshortly proved result attains an additional proof of the dense case of the\nedge-statistics conjecture.\n","authors":["Elad Aigner-Horev","Daniel Rosenberg","Roi Weiss"],"pdf_url":"https://arxiv.org/pdf/2402.10504v2.pdf","comment":"Numerous changes from the last version: 1. An oversight in the proof\n  fixed. 2. Added treatment of high degree polynomials 3. New results added"},{"id":"http://arxiv.org/abs/2408.02298v1","updated":"2024-08-05T08:14:32Z","published":"2024-08-05T08:14:32Z","title":"Backward Compatibility in Attributive Explanation and Enhanced Model\n  Training Method","summary":"  Model update is a crucial process in the operation of ML/AI systems. While\nupdating a model generally enhances the average prediction performance, it also\nsignificantly impacts the explanations of predictions. In real-world\napplications, even minor changes in explanations can have detrimental\nconsequences. To tackle this issue, this paper introduces BCX, a quantitative\nmetric that evaluates the backward compatibility of feature attribution\nexplanations between pre- and post-update models. BCX utilizes practical\nagreement metrics to calculate the average agreement between the explanations\nof pre- and post-update models, specifically among samples on which both models\naccurately predict. In addition, we propose BCXR, a BCX-aware model training\nmethod by designing surrogate losses which theoretically lower bounds agreement\nscores. Furthermore, we present a universal variant of BCXR that improves all\nagreement metrics, utilizing L2 distance among the explanations of the models.\nTo validate our approach, we conducted experiments on eight real-world\ndatasets, demonstrating that BCXR achieves superior trade-offs between\npredictive performances and BCX scores, showcasing the effectiveness of our\nBCXR methods.\n","authors":["Ryuta Matsuno"],"pdf_url":"https://arxiv.org/pdf/2408.02298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02296v1","updated":"2024-08-05T08:14:04Z","published":"2024-08-05T08:14:04Z","title":"Heart Rate and its Variability from Short-term ECG Recordings as\n  Biomarkers for Detecting Mild Cognitive Impairment in Indian Population","summary":"  Alterations in Heart Rate (HR) and Heart Rate Variability (HRV) can reflect\nautonomic dysfunction associated with neurodegeneration. We investigate the\ninfluence of Mild Cognitive Impairment (MCI) on HR and its variability measures\nin the Indian population by designing a complete signal processing pipeline to\ndetect the R-wave peaks and compute HR and HRV features from ECG recordings of\n10 seconds, for point-of-care applications. The study cohort involves 297 urban\nparticipants, among which 48.48% are male and 51.51% are female. From the\nAddenbrooke's Cognitive Examination-III (ACE-III), MCI is detected in 19.19% of\nparticipants and the rest, 80.8% of them are cognitively healthy. Statistical\nfeatures like central tendency (mean and root mean square (RMS) of the\nNormal-to-Normal (NN) intervals) and dispersion (standard deviation (SD) of all\nNN intervals (SDNN) and root mean square of successive differences of NN\nintervals (RMSSD)) of beat-to-beat intervals are computed. The Wilcoxon rank\nsum test reveals that mean of NN intervals (p = 0.0021), the RMS of NN\nintervals (p = 0.0014), the SDNN (p = 0.0192) and the RMSSD (p = 0.0206) values\ndiffer significantly between MCI and non-MCI classes, for a level of\nsignificance, 0.05. Machine learning classifiers like, Support Vector Machine\n(SVM), Discriminant Analysis (DA) and Naive Bayes (NB) driven by mean NN\nintervals, RMS, SDNN and RMSSD, show a high accuracy of 80.80% on each\nindividual feature input. Individuals with MCI are observed to have\ncomparatively higher HR than healthy subjects. HR and its variability can be\nconsidered as potential biomarkers for detecting MCI.\n","authors":["Anjo Xavier","Sneha Noble","Justin Joseph","Thomas Gregor Issac"],"pdf_url":"https://arxiv.org/pdf/2408.02296v1.pdf","comment":"Nil"},{"id":"http://arxiv.org/abs/2408.02295v1","updated":"2024-08-05T08:12:25Z","published":"2024-08-05T08:12:25Z","title":"Generalized Gaussian Temporal Difference Error For Uncertainty-aware\n  Reinforcement Learning","summary":"  Conventional uncertainty-aware temporal difference (TD) learning methods\noften rely on simplistic assumptions, typically including a zero-mean Gaussian\ndistribution for TD errors. Such oversimplification can lead to inaccurate\nerror representations and compromised uncertainty estimation. In this paper, we\nintroduce a novel framework for generalized Gaussian error modeling in deep\nreinforcement learning, applicable to both discrete and continuous control\nsettings. Our framework enhances the flexibility of error distribution modeling\nby incorporating higher-order moments, particularly kurtosis, thereby improving\nthe estimation and mitigation of data-dependent noise, i.e., aleatoric\nuncertainty. We examine the influence of the shape parameter of the generalized\nGaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form\nexpression that demonstrates an inverse relationship between uncertainty and\nthe shape parameter. Additionally, we propose a theoretically grounded\nweighting scheme to fully leverage the GGD. To address epistemic uncertainty,\nwe enhance the batch inverse variance weighting by incorporating bias reduction\nand kurtosis considerations, resulting in improved robustness. Extensive\nexperimental evaluations using policy gradient algorithms demonstrate the\nconsistent efficacy of our method, showcasing significant performance\nimprovements.\n","authors":["Seyeon Kim","Joonhun Lee","Namhoon Cho","Sungjun Han","Seungeon Baek"],"pdf_url":"https://arxiv.org/pdf/2408.02295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16458v2","updated":"2024-08-05T07:59:19Z","published":"2024-01-29T10:11:05Z","title":"Credit Risk Meets Large Language Models: Building a Risk Indicator from\n  Loan Descriptions in P2P Lending","summary":"  Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,\nlinking borrowers with lenders through online platforms. However, P2P lending\nfaces the challenge of information asymmetry, as lenders often lack sufficient\ndata to assess the creditworthiness of borrowers. This paper proposes a novel\napproach to address this issue by leveraging the textual descriptions provided\nby borrowers during the loan application process. Our methodology involves\nprocessing these textual descriptions using a Large Language Model (LLM), a\npowerful tool capable of discerning patterns and semantics within the text.\nTransfer learning is applied to adapt the LLM to the specific task at hand.\n  Our results derived from the analysis of the Lending Club dataset show that\nthe risk score generated by BERT, a widely used LLM, significantly improves the\nperformance of credit risk classifiers. However, the inherent opacity of\nLLM-based systems, coupled with uncertainties about potential biases,\nunderscores critical considerations for regulatory frameworks and engenders\ntrust-related concerns among end-users, opening new avenues for future research\nin the dynamic landscape of P2P lending and artificial intelligence.\n","authors":["Mario Sanz-Guerrero","Javier Arroyo"],"pdf_url":"https://arxiv.org/pdf/2401.16458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09780v2","updated":"2024-08-05T07:56:33Z","published":"2024-02-15T08:09:17Z","title":"TinyCL: An Efficient Hardware Architecture for Continual Learning on\n  Autonomous Systems","summary":"  The Continuous Learning (CL) paradigm consists of continuously evolving the\nparameters of the Deep Neural Network (DNN) model to progressively learn to\nperform new tasks without reducing the performance on previous tasks, i.e.,\navoiding the so-called catastrophic forgetting. However, the DNN parameter\nupdate in CL-based autonomous systems is extremely resource-hungry. The\nexisting DNN accelerators cannot be directly employed in CL because they only\nsupport the execution of the forward propagation. Only a few prior\narchitectures execute the backpropagation and weight update, but they lack the\ncontrol and management for CL. Towards this, we design a hardware architecture,\nTinyCL, to perform CL on resource-constrained autonomous systems. It consists\nof a processing unit that executes both forward and backward propagation, and a\ncontrol unit that manages memory-based CL workload. To minimize the memory\naccesses, the sliding window of the convolutional layer moves in a snake-like\nfashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at\nruntime to execute different operations. As per our knowledge, our proposed\nTinyCL represents the first hardware accelerator that executes CL on autonomous\nsystems. We synthesize the complete TinyCL architecture in a 65 nm CMOS\ntechnology node with the conventional ASIC design flow. It executes 1 epoch of\ntraining on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while\n1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,\nthus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.\n","authors":["Eugenio Ressa","Alberto Marchisio","Maurizio Martina","Guido Masera","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2402.09780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.02963v3","updated":"2024-08-05T07:55:34Z","published":"2022-06-07T01:49:22Z","title":"Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding","summary":"  Knowledge Graph Embedding (KGE), which projects entities and relations into\ncontinuous vector spaces, has garnered significant attention. Although\nhigh-dimensional KGE methods offer better performance, they come at the expense\nof significant computation and memory overheads. Decreasing embedding\ndimensions significantly deteriorates model performance. While several recent\nefforts utilize knowledge distillation or non-Euclidean representation learning\nto augment the effectiveness of low-dimensional KGE, they either necessitate a\npre-trained high-dimensional teacher model or involve complex non-Euclidean\noperations, thereby incurring considerable additional computational costs. To\naddress this, this work proposes Confidence-aware Self-Knowledge Distillation\n(CSD) that learns from the model itself to enhance KGE in a low-dimensional\nspace. Specifically, CSD extracts knowledge from embeddings in previous\niterations, which would be utilized to supervise the learning of the model in\nthe next iterations. Moreover, a specific semantic module is developed to\nfilter reliable knowledge by estimating the confidence of previously learned\nembeddings. This straightforward strategy bypasses the need for time-consuming\npre-training of teacher models and can be integrated into various KGE methods\nto improve their performance. Our comprehensive experiments on six KGE\nbackbones and four datasets underscore the effectiveness of the proposed CSD.\n","authors":["Yichen Liu","Jiawei Chen","Defang Chen","Zhehui Zhou","Yan Feng","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2206.02963v3.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2408.02280v1","updated":"2024-08-05T07:30:18Z","published":"2024-08-05T07:30:18Z","title":"Hardware Aware Ensemble Selection for Balancing Predictive Accuracy and\n  Cost","summary":"  Automated Machine Learning (AutoML) significantly simplifies the deployment\nof machine learning models by automating tasks from data preprocessing to model\nselection to ensembling. AutoML systems for tabular data often employ post hoc\nensembling, where multiple models are combined to improve predictive accuracy.\nThis typically results in longer inference times, a major limitation in\npractical deployments. Addressing this, we introduce a hardware-aware ensemble\nselection approach that integrates inference time into post hoc ensembling. By\nleveraging an existing framework for ensemble selection with quality diversity\noptimization, our method evaluates ensemble candidates for their predictive\naccuracy and hardware efficiency. This dual focus allows for a balanced\nconsideration of accuracy and operational efficiency. Thus, our approach\nenables practitioners to choose from a Pareto front of accurate and efficient\nensembles. Our evaluation using 83 classification datasets shows that our\napproach sustains competitive accuracy and can significantly improve ensembles'\noperational efficiency. The results of this study provide a foundation for\nextending these principles to additional hardware constraints, setting the\nstage for the development of more resource-efficient AutoML systems.\n","authors":["Jannis Maier","Felix Mller","Lennart Purucker"],"pdf_url":"https://arxiv.org/pdf/2408.02280v1.pdf","comment":"Accepted at Third International Conference on Automated Machine\n  Learning (AutoML 2024), Workshop Track; for code, see\n  https://github.com/Atraxus/HA-ES"},{"id":"http://arxiv.org/abs/2408.02279v1","updated":"2024-08-05T07:26:47Z","published":"2024-08-05T07:26:47Z","title":"DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for\n  Long Time-Series Forecasting","summary":"  Long-term time series forecasting (LTSF) has been widely applied in finance,\ntraffic prediction, and other domains. Recently, patch-based transformers have\nemerged as a promising approach, segmenting data into sub-level patches that\nserve as input tokens. However, existing methods mostly rely on predetermined\npatch lengths, necessitating expert knowledge and posing challenges in\ncapturing diverse characteristics across various scales. Moreover, time series\ndata exhibit diverse variations and fluctuations across different temporal\nscales, which traditional approaches struggle to model effectively. In this\npaper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm\nto capture diverse receptive fields and sparse patterns of time series data. In\norder to build hierarchical receptive fields, we develop a multi-scale\nTransformer model, coupled with multi-scale sequence extraction, capable of\ncapturing multi-resolution features. Additionally, we introduce a group-aware\nrotary position encoding technique to enhance intra- and inter-group position\nawareness among representations across different temporal scales. Our proposed\nmodel, named DRFormer, is evaluated on various real-world datasets, and\nexperimental results demonstrate its superiority compared to existing methods.\nOur code is available at: https://github.com/ruixindingECNU/DRFormer.\n","authors":["Ruixin Ding","Yuqi Chen","Yu-Ting Lan","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02266v1","updated":"2024-08-05T06:47:32Z","published":"2024-08-05T06:47:32Z","title":"One-Shot Collaborative Data Distillation","summary":"  Large machine-learning training datasets can be distilled into small\ncollections of informative synthetic data samples. These synthetic sets support\nefficient model learning and reduce the communication cost of data sharing.\nThus, high-fidelity distilled data can support the efficient deployment of\nmachine learning applications in distributed network environments. A naive way\nto construct a synthetic set in a distributed environment is to allow each\nclient to perform local data distillation and to merge local distillations at a\ncentral server. However, the quality of the resulting set is impaired by\nheterogeneity in the distributions of the local data held by clients. To\novercome this challenge, we introduce the first collaborative data distillation\ntechnique, called CollabDM, which captures the global distribution of the data\nand requires only a single round of communication between client and server.\nOur method outperforms the state-of-the-art one-shot learning method on skewed\ndata in distributed learning environments. We also show the promising practical\nbenefits of our method when applied to attack detection in 5G networks.\n","authors":["Rayne Holland","Chandra Thapa","Sarah Ali Siddiqui","Wei Shao","Seyit Camtepe"],"pdf_url":"https://arxiv.org/pdf/2408.02266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02247v1","updated":"2024-08-05T05:41:16Z","published":"2024-08-05T05:41:16Z","title":"Contrastive Learning and Abstract Concepts: The Case of Natural Numbers","summary":"  Contrastive Learning (CL) has been successfully applied to classification and\nother downstream tasks related to concrete concepts, such as objects contained\nin the ImageNet dataset. No attempts seem to have been made so far in applying\nthis promising scheme to more abstract entities. A prominent example of these\ncould be the concept of (discrete) Quantity. CL can be frequently interpreted\nas a self-supervised scheme guided by some profound and ubiquitous conservation\nprinciple (e.g. conservation of identity in object classification tasks). In\nthis introductory work we apply a suitable conservation principle to the\nsemi-abstract concept of natural numbers by which discrete quantities can be\nestimated or predicted. We experimentally show, by means of a toy problem, that\ncontrastive learning can be trained to count at a glance with high accuracy\nboth at human as well as at super-human ranges.. We compare this with the\nresults of a trained-to-count at a glance supervised learning (SL) neural\nnetwork scheme of similar architecture. We show that both schemes exhibit\nsimilar good performance on baseline experiments, where the distributions of\nthe training and testing stages are equal. Importantly, we demonstrate that in\nsome generalization scenarios, where training and testing distributions differ,\nCL boasts more robust and much better error performance.\n","authors":["Daniel N. Nissani"],"pdf_url":"https://arxiv.org/pdf/2408.02247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02242v1","updated":"2024-08-05T05:27:19Z","published":"2024-08-05T05:27:19Z","title":"Methods to improve run time of hydrologic models: opportunities and\n  challenges in the machine learning era","summary":"  The application of Machine Learning (ML) to hydrologic modeling is fledgling.\nIts applicability to capture the dependencies on watersheds to forecast better\nwithin a short period is fascinating. One of the key reasons to adopt ML\nalgorithms over physics-based models is its computational efficiency advantage\nand flexibility to work with various data sets. The diverse applications,\nparticularly in emergency response and expanding over a large scale, demand the\nhydrological model in a short time and make researchers adopt data-driven\nmodeling approaches unhesitatingly. In this work, in the era of ML and deep\nlearning (DL), how it can help to improve the overall run time of physics-based\nmodel and potential constraints that should be addressed while modeling. This\npaper covers the opportunities and challenges of adopting ML for hydrological\nmodeling and subsequently how it can help to improve the simulation time of\nphysics-based models and future works that should be addressed.\n","authors":["Supath Dhital"],"pdf_url":"https://arxiv.org/pdf/2408.02242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03560v3","updated":"2024-08-05T04:35:48Z","published":"2022-12-07T10:25:59Z","title":"SeqLink: A Robust Neural-ODE Architecture for Modelling Partially\n  Observed Time Series","summary":"  Ordinary Differential Equations (ODE) based models have become popular as\nfoundation models for solving many time series problems. Combining neural ODEs\nwith traditional RNN models has provided the best representation for irregular\ntime series. However, ODE-based models typically require the trajectory of\nhidden states to be defined based on either the initial observed value or the\nmost recent observation, raising questions about their effectiveness when\ndealing with longer sequences and extended time intervals. In this article, we\nexplore the behaviour of the ODE models in the context of time series data with\nvarying degrees of sparsity. We introduce SeqLink, an innovative neural\narchitecture designed to enhance the robustness of sequence representation.\nUnlike traditional approaches that solely rely on the hidden state generated\nfrom the last observed value, SeqLink leverages ODE latent representations\nderived from multiple data samples, enabling it to generate robust data\nrepresentations regardless of sequence length or data sparsity level. The core\nconcept behind our model is the definition of hidden states for the unobserved\nvalues based on the relationships between samples (links between sequences).\nThrough extensive experiments on partially observed synthetic and real-world\ndatasets, we demonstrate that SeqLink improves the modelling of intermittent\ntime series, consistently outperforming state-of-the-art approaches.\n","authors":["Futoon M. Abushaqra","Hao Xue","Yongli Ren","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2212.03560v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02223v1","updated":"2024-08-05T03:54:52Z","published":"2024-08-05T03:54:52Z","title":"Large Language Model Aided QoS Prediction for Service Recommendation","summary":"  Large language models (LLMs) have seen rapid improvement in the recent years,\nand are used in a wider range of applications. After being trained on large\ntext corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. Our proposed model is shown to overcome the data\nsparsity issue for QoS prediction. We show that on the WSDream dataset, llmQoS\noutperforms comparable baseline models consistently.\n","authors":["Huiying Liu","Zekun Zhang","Qilin Wu","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.00861v2","updated":"2024-08-05T03:47:54Z","published":"2022-06-02T03:48:29Z","title":"Dynamic Structure Estimation from Bandit Feedback using Nonvanishing\n  Exponential Sums","summary":"  This work tackles the dynamic structure estimation problems for periodically\nbehaved discrete dynamical system in the Euclidean space. We assume the\nobservations become sequentially available in a form of bandit feedback\ncontaminated by a sub-Gaussian noise. Under such fairly general assumptions on\nthe noise distribution, we carefully identify a set of recoverable information\nof periodic structures. Our main results are the (computation and sample)\nefficient algorithms that exploit asymptotic behaviors of exponential sums to\neffectively average out the noise effect while preventing the information to be\nestimated from vanishing. In particular, the novel use of the Weyl sum, a\nvariant of exponential sums, allows us to extract spectrum information for\nlinear systems. We provide sample complexity bounds for our algorithms, and we\nexperimentally validate our theoretical claims on simulations of toy examples,\nincluding Cellular Automata.\n","authors":["Motoya Ohnishi","Isao Ishikawa","Yuko Kuroki","Masahiro Ikeda"],"pdf_url":"https://arxiv.org/pdf/2206.00861v2.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02217v1","updated":"2024-08-05T03:38:38Z","published":"2024-08-05T03:38:38Z","title":"Climate-Driven Doubling of Maize Loss Probability in U.S. Crop\n  Insurance: Spatiotemporal Prediction and Possible Policy Responses","summary":"  Climate change not only threatens agricultural producers but also strains\nfinancial institutions. These important food system actors include government\nentities tasked with both insuring grower livelihoods and supporting response\nto continued global warming. We use an artificial neural network to predict\nfuture maize yields in the U.S. Corn Belt, finding alarming changes to\ninstitutional risk exposure within the Federal Crop Insurance Program.\nSpecifically, our machine learning method anticipates more frequent and more\nsevere yield losses that would result in the annual probability of Yield\nProtection (YP) claims to more than double at mid-century relative to\nsimulations without continued climate change. Furthermore, our dual finding of\nrelatively unchanged average yields paired with decreasing yield stability\nreveals targeted opportunities to adjust coverage formulas to include\nvariability. This important structural shift may help regulators support grower\nadaptation to continued climate change by recognizing the value of\nrisk-reducing strategies such as regenerative agriculture. Altogether, paired\nwith open source interactive tools for deeper investigation, our risk profile\nsimulations fill an actionable gap in current understanding, bridging granular\nhistoric yield estimation and climate-informed prediction of future\ninsurer-relevant loss.\n","authors":["A Samuel Pottinger","Lawson Connor","Brookie Guzder-Williams","Maya Weltman-Fahs","Timothy Bowles"],"pdf_url":"https://arxiv.org/pdf/2408.02217v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00657v2","updated":"2024-08-05T03:25:01Z","published":"2024-08-01T15:46:22Z","title":"Disentangling Dense Embeddings with Sparse Autoencoders","summary":"  Sparse autoencoders (SAEs) have shown promise in extracting interpretable\nfeatures from complex neural networks. We present one of the first applications\nof SAEs to dense text embeddings from large language models, demonstrating\ntheir effectiveness in disentangling semantic concepts. By training SAEs on\nembeddings of over 420,000 scientific paper abstracts from computer science and\nastronomy, we show that the resulting sparse representations maintain semantic\nfidelity while offering interpretability. We analyse these learned features,\nexploring their behaviour across different model capacities and introducing a\nnovel method for identifying ``feature families'' that represent related\nconcepts at varying levels of abstraction. To demonstrate the practical utility\nof our approach, we show how these interpretable features can be used to\nprecisely steer semantic search, allowing for fine-grained control over query\nsemantics. This work bridges the gap between the semantic richness of dense\nembeddings and the interpretability of sparse representations. We open source\nour embeddings, trained sparse autoencoders, and interpreted features, as well\nas a web app for exploring them.\n","authors":["Charles O'Neill","Christine Ye","Kartheik Iyer","John F. Wu"],"pdf_url":"https://arxiv.org/pdf/2408.00657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02903v4","updated":"2024-08-05T03:24:09Z","published":"2023-10-04T15:42:23Z","title":"FroSSL: Frobenius Norm Minimization for Efficient Multiview\n  Self-Supervised Learning","summary":"  Self-supervised learning (SSL) is a popular paradigm for representation\nlearning. Recent multiview methods can be classified as sample-contrastive,\ndimension-contrastive, or asymmetric network-based, with each family having its\nown approach to avoiding informational collapse. While these families converge\nto solutions of similar quality, it can be empirically shown that some methods\nare epoch-inefficient and require longer training to reach a target\nperformance. Two main approaches to improving efficiency are covariance\neigenvalue regularization and using more views. However, these two approaches\nare difficult to combine due to the computational complexity of computing\neigenvalues. We present the objective function FroSSL which reconciles both\napproaches while avoiding eigendecomposition entirely. FroSSL works by\nminimizing covariance Frobenius norms to avoid collapse and minimizing\nmean-squared error to achieve augmentation invariance. We show that FroSSL\nreaches competitive accuracies more quickly than any other SSL method and\nprovide theoretical and empirical support that this faster convergence is due\nto how FroSSL affects the eigenvalues of the embedding covariance matrices. We\nalso show that FroSSL learns competitive representations on linear probe\nevaluation when used to train a ResNet-18 on several datasets, including\nSTL-10, Tiny ImageNet, and ImageNet-100.\n","authors":["Oscar Skean","Aayush Dhakal","Nathan Jacobs","Luis Gonzalo Sanchez Giraldo"],"pdf_url":"https://arxiv.org/pdf/2310.02903v4.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.02208v1","updated":"2024-08-05T03:17:44Z","published":"2024-08-05T03:17:44Z","title":"Multi-level Traffic-Responsive Tilt Camera Surveillance through\n  Predictive Correlated Online Learning","summary":"  In urban traffic management, the primary challenge of dynamically and\nefficiently monitoring traffic conditions is compounded by the insufficient\nutilization of thousands of surveillance cameras along the intelligent\ntransportation system. This paper introduces the multi-level Traffic-responsive\nTilt Camera surveillance system (TTC-X), a novel framework designed for dynamic\nand efficient monitoring and management of traffic in urban networks. By\nleveraging widely deployed pan-tilt-cameras (PTCs), TTC-X overcomes the\nlimitations of a fixed field of view in traditional surveillance systems by\nproviding mobilized and 360-degree coverage. The innovation of TTC-X lies in\nthe integration of advanced machine learning modules, including a\ndetector-predictor-controller structure, with a novel Predictive Correlated\nOnline Learning (PiCOL) methodology and the Spatial-Temporal Graph Predictor\n(STGP) for real-time traffic estimation and PTC control. The TTC-X is tested\nand evaluated under three experimental scenarios (e.g., maximum traffic flow\ncapture, dynamic route planning, traffic state estimation) based on a\nsimulation environment calibrated using real-world traffic data in Brooklyn,\nNew York. The experimental results showed that TTC-X captured over 60\\% total\nnumber of vehicles at the network level, dynamically adjusted its route\nrecommendation in reaction to unexpected full-lane closure events, and\nreconstructed link-level traffic states with best MAE less than 1.25\nvehicle/hour. Demonstrating scalability, cost-efficiency, and adaptability,\nTTC-X emerges as a powerful solution for urban traffic management in both\ncyber-physical and real-world environments.\n","authors":["Tao Li","Zilin Bian","Haozhe Lei","Fan Zuo","Ya-Ting Yang","Quanyan Zhu","Zhenning Li","Kaan Ozbay"],"pdf_url":"https://arxiv.org/pdf/2408.02208v1.pdf","comment":"Accepted to Transportation Research Part C special issue: Modelling,\n  Learning, and Control of Conventional, Cooperative and Automated Motorway and\n  Urban Traffic Systems"},{"id":"http://arxiv.org/abs/2408.02201v1","updated":"2024-08-05T03:05:02Z","published":"2024-08-05T03:05:02Z","title":"Evaluating the Performance of Large Language Models for SDG Mapping\n  (Technical Report)","summary":"  The use of large language models (LLMs) is expanding rapidly, and open-source\nversions are becoming available, offering users safer and more adaptable\noptions. These models enable users to protect data privacy by eliminating the\nneed to provide data to third parties and can be customized for specific tasks.\nIn this study, we compare the performance of various language models on the\nSustainable Development Goal (SDG) mapping task, using the output of GPT-4o as\nthe baseline. The selected open-source models for comparison include Mixtral,\nLLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more\nspecialized version of GPT-4o, was included to extend the comparison. Given the\nmulti-label nature of the SDG mapping task, we employed metrics such as F1\nscore, precision, and recall with micro-averaging to evaluate different aspects\nof the models' performance. These metrics are derived from the confusion matrix\nto ensure a comprehensive evaluation. We provide a clear observation and\nanalysis of each model's performance by plotting curves based on F1 score,\nprecision, and recall at different thresholds. According to the results of this\nexperiment, LLaMA 2 and Gemma still have significant room for improvement. The\nother four models do not exhibit particularly large differences in performance.\nThe outputs from all seven models are available on Zenodo:\nhttps://doi.org/10.5281/zenodo.12789375.\n","authors":["Hui Yin","Amir Aryani","Nakul Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.02201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02198v1","updated":"2024-08-05T02:50:58Z","published":"2024-08-05T02:50:58Z","title":"Synergistic Learning with Multi-Task DeepONet for Efficient PDE Problem\n  Solving","summary":"  Multi-task learning (MTL) is an inductive transfer mechanism designed to\nleverage useful information from multiple tasks to improve generalization\nperformance compared to single-task learning. It has been extensively explored\nin traditional machine learning to address issues such as data sparsity and\noverfitting in neural networks. In this work, we apply MTL to problems in\nscience and engineering governed by partial differential equations (PDEs).\nHowever, implementing MTL in this context is complex, as it requires\ntask-specific modifications to accommodate various scenarios representing\ndifferent physical processes. To this end, we present a multi-task deep\noperator network (MT-DeepONet) to learn solutions across various functional\nforms of source terms in a PDE and multiple geometries in a single concurrent\ntraining session. We introduce modifications in the branch network of the\nvanilla DeepONet to account for various functional forms of a parameterized\ncoefficient in a PDE. Additionally, we handle parameterized geometries by\nintroducing a binary mask in the branch network and incorporating it into the\nloss term to improve convergence and generalization to new geometry tasks. Our\napproach is demonstrated on three benchmark problems: (1) learning different\nfunctional forms of the source term in the Fisher equation; (2) learning\nmultiple geometries in a 2D Darcy Flow problem and showcasing better transfer\nlearning capabilities to new geometries; and (3) learning 3D parameterized\ngeometries for a heat transfer problem and demonstrate the ability to predict\non new but similar geometries. Our MT-DeepONet framework offers a novel\napproach to solving PDE problems in engineering and science under a unified\numbrella based on synergistic learning that reduces the overall training cost\nfor neural operators.\n","authors":["Varun Kumar","Somdatta Goswami","Katiana Kontolati","Michael D. Shields","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2408.02198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00380v2","updated":"2024-08-05T02:45:50Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that Stain Normalized Pathology Foundational Model achieves excellent\nperformance relative to the number of WSIs used and the model's parameter\ncount. This suggests that the application of stain normalization has\nsubstantially improved the model's efficiency and generalization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02193v1","updated":"2024-08-05T02:38:48Z","published":"2024-08-05T02:38:48Z","title":"CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs","summary":"  Large language models (LLMs) have shown great potential in code-related\ntasks, yet open-source models lag behind their closed-source counterparts. To\nbridge this performance gap, existing methods generate vast amounts of\nsynthetic data for fine-tuning, leading to inefficiencies in training.\nMotivated by the need for more effective and efficient training, we propose the\nCode Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces\nthe Complexity and Diversity Aware Sampling (CDAS) method to select\nhigh-quality training data based on complexity and diversity, and the Dynamic\nPack padding strategy to reduce computational resource usage by minimizing\npadding tokens during training. Experimental results demonstrate that\nCodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,\nachieves an 8.6% performance increase on HumanEval, reduces training time by\n78%, and decreases peak GPU memory usage by 27%. These findings underscore\nCodeACT's ability to enhance the performance and efficiency of open-source\nmodels. By optimizing both the data selection and training processes, CodeACT\noffers a comprehensive approach to improving the capabilities of open-source\nLLMs while significantly reducing computational requirements, addressing the\ndual challenges of data quality and training efficiency, and paving the way for\nmore resource-efficient and performant models.\n","authors":["Weijie Lv","Xuan Xia","Sheng-Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2408.02193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02977v4","updated":"2024-08-05T01:24:52Z","published":"2024-02-05T12:58:29Z","title":"Variational Flow Models: Flowing in Your Style","summary":"  We propose a systematic training-free method to transform the probability\nflow of a \"linear\" stochastic process characterized by the equation\nX_{t}=a_{t}X_{0}+\\sigma_{t}X_{1} into a straight constant-speed (SC) flow,\nreminiscent of Rectified Flow. This transformation facilitates fast sampling\nalong the original probability flow via the Euler method without training a new\nmodel of the SC flow. The flexibility of our approach allows us to extend our\ntransformation to inter-convert two posterior flows of two distinct linear\nstochastic processes. Moreover, we can easily integrate high-order numerical\nsolvers into the transformed SC flow, further enhancing the sampling accuracy\nand efficiency. Rigorous theoretical analysis and extensive experimental\nresults substantiate the advantages of our framework. Our code is available at\nthis [https://github.com/clarken92/VFM||link].\n","authors":["Kien Do","Duc Kieu","Toan Nguyen","Dang Nguyen","Hung Le","Dung Nguyen","Thin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2402.02977v4.pdf","comment":"Our code is available at: https://github.com/clarken92/VFM"},{"id":"http://arxiv.org/abs/2302.00857v2","updated":"2024-08-05T01:10:35Z","published":"2023-02-02T04:02:49Z","title":"Algorithm Design for Online Meta-Learning with Task Boundary Detection","summary":"  Online meta-learning has recently emerged as a marriage between batch\nmeta-learning and online learning, for achieving the capability of quick\nadaptation on new tasks in a lifelong manner. However, most existing approaches\nfocus on the restrictive setting where the distribution of the online tasks\nremains fixed with known task boundaries. In this work, we relax these\nassumptions and propose a novel algorithm for task-agnostic online\nmeta-learning in non-stationary environments. More specifically, we first\npropose two simple but effective detection mechanisms of task switches and\ndistribution shift based on empirical observations, which serve as a key\nbuilding block for more elegant online model updates in our algorithm: the task\nswitch detection mechanism allows reusing of the best model available for the\ncurrent task at hand, and the distribution shift detection mechanism\ndifferentiates the meta model update in order to preserve the knowledge for\nin-distribution tasks and quickly learn the new knowledge for\nout-of-distribution tasks. In particular, our online meta model updates are\nbased only on the current data, which eliminates the need of storing previous\ndata as required in most existing methods. We further show that a sublinear\ntask-averaged regret can be achieved for our algorithm under mild conditions.\nEmpirical studies on three different benchmarks clearly demonstrate the\nsignificant advantage of our algorithm over related baseline approaches.\n","authors":["Daouda Sow","Sen Lin","Yingbin Liang","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.00857v2.pdf","comment":"CPAL 2024"},{"id":"http://arxiv.org/abs/2402.08225v3","updated":"2024-08-05T00:54:02Z","published":"2024-02-13T05:33:35Z","title":"Improving Black-box Robustness with In-Context Rewriting","summary":"  Machine learning models for text classification often excel on\nin-distribution (ID) data but struggle with unseen out-of-distribution (OOD)\ninputs. Most techniques for improving OOD robustness are not applicable to\nsettings where the model is effectively a black box, such as when the weights\nare frozen, retraining is costly, or the model is leveraged via an API.\nTest-time augmentation (TTA) is a simple post-hoc technique for improving\nrobustness that sidesteps black-box constraints by aggregating predictions\nacross multiple augmentations of the test input. TTA has seen limited use in\nNLP due to the challenge of generating effective natural language\naugmentations. In this work, we propose LLM-TTA, which uses LLM-generated\naugmentations as TTA's augmentation function. LLM-TTA outperforms conventional\naugmentation functions across sentiment, toxicity, and news classification\ntasks for BERT and T5 models, with BERT's OOD robustness improving by an\naverage of 4.48 percentage points without regressing average ID performance. We\nexplore selectively augmenting inputs based on prediction entropy to reduce the\nrate of expensive LLM augmentations, allowing us to maintain performance gains\nwhile reducing the average number of generated augmentations by 57.74\\%.\nLLM-TTA is agnostic to the task model architecture, does not require OOD\nlabels, and is effective across low and high-resource settings. We share our\ndata, models, and code for reproducibility.\n","authors":["Kyle O'Brien","Nathan Ng","Isha Puri","Jorge Mendez","Hamid Palangi","Yoon Kim","Marzyeh Ghassemi","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2402.08225v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02866v1","updated":"2024-08-05T23:33:24Z","published":"2024-08-05T23:33:24Z","title":"Back-Projection Diffusion: Solving the Wideband Inverse Scattering\n  Problem with Diffusion Models","summary":"  We present \\textit{Wideband back-projection diffusion}, an end-to-end\nprobabilistic framework for approximating the posterior distribution induced by\nthe inverse scattering map from wideband scattering data. This framework\nleverages conditional diffusion models coupled with the underlying physics of\nwave-propagation and symmetries in the problem, to produce highly accurate\nreconstructions. The framework introduces a factorization of the score function\ninto a physics-based latent representation inspired by the filtered\nback-propagation formula and a conditional score function conditioned on this\nlatent representation. These two steps are also constrained to obey symmetries\nin the formulation while being amenable to compression by imposing the rank\nstructure found in the filtered back-projection formula. As a result,\nempirically, our framework is able to provide sharp reconstructions\neffortlessly, even recovering sub-Nyquist features in the multiple-scattering\nregime. It has low-sample and computational complexity, its number of\nparameters scales sub-linearly with the target resolution, and it has stable\ntraining dynamics.\n","authors":["Borong Zhang","Martn Guerra","Qin Li","Leonardo Zepeda-Nez"],"pdf_url":"https://arxiv.org/pdf/2408.02866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09797v2","updated":"2024-08-05T23:23:14Z","published":"2023-07-19T07:31:37Z","title":"Probabilistic Forecasting with Coherent Aggregation","summary":"  Obtaining accurate probabilistic forecasts is an important operational\nchallenge in many applications, perhaps most obviously in energy management,\nclimate forecasting, supply chain planning, and resource allocation. In many of\nthese applications, there is a natural hierarchical structure over the\nforecasted quantities; and forecasting systems that adhere to this hierarchical\nstructure are said to be coherent. Furthermore, operational planning benefits\nfrom accuracy at all levels of the aggregation hierarchy. Building accurate and\ncoherent forecasting systems, however, is challenging: classic multivariate\ntime series tools and neural network methods are still being adapted for this\npurpose. In this paper, we augment an MQForecaster neural network architecture\nwith a novel deep Gaussian factor forecasting model that achieves coherence by\nconstruction, yielding a method we call the Deep Coherent Factor Model Neural\nNetwork (DeepCoFactor) model. DeepCoFactor generates samples that can be\ndifferentiated with respect to model parameters, allowing optimization on\nvarious sample-based learning objectives that align with the forecasting\nsystem's goals, including quantile loss and the scaled Continuous Ranked\nProbability Score (CRPS). In a comparison to state-of-the-art coherent\nforecasting methods, DeepCoFactor achieves significant improvements in scaled\nCRPS forecast accuracy, with gains between 4.16 and 54.40%, as measured on\nthree publicly available hierarchical forecasting datasets.\n","authors":["Kin G. Olivares","Geoffrey Ngiar","Ruijun Ma","O. Nangba Meetei","Mengfei Cao","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2307.09797v2.pdf","comment":"10 pages of main text. Updated method and results"},{"id":"http://arxiv.org/abs/2408.02861v1","updated":"2024-08-05T23:20:32Z","published":"2024-08-05T23:20:32Z","title":"A Framework for Fine-Tuning LLMs using Heterogeneous Feedback","summary":"  Large language models (LLMs) have been applied to a wide range of tasks,\nincluding text summarization, web navigation, and chatbots. They have\nbenefitted from supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) following an unsupervised pretraining. These datasets can\nbe difficult to collect, limited in scope, and vary in sample quality.\nAdditionally, datasets can vary extensively in supervision format, from\nnumerical to binary as well as multi-dimensional with many different values. We\npresent a framework for fine-tuning LLMs using heterogeneous feedback, which\nhas two main components. First, we combine the heterogeneous feedback data into\na single supervision format, compatible with methods like SFT and RLHF. Next,\ngiven this unified feedback dataset, we extract a high-quality and diverse\nsubset to obtain performance increases potentially exceeding the full dataset.\nWe conduct extensive experiments to understand the effectiveness of these\ntechniques for incorporating heterogeneous feedback, and demonstrate\nimprovements from using a high-quality and diverse subset of the data. We find\nthat our framework is able to improve models in multiple areas simultaneously,\nsuch as in instruction following and bias reduction.\n","authors":["Ryan Aponte","Ryan A. Rossi","Shunan Guo","Franck Dernoncourt","Tong Yu","Xiang Chen","Subrata Mitra","Nedim Lipka"],"pdf_url":"https://arxiv.org/pdf/2408.02861v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2405.18373v2","updated":"2024-08-05T22:25:10Z","published":"2024-05-28T17:11:34Z","title":"A Hessian-Aware Stochastic Differential Equation for Modelling SGD","summary":"  Continuous-time approximation of Stochastic Gradient Descent (SGD) is a\ncrucial tool to study its escaping behaviors from stationary points. However,\nexisting stochastic differential equation (SDE) models fail to fully capture\nthese behaviors, even for simple quadratic objectives. Built on a novel\nstochastic backward error analysis framework, we derive the Hessian-Aware\nStochastic Modified Equation (HA-SME), an SDE that incorporates Hessian\ninformation of the objective function into both its drift and diffusion terms.\nOur analysis shows that HA-SME matches the order-best approximation error\nguarantee among existing SDE models in the literature, while achieving a\nsignificantly reduced dependence on the smoothness parameter of the objective.\nFurther, for quadratic objectives, under mild conditions, HA-SME is proved to\nbe the first SDE model that recovers exactly the SGD dynamics in the\ndistributional sense. Consequently, when the local landscape near a stationary\npoint can be approximated by quadratics, HA-SME is expected to accurately\npredict the local escaping behaviors of SGD.\n","authors":["Xiang Li","Zebang Shen","Liang Zhang","Niao He"],"pdf_url":"https://arxiv.org/pdf/2405.18373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02849v1","updated":"2024-08-05T22:19:01Z","published":"2024-08-05T22:19:01Z","title":"Active Learning for WBAN-based Health Monitoring","summary":"  We consider a novel active learning problem motivated by the need of learning\nmachine learning models for health monitoring in wireless body area network\n(WBAN). Due to the limited resources at body sensors, collecting each unlabeled\nsample in WBAN incurs a nontrivial cost. Moreover, training health monitoring\nmodels typically requires labels indicating the patient's health state that\nneed to be generated by healthcare professionals, which cannot be obtained at\nthe same pace as data collection. These challenges make our problem\nfundamentally different from classical active learning, where unlabeled samples\nare free and labels can be queried in real time. To handle these challenges, we\npropose a two-phased active learning method, consisting of an online phase\nwhere a coreset construction algorithm is proposed to select a subset of\nunlabeled samples based on their noisy predictions, and an offline phase where\nthe selected samples are labeled to train the target model. The samples\nselected by our algorithm are proved to yield a guaranteed error in\napproximating the full dataset in evaluating the loss function. Our evaluation\nbased on real health monitoring data and our own experimentation demonstrates\nthat our solution can drastically save the data curation cost without\nsacrificing the quality of the target model.\n","authors":["Cho-Chun Chiu","Tuan Nguyen","Ting He","Shiqiang Wang","Beom-Su Kim","Ki-Il Kim"],"pdf_url":"https://arxiv.org/pdf/2408.02849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02845v1","updated":"2024-08-05T22:01:13Z","published":"2024-08-05T22:01:13Z","title":"Heterogeneous graph attention network improves cancer multiomics\n  integration","summary":"  The increase in high-dimensional multiomics data demands advanced integration\nmodels to capture the complexity of human diseases. Graph-based deep learning\nintegration models, despite their promise, struggle with small patient cohorts\nand high-dimensional features, often applying independent feature selection\nwithout modeling relationships among omics. Furthermore, conventional\ngraph-based omics models focus on homogeneous graphs, lacking multiple types of\nnodes and edges to capture diverse structures. We introduce a Heterogeneous\nGraph ATtention network for omics integration (HeteroGATomics) to improve\ncancer diagnosis. HeteroGATomics performs joint feature selection through a\nmulti-agent system, creating dedicated networks of feature and patient\nsimilarity for each omic modality. These networks are then combined into one\nheterogeneous graph for learning holistic omic-specific representations and\nintegrating predictions across modalities. Experiments on three cancer\nmultiomics datasets demonstrate HeteroGATomics' superior performance in cancer\ndiagnosis. Moreover, HeteroGATomics enhances interpretability by identifying\nimportant biomarkers contributing to the diagnosis outcomes.\n","authors":["Sina Tabakhi","Charlotte Vandermeulen","Ian Sudbery","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2408.02845v1.pdf","comment":"29 pages, 13 figures"},{"id":"http://arxiv.org/abs/2401.16803v3","updated":"2024-08-05T21:55:11Z","published":"2024-01-30T07:50:32Z","title":"PBSCR: The Piano Bootleg Score Composer Recognition Dataset","summary":"  This article motivates, describes, and presents the PBSCR dataset for\nstudying composer recognition of classical piano music. Our goal was to design\na dataset that facilitates large-scale research on composer recognition that is\nsuitable for modern architectures and training practices. To achieve this goal,\nwe utilize the abundance of sheet music images and rich metadata on IMSLP, use\na previously proposed feature representation called a bootleg score to encode\nthe location of noteheads relative to staff lines, and present the data in an\nextremely simple format (2D binary images) to encourage rapid exploration and\niteration. The dataset itself contains 40,000 62x64 bootleg score images for a\n9-class recognition task, 100,000 62x64 bootleg score images for a 100-class\nrecognition task, and 29,310 unlabeled variable-length bootleg score images for\npretraining. The labeled data is presented in a form that mirrors MNIST images,\nin order to make it extremely easy to visualize, manipulate, and train models\nin an efficient manner. We include relevant information to connect each bootleg\nscore image with its underlying raw sheet music image, and we scrape, organize,\nand compile metadata from IMSLP on all piano works to facilitate multimodal\nresearch and allow for convenient linking to other datasets. We release\nbaseline results in a supervised and low-shot setting for future works to\ncompare against, and we discuss open research questions that the PBSCR data is\nespecially well suited to facilitate research on.\n","authors":["Arhan Jain","Alec Bunn","Austin Pham","TJ Tsai"],"pdf_url":"https://arxiv.org/pdf/2401.16803v3.pdf","comment":"19 pages, 6 figures, to be published in Transactions of the\n  International Society for Music Information Retrieval"},{"id":"http://arxiv.org/abs/2309.08569v2","updated":"2024-08-05T21:54:54Z","published":"2023-09-15T17:35:51Z","title":"Local Differential Privacy in Graph Neural Networks: a Reconstruction\n  Approach","summary":"  Graph Neural Networks have achieved tremendous success in modeling complex\ngraph data in a variety of applications. However, there are limited studies\ninvestigating privacy protection in GNNs. In this work, we propose a learning\nframework that can provide node privacy at the user level, while incurring low\nutility loss. We focus on a decentralized notion of Differential Privacy,\nnamely Local Differential Privacy, and apply randomization mechanisms to\nperturb both feature and label data at the node level before the data is\ncollected by a central server for model training. Specifically, we investigate\nthe application of randomization mechanisms in high-dimensional feature\nsettings and propose an LDP protocol with strict privacy guarantees. Based on\nfrequency estimation in statistical analysis of randomized data, we develop\nreconstruction methods to approximate features and labels from perturbed data.\nWe also formulate this learning framework to utilize frequency estimates of\ngraph clusters to supervise the training procedure at a sub-graph level.\nExtensive experiments on real-world and semi-synthetic datasets demonstrate the\nvalidity of our proposed model.\n","authors":["Karuna Bhaila","Wen Huang","Yongkai Wu","Xintao Wu"],"pdf_url":"https://arxiv.org/pdf/2309.08569v2.pdf","comment":"2024 SIAM International Conference on Data Mining"},{"id":"http://arxiv.org/abs/2406.12038v2","updated":"2024-08-05T21:48:22Z","published":"2024-06-17T19:11:40Z","title":"Soft Prompting for Unlearning in Large Language Models","summary":"  The widespread popularity of Large Language Models (LLMs), partly due to\ntheir unique ability to perform in-context learning, has also brought to light\nthe importance of ethical and safety considerations when deploying these\npre-trained models. In this work, we focus on investigating machine unlearning\nfor LLMs motivated by data protection regulations. In contrast to the growing\nliterature on fine-tuning methods to achieve unlearning, we focus on a\ncomparatively lightweight alternative called soft prompting to realize the\nunlearning of a subset of training data. With losses designed to enforce\nforgetting as well as utility preservation, our framework \\textbf{S}oft\n\\textbf{P}rompting for \\textbf{U}n\\textbf{l}earning (SPUL) learns prompt tokens\nthat can be appended to an arbitrary query to induce unlearning of specific\nexamples at inference time without updating LLM parameters. We conduct a\nrigorous evaluation of the proposed method and our results indicate that SPUL\ncan significantly improve the trade-off between utility and forgetting in the\ncontext of text classification and question answering with LLMs. We further\nvalidate our method using multiple LLMs to highlight the scalability of our\nframework and provide detailed insights into the choice of hyperparameters and\nthe influence of the size of unlearning data. Our implementation is available\nat \\url{https://github.com/karuna-bhaila/llm_unlearning}.\n","authors":["Karuna Bhaila","Minh-Hao Van","Xintao Wu"],"pdf_url":"https://arxiv.org/pdf/2406.12038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02841v1","updated":"2024-08-05T21:35:51Z","published":"2024-08-05T21:35:51Z","title":"Evaluating Posterior Probabilities: Decision Theory, Proper Scoring\n  Rules, and Calibration","summary":"  Most machine learning classifiers are designed to output posterior\nprobabilities for the classes given the input sample. These probabilities may\nbe used to make the categorical decision on the class of the sample; provided\nas input to a downstream system; or provided to a human for interpretation.\nEvaluating the quality of the posteriors generated by these system is an\nessential problem which was addressed decades ago with the invention of proper\nscoring rules (PSRs). Unfortunately, much of the recent machine learning\nliterature uses calibration metrics -- most commonly, the expected calibration\nerror (ECE) -- as a proxy to assess posterior performance. The problem with\nthis approach is that calibration metrics reflect only one aspect of the\nquality of the posteriors, ignoring the discrimination performance. For this\nreason, we argue that calibration metrics should play no role in the assessment\nof posterior quality. Expected PSRs should instead be used for this job,\npreferably normalized for ease of interpretation. In this work, we first give a\nbrief review of PSRs from a practical perspective, motivating their definition\nusing Bayes decision theory. We discuss why expected PSRs provide a principled\nmeasure of the quality of a system's posteriors and why calibration metrics are\nnot the right tool for this job. We argue that calibration metrics, while not\nuseful for performance assessment, may be used as diagnostic tools during\nsystem development. With this purpose in mind, we discuss a simple and\npractical calibration metric, called calibration loss, derived from a\ndecomposition of expected PSRs. We compare this metric with the ECE and with\nthe expected score divergence calibration metric from the PSR literature and\nargue, using theoretical and empirical evidence, that calibration loss is\nsuperior to these two metrics.\n","authors":["Luciana Ferrer","Daniel Ramos"],"pdf_url":"https://arxiv.org/pdf/2408.02841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02839v1","updated":"2024-08-05T21:25:10Z","published":"2024-08-05T21:25:10Z","title":"Optimizing Cox Models with Stochastic Gradient Descent: Theoretical\n  Foundations and Practical Guidances","summary":"  Optimizing Cox regression and its neural network variants poses substantial\ncomputational challenges in large-scale studies. Stochastic gradient descent\n(SGD), known for its scalability in model optimization, has recently been\nadapted to optimize Cox models. Unlike its conventional application, which\ntypically targets a sum of independent individual loss, SGD for Cox models\nupdates parameters based on the partial likelihood of a subset of data. Despite\nits empirical success, the theoretical foundation for optimizing Cox partial\nlikelihood with SGD is largely underexplored. In this work, we demonstrate that\nthe SGD estimator targets an objective function that is batch-size-dependent.\nWe establish that the SGD estimator for the Cox neural network (Cox-NN) is\nconsistent and achieves the optimal minimax convergence rate up to a\npolylogarithmic factor. For Cox regression, we further prove the\n$\\sqrt{n}$-consistency and asymptotic normality of the SGD estimator, with\nvariance depending on the batch size. Furthermore, we quantify the impact of\nbatch size on Cox-NN training and its effect on the SGD estimator's asymptotic\nefficiency in Cox regression. These findings are validated by extensive\nnumerical experiments and provide guidance for selecting batch sizes in SGD\napplications. Finally, we demonstrate the effectiveness of SGD in a real-world\napplication where GD is unfeasible due to the large scale of data.\n","authors":["Lang Zeng","Weijing Tang","Zhao Ren","Ying Ding"],"pdf_url":"https://arxiv.org/pdf/2408.02839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02838v1","updated":"2024-08-05T21:22:36Z","published":"2024-08-05T21:22:36Z","title":"Interpretation of the Intent Detection Problem as Dynamics in a\n  Low-dimensional Space","summary":"  Intent detection is a text classification task whose aim is to recognize and\nlabel the semantics behind a users query. It plays a critical role in various\nbusiness applications. The output of the intent detection module strongly\nconditions the behavior of the whole system. This sequence analysis task is\nmainly tackled using deep learning techniques. Despite the widespread use of\nthese techniques, the internal mechanisms used by networks to solve the problem\nare poorly understood. Recent lines of work have analyzed the computational\nmechanisms learned by RNNs from a dynamical systems perspective. In this work,\nwe investigate how different RNN architectures solve the SNIPS intent detection\nproblem. Sentences injected into trained networks can be interpreted as\ntrajectories traversing a hidden state space. This space is constrained to a\nlow-dimensional manifold whose dimensionality is related to the embedding and\nhidden layer sizes. To generate predictions, RNN steers the trajectories\ntowards concrete regions, spatially aligned with the output layer matrix rows\ndirections. Underlying the system dynamics, an unexpected fixed point topology\nhas been identified with a limited number of attractors. Our results provide\nnew insights into the inner workings of networks that solve the intent\ndetection task.\n","authors":["Eduardo Sanchez-Karhunen","Jose F. Quesada-Moreno","Miguel A. Gutirrez-Naranjo"],"pdf_url":"https://arxiv.org/pdf/2408.02838v1.pdf","comment":"Camera-Ready version. Accepted paper at 27th European Conference on\n  Artificial Intelligence (ECAI-2024)"},{"id":"http://arxiv.org/abs/2408.02834v1","updated":"2024-08-05T21:11:34Z","published":"2024-08-05T21:11:34Z","title":"DaCapo: a modular deep learning framework for scalable 3D image\n  segmentation","summary":"  DaCapo is a specialized deep learning library tailored to expedite the\ntraining and application of existing machine learning approaches on large,\nnear-isotropic image data. In this correspondence, we introduce DaCapo's unique\nfeatures optimized for this specific domain, highlighting its modular\nstructure, efficient experiment management tools, and scalable deployment\ncapabilities. We discuss its potential to improve access to large-scale,\nisotropic image segmentation and invite the community to explore and contribute\nto this open-source initiative.\n","authors":["William Patton","Jeff L. Rhoades","Marwan Zouinkhi","David G. Ackerman","Caroline Malin-Mayor","Diane Adjavon","Larissa Heinrich","Davis Bennett","Yurii Zubov","CellMap Project Team","Aubrey V. Weigel","Jan Funke"],"pdf_url":"https://arxiv.org/pdf/2408.02834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19082v2","updated":"2024-08-05T21:09:50Z","published":"2024-07-26T21:02:11Z","title":"Regularized Multi-Decoder Ensemble for an Error-Aware Scene\n  Representation Network","summary":"  Feature grid Scene Representation Networks (SRNs) have been applied to\nscientific data as compact functional surrogates for analysis and\nvisualization. As SRNs are black-box lossy data representations, assessing the\nprediction quality is critical for scientific visualization applications to\nensure that scientists can trust the information being visualized. Currently,\nexisting architectures do not support inference time reconstruction quality\nassessment, as coordinate-level errors cannot be evaluated in the absence of\nground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN)\nensemble architecture consisting of a shared feature grid with multiple\nlightweight multi-layer perceptron decoders. MDSRN can generate a set of\nplausible predictions for a given input coordinate to compute the mean as the\nprediction of the multi-decoder ensemble and the variance as a confidence\nscore. The coordinate-level variance can be rendered along with the data to\ninform the reconstruction quality, or be integrated into uncertainty-aware\nvolume visualization algorithms. To prevent the misalignment between the\nquantified variance and the prediction quality, we propose a novel variance\nregularization loss for ensemble learning that promotes the Regularized\nmulti-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates\nclosely to the true model error. We comprehensively evaluate the quality of\nvariance quantification and data reconstruction of Monte Carlo Dropout, Mean\nField Variational Inference, Deep Ensemble, and Predicting Variance compared to\nthe proposed MDSRN and RMDSRN across diverse scalar field datasets. We\ndemonstrate that RMDSRN attains the most accurate data reconstruction and\ncompetitive variance-error correlation among uncertain SRNs under the same\nneural network parameter budgets.\n","authors":["Tianyu Xiong","Skylar W. Wurster","Hanqi Guo","Tom Peterka","Han-Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2407.19082v2.pdf","comment":"To be published in Proc. IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.02833v1","updated":"2024-08-05T21:09:01Z","published":"2024-08-05T21:09:01Z","title":"Adaptive Learning for Quantum Linear Regression","summary":"  The recent availability of quantum annealers as cloud-based services has\nenabled new ways to handle machine learning problems, and several relevant\nalgorithms have been adapted to run on these devices. In a recent work, linear\nregression was formulated as a quadratic binary optimization problem that can\nbe solved via quantum annealing. Although this approach promises a\ncomputational time advantage for large datasets, the quality of the solution is\nlimited by the necessary use of a precision vector, used to approximate the\nreal-numbered regression coefficients in the quantum formulation. In this work,\nwe focus on the practical challenge of improving the precision vector encoding:\ninstead of setting an array of generic values equal for all coefficients, we\nallow each one to be expressed by its specific precision, which is tuned with a\nsimple adaptive algorithm. This approach is evaluated on synthetic datasets of\nincreasing size, and linear regression is solved using the D-Wave Advantage\nquantum annealer, as well as classical solvers. To the best of our knowledge,\nthis is the largest dataset ever evaluated for linear regression on a quantum\nannealer. The results show that our formulation is able to deliver improved\nsolution quality in all instances, and could better exploit the potential of\ncurrent quantum devices.\n","authors":["Costantino Carugno","Maurizio Ferrari Dacrema","Paolo Cremonesi"],"pdf_url":"https://arxiv.org/pdf/2408.02833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02830v1","updated":"2024-08-05T20:55:14Z","published":"2024-08-05T20:55:14Z","title":"Setting the duration of online A/B experiments","summary":"  In designing an online A/B experiment, it is crucial to select a sample size\nand duration that ensure the resulting confidence interval (CI) for the\ntreatment effect is the right width to detect an effect of meaningful magnitude\nwith sufficient statistical power without wasting resources. While the\nrelationship between sample size and CI width is well understood, the effect of\nexperiment duration on CI width remains less clear. This paper provides an\nanalytical formula for the width of a CI based on a ratio treatment effect\nestimator as a function of both sample size (N) and duration (T). The formula\nis derived from a mixed effects model with two variance components. One\ncomponent, referred to as the temporal variance, persists over time for\nexperiments where the same users are kept in the same experiment arm across\ndifferent days. The remaining error variance component, by contrast, decays to\nzero as T gets large. The formula we derive introduces a key parameter that we\ncall the user-specific temporal correlation (UTC), which quantifies the\nrelative sizes of the two variance components and can be estimated from\nhistorical experiments. Higher UTC indicates a slower decay in CI width over\ntime. On the other hand, when the UTC is 0 -- as for experiments where users\nshuffle in and out of the experiment across days -- the CI width decays at the\nstandard parametric 1/T rate. We also study how access to pre-period data for\nthe users in the experiment affects the CI width decay. We show our formula\nclosely explains CI widths on real A/B experiments at YouTube.\n","authors":["Harrison H. Li","Chaoyu Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02824v1","updated":"2024-08-05T20:46:54Z","published":"2024-08-05T20:46:54Z","title":"Wave-RVFL: A Randomized Neural Network Based on Wave Loss Function","summary":"  The random vector functional link (RVFL) network is well-regarded for its\nstrong generalization capabilities in the field of machine learning. However,\nits inherent dependencies on the square loss function make it susceptible to\nnoise and outliers. Furthermore, the calculation of RVFL's unknown parameters\nnecessitates matrix inversion of the entire training sample, which constrains\nits scalability. To address these challenges, we propose the Wave-RVFL, an RVFL\nmodel incorporating the wave loss function. We formulate and solve the proposed\noptimization problem of the Wave-RVFL using the adaptive moment estimation\n(Adam) algorithm in a way that successfully eliminates the requirement for\nmatrix inversion and significantly enhances scalability. The Wave-RVFL exhibits\nrobustness against noise and outliers by preventing over-penalization of\ndeviations, thereby maintaining a balanced approach to managing noise and\noutliers. The proposed Wave-RVFL model is evaluated on multiple UCI datasets,\nboth with and without the addition of noise and outliers, across various\ndomains and sizes. Empirical results affirm the superior performance and\nrobustness of the Wave-RVFL compared to baseline models, establishing it as a\nhighly effective and scalable classification solution.\n","authors":["M. Sajid","A. Quadir","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2408.02824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02821v1","updated":"2024-08-05T20:39:06Z","published":"2024-08-05T20:39:06Z","title":"Continuous Monitoring via Repeated Significance","summary":"  Requiring statistical significance at multiple interim analyses to declare a\nstatistically significant result for an AB test allows less stringent\nrequirements for significance at each interim analysis. Repeated repeated\nsignificance competes well with methods built on assumptions about the test --\nassumptions that may be impossible to evaluate a priori and may require extra\ndata to evaluate empirically.\n  Instead, requiring repeated significance allows the data itself to prove\ndirectly that the required results are not due to chance alone. We explain how\nto apply tests with repeated significance to continuously monitor unbounded\ntests -- tests that do not have an a priori bound on running time or number of\nobservations. We show that it is impossible to maintain a constant requirement\nfor significance for unbounded tests, but that we can come arbitrarily close to\nthat goal.\n","authors":["Eric Bax","Arundhyoti Sarkar","Alex Shtoff"],"pdf_url":"https://arxiv.org/pdf/2408.02821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02814v1","updated":"2024-08-05T20:27:54Z","published":"2024-08-05T20:27:54Z","title":"Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream\n  Machine Learning Services","summary":"  Though pre-trained encoders can be easily accessed online to build downstream\nmachine learning (ML) services quickly, various attacks have been designed to\ncompromise the security and privacy of these encoders. While most attacks\ntarget encoders on the upstream side, it remains unknown how an encoder could\nbe threatened when deployed in a downstream ML service. This paper unveils a\nnew vulnerability: the Pre-trained Encoder Inference (PEI) attack, which posts\nprivacy threats toward encoders hidden behind downstream ML services. By only\nproviding API accesses to a targeted downstream service and a set of candidate\nencoders, the PEI attack can infer which encoder is secretly used by the\ntargeted service based on candidate ones. We evaluate the attack performance of\nPEI against real-world encoders on three downstream tasks: image\nclassification, text classification, and text-to-image generation. Experiments\nshow that the PEI attack succeeds in revealing the hidden encoder in most cases\nand seldom makes mistakes even when the hidden encoder is not in the candidate\nset. We also conducted a case study on one of the most recent vision-language\nmodels, LLaVA, to illustrate that the PEI attack is useful in assisting other\nML attacks such as adversarial attacks. The code is available at\nhttps://github.com/fshp971/encoder-inference.\n","authors":["Shaopeng Fu","Xuexue Sun","Ke Qing","Tianhang Zheng","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02813v1","updated":"2024-08-05T20:27:45Z","published":"2024-08-05T20:27:45Z","title":"Mitigating Malicious Attacks in Federated Learning via Confidence-aware\n  Defense","summary":"  Federated Learning (FL) is an emerging distributed machine learning paradigm\nthat allows multiple clients to collaboratively train a global model without\nsharing private local data. However, FL systems are vulnerable to attacks from\nmalicious clients, who can degrade the global model performance through data\npoisoning and model poisoning. Existing defense methods typically focus on a\nsingle type of attack, such as Byzantine attacks or backdoor attacks, and are\noften ineffective against potential data poisoning attacks like label flipping\nand label shuffling. Additionally, these methods often lack accuracy and\nrobustness in detecting and handling malicious updates. To address these\nissues, we propose a novel method based on model confidence scores, which\nevaluates the uncertainty of client model updates to detect and defend against\nmalicious clients. Our approach is comprehensively effective for both model\npoisoning and data poisoning attacks and is capable of accurately identifying\nand mitigating potential malicious updates from being aggregated. Experimental\nresults demonstrate that our method significantly improves the robustness of FL\nsystems against various types of attacks, also achieving higher model accuracy\nand stability across various scenarios.\n","authors":["Qilei Li","Ahmed M. Abdelmoniem"],"pdf_url":"https://arxiv.org/pdf/2408.02813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02802v1","updated":"2024-08-05T19:45:07Z","published":"2024-08-05T19:45:07Z","title":"Deciphering Air Travel Disruptions: A Machine Learning Approach","summary":"  This research investigates flight delay trends by examining factors such as\ndeparture time, airline, and airport. It employs regression machine learning\nmethods to predict the contributions of various sources to delays. Time-series\nmodels, including LSTM, Hybrid LSTM, and Bi-LSTM, are compared with baseline\nregression models such as Multiple Regression, Decision Tree Regression, Random\nForest Regression, and Neural Network. Despite considerable errors in the\nbaseline models, the study aims to identify influential features in delay\nprediction, potentially informing flight planning strategies. Unlike previous\nwork, this research focuses on regression tasks and explores the use of\ntime-series models for predicting flight delays. It offers insights into\naviation operations by independently analyzing each delay component (e.g.,\nsecurity, weather).\n","authors":["Aravinda Jatavallabha","Jacob Gerlach","Aadithya Naresh"],"pdf_url":"https://arxiv.org/pdf/2408.02802v1.pdf","comment":"10 pages, 11 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.02801v1","updated":"2024-08-05T19:38:45Z","published":"2024-08-05T19:38:45Z","title":"Sparse Deep Learning Models with the $\\ell_1$ Regularization","summary":"  Sparse neural networks are highly desirable in deep learning in reducing its\ncomplexity. The goal of this paper is to study how choices of regularization\nparameters influence the sparsity level of learned neural networks. We first\nderive the $\\ell_1$-norm sparsity-promoting deep learning models including\nsingle and multiple regularization parameters models, from a statistical\nviewpoint. We then characterize the sparsity level of a regularized neural\nnetwork in terms of the choice of the regularization parameters. Based on the\ncharacterizations, we develop iterative algorithms for selecting regularization\nparameters so that the weight parameters of the resulting deep neural network\nenjoy prescribed sparsity levels. Numerical experiments are presented to\ndemonstrate the effectiveness of the proposed algorithms in choosing desirable\nregularization parameters and obtaining corresponding neural networks having\nboth of predetermined sparsity levels and satisfactory approximation accuracy.\n","authors":["Lixin Shen","Rui Wang","Yuesheng Xu","Mingsong Yan"],"pdf_url":"https://arxiv.org/pdf/2408.02801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02798v1","updated":"2024-08-05T19:28:58Z","published":"2024-08-05T19:28:58Z","title":"Examining Gender and Power on Wikipedia Through Face and Politeness","summary":"  We propose a framework for analyzing discourse by combining two\ninterdependent concepts from sociolinguistic theory: face acts and politeness.\nWhile politeness has robust existing tools and data, face acts are less\nresourced. We introduce a new corpus created by annotating Wikipedia talk pages\nwith face acts and we use this to train a face act tagger. We then employ our\nframework to study how face and politeness interact with gender and power in\ndiscussions between Wikipedia editors. Among other findings, we observe that\nfemale Wikipedians are not only more polite, which is consistent with prior\nstudies, but that this difference corresponds with significantly more language\ndirected at humbling aspects of their own face. Interestingly, the distinction\nnearly vanishes once limiting to editors with administrative power.\n","authors":["Adil Soubki","Shyne Choi","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2408.02798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02797v1","updated":"2024-08-05T19:25:05Z","published":"2024-08-05T19:25:05Z","title":"Algorithm-Informed Graph Neural Networks for Leakage Detection and\n  Localization in Water Distribution Networks","summary":"  Detecting and localizing leakages is a significant challenge for the\nefficient and sustainable management of water distribution networks (WDN).\nLeveraging the inherent graph structure of WDNs, recent approaches have used\ngraph-based data-driven methods. However, these methods often learn shortcuts\nthat work well with in-distribution data but fail to generalize to\nout-of-distribution data. To address this limitation and inspired by the\nperfect generalization ability of classical algorithms, we propose an\nalgorithm-informed graph neural network (AIGNN). Recognizing that WDNs function\nas flow networks, incorporating max-flow information can be beneficial for\ninferring pressures. In the proposed framework, we first train AIGNN to emulate\nthe Ford-Fulkerson algorithm for solving max-flow problems. This algorithmic\nknowledge is then transferred to address the pressure estimation problem in\nWDNs. Two AIGNNs are deployed, one to reconstruct pressure based on the current\nmeasurements, and another to predict pressure based on previous measurements.\nLeakages are detected and localized by comparing the outputs of the\nreconstructor and the predictor. By pretraining AIGNNs to reason like\nalgorithms, they are expected to extract more task-relevant and generalizable\nfeatures. Experimental results demonstrate that the proposed algorithm-informed\napproach achieves superior results with better generalization ability compared\nto GNNs that do not incorporate algorithmic knowledge.\n","authors":["Zepeng Zhang","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2408.02797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05490v2","updated":"2024-08-05T18:57:42Z","published":"2023-09-11T14:32:04Z","title":"Learning Semantic Segmentation with Query Points Supervision on Aerial\n  Images","summary":"  Semantic segmentation is crucial in remote sensing, where high-resolution\nsatellite images are segmented into meaningful regions. Recent advancements in\ndeep learning have significantly improved satellite image segmentation.\nHowever, most of these methods are typically trained in fully supervised\nsettings that require high-quality pixel-level annotations, which are expensive\nand time-consuming to obtain. In this work, we present a weakly supervised\nlearning algorithm to train semantic segmentation algorithms that only rely on\nquery point annotations instead of full mask labels. Our proposed approach\nperforms accurate semantic segmentation and improves efficiency by\nsignificantly reducing the cost and time required for manual annotation.\nSpecifically, we generate superpixels and extend the query point labels into\nthose superpixels that group similar meaningful semantics. Then, we train\nsemantic segmentation models supervised with images partially labeled with the\nsuperpixel pseudo-labels. We benchmark our weakly supervised training approach\non an aerial image dataset and different semantic segmentation architectures,\nshowing that we can reach competitive performance compared to fully supervised\ntraining while reducing the annotation effort. The code of our proposed\napproach is publicly available at: https://github.com/santiago2205/LSSQPS.\n","authors":["Santiago Rivier","Carlos Hinojosa","Silvio Giancola","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2309.05490v2.pdf","comment":"Paper Accepted at ICIP 2024 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2312.00761v4","updated":"2024-08-05T18:40:07Z","published":"2023-12-01T18:29:08Z","title":"Deep Unlearning: Fast and Efficient Gradient-free Approach to Class\n  Forgetting","summary":"  Machine unlearning is a prominent and challenging field, driven by regulatory\ndemands for user data deletion and heightened privacy awareness. Existing\napproaches involve retraining model or multiple finetuning steps for each\ndeletion request, often constrained by computational limits and restricted data\naccess. In this work, we introduce a novel class unlearning algorithm designed\nto strategically eliminate specific classes from the learned model. Our\nalgorithm first estimates the Retain and the Forget Spaces using Singular Value\nDecomposition on the layerwise activations for a small subset of samples from\nthe retain and unlearn classes, respectively. We then compute the shared\ninformation between these spaces and remove it from the forget space to isolate\nclass-discriminatory feature space. Finally, we obtain the unlearned model by\nupdating the weights to suppress the class discriminatory features from the\nactivation spaces. We demonstrate our algorithm's efficacy on ImageNet using a\nVision Transformer with only $\\sim 1.5\\%$ drop in retain accuracy compared to\nthe original model while maintaining under $1\\%$ accuracy on the unlearned\nclass samples. Furthermore, our algorithm exhibits competitive unlearning\nperformance and resilience against Membership Inference Attacks (MIA). Compared\nto baselines, it achieves an average accuracy improvement of $1.38\\%$ on the\nImageNet dataset while requiring up to $10 \\times$ fewer samples for\nunlearning. Additionally, under stronger MIA attacks on the CIFAR-100 dataset\nusing a ResNet18 architecture, our approach outperforms the best baseline by\n$1.8\\%$. Our code is available at\nhttps://github.com/sangamesh-kodge/class_forgetting.\n","authors":["Sangamesh Kodge","Gobinda Saha","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2312.00761v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02767v1","updated":"2024-08-05T18:36:13Z","published":"2024-08-05T18:36:13Z","title":"4D-Var using Hessian approximation and backpropagation applied to\n  automatically-differentiable numerical and machine learning models","summary":"  Constraining a numerical weather prediction (NWP) model with observations via\n4D variational (4D-Var) data assimilation is often difficult to implement in\npractice due to the need to develop and maintain a software-based tangent\nlinear model and adjoint model. One of the most common 4D-Var algorithms uses\nan incremental update procedure, which has been shown to be an approximation of\nthe Gauss-Newton method. Here we demonstrate that when using a forecast model\nthat supports automatic differentiation, an efficient and in some cases more\naccurate alternative approximation of the Gauss-Newton method can be applied by\ncombining backpropagation of errors with Hessian approximation. This approach\ncan be used with either a conventional numerical model implemented within a\nsoftware framework that supports automatic differentiation, or a machine\nlearning (ML) based surrogate model. We test the new approach on a variety of\nLorenz-96 and quasi-geostrophic models. The results indicate potential for a\ndeeper integration of modeling, data assimilation, and new technologies in a\nnext-generation of operational forecast systems that leverage weather models\ndesigned to support automatic differentiation.\n","authors":["Kylen Solvik","Stephen G. Penny","Stephan Hoyer"],"pdf_url":"https://arxiv.org/pdf/2408.02767v1.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.02766v1","updated":"2024-08-05T18:34:15Z","published":"2024-08-05T18:34:15Z","title":"ConDL: Detector-Free Dense Image Matching","summary":"  In this work, we introduce a deep-learning framework designed for estimating\ndense image correspondences. Our fully convolutional model generates dense\nfeature maps for images, where each pixel is associated with a descriptor that\ncan be matched across multiple images. Unlike previous methods, our model is\ntrained on synthetic data that includes significant distortions, such as\nperspective changes, illumination variations, shadows, and specular highlights.\nUtilizing contrastive learning, our feature maps achieve greater invariance to\nthese distortions, enabling robust matching. Notably, our method eliminates the\nneed for a keypoint detector, setting it apart from many existing\nimage-matching techniques.\n","authors":["Monika Kwiatkowski","Simon Matern","Olaf Hellwich"],"pdf_url":"https://arxiv.org/pdf/2408.02766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02761v1","updated":"2024-08-05T18:24:48Z","published":"2024-08-05T18:24:48Z","title":"Dimensionality Reduction and Nearest Neighbors for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation","summary":"  Clinically deployed deep learning-based segmentation models are known to fail\non data outside of their training distributions. While clinicians review the\nsegmentations, these models tend to perform well in most instances, which could\nexacerbate automation bias. Therefore, detecting out-of-distribution images at\ninference is critical to warn the clinicians that the model likely failed. This\nwork applied the Mahalanobis distance (MD) post hoc to the bottleneck features\nof four Swin UNETR and nnU-net models that segmented the liver on T1-weighted\nmagnetic resonance imaging and computed tomography. By reducing the dimensions\nof the bottleneck features with either principal component analysis or uniform\nmanifold approximation and projection, images the models failed on were\ndetected with high performance and minimal computational load. In addition,\nthis work explored a non-parametric alternative to the MD, a k-th nearest\nneighbors distance (KNN). KNN drastically improved scalability and performance\nover MD when both were applied to raw and average-pooled bottleneck features.\n","authors":["McKell Woodland","Nihil Patel","Austin Castelo","Mais Al Taie","Mohamed Eltaher","Joshua P. Yung","Tucker J. Netherton","Tiffany L. Calderone","Jessica I. Sanchez","Darrel W. Cleere","Ahmed Elsaiey","Nakul Gupta","David Victor","Laura Beretta","Ankit B. Patel Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2408.02761v1.pdf","comment":"Expansion of \"Dimensionality Reduction for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation\" arXiv:2308.03723\n  . Submitted to the Journal for Machine Learning in Biomedical Imaging. Code\n  available at https://github.com/mckellwoodland/dimen_reduce_mahal"},{"id":"http://arxiv.org/abs/2408.02760v1","updated":"2024-08-05T18:24:09Z","published":"2024-08-05T18:24:09Z","title":"Classification of Raw MEG/EEG Data with Detach-Rocket Ensemble: An\n  Improved ROCKET Algorithm for Multivariate Time Series Analysis","summary":"  Multivariate Time Series Classification (MTSC) is a ubiquitous problem in\nscience and engineering, particularly in neuroscience, where most data\nacquisition modalities involve the simultaneous time-dependent recording of\nbrain activity in multiple brain regions. In recent years, Random Convolutional\nKernel models such as ROCKET and MiniRocket have emerged as highly effective\ntime series classification algorithms, capable of achieving state-of-the-art\naccuracy results with low computational load. Despite their success, these\ntypes of models face two major challenges when employed in neuroscience: 1)\nthey struggle to deal with high-dimensional data such as EEG and MEG, and 2)\nthey are difficult to interpret. In this work, we present a novel ROCKET-based\nalgorithm, named Detach-Rocket Ensemble, that is specifically designed to\naddress these two problems in MTSC. Our algorithm leverages pruning to provide\nan integrated estimation of channel importance, and ensembles to achieve better\naccuracy and provide a label probability. Using a synthetic multivariate time\nseries classification dataset in which we control the amount of information\ncarried by each of the channels, we first show that our algorithm is able to\ncorrectly recover the channel importance for classification. Then, using two\nreal-world datasets, a MEG dataset and an EEG dataset, we show that\nDetach-Rocket Ensemble is able to provide both interpretable channel relevance\nand competitive classification accuracy, even when applied directly to the raw\nbrain data, without the need for feature engineering.\n","authors":["Adri Solana","Erik Fransn","Gonzalo Uribarri"],"pdf_url":"https://arxiv.org/pdf/2408.02760v1.pdf","comment":"To be published in European Conference on Machine Learning and Data\n  Mining 2024, 20 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2405.01531v2","updated":"2024-08-05T18:20:39Z","published":"2024-05-02T17:59:01Z","title":"Improving Intervention Efficacy via Concept Realignment in Concept\n  Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) ground image classification on\nhuman-understandable concepts to allow for interpretable model decisions.\nCrucially, the CBM design inherently allows for human interventions, in which\nexpert users are given the ability to modify potentially misaligned concept\nchoices to influence the decision behavior of the model in an interpretable\nfashion. However, existing approaches often require numerous human\ninterventions per image to achieve strong performances, posing practical\nchallenges in scenarios where obtaining human feedback is expensive. In this\npaper, we find that this is noticeably driven by an independent treatment of\nconcepts during intervention, wherein a change of one concept does not\ninfluence the use of other ones in the model's final decision. To address this\nissue, we introduce a trainable concept intervention realignment module, which\nleverages concept relations to realign concept assignments post-intervention.\nAcross standard, real-world benchmarks, we find that concept realignment can\nsignificantly improve intervention efficacy; significantly reducing the number\nof interventions needed to reach a target classification performance or concept\nprediction accuracy. In addition, it easily integrates into existing\nconcept-based architectures without requiring changes to the models themselves.\nThis reduced cost of human-model collaboration is crucial to enhancing the\nfeasibility of CBMs in resource-constrained environments. Our code is available\nat: https://github.com/ExplainableML/concept_realignment.\n","authors":["Nishad Singhi","Jae Myung Kim","Karsten Roth","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2405.01531v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02751v1","updated":"2024-08-05T18:11:23Z","published":"2024-08-05T18:11:23Z","title":"A Novel Hybrid Approach for Tornado Prediction in the United States:\n  Kalman-Convolutional BiLSTM with Multi-Head Attention","summary":"  Tornadoes are among the most intense atmospheric vortex phenomena and pose\nsignificant challenges for detection and forecasting. Conventional methods,\nwhich heavily depend on ground-based observations and radar data, are limited\nby issues such as decreased accuracy over greater distances and a high rate of\nfalse positives. To address these challenges, this study utilizes the Seamless\nHybrid Scan Reflectivity (SHSR) dataset from the Multi-Radar Multi-Sensor\n(MRMS) system, which integrates data from multiple radar sources to enhance\naccuracy. A novel hybrid model, the Kalman-Convolutional BiLSTM with Multi-Head\nAttention, is introduced to improve dynamic state estimation and capture both\nspatial and temporal dependencies within the data. This model demonstrates\nsuperior performance in precision, recall, F1-Score, and accuracy compared to\nmethods such as K-Nearest Neighbors (KNN) and LightGBM. The results highlight\nthe considerable potential of advanced machine learning techniques to improve\ntornado prediction and reduce false alarm rates. Future research will focus on\nexpanding datasets, exploring innovative model architectures, and incorporating\nlarge language models (LLMs) to provide deeper insights. This research\nintroduces a novel model for tornado prediction, offering a robust framework\nfor enhancing forecasting accuracy and public safety.\n","authors":["Jiawei Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16778v2","updated":"2024-08-05T18:08:49Z","published":"2024-02-26T17:49:37Z","title":"On the Growth of Mistakes in Differentially Private Online Learning: A\n  Lower Bound Perspective","summary":"  In this paper, we provide lower bounds for Differentially Private (DP) Online\nLearning algorithms. Our result shows that, for a broad class of\n$(\\varepsilon,\\delta)$-DP online algorithms, for number of rounds $T$ such that\n$\\log T\\leq O(1 / \\delta)$, the expected number of mistakes incurred by the\nalgorithm grows as $\\Omega(\\log \\frac{T}{\\delta})$. This matches the upper\nbound obtained by Golowich and Livni (2021) and is in contrast to non-private\nonline learning where the number of mistakes is independent of $T$. To the best\nof our knowledge, our work is the first result towards settling lower bounds\nfor DP-Online learning and partially addresses the open question in Sanyal and\nRamponi (2022).\n","authors":["Daniil Dmitriev","Kristf Szab","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2402.16778v2.pdf","comment":"Accepted at the Conference on Learning Theory (COLT) 2024, Edmonton,\n  Canada"},{"id":"http://arxiv.org/abs/2406.11714v2","updated":"2024-08-05T18:02:55Z","published":"2024-06-17T16:32:57Z","title":"Scalable Expressiveness through Preprocessed Graph Perturbations","summary":"  Graph Neural Networks (GNNs) have emerged as the predominant method for\nanalyzing graph-structured data. However, canonical GNNs have limited\nexpressive power and generalization capability, thus triggering the development\nof more expressive yet computationally intensive methods. One such approach is\nto create a series of perturbed versions of input graphs and then repeatedly\nconduct multiple message-passing operations on all variations during training.\nDespite their expressive power, this approach does not scale well on larger\ngraphs. To address this scalability issue, we introduce Scalable Expressiveness\nthrough Preprocessed Graph Perturbation (SE2P). This model offers a flexible,\nconfigurable balance between scalability and generalizability with four\ndistinct configuration classes. At one extreme, the configuration prioritizes\nscalability through minimal learnable feature extraction and extensive\npreprocessing; at the other extreme, it enhances generalizability with more\nlearnable feature extractions, though this increases scalability costs. We\nconduct extensive experiments on real-world datasets to evaluate the\ngeneralizability and scalability of SE2P variants compared to various\nstate-of-the-art benchmarks. Our results indicate that, depending on the chosen\nSE2P configuration, the model can enhance generalizability compared to\nbenchmarks while achieving significant speed improvements of up to 8-fold.\n","authors":["Danial Saber","Amirali Salehi-Abari"],"pdf_url":"https://arxiv.org/pdf/2406.11714v2.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.02743v1","updated":"2024-08-05T18:01:07Z","published":"2024-08-05T18:01:07Z","title":"KAN we improve on HEP classification tasks? Kolmogorov-Arnold Networks\n  applied to an LHC physics example","summary":"  Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an\nalternative to multilayer perceptrons, suggesting advantages in performance and\ninterpretability. We study a typical binary event classification task in\nhigh-energy physics including high-level features and comment on the\nperformance and interpretability of KANs in this context. We find that the\nlearned activation functions of a one-layer KAN resemble the log-likelihood\nratio of the input features. In deeper KANs, the activations in the first KAN\nlayer differ from those in the one-layer KAN, which indicates that the deeper\nKANs learn more complex representations of the data. We study KANs with\ndifferent depths and widths and we compare them to multilayer perceptrons in\nterms of performance and number of trainable parameters. For the chosen\nclassification task, we do not find that KANs are more parameter efficient.\nHowever, small KANs may offer advantages in terms of interpretability that come\nat the cost of only a moderate loss in performance.\n","authors":["Johannes Erdmann","Florian Mausolf","Jan Lukas Sph"],"pdf_url":"https://arxiv.org/pdf/2408.02743v1.pdf","comment":"25 pages, 9 figures"}]},"2024-08-03T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.00677v2","updated":"2024-08-03T04:29:58Z","published":"2024-08-01T16:20:02Z","title":"Scaling Backwards: Minimal Synthetic Pre-training?","summary":"  Pre-training and transfer learning are an important building block of current\ncomputer vision systems. While pre-training is usually performed on large\nreal-world image datasets, in this paper we ask whether this is truly\nnecessary. To this end, we search for a minimal, purely synthetic pre-training\ndataset that allows us to achieve performance similar to the 1 million images\nof ImageNet-1k. We construct such a dataset from a single fractal with\nperturbations. With this, we contribute three main findings. (i) We show that\npre-training is effective even with minimal synthetic images, with performance\non par with large-scale pre-training datasets like ImageNet-1k for full\nfine-tuning. (ii) We investigate the single parameter with which we construct\nartificial categories for our dataset. We find that while the shape differences\ncan be indistinguishable to humans, they are crucial for obtaining strong\nperformances. (iii) Finally, we investigate the minimal requirements for\nsuccessful pre-training. Surprisingly, we find that a substantial reduction of\nsynthetic images from 1k to 1 can even lead to an increase in pre-training\nperformance, a motivation to further investigate ''scaling backwards''.\nFinally, we extend our method from synthetic images to real images to see if a\nsingle real image can show similar pre-training effect through shape\naugmentation. We find that the use of grayscale images and affine\ntransformations allows even real images to ''scale backwards''.\n","authors":["Ryo Nakamura","Ryu Tadokoro","Ryosuke Yamada","Yuki M. Asano","Iro Laina","Christian Rupprecht","Nakamasa Inoue","Rio Yokota","Hirokatsu Kataoka"],"pdf_url":"https://arxiv.org/pdf/2408.00677v2.pdf","comment":"Accepted to ECCV2024"},{"id":"http://arxiv.org/abs/2408.01878v1","updated":"2024-08-03T23:11:20Z","published":"2024-08-03T23:11:20Z","title":"FBINeRF: Feature-Based Integrated Recurrent Network for Pinhole and\n  Fisheye Neural Radiance Fields","summary":"  Previous studies aiming to optimize and bundle-adjust camera poses using\nNeural Radiance Fields (NeRFs), such as BARF and DBARF, have demonstrated\nimpressive capabilities in 3D scene reconstruction. However, these approaches\nhave been designed for pinhole-camera pose optimization and do not perform well\nunder radial image distortions such as those in fisheye cameras. Furthermore,\ninaccurate depth initialization in DBARF results in erroneous geometric\ninformation affecting the overall convergence and quality of results. In this\npaper, we propose adaptive GRUs with a flexible bundle-adjustment method\nadapted to radial distortions and incorporate feature-based recurrent neural\nnetworks to generate continuous novel views from fisheye datasets. Other NeRF\nmethods for fisheye images, such as SCNeRF and OMNI-NeRF, use projected ray\ndistance loss for distorted pose refinement, causing severe artifacts, long\nrendering time, and are difficult to use in downstream tasks, where the dense\nvoxel representation generated by a NeRF method needs to be converted into a\nmesh representation. We also address depth initialization issues by adding\nMiDaS-based depth priors for pinhole images. Through extensive experiments, we\ndemonstrate the generalization capacity of FBINeRF and show high-fidelity\nresults for both pinhole-camera and fisheye-camera NeRFs.\n","authors":["Yifan Wu","Tianyi Cheng","Peixu Xin","Janusz Konrad"],"pdf_url":"https://arxiv.org/pdf/2408.01878v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2403.02411v5","updated":"2024-08-03T23:05:44Z","published":"2024-03-04T19:08:20Z","title":"NiNformer: A Network in Network Transformer with Token Mixing as a\n  Gating Function Generator","summary":"  The attention mechanism is the main component of the transformer\narchitecture, and since its introduction, it has led to significant\nadvancements in deep learning that span many domains and multiple tasks. The\nattention mechanism was utilized in computer vision as the Vision Transformer\nViT, and its usage has expanded into many tasks in the vision domain, such as\nclassification, segmentation, object detection, and image generation. While\nthis mechanism is very expressive and capable, it comes with the drawback of\nbeing computationally expensive and requiring datasets of considerable size for\neffective optimization. To address these shortcomings, many designs have been\nproposed in the literature to reduce the computational burden and alleviate the\ndata size requirements. Examples of such attempts in the vision domain are the\nMLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper\nintroduces a new computational block as an alternative to the standard ViT\nblock that reduces the compute burdens by replacing the normal attention layers\nwith a Network in Network structure that enhances the static approach of the\nMLP-Mixer with a dynamic system of learning an element-wise gating function by\na token mixing process. Extensive experimentation shows that the proposed\ndesign provides better performance than the baseline architectures on multiple\ndatasets applied in the image classification task of the vision domain.\n","authors":["Abdullah Nazhat Abdullah","Tarkan Aydin"],"pdf_url":"https://arxiv.org/pdf/2403.02411v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01877v1","updated":"2024-08-03T22:55:26Z","published":"2024-08-03T22:55:26Z","title":"Is Generative Communication between Embodied Agents Good for Zero-Shot\n  ObjectNav?","summary":"  In Zero-Shot ObjectNav, an embodied ground agent is expected to navigate to a\ntarget object specified by a natural language label without any\nenvironment-specific fine-tuning. This is challenging, given the limited view\nof a ground agent and its independent exploratory behavior. To address these\nissues, we consider an assistive overhead agent with a bounded global view\nalongside the ground agent and present two coordinated navigation schemes for\njudicious exploration. We establish the influence of the Generative\nCommunication (GC) between the embodied agents equipped with Vision-Language\nModels (VLMs) in improving zero-shot ObjectNav, achieving a 10% improvement in\nthe ground agent's ability to find the target object in comparison with an\nunassisted setup in simulation. We further analyze the GC for unique traits\nquantifying the presence of hallucination and cooperation. In particular, we\nidentify a unique trait of \"preemptive hallucination\" specific to our embodied\nsetting, where the overhead agent assumes that the ground agent has executed an\naction in the dialogue when it is yet to move. Finally, we conduct real-world\ninferences with GC and showcase qualitative examples where countering\npre-emptive hallucination via prompt finetuning improves real-world ObjectNav\nperformance.\n","authors":["Vishnu Sashank Dorbala","Vishnu Dutt Sharma","Pratap Tokekar","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2408.01877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01872v1","updated":"2024-08-03T22:33:13Z","published":"2024-08-03T22:33:13Z","title":"Safe Semi-Supervised Contrastive Learning Using In-Distribution Data as\n  Positive Examples","summary":"  Semi-supervised learning methods have shown promising results in solving many\npractical problems when only a few labels are available. The existing methods\nassume that the class distributions of labeled and unlabeled data are equal;\nhowever, their performances are significantly degraded in class distribution\nmismatch scenarios where out-of-distribution (OOD) data exist in the unlabeled\ndata. Previous safe semi-supervised learning studies have addressed this\nproblem by making OOD data less likely to affect training based on labeled\ndata. However, even if the studies effectively filter out the unnecessary OOD\ndata, they can lose the basic information that all data share regardless of\nclass. To this end, we propose to apply a self-supervised contrastive learning\napproach to fully exploit a large amount of unlabeled data. We also propose a\ncontrastive loss function with coefficient schedule to aggregate as an anchor\nthe labeled negative examples of the same class into positive examples. To\nevaluate the performance of the proposed method, we conduct experiments on\nimage classification datasets - CIFAR-10, CIFAR-100, Tiny ImageNet, and\nCIFAR-100+Tiny ImageNet - under various mismatch ratios. The results show that\nself-supervised contrastive learning significantly improves classification\naccuracy. Moreover, aggregating the in-distribution examples produces better\nrepresentation and consequently further improves classification accuracy.\n","authors":["Min Gu Kwak","Hyungu Kahng","Seoung Bum Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21640v2","updated":"2024-08-03T22:22:30Z","published":"2024-07-31T14:41:10Z","title":"MSA$^2$Net: Multi-scale Adaptive Attention-guided Network for Medical\n  Image Segmentation","summary":"  Medical image segmentation involves identifying and separating object\ninstances in a medical image to delineate various tissues and structures, a\ntask complicated by the significant variations in size, shape, and density of\nthese features. Convolutional neural networks (CNNs) have traditionally been\nused for this task but have limitations in capturing long-range dependencies.\nTransformers, equipped with self-attention mechanisms, aim to address this\nproblem. However, in medical image segmentation it is beneficial to merge both\nlocal and global features to effectively integrate feature maps across various\nscales, capturing both detailed features and broader semantic elements for\ndealing with variations in structures. In this paper, we introduce MSA$^2$Net,\na new deep segmentation framework featuring an expedient design of\nskip-connections. These connections facilitate feature fusion by dynamically\nweighting and combining coarse-grained encoder features with fine-grained\ndecoder feature maps. Specifically, we propose a Multi-Scale Adaptive Spatial\nAttention Gate (MASAG), which dynamically adjusts the receptive field (Local\nand Global contextual information) to ensure that spatially relevant features\nare selectively highlighted while minimizing background distractions. Extensive\nevaluations involving dermatology, and radiological datasets demonstrate that\nour MSA$^2$Net outperforms state-of-the-art (SOTA) works or matches their\nperformance. The source code is publicly available at\nhttps://github.com/xmindflow/MSA-2Net.\n","authors":["Sina Ghorbani Kolahi","Seyed Kamal Chaharsooghi","Toktam Khatibi","Afshin Bozorgpour","Reza Azad","Moein Heidari","Ilker Hacihaliloglu","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2407.21640v2.pdf","comment":"Accepted at BMVC 2024. Supplementary materials included at the end of\n  the main paper (3 pages, 2 figures, 1 table)"},{"id":"http://arxiv.org/abs/2408.01859v1","updated":"2024-08-03T20:08:02Z","published":"2024-08-03T20:08:02Z","title":"Graph Unfolding and Sampling for Transitory Video Summarization via\n  Gershgorin Disc Alignment","summary":"  User-generated videos (UGVs) uploaded from mobile phones to social media\nsites like YouTube and TikTok are short and non-repetitive. We summarize a\ntransitory UGV into several keyframes in linear time via fast graph sampling\nbased on Gershgorin disc alignment (GDA). Specifically, we first model a\nsequence of $N$ frames in a UGV as an $M$-hop path graph $\\mathcal{G}^o$ for $M\n\\ll N$, where the similarity between two frames within $M$ time instants is\nencoded as a positive edge based on feature similarity. Towards efficient\nsampling, we then \"unfold\" $\\mathcal{G}^o$ to a $1$-hop path graph\n$\\mathcal{G}$, specified by a generalized graph Laplacian matrix $\\mathcal{L}$,\nvia one of two graph unfolding procedures with provable performance bounds. We\nshow that maximizing the smallest eigenvalue $\\lambda_{\\min}(\\mathbf{B})$ of a\ncoefficient matrix $\\mathbf{B} = \\textit{diag}\\left(\\mathbf{h}\\right) + \\mu\n\\mathcal{L}$, where $\\mathbf{h}$ is the binary keyframe selection vector, is\nequivalent to minimizing a worst-case signal reconstruction error. We maximize\ninstead the Gershgorin circle theorem (GCT) lower bound\n$\\lambda^-_{\\min}(\\mathbf{B})$ by choosing $\\mathbf{h}$ via a new fast graph\nsampling algorithm that iteratively aligns left-ends of Gershgorin discs for\nall graph nodes (frames). Extensive experiments on multiple short video\ndatasets show that our algorithm achieves comparable or better video\nsummarization performance compared to state-of-the-art methods, at a\nsubstantially reduced complexity.\n","authors":["Sadid Sahami","Gene Cheung","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2408.01859v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.01843v1","updated":"2024-08-03T18:51:04Z","published":"2024-08-03T18:51:04Z","title":"Supervised Image Translation from Visible to Infrared Domain for Object\n  Detection","summary":"  This study aims to learn a translation from visible to infrared imagery,\nbridging the domain gap between the two modalities so as to improve accuracy on\ndownstream tasks including object detection. Previous approaches attempt to\nperform bi-domain feature fusion through iterative optimization or end-to-end\ndeep convolutional networks. However, we pose the problem as similar to that of\nimage translation, adopting a two-stage training strategy with a Generative\nAdversarial Network and an object detection model. The translation model learns\na conversion that preserves the structural detail of visible images while\npreserving the texture and other characteristics of infrared images. Images so\ngenerated are used to train standard object detection frameworks including\nYolov5, Mask and Faster RCNN. We also investigate the usefulness of integrating\na super-resolution step into our pipeline to further improve model accuracy,\nand achieve an improvement of as high as 5.3% mAP.\n","authors":["Prahlad Anand","Qiranul Saadiyean","Aniruddh Sikdar","Nalini N","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2408.01843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02961v2","updated":"2024-08-03T18:49:02Z","published":"2024-05-05T15:01:00Z","title":"JOSENet: A Joint Stream Embedding Network for Violence Detection in\n  Surveillance Videos","summary":"  The increasing proliferation of video surveillance cameras and the escalating\ndemand for crime prevention have intensified interest in the task of violence\ndetection within the research community. Compared to other action recognition\ntasks, violence detection in surveillance videos presents additional issues,\nsuch as the wide variety of real fight scenes. Unfortunately, existing datasets\nfor violence detection are relatively small in comparison to those for other\naction recognition tasks. Moreover, surveillance footage often features\ndifferent individuals in each video and varying backgrounds for each camera. In\naddition, fast detection of violent actions in real-life surveillance videos is\ncrucial to prevent adverse outcomes, thus necessitating models that are\noptimized for reduced memory usage and computational costs. These challenges\ncomplicate the application of traditional action recognition methods. To tackle\nall these issues, we introduce JOSENet, a novel self-supervised framework that\nprovides outstanding performance for violence detection in surveillance videos.\nThe proposed model processes two spatiotemporal video streams, namely RGB\nframes and optical flows, and incorporates a new regularized self-supervised\nlearning approach for videos. JOSENet demonstrates improved performance\ncompared to state-of-the-art methods, while utilizing only one-fourth of the\nframes per video segment and operating at a reduced frame rate. The source code\nis available at https://github.com/ispamm/JOSENet.\n","authors":["Pietro Nardelli","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2405.02961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01195v4","updated":"2024-08-03T18:48:43Z","published":"2023-06-01T23:20:47Z","title":"Consistency-guided Prompt Learning for Vision-Language Models","summary":"  We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning\nmethod for vision-language models. Our approach improves the generalization of\nlarge foundation models when fine-tuned on downstream tasks in a few-shot\nsetting. The basic idea of CoPrompt is to enforce a consistency constraint in\nthe prediction of the trainable and pre-trained models to prevent overfitting\non the downstream task. Additionally, we introduce the following two components\ninto our consistency constraint to further boost the performance: enforcing\nconsistency on two perturbed inputs and combining two dominant paradigms of\ntuning, prompting and adapter. Enforcing consistency on perturbed input serves\nto further regularize the consistency constraint, thereby improving\ngeneralization. Moreover, the integration of adapters and prompts not only\nenhances performance on downstream tasks but also offers increased tuning\nflexibility in both input and output spaces. This facilitates more effective\nadaptation to downstream tasks in a few-shot learning setting. Experiments show\nthat CoPrompt outperforms existing methods on a range of evaluation suites,\nincluding base-to-novel generalization, domain generalization, and\ncross-dataset evaluation. On generalization, CoPrompt improves the\nstate-of-the-art on zero-shot tasks and the overall harmonic mean over 11\ndatasets. Detailed ablation studies show the effectiveness of each of the\ncomponents in CoPrompt. We make our code available at\nhttps://github.com/ShuvenduRoy/CoPrompt.\n","authors":["Shuvendu Roy","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2306.01195v4.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2408.01840v1","updated":"2024-08-03T18:47:31Z","published":"2024-08-03T18:47:31Z","title":"E$^3$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry\n  Images","summary":"  Neural Radiance Fields (NeRF) achieve impressive rendering performance by\nlearning volumetric 3D representation from several images of different views.\nHowever, it is difficult to reconstruct a sharp NeRF from blurry input as it\noften occurs in the wild. To solve this problem, we propose a novel Efficient\nEvent-Enhanced NeRF (E$^3$NeRF) by utilizing the combination of RGB images and\nevent streams. To effectively introduce event streams into the neural\nvolumetric representation learning process, we propose an event-enhanced blur\nrendering loss and an event rendering loss, which guide the network via\nmodeling the real blur process and event generation process, respectively.\nSpecifically, we leverage spatial-temporal information from the event stream to\nevenly distribute learning attention over temporal blur while simultaneously\nfocusing on blurry texture through the spatial attention. Moreover, a camera\npose estimation framework for real-world data is built with the guidance of the\nevents to generalize the method to practical applications. Compared to previous\nimage-based or event-based NeRF, our framework makes more profound use of the\ninternal relationship between events and images. Extensive experiments on both\nsynthetic data and real-world data demonstrate that E$^3$NeRF can effectively\nlearn a sharp NeRF from blurry images, especially in non-uniform motion and\nlow-light scenes.\n","authors":["Yunshan Qi","Jia Li","Yifan Zhao","Yu Zhang","Lin Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.01840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01835v1","updated":"2024-08-03T18:08:51Z","published":"2024-08-03T18:08:51Z","title":"TS-SAM: Fine-Tuning Segment-Anything Model for Downstream Tasks","summary":"  Adapter based fine-tuning has been studied for improving the performance of\nSAM on downstream tasks. However, there is still a significant performance gap\nbetween fine-tuned SAMs and domain-specific models. To reduce the gap, we\npropose Two-Stream SAM (TS-SAM). On the one hand, inspired by the side network\nin Parameter-Efficient Fine-Tuning (PEFT), we designed a lightweight\nConvolutional Side Adapter (CSA), which integrates the powerful features from\nSAM into side network training for comprehensive feature fusion. On the other\nhand, in line with the characteristics of segmentation tasks, we designed\nMulti-scale Refinement Module (MRM) and Feature Fusion Decoder (FFD) to keep\nboth the detailed and semantic features. Extensive experiments on ten public\ndatasets from three tasks demonstrate that TS-SAM not only significantly\noutperforms the recently proposed SAM-Adapter and SSOM, but achieves\ncompetitive performance with the SOTA domain-specific models. Our code is\navailable at: https://github.com/maoyangou147/TS-SAM.\n","authors":["Yang Yu","Chen Xu","Kai Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00569v4","updated":"2024-08-03T17:52:43Z","published":"2024-06-30T03:04:11Z","title":"Investigating and Mitigating the Multimodal Hallucination Snowballing in\n  Large Vision-Language Models","summary":"  Though advanced in understanding visual information with human languages,\nLarge Vision-Language Models (LVLMs) still suffer from multimodal\nhallucinations. A natural concern is that during multimodal interaction, the\ngenerated hallucinations could influence the LVLMs' subsequent generation.\nThus, we raise a question: When presented with a query relevant to the\npreviously generated hallucination, will LVLMs be misled and respond\nincorrectly, even though the ground visual information exists? To answer this,\nwe propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when\nencountering generated hallucinations, where LVLMs are required to answer\nspecific visual questions within a curated hallucinatory conversation.\nCrucially, our experiment shows that the performance of open-source LVLMs drops\nby at least $31\\%$, indicating that LVLMs are prone to accept the generated\nhallucinations and make false claims that they would not have supported without\ndistractions. We term this phenomenon Multimodal Hallucination Snowballing. To\nmitigate this, we further propose a training-free method called Residual Visual\nDecoding, where we revise the output distribution of LVLMs with the one derived\nfrom the residual visual input, providing models with direct access to the\nvisual information. Experiments show that our method can mitigate more than\n$24\\%$ of the snowballed multimodal hallucination while maintaining\ncapabilities.\n","authors":["Weihong Zhong","Xiaocheng Feng","Liang Zhao","Qiming Li","Lei Huang","Yuxuan Gu","Weitao Ma","Yuan Xu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2407.00569v4.pdf","comment":"Accepted to ACL 2024 Main Conference. 21 pages, 20 figures"},{"id":"http://arxiv.org/abs/2408.01831v1","updated":"2024-08-03T17:50:13Z","published":"2024-08-03T17:50:13Z","title":"A Deep CNN Model for Ringing Effect Attenuation of Vibroseis Data","summary":"  In the field of exploration geophysics, seismic vibrator is one of the widely\nused seismic sources to acquire seismic data, which is usually named vibroseis.\n\"Ringing effect\" is a common problem in vibroseis data processing due to the\nlimited frequency bandwidth of the vibrator, which degrades the performance of\nfirst-break picking. In this paper, we proposed a novel deringing model for\nvibroseis data using deep convolutional neural network (CNN). In this model we\nuse end-to-end training strategy to obtain the deringed data directly, and skip\nconnections to improve model training process and preserve the details of\nvibroseis data. For real vibroseis deringing task we synthesize training data\nand corresponding labels from real vibroseis data and utilize them to train the\ndeep CNN model. Experiments are conducted both on synthetic data and real\nvibroseis data. The experiment results show that deep CNN model can attenuate\nthe ringing effect effectively and expand the bandwidth of vibroseis data. The\nSTA/LTA ratio method for first-break picking also shows improvement on deringed\nvibroseis data using deep CNN model.\n","authors":["Zhuang Jia","Wenkai Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01827v1","updated":"2024-08-03T17:31:58Z","published":"2024-08-03T17:31:58Z","title":"ST-SACLF: Style Transfer Informed Self-Attention Classifier for\n  Bias-Aware Painting Classification","summary":"  Painting classification plays a vital role in organizing, finding, and\nsuggesting artwork for digital and classic art galleries. Existing methods\nstruggle with adapting knowledge from the real world to artistic images during\ntraining, leading to poor performance when dealing with different datasets. Our\ninnovation lies in addressing these challenges through a two-step process.\nFirst, we generate more data using Style Transfer with Adaptive Instance\nNormalization (AdaIN), bridging the gap between diverse styles. Then, our\nclassifier gains a boost with feature-map adaptive spatial attention modules,\nimproving its understanding of artistic details. Moreover, we tackle the\nproblem of imbalanced class representation by dynamically adjusting augmented\nsamples. Through a dual-stage process involving careful hyperparameter search\nand model fine-tuning, we achieve an impressive 87.24\\% accuracy using the\nResNet-50 backbone over 40 training epochs. Our study explores quantitative\nanalyses that compare different pretrained backbones, investigates model\noptimization through ablation studies, and examines how varying augmentation\nlevels affect model performance. Complementing this, our qualitative\nexperiments offer valuable insights into the model's decision-making process\nusing spatial attention and its ability to differentiate between easy and\nchallenging samples based on confidence ranking.\n","authors":["Mridula Vijendran","Frederick W. B. Li","Jingjing Deng","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2408.01827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01826v1","updated":"2024-08-03T17:18:26Z","published":"2024-08-03T17:18:26Z","title":"GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent\n  Diffusion Transformer","summary":"  3D speech-driven facial animation generation has received much attention in\nboth industrial applications and academic research. Since the non-verbal facial\ncues that exist across the face in reality are non-deterministic, the generated\nresults should be diverse. However, most recent methods are deterministic\nmodels that cannot learn a many-to-many mapping between audio and facial motion\nto generate diverse facial animations. To address this problem, we propose\nGLDiTalker, which introduces a motion prior along with some stochasticity to\nreduce the uncertainty of cross-modal mapping while increasing non-determinacy\nof the non-verbal facial cues that reside throughout the face. Particularly,\nGLDiTalker uses VQ-VAE to map facial motion mesh sequences into latent space in\nthe first stage, and then iteratively adds and removes noise to the latent\nfacial motion features in the second stage. In order to integrate different\nlevels of spatial information, the Spatial Pyramidal SpiralConv Encoder is also\ndesigned to extract multi-scale features. Extensive qualitative and\nquantitative experiments demonstrate that our method achieves the\nstate-of-the-art performance.\n","authors":["Yihong Lin","Lingyu Xiong","Xiandong Li","Wenxiong Kang","Xianjia Wu","Liang Peng","Songju Lei","Huang Xu","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2408.01826v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.04684v4","updated":"2024-08-03T15:49:01Z","published":"2023-07-10T16:37:46Z","title":"FreeDrag: Feature Dragging for Reliable Point-based Image Editing","summary":"  To serve the intricate and varied demands of image editing, precise and\nflexible manipulation in image content is indispensable. Recently, Drag-based\nediting methods have gained impressive performance. However, these methods\npredominantly center on point dragging, resulting in two noteworthy drawbacks,\nnamely \"miss tracking\", where difficulties arise in accurately tracking the\npredetermined handle points, and \"ambiguous tracking\", where tracked points are\npotentially positioned in wrong regions that closely resemble the handle\npoints. To address the above issues, we propose FreeDrag, a feature dragging\nmethodology designed to free the burden on point tracking. The FreeDrag\nincorporates two key designs, i.e., template feature via adaptive updating and\nline search with backtracking, the former improves the stability against\ndrastic content change by elaborately controls feature updating scale after\neach dragging, while the latter alleviates the misguidance from similar points\nby actively restricting the search area in a line. These two technologies\ntogether contribute to a more stable semantic dragging with higher efficiency.\nComprehensive experimental results substantiate that our approach significantly\noutperforms pre-existing methodologies, offering reliable point-based editing\neven in various complex scenarios.\n","authors":["Pengyang Ling","Lin Chen","Pan Zhang","Huaian Chen","Yi Jin","Jinjin Zheng"],"pdf_url":"https://arxiv.org/pdf/2307.04684v4.pdf","comment":"13 pages, 16 figures"},{"id":"http://arxiv.org/abs/2408.01812v1","updated":"2024-08-03T15:43:56Z","published":"2024-08-03T15:43:56Z","title":"SkyDiffusion: Street-to-Satellite Image Synthesis with Diffusion Models\n  and BEV Paradigm","summary":"  Street-to-satellite image synthesis focuses on generating realistic satellite\nimages from corresponding ground street-view images while maintaining a\nconsistent content layout, similar to looking down from the sky. The\nsignificant differences in perspectives create a substantial domain gap between\nthe views, making this cross-view generation task particularly challenging. In\nthis paper, we introduce SkyDiffusion, a novel cross-view generation method for\nsynthesizing satellite images from street-view images, leveraging diffusion\nmodels and Bird's Eye View (BEV) paradigm. First, we design a Curved-BEV method\nto transform street-view images to the satellite view, reformulating the\nchallenging cross-domain image synthesis task into a conditional generation\nproblem. Curved-BEV also includes a \"Multi-to-One\" mapping strategy for\ncombining multiple street-view images within the same satellite coverage area,\neffectively solving the occlusion issues in dense urban scenes. Next, we design\na BEV-controlled diffusion model to generate satellite images consistent with\nthe street-view content, which also incorporates a light manipulation module to\noptimize the lighting condition of the synthesized image using a reference\nsatellite. Experimental results demonstrate that SkyDiffusion outperforms\nstate-of-the-art methods on both suburban (CVUSA & CVACT) and urban\n(VIGOR-Chicago) cross-view datasets, with an average SSIM increase of 14.5% and\na FID reduction of 29.6%, achieving realistic and content-consistent satellite\nimage generation. The code and models of this work will be released at\nhttps://opendatalab.github.io/skydiffusion/.\n","authors":["Junyan Ye","Jun He","Weijia Li","Zhutao Lv","Jinhua Yu","Haote Yang","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2408.01812v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.01800v1","updated":"2024-08-03T15:02:21Z","published":"2024-08-03T15:02:21Z","title":"MiniCPM-V: A GPT-4V Level MLLM on Your Phone","summary":"  The recent surge of Multimodal Large Language Models (MLLMs) has\nfundamentally reshaped the landscape of AI research and industry, shedding\nlight on a promising path toward the next AI milestone. However, significant\nchallenges remain preventing MLLMs from being practical in real-world\napplications. The most notable challenge comes from the huge cost of running an\nMLLM with a massive number of parameters and extensive computation. As a\nresult, most MLLMs need to be deployed on high-performing cloud servers, which\ngreatly limits their application scopes such as mobile, offline,\nenergy-sensitive, and privacy-protective scenarios. In this work, we present\nMiniCPM-V, a series of efficient MLLMs deployable on end-side devices. By\nintegrating the latest MLLM techniques in architecture, pretraining and\nalignment, the latest MiniCPM-Llama3-V 2.5 has several notable features: (1)\nStrong performance, outperforming GPT-4V-1106, Gemini Pro and Claude 3 on\nOpenCompass, a comprehensive evaluation over 11 popular benchmarks, (2) strong\nOCR capability and 1.8M pixel high-resolution image perception at any aspect\nratio, (3) trustworthy behavior with low hallucination rates, (4) multilingual\nsupport for 30+ languages, and (5) efficient deployment on mobile phones. More\nimportantly, MiniCPM-V can be viewed as a representative example of a promising\ntrend: The model sizes for achieving usable (e.g., GPT-4V) level performance\nare rapidly decreasing, along with the fast growth of end-side computation\ncapacity. This jointly shows that GPT-4V level MLLMs deployed on end devices\nare becoming increasingly possible, unlocking a wider spectrum of real-world AI\napplications in the near future.\n","authors":["Yuan Yao","Tianyu Yu","Ao Zhang","Chongyi Wang","Junbo Cui","Hongji Zhu","Tianchi Cai","Haoyu Li","Weilin Zhao","Zhihui He","Qianyu Chen","Huarong Zhou","Zhensheng Zou","Haoye Zhang","Shengding Hu","Zhi Zheng","Jie Zhou","Jie Cai","Xu Han","Guoyang Zeng","Dahai Li","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01800v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2404.12634v2","updated":"2024-08-03T14:53:25Z","published":"2024-04-19T05:31:37Z","title":"Transformer-Based Classification Outcome Prediction for Multimodal\n  Stroke Treatment","summary":"  This study proposes a multi-modal fusion framework Multitrans based on the\nTransformer architecture and self-attention mechanism. This architecture\ncombines the study of non-contrast computed tomography (NCCT) images and\ndischarge diagnosis reports of patients undergoing stroke treatment, using a\nvariety of methods based on Transformer architecture approach to predicting\nfunctional outcomes of stroke treatment. The results show that the performance\nof single-modal text classification is significantly better than single-modal\nimage classification, but the effect of multi-modal combination is better than\nany single modality. Although the Transformer model only performs worse on\nimaging data, when combined with clinical meta-diagnostic information, both can\nlearn better complementary information and make good contributions to\naccurately predicting stroke treatment effects..\n","authors":["Danqing Ma","Meng Wang","Ao Xiang","Zongqing Qi","Qin Yang"],"pdf_url":"https://arxiv.org/pdf/2404.12634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01797v1","updated":"2024-08-03T14:48:34Z","published":"2024-08-03T14:48:34Z","title":"NuLite -- Lightweight and Fast Model for Nuclei Instance Segmentation\n  and Classification","summary":"  In pathology, accurate and efficient analysis of Hematoxylin and Eosin (H\\&E)\nslides is crucial for timely and effective cancer diagnosis. Although many deep\nlearning solutions for nuclei instance segmentation and classification exist in\nthe literature, they often entail high computational costs and resource\nrequirements, thus limiting their practical usage in medical applications. To\naddress this issue, we introduce a novel convolutional neural network, NuLite,\na U-Net-like architecture designed explicitly on Fast-ViT, a state-of-the-art\n(SOTA) lightweight CNN. We obtained three versions of our model, NuLite-S,\nNuLite-M, and NuLite-H, trained on the PanNuke dataset. The experimental\nresults prove that our models equal CellViT (SOTA) in terms of panoptic quality\nand detection. However, our lightest model, NuLite-S, is 40 times smaller in\nterms of parameters and about 8 times smaller in terms of GFlops, while our\nheaviest model is 17 times smaller in terms of parameters and about 7 times\nsmaller in terms of GFlops. Moreover, our model is up to about 8 times faster\nthan CellViT. Lastly, to prove the effectiveness of our solution, we provide a\nrobust comparison of external datasets, namely CoNseP, MoNuSeg, and GlySAC. Our\nmodel is publicly available at https://github.com/CosmoIknosLab/NuLite\n","authors":["Cristian Tommasino","Cristiano Russo","Antonio Maria Rinaldi"],"pdf_url":"https://arxiv.org/pdf/2408.01797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19524v2","updated":"2024-08-03T14:36:06Z","published":"2024-07-28T16:24:07Z","title":"VersusDebias: Universal Zero-Shot Debiasing for Text-to-Image Models via\n  SLM-Based Prompt Engineering and Generative Adversary","summary":"  With the rapid development of Text-to-Image models, biases in human image\ngeneration against demographic groups social attract more and more concerns.\nExisting methods are designed based on certain models with fixed prompts,\nunable to accommodate the trend of high-speed updating of Text-to-Image (T2I)\nmodels and variable prompts in practical scenes. Additionally, they fail to\nconsider the possibility of hallucinations, leading to deviations between\nexpected and actual results. To address this issue, we introduce VersusDebias,\na novel and universal debiasing framework for biases in T2I models, consisting\nof one generative adversarial mechanism (GAM) and one debiasing generation\nmechanism using a small language model (SLM). The self-adaptive GAM generates\nspecialized attribute arrays for each prompts for diminishing the influence of\nhallucinations from T2I models. The SLM uses prompt engineering to generate\ndebiased prompts for the T2I model, providing zero-shot debiasing ability and\ncustom optimization for different models. Extensive experiments demonstrate\nVersusDebias's capability to rectify biases on arbitrary models across multiple\nprotected attributes simultaneously, including gender, race, and age.\nFurthermore, VersusDebias outperforms existing methods in both zero-shot and\nfew-shot situations, illustrating its extraordinary utility. Our work is openly\naccessible to the research community to ensure the reproducibility.\n","authors":["Hanjun Luo","Ziye Deng","Haoyu Huang","Xuecheng Liu","Ruizhe Chen","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2407.19524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08748v2","updated":"2024-08-03T13:50:40Z","published":"2024-04-12T18:21:08Z","title":"Multi-Branch Generative Models for Multichannel Imaging with an\n  Application to PET/CT Synergistic Reconstruction","summary":"  This paper presents a novel approach for learned synergistic reconstruction\nof medical images using multi-branch generative models. Leveraging variational\nautoencoders (VAEs), our model learns from pairs of images simultaneously,\nenabling effective denoising and reconstruction. Synergistic image\nreconstruction is achieved by incorporating the trained models in a regularizer\nthat evaluates the distance between the images and the model. We demonstrate\nthe efficacy of our approach on both Modified National Institute of Standards\nand Technology (MNIST) and positron emission tomography (PET)/computed\ntomography (CT) datasets, showcasing improved image quality for low-dose\nimaging. Despite challenges such as patch decomposition and model limitations,\nour results underscore the potential of generative models for enhancing medical\nimaging reconstruction.\n","authors":["Noel Jeffrey Pinton","Alexandre Bousse","Catherine Cheze-Le-Rest","Dimitris Visvikis"],"pdf_url":"https://arxiv.org/pdf/2404.08748v2.pdf","comment":"12 pages, 16 figures, 2 tables, submitted to IEEE TRPMS"},{"id":"http://arxiv.org/abs/2407.18449v2","updated":"2024-08-03T13:36:24Z","published":"2024-07-26T01:12:54Z","title":"Towards A Generalizable Pathology Foundation Model via Unified Knowledge\n  Distillation","summary":"  Foundation models pretrained on large-scale datasets are revolutionizing the\nfield of computational pathology (CPath). The generalization ability of\nfoundation models is crucial for the success in various downstream clinical\ntasks. However, current foundation models have only been evaluated on a limited\ntype and number of tasks, leaving their generalization ability and overall\nperformance unclear. To address this gap, we established a most comprehensive\nbenchmark to evaluate the performance of off-the-shelf foundation models across\nsix distinct clinical task types, encompassing a total of 39 specific tasks.\nOur findings reveal that existing foundation models excel at certain task types\nbut struggle to effectively handle the full breadth of clinical tasks. To\nimprove the generalization of pathology foundation models, we propose a unified\nknowledge distillation framework consisting of both expert and self knowledge\ndistillation, where the former allows the model to learn from the knowledge of\nmultiple expert models, while the latter leverages self-distillation to enable\nimage representation learning via local-global alignment. Based on this\nframework, a Generalizable Pathology Foundation Model (GPFM) is pretrained on a\nlarge-scale dataset consisting of 190 million images from around 86,000 public\nH&E whole slides across 34 major tissue types. Evaluated on the established\nbenchmark, GPFM achieves an impressive average rank of 1.36, with 29 tasks\nranked 1st, while the the second-best model, UNI, attains an average rank of\n2.96, with only 4 tasks ranked 1st. The superior generalization of GPFM\ndemonstrates its exceptional modeling capabilities across a wide range of\nclinical tasks, positioning it as a new cornerstone for feature representation\nin CPath.\n","authors":["Jiabo Ma","Zhengrui Guo","Fengtao Zhou","Yihui Wang","Yingxue Xu","Yu Cai","Zhengjie Zhu","Cheng Jin","Yi Lin","Xinrui Jiang","Anjia Han","Li Liang","Ronald Cheong Kin Chan","Jiguang Wang","Kwang-Ting Cheng","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.18449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01774v1","updated":"2024-08-03T13:06:04Z","published":"2024-08-03T13:06:04Z","title":"STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver\n  Attention to Predict Driver Behaviors Under Safety-Critical Scenarios","summary":"  Accurate behavior prediction for vehicles is essential but challenging for\nautonomous driving. Most existing studies show satisfying performance under\nregular scenarios, but most neglected safety-critical scenarios. In this study,\na spatio-temporal dual-encoder network named STDA for safety-critical scenarios\nwas developed. Considering the exceptional capabilities of human drivers in\nterms of situational awareness and comprehending risks, driver attention was\nincorporated into STDA to facilitate swift identification of the critical\nregions, which is expected to improve both performance and interpretability.\nSTDA contains four parts: the driver attention prediction module, which\npredicts driver attention; the fusion module designed to fuse the features\nbetween driver attention and raw images; the temporary encoder module used to\nenhance the capability to interpret dynamic scenes; and the behavior prediction\nmodule to predict the behavior. The experiment data are used to train and\nvalidate the model. The results show that STDA improves the G-mean from 0.659\nto 0.719 when incorporating driver attention and adopting a temporal encoder\nmodule. In addition, extensive experimentation has been conducted to validate\nthat the proposed module exhibits robust generalization capabilities and can be\nseamlessly integrated into other mainstream models.\n","authors":["Dongyang Xu","Yiran Luo","Tianle Lu","Qingfan Wang","Qing Zhou","Bingbing Nie"],"pdf_url":"https://arxiv.org/pdf/2408.01774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01767v1","updated":"2024-08-03T12:34:30Z","published":"2024-08-03T12:34:30Z","title":"Comparison of Embedded Spaces for Deep Learning Classification","summary":"  Embedded spaces are a key feature in deep learning. Good embedded spaces\nrepresent the data well to support classification and advanced techniques such\nas open-set recognition, few-short learning and explainability. This paper\npresents a compact overview of different techniques to design embedded spaces\nfor classification. It compares different loss functions and constraints on the\nnetwork parameters with respect to the achievable geometric structure of the\nembedded space. The techniques are demonstrated with two and three-dimensional\nembeddings for the MNIST, Fashion MNIST and CIFAR-10 datasets, allowing visual\ninspection of the embedded spaces.\n","authors":["Stefan Scholl"],"pdf_url":"https://arxiv.org/pdf/2408.01767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01766v1","updated":"2024-08-03T12:33:21Z","published":"2024-08-03T12:33:21Z","title":"MultiFuser: Multimodal Fusion Transformer for Enhanced Driver Action\n  Recognition","summary":"  Driver action recognition, aiming to accurately identify drivers' behaviours,\nis crucial for enhancing driver-vehicle interactions and ensuring driving\nsafety. Unlike general action recognition, drivers' environments are often\nchallenging, being gloomy and dark, and with the development of sensors,\nvarious cameras such as IR and depth cameras have emerged for analyzing\ndrivers' behaviors. Therefore, in this paper, we propose a novel multimodal\nfusion transformer, named MultiFuser, which identifies cross-modal\ninterrelations and interactions among multimodal car cabin videos and\nadaptively integrates different modalities for improved representations.\nSpecifically, MultiFuser comprises layers of Bi-decomposed Modules to model\nspatiotemporal features, with a modality synthesizer for multimodal features\nintegration. Each Bi-decomposed Module includes a Modal Expertise ViT block for\nextracting modality-specific features and a Patch-wise Adaptive Fusion block\nfor efficient cross-modal fusion. Extensive experiments are conducted on\nDrive&Act dataset and the results demonstrate the efficacy of our proposed\napproach.\n","authors":["Ruoyu Wang","Wenqian Wang","Jianjun Gao","Dan Lin","Kim-Hui Yap","Bingbing Li"],"pdf_url":"https://arxiv.org/pdf/2408.01766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11340v3","updated":"2024-08-03T12:23:25Z","published":"2024-06-17T08:57:00Z","title":"CM2-Net: Continual Cross-Modal Mapping Network for Driver Action\n  Recognition","summary":"  Driver action recognition has significantly advanced in enhancing\ndriver-vehicle interactions and ensuring driving safety by integrating multiple\nmodalities, such as infrared and depth. Nevertheless, compared to RGB modality\nonly, it is always laborious and costly to collect extensive data for all types\nof non-RGB modalities in car cabin environments. Therefore, previous works have\nsuggested independently learning each non-RGB modality by fine-tuning a model\npre-trained on RGB videos, but these methods are less effective in extracting\ninformative features when faced with newly-incoming modalities due to large\ndomain gaps. In contrast, we propose a Continual Cross-Modal Mapping Network\n(CM2-Net) to continually learn each newly-incoming modality with instructive\nprompts from the previously-learned modalities. Specifically, we have developed\nAccumulative Cross-modal Mapping Prompting (ACMP), to map the discriminative\nand informative features learned from previous modalities into the feature\nspace of newly-incoming modalities. Then, when faced with newly-incoming\nmodalities, these mapped features are able to provide effective prompts for\nwhich features should be extracted and prioritized. These prompts are\naccumulating throughout the continual learning process, thereby boosting\nfurther recognition performances. Extensive experiments conducted on the\nDrive&Act dataset demonstrate the performance superiority of CM2-Net on both\nuni- and multi-modal driver action recognition.\n","authors":["Ruoyu Wang","Chen Cai","Wenqian Wang","Jianjun Gao","Dan Lin","Wenyang Liu","Kim-Hui Yap"],"pdf_url":"https://arxiv.org/pdf/2406.11340v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01752v1","updated":"2024-08-03T11:16:00Z","published":"2024-08-03T11:16:00Z","title":"Advancing Green AI: Efficient and Accurate Lightweight CNNs for Rice\n  Leaf Disease Identification","summary":"  Rice plays a vital role as a primary food source for over half of the world's\npopulation, and its production is critical for global food security.\nNevertheless, rice cultivation is frequently affected by various diseases that\ncan severely decrease yield and quality. Therefore, early and accurate\ndetection of rice diseases is necessary to prevent their spread and minimize\ncrop losses. In this research, we explore three mobile-compatible CNN\narchitectures, namely ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice\nleaf disease classification. These models are selected due to their\ncompatibility with mobile devices, as they demand less computational power and\nmemory compared to other CNN models. To enhance the performance of the three\nmodels, we added two fully connected layers separated by a dropout layer. We\nused early stop creation to prevent the model from being overfiting. The\nresults of the study showed that the best performance was achieved by the\nEfficientNet-B0 model with an accuracy of 99.8%. Meanwhile, MobileNetV2 and\nShuffleNet only achieved accuracies of 84.21% and 66.51%, respectively. This\nstudy shows that EfficientNet-B0 when combined with the proposed layer and\nearly stop, can produce a high-accuracy model.\n  Keywords: rice leaf detection; green AI; smart agriculture; EfficientNet;\n","authors":["Khairun Saddami","Yudha Nurdin","Mutia Zahramita","Muhammad Shahreeza Safiruz"],"pdf_url":"https://arxiv.org/pdf/2408.01752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01746v1","updated":"2024-08-03T11:06:47Z","published":"2024-08-03T11:06:47Z","title":"Domain penalisation for improved Out-of-Distribution Generalisation","summary":"  In the field of object detection, domain generalisation (DG) aims to ensure\nrobust performance across diverse and unseen target domains by learning the\nrobust domain-invariant features corresponding to the objects of interest\nacross multiple source domains. While there are many approaches established for\nperforming DG for the task of classification, there has been a very little\nfocus on object detection. In this paper, we propose a domain penalisation (DP)\nframework for the task of object detection, where the data is assumed to be\nsampled from multiple source domains and tested on completely unseen test\ndomains. We assign penalisation weights to each domain, with the values updated\nbased on the detection networks performance on the respective source domains.\nBy prioritising the domains that needs more attention, our approach effectively\nbalances the training process. We evaluate our solution on the GWHD 2021\ndataset, a component of the WiLDS benchmark and we compare against ERM and\nGroupDRO as these are primarily loss function based. Our extensive experimental\nresults reveals that the proposed approach improves the accuracy by 0.3 percent\nand 0.5 percent on validation and test out-of-distribution (OOD) sets,\nrespectively for FasterRCNN. We also compare the performance of our approach on\nFCOS detector and show that our approach improves the baseline OOD performance\nover the existing approaches by 1.3 percent and 1.4 percent on validation and\ntest sets, respectively. This study underscores the potential of performance\nbased domain penalisation in enhancing the generalisation ability of object\ndetection models across diverse environments.\n","authors":["Shuvam Jena","Sushmetha Sumathi Rajendran","Karthik Seemakurthy","Sasithradevi A","Vijayalakshmi M","Prakash Poornachari"],"pdf_url":"https://arxiv.org/pdf/2408.01746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10269v2","updated":"2024-08-03T10:50:21Z","published":"2023-08-20T14:00:33Z","title":"Domain Reduction Strategy for Non Line of Sight Imaging","summary":"  This paper presents a novel optimization-based method for non-line-of-sight\n(NLOS) imaging that aims to reconstruct hidden scenes under general setups with\nsignificantly reduced reconstruction time. In NLOS imaging, the visible\nsurfaces of the target objects are notably sparse. To mitigate unnecessary\ncomputations arising from empty regions, we design our method to render the\ntransients through partial propagations from a continuously sampled set of\npoints from the hidden space. Our method is capable of accurately and\nefficiently modeling the view-dependent reflectance using surface normals,\nwhich enables us to obtain surface geometry as well as albedo. In this\npipeline, we propose a novel domain reduction strategy to eliminate superfluous\ncomputations in empty regions. During the optimization process, our domain\nreduction procedure periodically prunes the empty regions from our sampling\ndomain in a coarse-to-fine manner, leading to substantial improvement in\nefficiency. We demonstrate the effectiveness of our method in various NLOS\nscenarios with sparse scanning patterns. Experiments conducted on both\nsynthetic and real-world data support the efficacy in general NLOS scenarios,\nand the improved efficiency of our method compared to the previous\noptimization-based solutions. Our code is available at\nhttps://github.com/hyunbo9/domain-reduction-strategy.\n","authors":["Hyunbo Shim","In Cho","Daekyu Kwon","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2308.10269v2.pdf","comment":"27 pages, 15 figures"},{"id":"http://arxiv.org/abs/2408.01739v1","updated":"2024-08-03T10:50:07Z","published":"2024-08-03T10:50:07Z","title":"LAM3D: Leveraging Attention for Monocular 3D Object Detection","summary":"  Since the introduction of the self-attention mechanism and the adoption of\nthe Transformer architecture for Computer Vision tasks, the Vision\nTransformer-based architectures gained a lot of popularity in the field, being\nused for tasks such as image classification, object detection and image\nsegmentation. However, efficiently leveraging the attention mechanism in vision\ntransformers for the Monocular 3D Object Detection task remains an open\nquestion. In this paper, we present LAM3D, a framework that Leverages\nself-Attention mechanism for Monocular 3D object Detection. To do so, the\nproposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as\nfeature extraction backbone and 2D/3D detection machinery. We evaluate the\nproposed method on the KITTI 3D Object Detection Benchmark, proving the\napplicability of the proposed solution in the autonomous driving domain and\noutperforming reference methods. Moreover, due to the usage of self-attention,\nLAM3D is able to systematically outperform the equivalent architecture that\ndoes not employ self-attention.\n","authors":["Diana-Alexandra Sas","Leandro Di Bella","Yangxintong Lyu","Florin Oniga","Adrian Munteanu"],"pdf_url":"https://arxiv.org/pdf/2408.01739v1.pdf","comment":"6 pages. Accepted to MMSP 2024"},{"id":"http://arxiv.org/abs/2408.01732v1","updated":"2024-08-03T10:19:38Z","published":"2024-08-03T10:19:38Z","title":"Landmark-guided Diffusion Model for High-fidelity and Temporally\n  Coherent Talking Head Generation","summary":"  Audio-driven talking head generation is a significant and challenging task\napplicable to various fields such as virtual avatars, film production, and\nonline conferences. However, the existing GAN-based models emphasize generating\nwell-synchronized lip shapes but overlook the visual quality of generated\nframes, while diffusion-based models prioritize generating high-quality frames\nbut neglect lip shape matching, resulting in jittery mouth movements. To\naddress the aforementioned problems, we introduce a two-stage diffusion-based\nmodel. The first stage involves generating synchronized facial landmarks based\non the given speech. In the second stage, these generated landmarks serve as a\ncondition in the denoising process, aiming to optimize mouth jitter issues and\ngenerate high-fidelity, well-synchronized, and temporally coherent talking head\nvideos. Extensive experiments demonstrate that our model yields the best\nperformance.\n","authors":["Jintao Tan","Xize Cheng","Lingyu Xiong","Lei Zhu","Xiandong Li","Xianjia Wu","Kai Gong","Minglei Li","Yi Cai"],"pdf_url":"https://arxiv.org/pdf/2408.01732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01728v1","updated":"2024-08-03T10:01:29Z","published":"2024-08-03T10:01:29Z","title":"Survey on Emotion Recognition through Posture Detection and the\n  possibility of its application in Virtual Reality","summary":"  A survey is presented focused on using pose estimation techniques in\nEmotional recognition using various technologies normal cameras, and depth\ncameras for real-time, and the potential use of VR and inputs including images,\nvideos, and 3-dimensional poses described in vector space. We discussed 19\nresearch papers collected from selected journals and databases highlighting\ntheir methodology, classification algorithm, and the used datasets that relate\nto emotion recognition and pose estimation. A benchmark has been made according\nto their accuracy as it was the most common performance measurement metric\nused. We concluded that the multimodal Approaches overall made the best\naccuracy and then we mentioned futuristic concerns that can improve the\ndevelopment of this research topic.\n","authors":["Leina Elansary","Zaki Taha","Walaa Gad"],"pdf_url":"https://arxiv.org/pdf/2408.01728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01723v1","updated":"2024-08-03T09:27:57Z","published":"2024-08-03T09:27:57Z","title":"A Novel Evaluation Framework for Image2Text Generation","summary":"  Evaluating the quality of automatically generated image descriptions is\nchallenging, requiring metrics that capture various aspects such as\ngrammaticality, coverage, correctness, and truthfulness. While human evaluation\noffers valuable insights, its cost and time-consuming nature pose limitations.\nExisting automated metrics like BLEU, ROUGE, METEOR, and CIDEr aim to bridge\nthis gap but often show weak correlations with human judgment. We address this\nchallenge by introducing a novel evaluation framework rooted in a modern large\nlanguage model (LLM), such as GPT-4 or Gemini, capable of image generation. In\nour proposed framework, we begin by feeding an input image into a designated\nimage captioning model, chosen for evaluation, to generate a textual\ndescription. Using this description, an LLM then creates a new image. By\nextracting features from both the original and LLM-created images, we measure\ntheir similarity using a designated similarity metric. A high similarity score\nsuggests that the image captioning model has accurately generated textual\ndescriptions, while a low similarity score indicates discrepancies, revealing\npotential shortcomings in the model's performance. Human-annotated reference\ncaptions are not required in our proposed evaluation framework, which serves as\na valuable tool for evaluating the effectiveness of image captioning models.\nIts efficacy is confirmed through human evaluation.\n","authors":["Jia-Hong Huang","Hongyi Zhu","Yixian Shen","Stevan Rudinac","Alessio M. Pacces","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2408.01723v1.pdf","comment":"The paper has been accepted for presentation at the 47th\n  International ACM SIGIR Conference on Research and Development in Information\n  Retrieval, specifically in the Large Language Model for Evaluation in IR\n  (LLM4Eval) Workshop in 2024"},{"id":"http://arxiv.org/abs/2405.01726v7","updated":"2024-08-03T09:18:32Z","published":"2024-05-02T20:44:26Z","title":"SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral\n  Image Denoising","summary":"  Denoising is a crucial preprocessing step for hyperspectral images (HSIs) due\nto noise arising from intra-imaging mechanisms and environmental factors.\nLong-range spatial-spectral correlation modeling is beneficial for HSI\ndenoising but often comes with high computational complexity. Based on the\nstate space model (SSM), Mamba is known for its remarkable long-range\ndependency modeling capabilities and computational efficiency. Building on\nthis, we introduce a memory-efficient spatial-spectral UMamba (SSUMamba) for\nHSI denoising, with the spatial-spectral continuous scan (SSCS) Mamba being the\ncore component. SSCS Mamba alternates the row, column, and band in six\ndifferent orders to generate the sequence and uses the bidirectional SSM to\nexploit long-range spatial-spectral dependencies. In each order, the images are\nrearranged between adjacent scans to ensure spatial-spectral continuity.\nAdditionally, 3D convolutions are embedded into the SSCS Mamba to enhance local\nspatial-spectral modeling. Experiments demonstrate that SSUMamba achieves\nsuperior denoising results with lower memory consumption per batch compared to\ntransformer-based methods. The source code is available at\nhttps://github.com/lronkitty/SSUMamba.\n","authors":["Guanyiman Fu","Fengchao Xiong","Jianfeng Lu","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.01726v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01716v1","updated":"2024-08-03T09:10:38Z","published":"2024-08-03T09:10:38Z","title":"Visual-Inertial SLAM for Agricultural Robotics: Benchmarking the\n  Benefits and Computational Costs of Loop Closing","summary":"  Simultaneous Localization and Mapping (SLAM) is essential for mobile\nrobotics, enabling autonomous navigation in dynamic, unstructured outdoor\nenvironments without relying on external positioning systems. In agricultural\napplications, where environmental conditions can be particularly challenging\ndue to variable lighting or weather conditions, Visual-Inertial SLAM has\nemerged as a potential solution. This paper benchmarks several open-source\nVisual-Inertial SLAM systems, including ORB-SLAM3, VINS-Fusion, OpenVINS,\nKimera, and SVO Pro, to evaluate their performance in agricultural settings. We\nfocus on the impact of loop closing on localization accuracy and computational\ndemands, providing a comprehensive analysis of these systems' effectiveness in\nreal-world environments and especially their application to embedded systems in\nagricultural robotics. Our contributions further include an assessment of\nvarying frame rates on localization accuracy and computational load. The\nfindings highlight the importance of loop closing in improving localization\naccuracy while managing computational resources efficiently, offering valuable\ninsights for optimizing Visual-Inertial SLAM systems for practical outdoor\napplications in mobile robotics.\n","authors":["Fabian Schmidt","Constantin Blessing","Markus Enzweiler","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2408.01716v1.pdf","comment":"18 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.01059v2","updated":"2024-08-03T08:52:32Z","published":"2024-06-03T07:14:19Z","title":"VIP: Versatile Image Outpainting Empowered by Multimodal Large Language\n  Model","summary":"  In this paper, we focus on resolving the problem of image outpainting, which\naims to extrapolate the surrounding parts given the center contents of an\nimage. Although recent works have achieved promising performance, the lack of\nversatility and customization hinders their practical applications in broader\nscenarios. Therefore, this work presents a novel image outpainting framework\nthat is capable of customizing the results according to the requirement of\nusers. First of all, we take advantage of a Multimodal Large Language Model\n(MLLM) that automatically extracts and organizes the corresponding textual\ndescriptions of the masked and unmasked part of a given image. Accordingly, the\nobtained text prompts are introduced to endow our model with the capacity to\ncustomize the outpainting results. In addition, a special Cross-Attention\nmodule, namely Center-Total-Surrounding (CTS), is elaborately designed to\nenhance further the the interaction between specific space regions of the image\nand corresponding parts of the text prompts. Note that unlike most existing\nmethods, our approach is very resource-efficient since it is just slightly\nfine-tuned on the off-the-shelf stable diffusion (SD) model rather than being\ntrained from scratch. Finally, the experimental results on three commonly used\ndatasets, i.e. Scenery, Building, and WikiArt, demonstrate our model\nsignificantly surpasses the SoTA methods. Moreover, versatile outpainting\nresults are listed to show its customized ability.\n","authors":["Jinze Yang","Haoran Wang","Zining Zhu","Chenglong Liu","Meng Wymond Wu","Zeke Xie","Zhong Ji","Jungong Han","Mingming Sun"],"pdf_url":"https://arxiv.org/pdf/2406.01059v2.pdf","comment":"Our source code is available at: https://github.com/ucasyjz/VIP, 15\n  pages"},{"id":"http://arxiv.org/abs/2408.01712v1","updated":"2024-08-03T08:41:07Z","published":"2024-08-03T08:41:07Z","title":"A General Ambiguity Model for Binary Edge Images with Edge Tracing and\n  its Implementation","summary":"  We present a general and intuitive ambiguity model for intersections,\njunctions and other structures in binary edge images. The model is combined\nwith edge tracing, where edges are ordered sequences of connected pixels. The\nobjective is to provide a versatile preprocessing method for tasks such as\nfigure-ground segmentation, object recognition, topological analysis, etc. By\nusing only a small set of straightforward principles, the results are intuitive\nto describe. This helps to implement subsequent processing steps, such as\nresolving ambiguous edge connections at junctions. By using an augmented edge\nmap, neighboring edges can be directly accessed using quick local search\noperations. The edge tracing uses recursion, which leads to compact programming\ncode. We explain our algorithm using pseudocode, compare it with related\nmethods, and show how simple modular postprocessing steps can be used to\noptimize the results. The complete algorithm, including all data structures,\nrequires less than 50 lines of pseudocode. We also provide a C++ implementation\nof our method.\n","authors":["Markus Hennig","Marc Leineke","Brbel Mertsching"],"pdf_url":"https://arxiv.org/pdf/2408.01712v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.01708v1","updated":"2024-08-03T08:25:26Z","published":"2024-08-03T08:25:26Z","title":"AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual\n  Segmentation","summary":"  Recently, transformer-based models have demonstrated remarkable performance\non audio-visual segmentation (AVS) tasks. However, their expensive\ncomputational cost makes real-time inference impractical. By characterizing\nattention maps of the network, we identify two key obstacles in AVS models: 1)\nattention dissipation, corresponding to the over-concentrated attention weights\nby Softmax within restricted frames, and 2) inefficient, burdensome transformer\ndecoder, caused by narrow focus patterns in early stages. In this paper, we\nintroduce AVESFormer, the first real-time Audio-Visual Efficient Segmentation\ntransformer that achieves fast, efficient and light-weight simultaneously. Our\nmodel leverages an efficient prompt query generator to correct the behaviour of\ncross-attention. Additionally, we propose ELF decoder to bring greater\nefficiency by facilitating convolutions suitable for local features to reduce\ncomputational burdens. Extensive experiments demonstrate that our AVESFormer\nsignificantly enhances model performance, achieving 79.9% on S4, 57.9% on MS3\nand 31.2% on AVSS, outperforming previous state-of-the-art and achieving an\nexcellent trade-off between performance and speed. Code can be found at\nhttps://github.com/MarkXCloud/AVESFormer.git.\n","authors":["Zili Wang","Qi Yang","Linsu Shi","Jiazhong Yu","Qinghua Liang","Fei Li","Shiming Xiang"],"pdf_url":"https://arxiv.org/pdf/2408.01708v1.pdf","comment":null}]},"2024-08-06T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.02336v2","updated":"2024-08-06T04:04:23Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.04485v3","updated":"2024-08-06T16:35:50Z","published":"2024-06-06T20:15:42Z","title":"GenAI Arena: An Open Evaluation Platform for Generative Models","summary":"  Generative AI has made remarkable strides to revolutionize fields such as\nimage and video generation. These advancements are driven by innovative\nalgorithms, architecture, and data. However, the rapid proliferation of\ngenerative models has highlighted a critical gap: the absence of trustworthy\nevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc\noften fail to capture the nuanced quality and user satisfaction associated with\ngenerative outputs. This paper proposes an open platform GenAI-Arena to\nevaluate different image and video generative models, where users can actively\nparticipate in evaluating these models. By leveraging collective user feedback\nand votes, GenAI-Arena aims to provide a more democratic and accurate measure\nof model performance. It covers three arenas for text-to-image generation,\ntext-to-video generation, and image editing respectively. Currently, we cover a\ntotal of 27 open-source generative models. GenAI-Arena has been operating for\nfour months, amassing over 6000 votes from the community. We describe our\nplatform, analyze the data, and explain the statistical methods for ranking the\nmodels. To further promote the research in building model-based evaluation\nmetrics, we release a cleaned version of our preference data for the three\ntasks, namely GenAI-Bench. We prompt the existing multi-modal models like\nGemini, GPT-4o to mimic human voting. We compute the correlation between model\nvoting with human voting to understand their judging abilities. Our results\nshow existing multimodal models are still lagging in assessing the generated\nvisual content, even the best model GPT-4o only achieves a Pearson correlation\nof 0.22 in the quality subscore, and behaves like random guessing in others.\n","authors":["Dongfu Jiang","Max Ku","Tianle Li","Yuansheng Ni","Shizhuo Sun","Rongqi Fan","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.04485v3.pdf","comment":"9 pages,7 figures"},{"id":"http://arxiv.org/abs/2408.02209v2","updated":"2024-08-06T11:46:39Z","published":"2024-08-05T03:18:58Z","title":"Source-Free Domain-Invariant Performance Prediction","summary":"  Accurately estimating model performance poses a significant challenge,\nparticularly in scenarios where the source and target domains follow different\ndata distributions. Most existing performance prediction methods heavily rely\non the source data in their estimation process, limiting their applicability in\na more realistic setting where only the trained model is accessible. The few\nmethods that do not require source data exhibit considerably inferior\nperformance. In this work, we propose a source-free approach centred on\nuncertainty-based estimation, using a generative model for calibration in the\nabsence of source data. We establish connections between our approach for\nunsupervised calibration and temperature scaling. We then employ a\ngradient-based strategy to evaluate the correctness of the calibrated\npredictions. Our experiments on benchmark object recognition datasets reveal\nthat existing source-based methods fall short with limited source sample\navailability. Furthermore, our approach significantly outperforms the current\nstate-of-the-art source-free and source-based methods, affirming its\neffectiveness in domain-invariant performance estimation.\n","authors":["Ekaterina Khramtsova","Mahsa Baktashmotlagh","Guido Zuccon","Xi Wang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2408.02209v2.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2404.07950v3","updated":"2024-08-06T02:42:32Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v3.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2408.02085v2","updated":"2024-08-06T03:19:25Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v2.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.01934v2","updated":"2024-08-06T02:39:46Z","published":"2024-08-04T05:22:08Z","title":"A Survey and Evaluation of Adversarial Attacks for Object Detection","summary":"  Deep learning models excel in various computer vision tasks but are\nsusceptible to adversarial examples-subtle perturbations in input data that\nlead to incorrect predictions. This vulnerability poses significant risks in\nsafety-critical applications such as autonomous vehicles, security\nsurveillance, and aircraft health monitoring. While numerous surveys focus on\nadversarial attacks in image classification, the literature on such attacks in\nobject detection is limited. This paper offers a comprehensive taxonomy of\nadversarial attacks specific to object detection, reviews existing adversarial\nrobustness evaluation metrics, and systematically assesses open-source attack\nmethods and model robustness. Key observations are provided to enhance the\nunderstanding of attack effectiveness and corresponding countermeasures.\nAdditionally, we identify crucial research challenges to guide future efforts\nin securing automated object detection systems.\n","authors":["Khoi Nguyen Tiet Nguyen","Wenyu Zhang","Kangkang Lu","Yuhuan Wu","Xingjian Zheng","Hui Li Tan","Liangli Zhen"],"pdf_url":"https://arxiv.org/pdf/2408.01934v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.03326v1","updated":"2024-08-06T17:59:44Z","published":"2024-08-06T17:59:44Z","title":"LLaVA-OneVision: Easy Visual Task Transfer","summary":"  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.\n","authors":["Bo Li","Yuanhan Zhang","Dong Guo","Renrui Zhang","Feng Li","Hao Zhang","Kaichen Zhang","Yanwei Li","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03326v1.pdf","comment":"Project Homepage:\n  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/"},{"id":"http://arxiv.org/abs/2408.03322v1","updated":"2024-08-06T17:58:18Z","published":"2024-08-06T17:58:18Z","title":"Segment Anything in Medical Images and Videos: Benchmark and Deployment","summary":"  Recent advances in segmentation foundation models have enabled accurate and\nefficient segmentation across a wide range of natural images and videos, but\ntheir utility to medical data remains unclear. In this work, we first present a\ncomprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11\nmedical image modalities and videos and point out its strengths and weaknesses\nby comparing it to SAM1 and MedSAM. Then, we develop a transfer learning\npipeline and demonstrate SAM2 can be quickly adapted to medical domain by\nfine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio\nAPI for efficient 3D image and video segmentation. The code has been made\npublicly available at \\url{https://github.com/bowang-lab/MedSAM}.\n","authors":["Jun Ma","Sumin Kim","Feifei Li","Mohammed Baharoon","Reza Asakereh","Hongwei Lyu","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01197v2","updated":"2024-08-06T17:58:00Z","published":"2024-04-01T15:55:25Z","title":"Getting it Right: Improving Spatial Consistency in Text-to-Image Models","summary":"  One of the key shortcomings in current text-to-image (T2I) models is their\ninability to consistently generate images which faithfully follow the spatial\nrelationships specified in the text prompt. In this paper, we offer a\ncomprehensive investigation of this limitation, while also developing datasets\nand methods that support algorithmic solutions to improve spatial reasoning in\nT2I models. We find that spatial relationships are under-represented in the\nimage descriptions found in current vision-language datasets. To alleviate this\ndata bottleneck, we create SPRIGHT, the first spatially focused, large-scale\ndataset, by re-captioning 6 million images from 4 widely used vision datasets\nand through a 3-fold evaluation and analysis pipeline, show that SPRIGHT\nimproves the proportion of spatial relationships in existing datasets. We show\nthe efficacy of SPRIGHT data by showing that using only $\\sim$0.25% of SPRIGHT\nresults in a 22% improvement in generating spatially accurate images while also\nimproving FID and CMMD scores. We also find that training on images containing\na larger number of objects leads to substantial improvements in spatial\nconsistency, including state-of-the-art results on T2I-CompBench with a spatial\nscore of 0.2133, by fine-tuning on <500 images. Through a set of controlled\nexperiments and ablations, we document additional findings that could support\nfuture work that seeks to understand factors that affect spatial consistency in\ntext-to-image models.\n","authors":["Agneet Chatterjee","Gabriela Ben Melech Stan","Estelle Aflalo","Sayak Paul","Dhruba Ghosh","Tejas Gokhale","Ludwig Schmidt","Hannaneh Hajishirzi","Vasudev Lal","Chitta Baral","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2404.01197v2.pdf","comment":"Accepted to ECCV 2024. Project Page : https://spright-t2i.github.io/"},{"id":"http://arxiv.org/abs/2404.01282v2","updated":"2024-08-06T17:56:53Z","published":"2024-04-01T17:54:34Z","title":"LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action\n  Localization","summary":"  Temporal Action Localization (TAL) involves localizing and classifying action\nsnippets in an untrimmed video. The emergence of large video foundation models\nhas led RGB-only video backbones to outperform previous methods needing both\nRGB and optical flow modalities. Leveraging these large models is often limited\nto training only the TAL head due to the prohibitively large GPU memory\nrequired to adapt the video backbone for TAL. To overcome this limitation, we\nintroduce LoSA, the first memory-and-parameter-efficient backbone adapter\ndesigned specifically for TAL to handle untrimmed videos. LoSA specializes for\nTAL by introducing Long-Short-range Adapters that adapt the intermediate layers\nof the video backbone over different temporal ranges. These adapters run\nparallel to the video backbone to significantly reduce memory footprint. LoSA\nalso includes Long-Short-range Gated Fusion that strategically combines the\noutput of these adapters from the video backbone layers to enhance the video\nfeatures provided to the TAL head. Experiments show that LoSA significantly\noutperforms all existing methods on standard TAL benchmarks, THUMOS-14 and\nActivityNet-v1.3, by scaling end-to-end backbone adaptation to\nbillion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them\nbeyond head-only transfer learning.\n","authors":["Akshita Gupta","Gaurav Mittal","Ahmed Magooda","Ye Yu","Graham W. Taylor","Mei Chen"],"pdf_url":"https://arxiv.org/pdf/2404.01282v2.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2408.00756v2","updated":"2024-08-06T17:40:07Z","published":"2024-08-01T17:57:25Z","title":"Segment anything model 2: an application to 2D and 3D medical images","summary":"  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment varous objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we extensively evaluate SAM 2's ability\nto segment both 2D and 3D medical images by first collecting 18 medical imaging\ndatasets, including common 3D modalities such as computed tomography (CT),\nmagnetic resonance imaging (MRI), and positron emission tomography (PET) as\nwell as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of\nSAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are\nprovided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. Our results show that SAM 2 exhibits similar performance as\nSAM under single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.\n","authors":["Haoyu Dong","Hanxue Gu","Yaqian Chen","Jichen Yang","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.00756v2.pdf","comment":"12 pages, 9 figures. An updated version with new results and\n  corrections"},{"id":"http://arxiv.org/abs/2402.00035v4","updated":"2024-08-06T17:36:06Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mlanie Ducoffe","Audrey Galametz","Guillaume Povda","Ryma Boumazouza","Nomie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v4.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2402.04492v2","updated":"2024-08-06T17:31:33Z","published":"2024-02-07T00:31:49Z","title":"ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation","summary":"  This paper introduces the ColorSwap dataset, designed to assess and improve\nthe proficiency of multimodal models in matching objects with their colors. The\ndataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000\nexamples. Each example includes a caption-image pair, along with a\n``color-swapped'' pair. We follow the Winoground schema: the two captions in an\nexample have the same words, but the color words have been rearranged to modify\ndifferent objects. The dataset was created through a novel blend of automated\ncaption and image generation with humans in the loop. We evaluate image-text\nmatching (ITM) and visual language models (VLMs) and find that even the latest\nones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on\nour main VLM metric, although they may improve with more advanced prompting\ntechniques. On the main ITM metric, contrastive models such as CLIP and SigLIP\nperform close to chance (at 12% and 30%, respectively), although the\nnon-contrastive BLIP ITM model is stronger (87%). We also find that finetuning\non fewer than 2,000 examples yields significant performance gains on this\nout-of-distribution word-order understanding task. The dataset is here:\nhttps://github.com/Top34051/colorswap and here:\nhttps://huggingface.co/datasets/stanfordnlp/colorswap.\n","authors":["Jirayu Burapacheep","Ishan Gaur","Agam Bhatia","Tristan Thrush"],"pdf_url":"https://arxiv.org/pdf/2402.04492v2.pdf","comment":"ACL Findings 2024"},{"id":"http://arxiv.org/abs/2408.03312v1","updated":"2024-08-06T17:29:01Z","published":"2024-08-06T17:29:01Z","title":"MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture\n  Generation","summary":"  Recent advancements in the field of Diffusion Transformers have substantially\nimproved the generation of high-quality 2D images, 3D videos, and 3D shapes.\nHowever, the effectiveness of the Transformer architecture in the domain of\nco-speech gesture generation remains relatively unexplored, as prior\nmethodologies have predominantly employed the Convolutional Neural Network\n(CNNs) or simple a few transformer layers. In an attempt to bridge this\nresearch gap, we introduce a novel Masked Diffusion Transformer for co-speech\ngesture generation, referred to as MDT-A2G, which directly implements the\ndenoising process on gesture sequences. To enhance the contextual reasoning\ncapability of temporally aligned speech-driven gestures, we incorporate a novel\nMasked Diffusion Transformer. This model employs a mask modeling scheme\nspecifically designed to strengthen temporal relation learning among sequence\ngestures, thereby expediting the learning process and leading to coherent and\nrealistic motions. Apart from audio, Our MDT-A2G model also integrates\nmulti-modal information, encompassing text, emotion, and identity. Furthermore,\nwe propose an efficient inference strategy that diminishes the denoising\ncomputation by leveraging previously calculated results, thereby achieving a\nspeedup with negligible performance degradation. Experimental results\ndemonstrate that MDT-A2G excels in gesture generation, boasting a learning\nspeed that is over 6$\\times$ faster than traditional diffusion transformers and\nan inference speed that is 5.7$\\times$ than the standard diffusion model.\n","authors":["Xiaofeng Mao","Zhengkai Jiang","Qilin Wang","Chencan Fu","Jiangning Zhang","Jiafu Wu","Yabiao Wang","Chengjie Wang","Wei Li","Mingmin Chi"],"pdf_url":"https://arxiv.org/pdf/2408.03312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19308v2","updated":"2024-08-06T17:22:17Z","published":"2024-07-27T17:45:20Z","title":"Comprehensive Attribution: Inherently Explainable Vision Model with\n  Feature Detector","summary":"  As deep vision models' popularity rapidly increases, there is a growing\nemphasis on explanations for model predictions. The inherently explainable\nattribution method aims to enhance the understanding of model behavior by\nidentifying the important regions in images that significantly contribute to\npredictions. It is achieved by cooperatively training a selector (generating an\nattribution map to identify important features) and a predictor (making\npredictions using the identified features). Despite many advancements, existing\nmethods suffer from the incompleteness problem, where discriminative features\nare masked out, and the interlocking problem, where the non-optimized selector\ninitially selects noise, causing the predictor to fit on this noise and\nperpetuate the cycle. To address these problems, we introduce a new objective\nthat discourages the presence of discriminative features in the masked-out\nregions thus enhancing the comprehensiveness of feature selection. A\npre-trained detector is introduced to detect discriminative features in the\nmasked-out region. If the selector selects noise instead of discriminative\nfeatures, the detector can observe and break the interlocking situation by\npenalizing the selector. Extensive experiments show that our model makes\naccurate predictions with higher accuracy than the regular black-box model, and\nproduces attribution maps with high feature coverage, localization ability,\nfidelity and robustness. Our code will be available at\n\\href{https://github.com/Zood123/COMET}{https://github.com/Zood123/COMET}.\n","authors":["Xianren Zhang","Dongwon Lee","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19308v2.pdf","comment":"Accepted as a conference paper by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03304v1","updated":"2024-08-06T17:11:40Z","published":"2024-08-06T17:11:40Z","title":"Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks","summary":"  Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.\n","authors":["Rafael Sterzinger","Christian Stippel","Robert Sablatnig"],"pdf_url":"https://arxiv.org/pdf/2408.03304v1.pdf","comment":"16 pages, accepted at ICPR2024"},{"id":"http://arxiv.org/abs/2304.05339v2","updated":"2024-08-06T17:09:59Z","published":"2023-04-11T16:58:59Z","title":"Deep-learning Assisted Detection and Quantification of (oo)cysts of\n  Giardia and Cryptosporidium on Smartphone Microscopy Images","summary":"  The consumption of microbial-contaminated food and water is responsible for\nthe deaths of millions of people annually. Smartphone-based microscopy systems\nare portable, low-cost, and more accessible alternatives for the detection of\nGiardia and Cryptosporidium than traditional brightfield microscopes. However,\nthe images from smartphone microscopes are noisier and require manual cyst\nidentification by trained technicians, usually unavailable in resource-limited\nsettings. Automatic detection of (oo)cysts using deep-learning-based object\ndetection could offer a solution for this limitation. We evaluate the\nperformance of four state-of-the-art object detectors to detect (oo)cysts of\nGiardia and Cryptosporidium on a custom dataset that includes both smartphone\nand brightfield microscopic images from vegetable samples. Faster RCNN,\nRetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer\n(Deformable DETR) deep-learning models were employed to explore their efficacy\nand limitations. Our results show that while the deep-learning models perform\nbetter with the brightfield microscopy image dataset than the smartphone\nmicroscopy image dataset, the smartphone microscopy predictions are still\ncomparable to the prediction performance of non-experts. Also, we publicly\nrelease brightfield and smartphone microscopy datasets with the benchmark\nresults for the detection of Giardia and Cryptosporidium, independently\ncaptured on reference (or standard lab setting) and vegetable samples. Our code\nand dataset are available at\nhttps://github.com/naamiinepal/smartphone_microscopy and\nhttps://doi.org/10.5281/zenodo.7813183, respectively.\n","authors":["Suprim Nakarmi","Sanam Pudasaini","Safal Thapaliya","Pratima Upretee","Retina Shrestha","Basant Giri","Bhanu Bhakta Neupane","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2304.05339v2.pdf","comment":"21 pages (including supplementary information), 5 figures, 7 tables,\n  Accepted for publication at the Journal of Machine Learning for Biomedical\n  Imaging (MELBA) https://melba-journal.org/2024:014"},{"id":"http://arxiv.org/abs/2408.03302v1","updated":"2024-08-06T17:08:05Z","published":"2024-08-06T17:08:05Z","title":"TextIM: Part-aware Interactive Motion Synthesis from Text","summary":"  In this work, we propose TextIM, a novel framework for synthesizing\nTEXT-driven human Interactive Motions, with a focus on the precise alignment of\npart-level semantics. Existing methods often overlook the critical roles of\ninteractive body parts and fail to adequately capture and align part-level\nsemantics, resulting in inaccuracies and even erroneous movement outcomes. To\naddress these issues, TextIM utilizes a decoupled conditional diffusion\nframework to enhance the detailed alignment between interactive movements and\ncorresponding semantic intents from textual descriptions. Our approach\nleverages large language models, functioning as a human brain, to identify\ninteracting human body parts and to comprehend interaction semantics to\ngenerate complicated and subtle interactive motion. Guided by the refined\nmovements of the interacting parts, TextIM further extends these movements into\na coherent whole-body motion. We design a spatial coherence module to\ncomplement the entire body movements while maintaining consistency and harmony\nacross body parts using a part graph convolutional network. For training and\nevaluation, we carefully selected and re-labeled interactive motions from\nHUMANML3D to develop a specialized dataset. Experimental results demonstrate\nthat TextIM produces semantically accurate human interactive motions,\nsignificantly enhancing the realism and applicability of synthesized\ninteractive motions in diverse scenarios, even including interactions with\ndeformable and dynamically changing objects.\n","authors":["Siyuan Fan","Bo Du","Xiantao Cai","Bo Peng","Longling Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11914v2","updated":"2024-08-06T17:00:49Z","published":"2024-05-20T09:49:13Z","title":"PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single\n  Highly-Ambiguous RGB Images","summary":"  Generating 3D shapes from single RGB images is essential in various\napplications such as robotics. Current approaches typically target images\ncontaining clear and complete visual descriptions of the object, without\nconsidering common realistic cases where observations of objects that are\nlargely occluded or truncated. We thus propose a transformer-based\nautoregressive model to generate the probabilistic distribution of 3D shapes\nconditioned on an RGB image containing potentially highly ambiguous\nobservations of the object. To handle realistic scenarios such as occlusion or\nfield-of-view truncation, we create simulated image-to-shape training pairs\nthat enable improved fine-tuning for real-world scenarios. We then adopt\ncross-attention to effectively identify the most relevant region of interest\nfrom the input image for shape generation. This enables inference of sampled\nshapes with reasonable diversity and strong alignment with the input image. We\ntrain and test our model on our synthetic data then fine-tune and test it on\nreal-world data. Experiments demonstrate that our model outperforms state of\nthe art in both scenarios.\n","authors":["Yiheng Xiong","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2405.11914v2.pdf","comment":"10 pages, 6 figures. Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2408.03291v1","updated":"2024-08-06T16:40:04Z","published":"2024-08-06T16:40:04Z","title":"DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training\n  Quantization for Vision Transformers","summary":"  Vision transformers (ViTs) have garnered significant attention for their\nperformance in vision tasks; however, the high computational cost and\nsignificant latency issues have hinder widespread adoption. Post-training\nquantization (PTQ), a promising method for model compression, still faces\naccuracy degradation challenges with ViTs. There are two reasons for this: the\nexisting quantization paradigm does not fit the power-law distribution of\npost-Softmax activations well, and accuracy inevitably decreases after\nreparameterizing post-LayerNorm activations. We propose a Distribution-Friendly\nand Outlier-Aware Post-training Quantization method for Vision Transformers,\nnamed DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and\nintroduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more\non values near 1, more accurately preserving the power-law distribution of\npost-Softmax activations, and achieves favorable results. Moreover, when\nreparameterizing post-LayerNorm activations from channel-wise to layer-wise\nquantization, the accuracy degradation is mainly due to the significant impact\nof outliers in the scaling factors. Therefore, DopQ-ViT proposes a method to\nSearch for the Optimal Scaling Factor, denoted as SOSF, which compensates for\nthe influence of outliers and preserves the performance of the quantization\nmodel. DopQ-ViT has undergone extensive validation and demonstrates significant\nperformance improvements in quantization models, particularly in low-bit\nsettings.\n","authors":["Lianwei Yang","Haisong Gong"],"pdf_url":"https://arxiv.org/pdf/2408.03291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14242v2","updated":"2024-08-06T16:36:11Z","published":"2023-11-24T01:15:57Z","title":"RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with\n  Occlusion Handling","summary":"  In the domain of 3D Human Pose Estimation, which finds widespread daily\napplications, the requirement for convenient acquisition equipment continues to\ngrow. To satisfy this demand, we set our sights on a short-baseline binocular\nsetting that offers both portability and a geometric measurement property that\nradically mitigates depth ambiguity. However, as the binocular baseline\nshortens, two serious challenges emerge: first, the robustness of 3D\nreconstruction against 2D errors deteriorates; and second, occlusion reoccurs\ndue to the limited visual differences between two views. To address the first\nchallenge, we propose the Stereo Co-Keypoints Estimation module to improve the\nview consistency of 2D keypoints and enhance the 3D robustness. In this module,\nthe disparity is utilized to represent the correspondence of binocular 2D\npoints and the Stereo Volume Feature is introduced to contain binocular\nfeatures across different disparities. Through the regression of SVF, two-view\n2D keypoints are simultaneously estimated in a collaborative way which\nrestricts their view consistency. Furthermore, to deal with occlusions, a\nPre-trained Pose Transformer module is introduced. Through this module, 3D\nposes are refined by perceiving pose coherence, a representation of joint\ncorrelations. This perception is injected by the Pose Transformer network and\nlearned through a pre-training task that recovers iterative masked joints.\nComprehensive experiments carried out on H36M and MHAD datasets, complemented\nby visualizations, validate the effectiveness of our approach in the\nshort-baseline binocular 3D Human Pose Estimation and occlusion handling.\n","authors":["Xiaoyue Wan","Zhuo Chen","Yiming Bao","Xu Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.14242v2.pdf","comment":"13 pages, 8 figures, currently under review at IEEE Transactions on\n  Image Processing journal"},{"id":"http://arxiv.org/abs/2408.03286v1","updated":"2024-08-06T16:34:04Z","published":"2024-08-06T16:34:04Z","title":"Biomedical SAM 2: Segment Anything in Biomedical Images and Videos","summary":"  Medical image segmentation and video object segmentation are essential for\ndiagnosing and analyzing diseases by identifying and measuring biological\nstructures. Recent advances in natural domain have been driven by foundation\nmodels like the Segment Anything Model 2 (SAM 2). To explore the performance of\nSAM 2 in biomedical applications, we designed two evaluation pipelines for\nsingle-frame image segmentation and multi-frame video segmentation with varied\nprompt designs, revealing SAM 2's limitations in medical contexts.\nConsequently, we developed BioSAM 2, an enhanced foundation model optimized for\nbiomedical data based on SAM 2. Our experiments show that BioSAM 2 not only\nsurpasses the performance of existing state-of-the-art foundation models but\nalso matches or even exceeds specialist models, demonstrating its efficacy and\npotential in the medical domain.\n","authors":["Zhiling Yan","Weixiang Sun","Rong Zhou","Zhengqing Yuan","Kai Zhang","Yiwei Li","Tianming Liu","Quanzheng Li","Xiang Li","Lifang He","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03284v1","updated":"2024-08-06T16:31:45Z","published":"2024-08-06T16:31:45Z","title":"ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually\n  Synced Facial Performer","summary":"  Lip-syncing videos with given audio is the foundation for various\napplications including the creation of virtual presenters or performers. While\nrecent studies explore high-fidelity lip-sync with different techniques, their\ntask-orientated models either require long-term videos for clip-specific\ntraining or retain visible artifacts. In this paper, we propose a unified and\neffective framework ReSyncer, that synchronizes generalized audio-visual facial\ninformation. The key design is revisiting and rewiring the Style-based\ngenerator to efficiently adopt 3D facial dynamics predicted by a principled\nstyle-injected Transformer. By simply re-configuring the information insertion\nmechanisms within the noise and style space, our framework fuses motion and\nappearance with unified training. Extensive experiments demonstrate that\nReSyncer not only produces high-fidelity lip-synced videos according to audio,\nbut also supports multiple appealing properties that are suitable for creating\nvirtual presenters and performers, including fast personalized fine-tuning,\nvideo-driven lip-syncing, the transfer of speaking styles, and even face\nswapping. Resources can be found at\nhttps://guanjz20.github.io/projects/ReSyncer.\n","authors":["Jiazhi Guan","Zhiliang Xu","Hang Zhou","Kaisiyuan Wang","Shengyi He","Zhanwang Zhang","Borong Liang","Haocheng Feng","Errui Ding","Jingtuo Liu","Jingdong Wang","Youjian Zhao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03284v1.pdf","comment":"Accepted to European Conference on Computer Vision (ECCV), 2024.\n  Project page: https://guanjz20.github.io/projects/ReSyncer"},{"id":"http://arxiv.org/abs/2408.03282v1","updated":"2024-08-06T16:29:51Z","published":"2024-08-06T16:29:51Z","title":"AMES: Asymmetric and Memory-Efficient Similarity Estimation for\n  Instance-level Retrieval","summary":"  This work investigates the problem of instance-level image retrieval\nre-ranking with the constraint of memory efficiency, ultimately aiming to limit\nmemory usage to 1KB per image. Departing from the prevalent focus on\nperformance enhancements, this work prioritizes the crucial trade-off between\nperformance and memory requirements. The proposed model uses a\ntransformer-based architecture designed to estimate image-to-image similarity\nby capturing interactions within and across images based on their local\ndescriptors. A distinctive property of the model is the capability for\nasymmetric similarity estimation. Database images are represented with a\nsmaller number of descriptors compared to query images, enabling performance\nimprovements without increasing memory consumption. To ensure adaptability\nacross different applications, a universal model is introduced that adjusts to\na varying number of local descriptors during the testing phase. Results on\nstandard benchmarks demonstrate the superiority of our approach over both\nhand-crafted and learned models. In particular, compared with current\nstate-of-the-art methods that overlook their memory footprint, our approach not\nonly attains superior performance but does so with a significantly reduced\nmemory footprint. The code and pretrained models are publicly available at:\nhttps://github.com/pavelsuma/ames\n","authors":["Pavel Suma","Giorgos Kordopatis-Zilos","Ahmet Iscen","Giorgos Tolias"],"pdf_url":"https://arxiv.org/pdf/2408.03282v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2403.15313v2","updated":"2024-08-06T15:58:35Z","published":"2024-03-22T16:06:05Z","title":"CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking","summary":"  To enable self-driving vehicles accurate detection and tracking of\nsurrounding objects is essential. While Light Detection and Ranging (LiDAR)\nsensors have set the benchmark for high-performance systems, the appeal of\ncamera-only solutions lies in their cost-effectiveness. Notably, despite the\nprevalent use of Radio Detection and Ranging (RADAR) sensors in automotive\nsystems, their potential in 3D detection and tracking has been largely\ndisregarded due to data sparsity and measurement noise. As a recent\ndevelopment, the combination of RADARs and cameras is emerging as a promising\nsolution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a\ncamera-RADAR fusion model for 3D object detection, and Multi-Object Tracking\n(MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only\nBEVDet architecture, CR3DT demonstrates substantial improvements in both\ndetection and tracking capabilities, by incorporating the spatial and velocity\ninformation of the RADAR sensor. Experimental results demonstrate an absolute\nimprovement in detection performance of 5.3% in mean Average Precision (mAP)\nand a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the\nnuScenes dataset when leveraging both modalities. CR3DT bridges the gap between\nhigh-performance and cost-effective perception systems in autonomous driving,\nby capitalizing on the ubiquitous presence of RADAR in automotive applications.\nThe code is available at: https://github.com/ETH-PBL/CR3DT.\n","authors":["Nicolas Baumann","Michael Baumgartner","Edoardo Ghignone","Jonas Khne","Tobias Fischer","Yung-Hsu Yang","Marc Pollefeys","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.15313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10344v3","updated":"2024-08-06T15:39:03Z","published":"2024-02-15T22:17:17Z","title":"Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry\n  Reconstruction in Field Conditions","summary":"  We evaluate different Neural Radiance Fields (NeRFs) techniques for the 3D\nreconstruction of plants in varied environments, from indoor settings to\noutdoor fields. Traditional methods usually fail to capture the complex\ngeometric details of plants, which is crucial for phenotyping and breeding\nstudies. We evaluate the reconstruction fidelity of NeRFs in three scenarios\nwith increasing complexity and compare the results with the point cloud\nobtained using LiDAR as ground truth. In the most realistic field scenario, the\nNeRF models achieve a 74.6% F1 score after 30 minutes of training on the GPU,\nhighlighting the efficacy of NeRFs for 3D reconstruction in challenging\nenvironments. Additionally, we propose an early stopping technique for NeRF\ntraining that almost halves the training time while achieving only a reduction\nof 7.4% in the average F1 score. This optimization process significantly\nenhances the speed and efficiency of 3D reconstruction using NeRFs. Our\nfindings demonstrate the potential of NeRFs in detailed and realistic 3D plant\nreconstruction and suggest practical approaches for enhancing the speed and\nefficiency of NeRFs in the 3D reconstruction process.\n","authors":["Muhammad Arbab Arshad","Talukder Jubery","James Afful","Anushrut Jignasu","Aditya Balu","Baskar Ganapathysubramanian","Soumik Sarkar","Adarsh Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2402.10344v3.pdf","comment":"Published in 'Plant Phenomics'"},{"id":"http://arxiv.org/abs/2402.15745v2","updated":"2024-08-06T15:28:30Z","published":"2024-02-24T06:57:15Z","title":"GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models\n  Evaluation","summary":"  The Large Vision-Language Models (LVLMs) have demonstrated great abilities in\nimage perception and language understanding. However, existing multimodal\nbenchmarks focus on primary perception abilities and commonsense knowledge\nwhich are insufficient to reflect the comprehensive capabilities of LVLMs. We\npropose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance\nExamination (GAOKAO), comprising of 8 subjects and 12 types of images, such as\ndiagrams, function graphs, maps and photos. GAOKAO-MM derives from native\nChinese context and sets human-level requirements for the model's abilities,\nincluding perception, understanding, knowledge and reasoning. We evaluate 10\nLVLMs and find that the accuracies of all of them are lower than 50%, with\nGPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking\nin the top three positions. The results of our multi-dimension analysis\nindicate that LVLMs have moderate distance towards Artificial General\nIntelligence (AGI) and provide insights facilitating the development of\nmultilingual LVLMs.\n","authors":["Yi Zong","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2402.15745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03291v2","updated":"2024-08-06T15:14:01Z","published":"2024-07-03T17:24:36Z","title":"VCHAR:Variance-Driven Complex Human Activity Recognition framework with\n  Generative Representation","summary":"  Complex human activity recognition (CHAR) remains a pivotal challenge within\nubiquitous computing, especially in the context of smart environments. Existing\nstudies typically require meticulous labeling of both atomic and complex\nactivities, a task that is labor-intensive and prone to errors due to the\nscarcity and inaccuracies of available datasets. Most prior research has\nfocused on datasets that either precisely label atomic activities or, at\nminimum, their sequence approaches that are often impractical in real world\nsettings.In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic\nactivities as a distribution over specified intervals. Leveraging generative\nmethodologies, VCHAR elucidates the reasoning behind complex activity\nclassifications through video-based explanations, accessible to users without\nprior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex\nactivity recognition without necessitating precise temporal or sequential\nlabeling of atomic activities. Furthermore, user studies confirm that VCHAR's\nexplanations are more intelligible compared to existing methods, facilitating a\nbroader understanding of complex activity recognition among non-experts.\n","authors":["Yuan Sun","Navid Salami Pargoo","Taqiya Ehsan","Zhao Zhang","Jorge Ortiz"],"pdf_url":"https://arxiv.org/pdf/2407.03291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03238v1","updated":"2024-08-06T14:50:48Z","published":"2024-08-06T14:50:48Z","title":"LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for\n  Accurate Robotic Grasping Under the Occlusion","summary":"  This paper addresses the challenge of perceiving complete object shapes\nthrough visual perception. While prior studies have demonstrated encouraging\noutcomes in segmenting the visible parts of objects within a scene, amodal\nsegmentation, in particular, has the potential to allow robots to infer the\noccluded parts of objects. To this end, this paper introduces a new framework\nthat explores amodal segmentation for robotic grasping in cluttered scenes,\nthus greatly enhancing robotic grasping abilities. Initially, we use a\nconventional segmentation algorithm to detect the visible segments of the\ntarget object, which provides shape priors for completing the full object mask.\nParticularly, to explore how to utilize semantic features from RGB images and\ngeometric information from depth images, we propose a Linear-fusion\nAttention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the\nlinear-fusion strategy to effectively fuse this cross-modal data, and then uses\nthe prior visible mask as attention map to guide the network to focus on target\nfeature locations for further complete mask recovery. Using the amodal mask of\nthe target object provides advantages in selecting more accurate and robust\ngrasp points compared to relying solely on the visible segments. The results on\ndifferent datasets show that our method achieves state-of-the-art performance.\nFurthermore, the robot experiments validate the feasibility and robustness of\nthis method in the real world. Our code and demonstrations are available on the\nproject page: https://jrryzh.github.io/LAC-Net.\n","authors":["Jinyu Zhang","Yongchong Gu","Jianxiong Gao","Haitao Lin","Qiang Sun","Xinwei Sun","Xiangyang Xue","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2408.03238v1.pdf","comment":"accepted by IROS2024"},{"id":"http://arxiv.org/abs/2408.03230v1","updated":"2024-08-06T14:44:55Z","published":"2024-08-06T14:44:55Z","title":"Contrastive Learning for Image Complexity Representation","summary":"  Quantifying and evaluating image complexity can be instrumental in enhancing\nthe performance of various computer vision tasks. Supervised learning can\neffectively learn image complexity features from well-annotated datasets.\nHowever, creating such datasets requires expensive manual annotation costs. The\nmodels may learn human subjective biases from it. In this work, we introduce\nthe MoCo v2 framework. We utilize contrastive learning to represent image\ncomplexity, named CLIC (Contrastive Learning for Image Complexity). We find\nthat there are complexity differences between different local regions of an\nimage, and propose Random Crop and Mix (RCM), which can produce positive\nsamples consisting of multi-scale local crops. RCM can also expand the train\nset and increase data diversity without introducing additional data. We conduct\nextensive experiments with CLIC, comparing it with both unsupervised and\nsupervised methods. The results demonstrate that the performance of CLIC is\ncomparable to that of state-of-the-art supervised methods. In addition, we\nestablish the pipelines that can apply CLIC to computer vision tasks to\neffectively improve their performance.\n","authors":["Shipeng Liu","Liang Zhao","Dengfeng Chen","Zhanping Song"],"pdf_url":"https://arxiv.org/pdf/2408.03230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03225v1","updated":"2024-08-06T14:36:43Z","published":"2024-08-06T14:36:43Z","title":"Line-based 6-DoF Object Pose Estimation and Tracking With an Event\n  Camera","summary":"  Pose estimation and tracking of objects is a fundamental application in 3D\nvision. Event cameras possess remarkable attributes such as high dynamic range,\nlow latency, and resilience against motion blur, which enables them to address\nchallenging high dynamic range scenes or high-speed motion. These features make\nevent cameras an ideal complement over standard cameras for object pose\nestimation. In this work, we propose a line-based robust pose estimation and\ntracking method for planar or non-planar objects using an event camera.\nFirstly, we extract object lines directly from events, then provide an initial\npose using a globally-optimal Branch-and-Bound approach, where 2D-3D line\ncorrespondences are not known in advance. Subsequently, we utilize event-line\nmatching to establish correspondences between 2D events and 3D models.\nFurthermore, object poses are refined and continuously tracked by minimizing\nevent-line distances. Events are assigned different weights based on these\ndistances, employing robust estimation algorithms. To evaluate the precision of\nthe proposed methods in object pose estimation and tracking, we have devised\nand established an event-based moving object dataset. Compared against\nstate-of-the-art methods, the robustness and accuracy of our methods have been\nvalidated both on synthetic experiments and the proposed dataset. The source\ncode is available at https://github.com/Zibin6/LOPET.\n","authors":["Zibin Liu","Banglei Guan","Yang Shang","Qifeng Yu","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2408.03225v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing,2024"},{"id":"http://arxiv.org/abs/2408.03219v1","updated":"2024-08-06T14:25:23Z","published":"2024-08-06T14:25:23Z","title":"Learning to Learn without Forgetting using Attention","summary":"  Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.\n","authors":["Anna Vettoruzzo","Joaquin Vanschoren","Mohamed-Rafik Bouguelia","Thorsteinn Rgnvaldsson"],"pdf_url":"https://arxiv.org/pdf/2408.03219v1.pdf","comment":"Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),\n  2024"},{"id":"http://arxiv.org/abs/2408.03209v1","updated":"2024-08-06T14:08:22Z","published":"2024-08-06T14:08:22Z","title":"IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning\n  using Instruct Prompts","summary":"  Diffusion models continuously push the boundary of state-of-the-art image\ngeneration, but the process is hard to control with any nuance: practice proves\nthat textual prompts are inadequate for accurately describing image style or\nfine structural details (such as faces). ControlNet and IPAdapter address this\nshortcoming by conditioning the generative process on imagery instead, but each\nindividual instance is limited to modeling a single conditional posterior: for\npractical use-cases, where multiple different posteriors are desired within the\nsame workflow, training and using multiple adapters is cumbersome. We propose\nIPAdapter-Instruct, which combines natural-image conditioning with ``Instruct''\nprompts to swap between interpretations for the same conditioning image: style\ntransfer, object extraction, both, or something else still? IPAdapterInstruct\nefficiently learns multiple tasks with minimal loss in quality compared to\ndedicated per-task models.\n","authors":["Ciara Rowles","Shimon Vainer","Dante De Nigris","Slava Elizarov","Konstantin Kutsy","Simon Donn"],"pdf_url":"https://arxiv.org/pdf/2408.03209v1.pdf","comment":"17 pages, 10 figures, Project page:\n  https://unity-research.github.io/IP-Adapter-Instruct.github.io/"},{"id":"http://arxiv.org/abs/2408.03208v1","updated":"2024-08-06T14:06:53Z","published":"2024-08-06T14:06:53Z","title":"Personalizing Federated Instrument Segmentation with Visual Trait Priors\n  in Robotic Surgery","summary":"  Personalized federated learning (PFL) for surgical instrument segmentation\n(SIS) is a promising approach. It enables multiple clinical sites to\ncollaboratively train a series of models in privacy, with each model tailored\nto the individual distribution of each site. Existing PFL methods rarely\nconsider the personalization of multi-headed self-attention, and do not account\nfor appearance diversity and instrument shape similarity, both inherent in\nsurgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait\npriors for SIS, incorporating global-personalized disentanglement (GPD),\nappearance-regulation personalized enhancement (APE), and shape-similarity\nglobal enhancement (SGE), to boost SIS performance in each site. GPD represents\nthe first attempt at head-wise assignment for multi-headed self-attention\npersonalization. To preserve the unique appearance representation of each site\nand gradually leverage the inter-site difference, APE introduces appearance\nregulation and provides customized layer-wise aggregation solutions via\nhypernetworks for each site's personalized parameters. The mutual shape\ninformation of instruments is maintained and shared via SGE, which enhances the\ncross-style shape consistency on the image level and computes the\nshape-similarity contribution of each site on the prediction level for updating\nthe global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%\nDice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding\ncode and models will be released at https://github.com/wzjialang/PFedSIS.\n","authors":["Jialang Xu","Jiacheng Wang","Lequan Yu","Danail Stoyanov","Yueming Jin","Evangelos B. Mazomenos"],"pdf_url":"https://arxiv.org/pdf/2408.03208v1.pdf","comment":"9 pages, 3 figures, under review"},{"id":"http://arxiv.org/abs/2408.03194v1","updated":"2024-08-06T13:53:45Z","published":"2024-08-06T13:53:45Z","title":"SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via\n  Spatio-Frequency Co-Query Attention","summary":"  Magnetic Resonance Imaging (MRI) is a leading diagnostic modality for a wide\nrange of exams, where multiple contrast images are often acquired for\ncharacterizing different tissues. However, acquiring high-resolution MRI\ntypically extends scan time, which can introduce motion artifacts.\nSuper-resolution of MRI therefore emerges as a promising approach to mitigate\nthese challenges. Earlier studies have investigated the use of multiple\ncontrasts for MRI super-resolution (MCSR), whereas majority of them did not\nfully exploit the rich contrast-invariant structural information. To fully\nutilize such crucial prior knowledge of multi-contrast MRI, in this work, we\npropose a novel structure-guided MCSR (SGSR) framework based on a new\nspatio-frequency co-query attention (CQA) mechanism. Specifically, CQA performs\nattention on features of multiple contrasts with a shared structural query,\nwhich is particularly designed to extract, fuse, and refine the common\nstructures from different contrasts. We further propose a novel\nfrequency-domain CQA module in addition to the spatial domain, to enable more\nfine-grained structural refinement. Extensive experiments on fastMRI knee data\nand low-field brain MRI show that SGSR outperforms state-of-the-art MCSR\nmethods with statistical significance.\n","authors":["Shaoming Zheng","Yinsong Wang","Siyi Du","Chen Qin"],"pdf_url":"https://arxiv.org/pdf/2408.03194v1.pdf","comment":"The 15th International Workshop on Machine Learning in Medical\n  Imaging (MLMI 2024)"},{"id":"http://arxiv.org/abs/2408.03193v1","updated":"2024-08-06T13:49:01Z","published":"2024-08-06T13:49:01Z","title":"Efficient NeRF Optimization -- Not All Samples Remain Equally Hard","summary":"  We propose an application of online hard sample mining for efficient training\nof Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality\nfor many 3D reconstruction and rendering tasks but require substantial\ncomputational resources. The encoding of the scene information within the NeRF\nnetwork parameters necessitates stochastic sampling. We observe that during the\ntraining, a major part of the compute time and memory usage is spent on\nprocessing already learnt samples, which no longer affect the model update\nsignificantly. We identify the backward pass on the stochastic samples as the\ncomputational bottleneck during the optimization. We thus perform the first\nforward pass in inference mode as a relatively low-cost search for hard\nsamples. This is followed by building the computational graph and updating the\nNeRF network parameters using only the hard samples. To demonstrate the\neffectiveness of the proposed approach, we apply our method to Instant-NGP,\nresulting in significant improvements of the view-synthesis quality over the\nbaseline (1 dB improvement on average per training time, or 2x speedup to reach\nthe same PSNR level) along with approx. 40% memory savings coming from using\nonly the hard samples to build the computational graph. As our method only\ninterfaces with the network module, we expect it to be widely applicable.\n","authors":["Juuso Korhonen","Goutham Rangu","Hamed R. Tavakoli","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2408.03193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15890v3","updated":"2024-08-06T13:47:24Z","published":"2023-11-27T14:56:47Z","title":"Stability-Informed Initialization of Neural Ordinary Differential\n  Equations","summary":"  This paper addresses the training of Neural Ordinary Differential Equations\n(neural ODEs), and in particular explores the interplay between numerical\nintegration techniques, stability regions, step size, and initialization\ntechniques. It is shown how the choice of integration technique implicitly\nregularizes the learned model, and how the solver's corresponding stability\nregion affects training and prediction performance. From this analysis, a\nstability-informed parameter initialization technique is introduced. The\neffectiveness of the initialization method is displayed across several learning\nbenchmarks and industrial applications.\n","authors":["Theodor Westny","Arman Mohammadi","Daniel Jung","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2311.15890v3.pdf","comment":"In Proceedings of the 41 st International Conference on Machine\n  Learning"},{"id":"http://arxiv.org/abs/2308.10015v2","updated":"2024-08-06T13:25:29Z","published":"2023-08-19T13:46:49Z","title":"DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for\n  Fingerprint Presentation Attack Detection","summary":"  Automatic fingerprint recognition systems suffer from the threat of\npresentation attacks due to their wide range of deployment in areas including\nnational borders and commercial applications. A presentation attack can be\nperformed by creating a spoof of a user's fingerprint with or without their\nconsent. This paper presents a dynamic ensemble of deep CNN and handcrafted\nfeatures to detect presentation attacks in known-material and unknown-material\nprotocols of the livness detection competition. The proposed presentation\nattack detection model, in this way, utilizes the capabilities of both deep CNN\nand handcrafted features techniques and exhibits better performance than their\nindividual performances. The proposed method is validated using benchmark\ndatabases from the Liveness Detection Competition in 2015, 2017, and 2019,\nyielding overall accuracy of 96.10\\%, 96.49\\%, and 94.99\\% on them,\nrespectively. The proposed method outperforms state-of-the-art methods in terms\nof classification accuracy.\n","authors":["Anuj Rai","Parsheel Kumar Tiwari","Jyotishna Baishya","Ram Prakash Sharma","Somnath Dey"],"pdf_url":"https://arxiv.org/pdf/2308.10015v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.09397"},{"id":"http://arxiv.org/abs/2408.03178v1","updated":"2024-08-06T13:22:51Z","published":"2024-08-06T13:22:51Z","title":"An Object is Worth 64x64 Pixels: Generating 3D Object via Image\n  Diffusion","summary":"  We introduce a new approach for generating realistic 3D models with UV maps\nthrough a representation termed \"Object Images.\" This approach encapsulates\nsurface geometry, appearance, and patch structures within a 64x64 pixel image,\neffectively converting complex 3D shapes into a more manageable 2D format. By\ndoing so, we address the challenges of both geometric and semantic irregularity\ninherent in polygonal meshes. This method allows us to use image generation\nmodels, such as Diffusion Transformers, directly for 3D shape generation.\nEvaluated on the ABO dataset, our generated shapes with patch structures\nachieve point cloud FID comparable to recent 3D generative models, while\nnaturally supporting PBR material generation.\n","authors":["Xingguang Yan","Han-Hung Lee","Ziyu Wan","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2408.03178v1.pdf","comment":"Project Page: https://omages.github.io/"},{"id":"http://arxiv.org/abs/2407.15706v5","updated":"2024-08-06T13:20:16Z","published":"2024-07-22T15:16:47Z","title":"Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition","summary":"  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n","authors":["Jinfu Liu","Chen Chen","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15706v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00956v3","updated":"2024-08-06T13:07:37Z","published":"2024-05-02T02:34:19Z","title":"SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery\n  Videos via Physics-embedded 3D Gaussians","summary":"  Surgical scene simulation plays a crucial role in surgical education and\nsimulator-based robot learning. Traditional approaches for creating these\nenvironments with surgical scene involve a labor-intensive process where\ndesigners hand-craft tissues models with textures and geometries for soft body\nsimulations. This manual approach is not only time-consuming but also limited\nin the scalability and realism. In contrast, data-driven simulation offers a\ncompelling alternative. It has the potential to automatically reconstruct 3D\nsurgical scenes from real-world surgical video data, followed by the\napplication of soft body physics. This area, however, is relatively uncharted.\nIn our research, we introduce 3D Gaussian as a learnable representation for\nsurgical scene, which is learned from stereo endoscopic video. To prevent\nover-fitting and ensure the geometrical correctness of these scenes, we\nincorporate depth supervision and anisotropy regularization into the Gaussian\nlearning process. Furthermore, we apply the Material Point Method, which is\nintegrated with physical properties, to the 3D Gaussians to achieve realistic\nscene deformations. Our method was evaluated on our collected in-house and\npublic surgical videos datasets. Results show that it can reconstruct and\nsimulate surgical scenes from endoscopic videos efficiently-taking only a few\nminutes to reconstruct the surgical scene-and produce both visually and\nphysically plausible deformations at a speed approaching real-time. The results\ndemonstrate great potential of our proposed method to enhance the efficiency\nand variety of simulations available for surgical education and robot learning.\n","authors":["Zhenya Yang","Kai Chen","Yonghao Long","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2405.00956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12705v2","updated":"2024-08-06T13:06:26Z","published":"2024-07-17T16:26:30Z","title":"IMAGDressing-v1: Customizable Virtual Dressing","summary":"  Latest advances have achieved realistic virtual try-on (VTON) through\nlocalized garment inpainting using latent diffusion models, significantly\nenhancing consumers' online shopping experience. However, existing VTON\ntechnologies neglect the need for merchants to showcase garments\ncomprehensively, including flexible control over garments, optional faces,\nposes, and scenes. To address this issue, we define a virtual dressing (VD)\ntask focused on generating freely editable human images with fixed garments and\noptional conditions. Meanwhile, we design a comprehensive affinity metric index\n(CAMI) to evaluate the consistency between generated images and reference\ngarments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet\nthat captures semantic features from CLIP and texture features from VAE. We\npresent a hybrid attention module, including a frozen self-attention and a\ntrainable cross-attention, to integrate garment features from the garment UNet\ninto a frozen denoising UNet, ensuring users can control different scenes\nthrough text. IMAGDressing-v1 can be combined with other extension plugins,\nsuch as ControlNet and IP-Adapter, to enhance the diversity and controllability\nof generated images. Furthermore, to address the lack of data, we release the\ninteractive garment pairing (IGPair) dataset, containing over 300,000 pairs of\nclothing and dressed images, and establish a standard pipeline for data\nassembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves\nstate-of-the-art human image synthesis performance under various controlled\nconditions. The code and model will be available at\nhttps://github.com/muzishen/IMAGDressing.\n","authors":["Fei Shen","Xin Jiang","Xin He","Hu Ye","Cong Wang","Xiaoyu Du","Zechao Li","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2407.12705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03164v1","updated":"2024-08-06T13:05:32Z","published":"2024-08-06T13:05:32Z","title":"Dilated Convolution with Learnable Spacings makes visual models more\n  aligned with humans: a Grad-CAM study","summary":"  Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced\nconvolution method that allows enlarging the receptive fields (RF) without\nincreasing the number of parameters, like the dilated convolution, yet without\nimposing a regular grid. DCLS has been shown to outperform the standard and\ndilated convolutions on several computer vision benchmarks. Here, we show that,\nin addition, DCLS increases the models' interpretability, defined as the\nalignment with human visual strategies. To quantify it, we use the Spearman\ncorrelation between the models' GradCAM heatmaps and the ClickMe dataset\nheatmaps, which reflect human visual attention. We took eight reference models\n- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and\n36) - and drop-in replaced the standard convolution layers with DCLS ones. This\nimproved the interpretability score in seven of them. Moreover, we observed\nthat Grad-CAM generated random heatmaps for two models in our study: CAFormer\nand ConvFormer models, leading to low interpretability scores. We addressed\nthis issue by introducing Threshold-Grad-CAM, a modification built on top of\nGrad-CAM that enhanced interpretability across nearly all models. The code and\ncheckpoints to reproduce this study are available at:\nhttps://github.com/rabihchamas/DCLS-GradCAM-Eval.\n","authors":["Rabih Chamas","Ismail Khalfaoui-Hassani","Timothee Masquelier"],"pdf_url":"https://arxiv.org/pdf/2408.03164v1.pdf","comment":"Accepted at The Trustworthy AI Workshop, IJCAI 2024"},{"id":"http://arxiv.org/abs/2408.03156v1","updated":"2024-08-06T12:55:17Z","published":"2024-08-06T12:55:17Z","title":"Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models","summary":"  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n","authors":["Sho Ozaki","Shizuo Kaji","Toshikazu Imae","Kanabu Nawa","Hideomi Yamashita","Keiichi Nakagawa"],"pdf_url":"https://arxiv.org/pdf/2408.03156v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.15488v3","updated":"2024-08-06T12:54:41Z","published":"2024-07-22T09:05:16Z","title":"DiffX: Guide Your Layout to Cross-Modal Generative Modeling","summary":"  Diffusion models have made significant strides in language-driven and\nlayout-driven image generation. However, most diffusion models are limited to\nvisible RGB image generation. In fact, human perception of the world is\nenriched by diverse viewpoints, such as chromatic contrast, thermal\nillumination, and depth information. In this paper, we introduce a novel\ndiffusion model for general layout-guided cross-modal generation, called DiffX.\nNotably, DiffX presents a simple yet effective cross-modal generative modeling\npipeline, which conducts diffusion and denoising processes in the\nmodality-shared latent space. Moreover, we introduce the Joint-Modality\nEmbedder (JME) to enhance interaction between layout and text conditions by\nincorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP is\nemployed for long caption embedding for user instruction. To facilitate the\nuser-instructed generative training, we construct the cross-modal image\ndatasets with detailed text captions assisted by the Large-Multimodal Model\n(LMM). Through extensive experiments, DiffX demonstrates robustness in\ncross-modal generation across three ``RGB+X'' datasets: FLIR, MFNet, and\nCOME15K, guided by various layout conditions. It also shows the potential for\nthe adaptive generation of ``RGB+X+Y+Z'' images or more diverse modalities on\nCOME15K and MCXFace datasets. Our code and constructed cross-modal image\ndatasets are available at https://github.com/zeyuwang-zju/DiffX.\n","authors":["Zeyu Wang","Jingyu Lin","Yifei Qian","Yi Huang","Shicen Tian","Bosong Chai","Juncan Deng","Lan Du","Cunjian Chen","Yufei Guo","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2407.15488v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01661v3","updated":"2024-08-06T12:54:26Z","published":"2024-05-02T18:31:47Z","title":"When a Relation Tells More Than a Concept: Exploring and Evaluating\n  Classifier Decisions with CoReX","summary":"  Explanations for Convolutional Neural Networks (CNNs) based on relevance of\ninput pixels might be too unspecific to evaluate which and how input features\nimpact model decisions. Especially in complex real-world domains like biology,\nthe presence of specific concepts and of relations between concepts might be\ndiscriminating between classes. Pixel relevance is not expressive enough to\nconvey this type of information. In consequence, model evaluation is limited\nand relevant aspects present in the data and influencing the model decisions\nmight be overlooked. This work presents a novel method to explain and evaluate\nCNN models, which uses a concept- and relation-based explainer (CoReX). It\nexplains the predictive behavior of a model on a set of images by masking\n(ir-)relevant concepts from the decision-making process and by constraining\nrelations in a learned interpretable surrogate model. We test our approach with\nseveral image data sets and CNN architectures. Results show that CoReX\nexplanations are faithful to the CNN model in terms of predictive outcomes. We\nfurther demonstrate through a human evaluation that CoReX is a suitable tool\nfor generating combined explanations that help assessing the classification\nquality of CNNs. We further show that CoReX supports the identification and\nre-classification of incorrect or ambiguous classifications.\n","authors":["Bettina Finzel","Patrick Hilme","Johannes Rabold","Ute Schmid"],"pdf_url":"https://arxiv.org/pdf/2405.01661v3.pdf","comment":"preliminary version, submitted to Machine Learning"},{"id":"http://arxiv.org/abs/2408.03149v1","updated":"2024-08-06T12:45:56Z","published":"2024-08-06T12:45:56Z","title":"Leveraging Entity Information for Cross-Modality Correlation Learning:\n  The Entity-Guided Multimodal Summarization","summary":"  The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.\n","authors":["Yanghai Zhang","Ye Liu","Shiwei Wu","Kai Zhang","Xukai Liu","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03149v1.pdf","comment":"In ACL-Findings 2024"},{"id":"http://arxiv.org/abs/2408.03143v1","updated":"2024-08-06T12:37:47Z","published":"2024-08-06T12:37:47Z","title":"SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast\n  and Reliable Surface Defect Detection","summary":"  The aim of surface defect detection is to identify and localise abnormal\nregions on the surfaces of captured objects, a task that's increasingly\ndemanded across various industries. Current approaches frequently fail to\nfulfil the extensive demands of these industries, which encompass high\nperformance, consistency, and fast operation, along with the capacity to\nleverage the entirety of the available training data. Addressing these gaps, we\nintroduce SuperSimpleNet, an innovative discriminative model that evolved from\nSimpleNet. This advanced model significantly enhances its predecessor's\ntraining consistency, inference time, as well as detection performance.\nSuperSimpleNet operates in an unsupervised manner using only normal training\nimages but also benefits from labelled abnormal training images when they are\navailable. SuperSimpleNet achieves state-of-the-art results in both the\nsupervised and the unsupervised settings, as demonstrated by experiments across\nfour challenging benchmark datasets. Code:\nhttps://github.com/blaz-r/SuperSimpleNet .\n","authors":["Bla Rolih","Matic Fuka","Danijel Skoaj"],"pdf_url":"https://arxiv.org/pdf/2408.03143v1.pdf","comment":"Accepted to ICPR 2024"},{"id":"http://arxiv.org/abs/2402.17485v2","updated":"2024-08-06T12:33:30Z","published":"2024-02-27T13:10:11Z","title":"EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions","summary":"  In this work, we tackle the challenge of enhancing the realism and\nexpressiveness in talking head video generation by focusing on the dynamic and\nnuanced relationship between audio cues and facial movements. We identify the\nlimitations of traditional techniques that often fail to capture the full\nspectrum of human expressions and the uniqueness of individual facial styles.\nTo address these issues, we propose EMO, a novel framework that utilizes a\ndirect audio-to-video synthesis approach, bypassing the need for intermediate\n3D models or facial landmarks. Our method ensures seamless frame transitions\nand consistent identity preservation throughout the video, resulting in highly\nexpressive and lifelike animations. Experimental results demonsrate that EMO is\nable to produce not only convincing speaking videos but also singing videos in\nvarious styles, significantly outperforming existing state-of-the-art\nmethodologies in terms of expressiveness and realism.\n","authors":["Linrui Tian","Qi Wang","Bang Zhang","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2402.17485v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20962v2","updated":"2024-08-06T12:25:48Z","published":"2024-07-30T16:43:24Z","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions","summary":"  Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.\n","authors":["Xiaowei Chi","Yatian Wang","Aosong Cheng","Pengjun Fang","Zeyue Tian","Yingqing He","Zhaoyang Liu","Xingqun Qi","Jiahao Pan","Rongyu Zhang","Mengfei Li","Ruibin Yuan","Yanbing Jiang","Wei Xue","Wenhan Luo","Qifeng Chen","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2407.20962v2.pdf","comment":"15 Pages. Dataset report"},{"id":"http://arxiv.org/abs/2408.00998v2","updated":"2024-08-06T12:01:17Z","published":"2024-08-02T04:13:38Z","title":"FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation","summary":"  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.\n","authors":["Xiang Gao","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00998v2.pdf","comment":"Accepted conference paper of ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.03120v1","updated":"2024-08-06T11:49:13Z","published":"2024-08-06T11:49:13Z","title":"Benchmarking In-the-wild Multimodal Disease Recognition and A Versatile\n  Baseline","summary":"  Existing plant disease classification models have achieved remarkable\nperformance in recognizing in-laboratory diseased images. However, their\nperformance often significantly degrades in classifying in-the-wild images.\nFurthermore, we observed that in-the-wild plant images may exhibit similar\nappearances across various diseases (i.e., small inter-class discrepancy) while\nthe same diseases may look quite different (i.e., large intra-class variance).\nMotivated by this observation, we propose an in-the-wild multimodal plant\ndisease recognition dataset that contains the largest number of disease classes\nbut also text-based descriptions for each disease. Particularly, the newly\nprovided text descriptions are introduced to provide rich information in\ntextual modality and facilitate in-the-wild disease classification with small\ninter-class discrepancy and large intra-class variance issues. Therefore, our\nproposed dataset can be regarded as an ideal testbed for evaluating disease\nrecognition methods in the real world. In addition, we further present a strong\nyet versatile baseline that models text descriptions and visual data through\nmultiple prototypes for a given class. By fusing the contributions of\nmultimodal prototypes in classification, our baseline can effectively address\nthe small inter-class discrepancy and large intra-class variance issues.\nRemarkably, our baseline model can not only classify diseases but also\nrecognize diseases in few-shot or training-free scenarios. Extensive\nbenchmarking results demonstrate that our proposed in-the-wild multimodal\ndataset sets many new challenges to the plant disease recognition task and\nthere is a large space to improve for future works.\n","authors":["Tianqi Wei","Zhi Chen","Zi Huang","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2408.03120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02558v3","updated":"2024-08-06T11:35:04Z","published":"2023-11-05T03:53:42Z","title":"Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity\n  with Free-Flying Robots","summary":"  Assistive free-flyer robots autonomously caring for future crewed outposts --\nsuch as NASA's Astrobee robots on the International Space Station (ISS) -- must\nbe able to detect day-to-day interior changes to track inventory, detect and\ndiagnose faults, and monitor the outpost status. This work presents a framework\nfor multi-agent cooperative mapping and change detection to enable robotic\nmaintenance of space outposts. One agent is used to reconstruct a 3D model of\nthe environment from sequences of images and corresponding depth information.\nAnother agent is used to periodically scan the environment for inconsistencies\nagainst the 3D model. Change detection is validated after completing the\nsurveys using real image and pose data collected by Astrobee robots in a ground\ntesting environment and from microgravity aboard the ISS. This work outlines\nthe objectives, requirements, and algorithmic modules for the multi-agent\nreconstruction system, including recommendations for its use by assistive\nfree-flyers aboard future microgravity outposts.\n  *Denotes Equal Contribution\n","authors":["Holly Dinkel","Julia Di","Jamie Santos","Keenan Albee","Paulo Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2311.02558v3.pdf","comment":"11 pages, 8 figures, Manuscript presented at the 74th International\n  Astronautical Congress, IAC 2023, Baku, Azerbaijan, 2 - 6 October 2023. Video\n  presentation: [https://www.youtube.com/watch?v=VfjV-zwFEtU]. Code:\n  [https://github.com/hollydinkel/astrobeecd]"},{"id":"http://arxiv.org/abs/2408.03097v1","updated":"2024-08-06T10:56:53Z","published":"2024-08-06T10:56:53Z","title":"Prototype Learning for Micro-gesture Classification","summary":"  In this paper, we briefly introduce the solution developed by our team,\nHFUT-VUT, for the track of Micro-gesture Classification in the MiGA challenge\nat IJCAI 2024. The task of micro-gesture classification task involves\nrecognizing the category of a given video clip, which focuses on more\nfine-grained and subtle body movements compared to typical action recognition\ntasks. Given the inherent complexity of micro-gesture recognition, which\nincludes large intra-class variability and minimal inter-class differences, we\nutilize two innovative modules, i.e., the cross-modal fusion module and\nprototypical refinement module, to improve the discriminative ability of MG\nfeatures, thereby improving the classification accuracy. Our solution achieved\nsignificant success, ranking 1st in the track of Micro-gesture Classification.\nWe surpassed the performance of last year's leading team by a substantial\nmargin, improving Top-1 accuracy by 6.13%.\n","authors":["Guoliang Chen","Fei Wang","Kun Li","Zhiliang Wu","Hehe Fan","Yi Yang","Meng Wang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2408.03097v1.pdf","comment":"1st Place in Micro-gesture Classification in MiGA at IJCAI-2024"},{"id":"http://arxiv.org/abs/2408.03078v1","updated":"2024-08-06T10:13:57Z","published":"2024-08-06T10:13:57Z","title":"BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications","summary":"  Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Simultaneous\nLocalization and Mapping (SLAM) has emerged as a promising solution to address\nthese limitations, its implementation in endoscopic procedures presents\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents a\nrobust deep learning-based SLAM approach that combines state-of-the-art and\nnewly developed models. It consists of three main parts: the Monocular Pose\nEstimation Module that introduces a novel unsupervised method based on the\nCycleGAN architecture, the Monocular Depth Estimation Module that leverages the\nnovel Zoe architecture, and the 3D Reconstruction Module which uses information\nfrom the previous models to create a coherent surgical map. The performance of\nthe procedure was rigorously evaluated using three publicly available datasets\n(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art\nmethods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM\ndemonstrated superior performance compared to state-of-the-art depth estimation\nalgorithms in endoscopy, whereas the novel approach in the MPEM exhibited\ncompetitive performance and the lowest inference time. The results showcase the\nrobustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three\ndifferent scenarios in endoscopic surgery. The proposed SLAM approach has the\npotential to improve the accuracy and efficiency of endoscopic procedures by\nproviding surgeons with enhanced depth perception and 3D reconstruction\ncapabilities.\n","authors":["G. Manni","C. Lauretti","F. Prata","R. Papalia","L. Zollo","P. Soda"],"pdf_url":"https://arxiv.org/pdf/2408.03078v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.02265v3","updated":"2024-08-06T10:10:58Z","published":"2024-06-04T12:41:54Z","title":"Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning","summary":"  Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.\n","authors":["Wenyan Li","Jiaang Li","Rita Ramos","Raphael Tang","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2406.02265v3.pdf","comment":"9 pages, long paper at ACL 2024"},{"id":"http://arxiv.org/abs/2407.14086v2","updated":"2024-08-06T09:56:36Z","published":"2024-07-19T07:48:45Z","title":"Temporal Correlation Meets Embedding: Towards a 2nd Generation of\n  JDE-based Real-Time Multi-Object Tracking","summary":"  Joint Detection and Embedding (JDE) trackers have demonstrated excellent\nperformance in Multi-Object Tracking (MOT) tasks by incorporating the\nextraction of appearance features as auxiliary tasks through embedding\nRe-Identification task (ReID) into the detector, achieving a balance between\ninference speed and tracking performance. However, solving the competition\nbetween the detector and the feature extractor has always been a challenge.\nMeanwhile, the issue of directly embedding the ReID task into MOT has remained\nunresolved. The lack of high discriminability in appearance features results in\ntheir limited utility. In this paper, a new learning approach using\ncross-correlation to capture temporal information of objects is proposed. The\nfeature extraction network is no longer trained solely on appearance features\nfrom each frame but learns richer motion features by utilizing feature heatmaps\nfrom consecutive frames, which addresses the challenge of inter-class feature\nsimilarity. Furthermore, our learning approach is applied to a more lightweight\nfeature extraction network, and treat the feature matching scores as strong\ncues rather than auxiliary cues, with an appropriate weight calculation to\nreflect the compatibility between our obtained features and the MOT task. Our\ntracker, named TCBTrack, achieves state-of-the-art performance on multiple\npublic benchmarks, i.e., MOT17, MOT20, and DanceTrack datasets. Specifically,\non the DanceTrack test set, we achieve 56.8 HOTA, 58.1 IDF1 and 92.5 MOTA,\nmaking it the best online tracker capable of achieving real-time performance.\nComparative evaluations with other trackers prove that our tracker achieves the\nbest balance between speed, robustness and accuracy. Code is available at\nhttps://github.com/yfzhang1214/TCBTrack.\n","authors":["Yunfei Zhang","Chao Liang","Jin Gao","Zhipeng Zhang","Weiming Hu","Stephen Maybank","Xue Zhou","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2407.14086v2.pdf","comment":"A submission to IJCV"},{"id":"http://arxiv.org/abs/2408.03065v1","updated":"2024-08-06T09:35:50Z","published":"2024-08-06T09:35:50Z","title":"SCOPE: A Synthetic Multi-Modal Dataset for Collective Perception\n  Including Physical-Correct Weather Conditions","summary":"  Collective perception has received considerable attention as a promising\napproach to overcome occlusions and limited sensing ranges of vehicle-local\nperception in autonomous driving. In order to develop and test novel collective\nperception technologies, appropriate datasets are required. These datasets must\ninclude not only different environmental conditions, as they strongly influence\nthe perception capabilities, but also a wide range of scenarios with different\nroad users as well as realistic sensor models. Therefore, we propose the\nSynthetic COllective PErception (SCOPE) dataset. SCOPE is the first synthetic\nmulti-modal dataset that incorporates realistic camera and LiDAR models as well\nas parameterized and physically accurate weather simulations for both sensor\ntypes. The dataset contains 17,600 frames from over 40 diverse scenarios with\nup to 24 collaborative agents, infrastructure sensors, and passive traffic,\nincluding cyclists and pedestrians. In addition, recordings from two novel\ndigital-twin maps from Karlsruhe and T\\\"ubingen are included. The dataset is\navailable at https://ekut-es.github.io/scope\n","authors":["Jrg Gamerdinger","Sven Teufel","Patrick Schulz","Stephan Amann","Jan-Patrick Kirchner","Oliver Bringmann"],"pdf_url":"https://arxiv.org/pdf/2408.03065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03060v1","updated":"2024-08-06T09:23:24Z","published":"2024-08-06T09:23:24Z","title":"MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View\n  Images","summary":"  Over the last few decades, image-based building surface reconstruction has\ngarnered substantial research interest and has been applied across various\nfields, such as heritage preservation, architectural planning, etc. Compared to\nthe traditional photogrammetric and NeRF-based solutions, recently, Gaussian\nfields-based methods have exhibited significant potential in generating surface\nmeshes due to their time-efficient training and detailed 3D information\npreservation. However, most gaussian fields-based methods are trained with all\nimage pixels, encompassing building and nonbuilding areas, which results in a\nsignificant noise for building meshes and degeneration in time efficiency. This\npaper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to\ngenerate accurate surface reconstruction for building in a time-efficient way.\nThe framework first applies EfficientSAM and COLMAP to generate multi-level\nmasks of building and the corresponding masked point clouds. Subsequently, the\nmasked gaussian fields are trained by integrating two innovative losses: a\nmulti-level perceptual masked loss focused on constructing building regions and\na boundary loss aimed at enhancing the details of the boundaries between\ndifferent masks. Finally, we improve the tetrahedral surface mesh extraction\nmethod based on the masked gaussian spheres. Comprehensive experiments on UAV\nimages demonstrate that, compared to the traditional method and several\nNeRF-based and Gaussian-based SOTA solutions, our approach significantly\nimproves both the accuracy and efficiency of building surface reconstruction.\nNotably, as a byproduct, there is an additional gain in the novel view\nsynthesis of building.\n","authors":["Tengfei Wang","Zongqian Zhan","Rui Xia","Linxia Ji","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03046v1","updated":"2024-08-06T09:02:31Z","published":"2024-08-06T09:02:31Z","title":"Comb, Prune, Distill: Towards Unified Pruning for Vision Model\n  Compression","summary":"  Lightweight and effective models are essential for devices with limited\nresources, such as intelligent vehicles. Structured pruning offers a promising\napproach to model compression and efficiency enhancement. However, existing\nmethods often tie pruning techniques to specific model architectures or vision\ntasks. To address this limitation, we propose a novel unified pruning framework\nComb, Prune, Distill (CPD), which addresses both model-agnostic and\ntask-agnostic concerns simultaneously. Our framework employs a combing step to\nresolve hierarchical layer-wise dependency issues, enabling architecture\nindependence. Additionally, the pruning pipeline adaptively remove parameters\nbased on the importance scoring metrics regardless of vision tasks. To support\nthe model in retaining its learned information, we introduce knowledge\ndistillation during the pruning step. Extensive experiments demonstrate the\ngeneralizability of our framework, encompassing both convolutional neural\nnetwork (CNN) and transformer models, as well as image classification and\nsegmentation tasks. In image classification we achieve a speedup of up to x4.3\nwith a accuracy loss of 1.8% and in semantic segmentation up to x1.89 with a\n5.1% loss in mIoU.\n","authors":["Jonas Schmitt","Ruiping Liu","Junwei Zheng","Jiaming Zhang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2408.03046v1.pdf","comment":"Accepted by ITSC 2024. Code is publicly available at:\n  https://github.com/Cranken/CPD"},{"id":"http://arxiv.org/abs/2408.03043v1","updated":"2024-08-06T08:58:20Z","published":"2024-08-06T08:58:20Z","title":"Targeted Visual Prompting for Medical Visual Question Answering","summary":"  With growing interest in recent years, medical visual question answering\n(Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs)\nemerging as an alternative to classical model architectures. Specifically,\ntheir ability to add visual information to the input of pre-trained LLMs brings\nnew capabilities for image interpretation. However, simple visual errors cast\ndoubt on the actual visual understanding abilities of these models. To address\nthis, region-based questions have been proposed as a means to assess and\nenhance actual visual understanding through compositional evaluation. To\ncombine these two perspectives, this paper introduces targeted visual prompting\nto equip MLLMs with region-based questioning capabilities. By presenting the\nmodel with both the isolated region and the region in its context in a\ncustomized visual prompt, we show the effectiveness of our method across\nmultiple datasets while comparing it to several baseline models. Our code and\ndata are available at https://github.com/sergiotasconmorales/locvqallm.\n","authors":["Sergio Tascon-Morales","Pablo Mrquez-Neila","Raphael Sznitman"],"pdf_url":"https://arxiv.org/pdf/2408.03043v1.pdf","comment":"Accepted at the MICCAI AMAI Workshop 2024"},{"id":"http://arxiv.org/abs/2407.20171v2","updated":"2024-08-06T08:42:47Z","published":"2024-07-29T17:00:09Z","title":"Diffusion Feedback Helps CLIP See Better","summary":"  Contrastive Language-Image Pre-training (CLIP), which excels at abstracting\nopen-world representations across domains and modalities, has become a\nfoundation for a variety of vision and multimodal tasks. However, recent\nstudies reveal that CLIP has severe visual shortcomings, such as which can\nhardly distinguish orientation, quantity, color, structure, etc. These visual\nshortcomings also limit the perception capabilities of multimodal large\nlanguage models (MLLMs) built on CLIP. The main reason could be that the\nimage-text pairs used to train CLIP are inherently biased, due to the lack of\nthe distinctiveness of the text and the diversity of images. In this work, we\npresent a simple post-training approach for CLIP models, which largely\novercomes its visual shortcomings via a self-supervised diffusion process. We\nintroduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP.\nSpecifically, DIVA leverages generative feedback from text-to-image diffusion\nmodels to optimize CLIP representations, with only images (without\ncorresponding text). We demonstrate that DIVA improves CLIP's performance on\nthe challenging MMVP-VLM benchmark which assesses fine-grained visual abilities\nto a large extent (e.g., 3-7%), and enhances the performance of MLLMs and\nvision models on multimodal understanding and segmentation tasks. Extensive\nevaluation on 29 image classification and retrieval benchmarks confirms that\nour framework preserves CLIP's strong zero-shot capabilities. The code is\navailable at https://github.com/baaivision/DIVA.\n","authors":["Wenxuan Wang","Quan Sun","Fan Zhang","Yepeng Tang","Jing Liu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.20171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03035v1","updated":"2024-08-06T08:31:34Z","published":"2024-08-06T08:31:34Z","title":"Training-Free Condition Video Diffusion Models for single frame\n  Spatial-Semantic Echocardiogram Synthesis","summary":"  Conditional video diffusion models (CDM) have shown promising results for\nvideo synthesis, potentially enabling the generation of realistic\nechocardiograms to address the problem of data scarcity. However, current CDMs\nrequire a paired segmentation map and echocardiogram dataset. We present a new\nmethod called Free-Echo for generating realistic echocardiograms from a single\nend-diastolic segmentation map without additional training data. Our method is\nbased on the 3D-Unet with Temporal Attention Layers model and is conditioned on\nthe segmentation map using a training-free conditioning method based on SDEdit.\nWe evaluate our model on two public echocardiogram datasets, CAMUS and\nEchoNet-Dynamic. We show that our model can generate plausible echocardiograms\nthat are spatially aligned with the input segmentation map, achieving\nperformance comparable to training-based CDMs. Our work opens up new\npossibilities for generating echocardiograms from a single segmentation map,\nwhich can be used for data augmentation, domain adaptation, and other\napplications in medical imaging. Our code is available at\n\\url{https://github.com/gungui98/echo-free}\n","authors":["Van Phi Nguyen","Tri Nhan Luong Ha","Huy Hieu Pham","Quoc Long Tran"],"pdf_url":"https://arxiv.org/pdf/2408.03035v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.09913v3","updated":"2024-08-06T08:27:55Z","published":"2024-06-14T10:47:52Z","title":"OpenECAD: An Efficient Visual Language Model for Editable 3D-CAD Design","summary":"  Computer-aided design (CAD) tools are utilized in the manufacturing industry\nfor modeling everything from cups to spacecraft. These programs are complex to\nuse and typically require years of training and experience to master.\nStructured and well-constrained 2D sketches and 3D constructions are crucial\ncomponents of CAD modeling. A well-executed CAD model can be seamlessly\nintegrated into the manufacturing process, thereby enhancing production\nefficiency. Deep generative models of 3D shapes and 3D object reconstruction\nmodels have garnered significant research interest. However, most of these\nmodels produce discrete forms of 3D objects that are not editable. Moreover,\nthe few models based on CAD operations often have substantial input\nrestrictions. In this work, we fine-tuned pre-trained models to create OpenECAD\nmodels (0.55B, 0.89B, 2.4B and 3.1B), leveraging the visual, logical, coding,\nand general capabilities of visual language models. OpenECAD models can process\nimages of 3D designs as input and generate highly structured 2D sketches and 3D\nconstruction commands, ensuring that the designs are editable. These outputs\ncan be directly used with existing CAD tools' APIs to generate project files.\nTo train our network, we created a series of OpenECAD datasets. These datasets\nare derived from existing public CAD datasets, adjusted and augmented to meet\nthe specific requirements of vision language model (VLM) training.\nAdditionally, we have introduced an approach that utilizes dependency\nrelationships to define and generate sketches, further enriching the content\nand functionality of the datasets.\n","authors":["Zhe Yuan","Jianqi Shi","Yanhong Huang"],"pdf_url":"https://arxiv.org/pdf/2406.09913v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03030v1","updated":"2024-08-06T08:24:47Z","published":"2024-08-06T08:24:47Z","title":"Nighttime Pedestrian Detection Based on Fore-Background Contrast\n  Learning","summary":"  The significance of background information is frequently overlooked in\ncontemporary research concerning channel attention mechanisms. This study\naddresses the issue of suboptimal single-spectral nighttime pedestrian\ndetection performance under low-light conditions by incorporating background\ninformation into the channel attention mechanism. Despite numerous studies\nfocusing on the development of efficient channel attention mechanisms, the\nrelevance of background information has been largely disregarded. By adopting a\ncontrast learning approach, we reexamine channel attention with regard to\npedestrian objects and background information for nighttime pedestrian\ndetection, resulting in the proposed Fore-Background Contrast Attention (FBCA).\nFBCA possesses two primary attributes: (1) channel descriptors form remote\ndependencies with global spatial feature information; (2) the integration of\nbackground information enhances the distinction between channels concentrating\non low-light pedestrian features and those focusing on background information.\nConsequently, the acquired channel descriptors exhibit a higher semantic level\nand spatial accuracy. Experimental outcomes demonstrate that FBCA significantly\noutperforms existing methods in single-spectral nighttime pedestrian detection,\nachieving state-of-the-art results on the NightOwls and TJU-DHD-pedestrian\ndatasets. Furthermore, this methodology also yields performance improvements\nfor the multispectral LLVIP dataset. These findings indicate that integrating\nbackground information into the channel attention mechanism effectively\nmitigates detector performance degradation caused by illumination factors in\nnighttime scenarios.\n","authors":["He Yao","Yongjun Zhang","Huachun Jian","Li Zhang","Ruzhong Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.03030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03014v1","updated":"2024-08-06T07:51:20Z","published":"2024-08-06T07:51:20Z","title":"CKNN: Cleansed k-Nearest Neighbor for Unsupervised Video Anomaly\n  Detection","summary":"  In this paper, we address the problem of unsupervised video anomaly detection\n(UVAD). The task aims to detect abnormal events in test video using unlabeled\nvideos as training data. The presence of anomalies in the training data poses a\nsignificant challenge in this task, particularly because they form clusters in\nthe feature space. We refer to this property as the \"Anomaly Cluster\" issue.\nThe condensed nature of these anomalies makes it difficult to distinguish\nbetween normal and abnormal data in the training set. Consequently, training\nconventional anomaly detection techniques using an unlabeled dataset often\nleads to sub-optimal results. To tackle this difficulty, we propose a new\nmethod called Cleansed k-Nearest Neighbor (CKNN), which explicitly filters out\nthe Anomaly Clusters by cleansing the training dataset. Following the k-nearest\nneighbor algorithm in the feature space provides powerful anomaly detection\ncapability. Although the identified Anomaly Cluster issue presents a\nsignificant challenge to applying k-nearest neighbor in UVAD, our proposed\ncleansing scheme effectively addresses this problem. We evaluate the proposed\nmethod on various benchmark datasets and demonstrate that CKNN outperforms the\nprevious state-of-the-art UVAD method by up to 8.5% (from 82.0 to 89.0) in\nterms of AUROC. Moreover, we emphasize that the performance of the proposed\nmethod is comparable to that of the state-of-the-art method trained using\nanomaly-free data.\n","authors":["Jihun Yi","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2408.03014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17572v3","updated":"2024-08-06T07:36:21Z","published":"2024-07-24T18:05:13Z","title":"CityX: Controllable Procedural Content Generation for Unbounded 3D\n  Cities","summary":"  Generating a realistic, large-scale 3D virtual city remains a complex\nchallenge due to the involvement of numerous 3D assets, various city styles,\nand strict layout constraints. Existing approaches provide promising attempts\nat procedural content generation to create large-scale scenes using Blender\nagents. However, they face crucial issues such as difficulties in scaling up\ngeneration capability and achieving fine-grained control at the semantic layout\nlevel. To address these problems, we propose a novel multi-modal controllable\nprocedural content generation method, named CityX, which enhances realistic,\nunbounded 3D city generation guided by multiple layout conditions, including\nOSM, semantic maps, and satellite images. Specifically, the proposed method\ncontains a general protocol for integrating various PCG plugins and a\nmulti-agent framework for transforming instructions into executable Blender\nactions. Through this effective framework, CityX shows the potential to build\nan innovative ecosystem for 3D scene generation by bridging the gap between the\nquality of generated assets and industrial requirements. Extensive experiments\nhave demonstrated the effectiveness of our method in creating high-quality,\ndiverse, and unbounded cities guided by multi-modal conditions. Our project\npage: https://cityx-lab.github.io.\n","authors":["Shougao Zhang","Mengqi Zhou","Yuxi Wang","Chuanchen Luo","Rongyu Wang","Yiwei Li","Xucheng Yin","Zhaoxiang Zhang","Junran Peng"],"pdf_url":"https://arxiv.org/pdf/2407.17572v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14362v3","updated":"2024-08-06T07:32:46Z","published":"2024-03-21T12:45:01Z","title":"Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics","summary":"  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.\n","authors":["Jiaqi Yue","Jiancheng Zhao","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.14362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03006v1","updated":"2024-08-06T07:30:53Z","published":"2024-08-06T07:30:53Z","title":"Dual-path Collaborative Generation Network for Emotional Video\n  Captioning","summary":"  Emotional Video Captioning is an emerging task that aims to describe factual\ncontent with the intrinsic emotions expressed in videos. The essential of the\nEVC task is to effectively perceive subtle and ambiguous visual emotional cues\nduring the caption generation, which is neglected by the traditional video\ncaptioning. Existing emotional video captioning methods perceive global visual\nemotional cues at first, and then combine them with the video features to guide\nthe emotional caption generation, which neglects two characteristics of the EVC\ntask. Firstly, their methods neglect the dynamic subtle changes in the\nintrinsic emotions of the video, which makes it difficult to meet the needs of\ncommon scenes with diverse and changeable emotions. Secondly, as their methods\nincorporate emotional cues into each step, the guidance role of emotion is\noveremphasized, which makes factual content more or less ignored during\ngeneration. To this end, we propose a dual-path collaborative generation\nnetwork, which dynamically perceives visual emotional cues evolutions while\ngenerating emotional captions by collaborative learning. Specifically, in the\ndynamic emotion perception path, we propose a dynamic emotion evolution module,\nwhich first aggregates visual features and historical caption features to\nsummarize the global visual emotional cues, and then dynamically selects\nemotional cues required to be re-composed at each stage. Besides, in the\nadaptive caption generation path, to balance the description of factual content\nand emotional cues, we propose an emotion adaptive decoder. Thus, our methods\ncan generate emotion-related words at the necessary time step, and our caption\ngeneration balances the guidance of factual content and emotional cues well.\nExtensive experiments on three challenging datasets demonstrate the superiority\nof our approach and each proposed module.\n","authors":["Cheng Ye","Weidong Chen","Jingyu Li","Lei Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2408.03006v1.pdf","comment":"Acccepted by ACM Multimedia 2024, oral"},{"id":"http://arxiv.org/abs/2402.16907v2","updated":"2024-08-06T07:24:35Z","published":"2024-02-25T04:24:28Z","title":"Diffusion Posterior Proximal Sampling for Image Restoration","summary":"  Diffusion models have demonstrated remarkable efficacy in generating\nhigh-quality samples. Existing diffusion-based image restoration algorithms\nexploit pre-trained diffusion models to leverage data priors, yet they still\npreserve elements inherited from the unconditional generation paradigm. These\nstrategies initiate the denoising process with pure white noise and incorporate\nrandom noise at each generative step, leading to over-smoothed results. In this\npaper, we present a refined paradigm for diffusion-based image restoration.\nSpecifically, we opt for a sample consistent with the measurement identity at\neach generative step, exploiting the sampling selection as an avenue for output\nstability and enhancement. The number of candidate samples used for selection\nis adaptively determined based on the signal-to-noise ratio of the timestep.\nAdditionally, we start the restoration process with an initialization combined\nwith the measurement signal, providing supplementary information to better\nalign the generative process. Extensive experimental results and analyses\nvalidate that our proposed method significantly enhances image restoration\nperformance while consuming negligible additional computational resources.\n","authors":["Hongjie Wu","Linchao He","Mingqin Zhang","Dongdong Chen","Kunming Luo","Mengting Luo","Ji-Zhe Zhou","Hu Chen","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2402.16907v2.pdf","comment":"ACM Multimedia 2024 Oral"},{"id":"http://arxiv.org/abs/2408.03001v1","updated":"2024-08-06T07:19:51Z","published":"2024-08-06T07:19:51Z","title":"Multitask and Multimodal Neural Tuning for Large Models","summary":"  In recent years, large-scale multimodal models have demonstrated impressive\ncapabilities across various domains. However, enabling these models to\neffectively perform multiple multimodal tasks simultaneously remains a\nsignificant challenge. To address this, we introduce a novel tuning method\ncalled neural tuning, designed to handle diverse multimodal tasks concurrently,\nincluding reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. Neural tuning emulates sparse distributed\nrepresentation in human brain, where only specific subsets of neurons are\nactivated for each task. Additionally, we present a new benchmark, MMUD, where\neach sample is annotated with multiple task labels. By applying neural tuning\nto pretrained large models on the MMUD benchmark, we achieve simultaneous task\nhandling in a streamlined and efficient manner. All models, code, and datasets\nwill be publicly available after publication, facilitating further research and\ndevelopment in this field.\n","authors":["Hao Sun","Yu Song","Jihong Hu","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2408.03001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02993v1","updated":"2024-08-06T06:59:15Z","published":"2024-08-06T06:59:15Z","title":"DreamLCM: Towards High-Quality Text-to-3D Generation via Latent\n  Consistency Model","summary":"  Recently, the text-to-3D task has developed rapidly due to the appearance of\nthe SDS method. However, the SDS method always generates 3D objects with poor\nquality due to the over-smooth issue. This issue is attributed to two factors:\n1) the DDPM single-step inference produces poor guidance gradients; 2) the\nrandomness from the input noises and timesteps averages the details of the 3D\ncontents.In this paper, to address the issue, we propose DreamLCM which\nincorporates the Latent Consistency Model (LCM). DreamLCM leverages the\npowerful image generation capabilities inherent in LCM, enabling generating\nconsistent and high-quality guidance, i.e., predicted noises or images. Powered\nby the improved guidance, the proposed method can provide accurate and detailed\ngradients to optimize the target 3D models.In addition, we propose two\nstrategies to enhance the generation quality further. Firstly, we propose a\nguidance calibration strategy, utilizing Euler Solver to calibrate the guidance\ndistribution to accelerate 3D models to converge. Secondly, we propose a dual\ntimestep strategy, increasing the consistency of guidance and optimizing 3D\nmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCM\nachieves state-of-the-art results in both generation quality and training\nefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.\n","authors":["Yiming Zhong","Xiaolin Zhang","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2408.02993v1.pdf","comment":"15 pages, 9 figures, ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.02983v1","updated":"2024-08-06T06:33:24Z","published":"2024-08-06T06:33:24Z","title":"Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond","summary":"  Non-exemplar class-incremental learning (NECIL) is to resist catastrophic\nforgetting without saving old class samples. Prior methodologies generally\nemploy simple rules to generate features for replaying, suffering from large\ndistribution gap between replayed features and real ones. To address the\naforementioned issue, we propose a simple, yet effective\n\\textbf{Diff}usion-based \\textbf{F}eature \\textbf{R}eplay (\\textbf{DiffFR})\nmethod for NECIL. First, to alleviate the limited representational capacity\ncaused by fixing the feature extractor, we employ Siamese-based self-supervised\nlearning for initial generalizable features. Second, we devise diffusion models\nto generate class-representative features highly similar to real features,\nwhich provides an effective way for exemplar-free knowledge memorization.\nThird, we introduce prototype calibration to direct the diffusion model's focus\ntowards learning the distribution shapes of features, rather than the entire\ndistribution. Extensive experiments on public datasets demonstrate significant\nperformance gains of our DiffFR, outperforming the state-of-the-art NECIL\nmethods by 3.0\\% in average. The code will be made publicly available soon.\n","authors":["Jichuan Zhang","Yali Li","Xin Liu","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16499v2","updated":"2024-08-06T06:31:34Z","published":"2023-11-27T15:49:41Z","title":"InceptionHuman: Controllable Prompt-to-NeRF for Photorealistic 3D Human\n  Generation","summary":"  This paper presents InceptionHuman, a prompt-to-NeRF framework that allows\neasy control via a combination of prompts in different modalities (e.g., text,\nposes, edge, segmentation map, etc) as inputs to generate photorealistic 3D\nhumans. While many works have focused on generating 3D human models, they\nsuffer one or more of the following: lack of distinctive features, unnatural\nshading/shadows, unnatural poses/clothes, limited views, etc. InceptionHuman\nachieves consistent 3D human generation within a progressively refined NeRF\nspace with two novel modules, Iterative Pose-Aware Refinement (IPAR) and\nProgressive-Augmented Reconstruction (PAR). IPAR iteratively refines the\ndiffusion-generated images and synthesizes high-quality 3D-aware views\nconsidering the close-pose RGB values. PAR employs a pretrained diffusion prior\nto augment the generated synthetic views and adds regularization for\nview-independent appearance. Overall, the synthesis of photorealistic novel\nviews empowers the resulting 3D human NeRF from 360-degree perspectives.\nExtensive qualitative and quantitative experimental comparison show that our\nInceptionHuman models achieve state-of-the-art application quality.\n","authors":["Shiu-hong Kao","Xinhang Liu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2311.16499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02980v1","updated":"2024-08-06T06:25:39Z","published":"2024-08-06T06:25:39Z","title":"Sample-agnostic Adversarial Perturbation for Vision-Language\n  Pre-training Models","summary":"  Recent studies on AI security have highlighted the vulnerability of\nVision-Language Pre-training (VLP) models to subtle yet intentionally designed\nperturbations in images and texts. Investigating multimodal systems' robustness\nvia adversarial attacks is crucial in this field. Most multimodal attacks are\nsample-specific, generating a unique perturbation for each sample to construct\nadversarial samples. To the best of our knowledge, it is the first work through\nmultimodal decision boundaries to explore the creation of a universal,\nsample-agnostic perturbation that applies to any image. Initially, we explore\nstrategies to move sample points beyond the decision boundaries of linear\nclassifiers, refining the algorithm to ensure successful attacks under the top\n$k$ accuracy metric. Based on this foundation, in visual-language tasks, we\ntreat visual and textual modalities as reciprocal sample points and decision\nhyperplanes, guiding image embeddings to traverse text-constructed decision\nboundaries, and vice versa. This iterative process consistently refines a\nuniversal perturbation, ultimately identifying a singular direction within the\ninput space which is exploitable to impair the retrieval performance of VLP\nmodels. The proposed algorithms support the creation of global perturbations or\nadversarial patches. Comprehensive experiments validate the effectiveness of\nour method, showcasing its data, task, and model transferability across various\nVLP models and datasets. Code: https://github.com/LibertazZ/MUAP\n","authors":["Haonan Zheng","Wen Jiang","Xinyang Deng","Wenrui Li"],"pdf_url":"https://arxiv.org/pdf/2408.02980v1.pdf","comment":"13 pages, 8 figures, published in ACMMM2024"},{"id":"http://arxiv.org/abs/2408.02978v1","updated":"2024-08-06T06:24:10Z","published":"2024-08-06T06:24:10Z","title":"ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval","summary":"  E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.\n","authors":["Ruixiang Zhao","Jian Jia","Yan Li","Xuehan Bai","Quan Chen","Han Li","Peng Jiang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02978v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.18136v2","updated":"2024-08-06T06:04:41Z","published":"2024-04-28T10:16:35Z","title":"SafePaint: Anti-forensic Image Inpainting with Domain Adaptation","summary":"  Existing image inpainting methods have achieved remarkable accomplishments in\ngenerating visually appealing results, often accompanied by a trend toward\ncreating more intricate structural textures. However, while these models excel\nat creating more realistic image content, they often leave noticeable traces of\ntampering, posing a significant threat to security. In this work, we take the\nanti-forensic capabilities into consideration, firstly proposing an end-to-end\ntraining framework for anti-forensic image inpainting named SafePaint.\nSpecifically, we innovatively formulated image inpainting as two major tasks:\nsemantically plausible content completion and region-wise optimization. The\nformer is similar to current inpainting methods that aim to restore the missing\nregions of corrupted images. The latter, through domain adaptation, endeavors\nto reconcile the discrepancies between the inpainted region and the unaltered\narea to achieve anti-forensic goals. Through comprehensive theoretical\nanalysis, we validate the effectiveness of domain adaptation for anti-forensic\nperformance. Furthermore, we meticulously crafted a region-wise separated\nattention (RWSA) module, which not only aligns with our objective of\nanti-forensics but also enhances the performance of the model. Extensive\nqualitative and quantitative evaluations show our approach achieves comparable\nresults to existing image inpainting methods while offering anti-forensic\ncapabilities not available in other methods.\n","authors":["Dunyun Chen","Xin Liao","Xiaoshuai Wu","Shiwei Chen"],"pdf_url":"https://arxiv.org/pdf/2404.18136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21317v2","updated":"2024-08-06T05:42:42Z","published":"2024-07-31T03:58:48Z","title":"Pathology Foundation Models","summary":"  Pathology has played a crucial role in the diagnosis and evaluation of\npatient tissue samples obtained from surgeries and biopsies for many years. The\nadvent of Whole Slide Scanners and the development of deep learning\ntechnologies have significantly advanced the field, leading to extensive\nresearch and development in pathology AI (Artificial Intelligence). These\nadvancements have contributed to reducing the workload of pathologists and\nsupporting decision-making in treatment plans. Recently, large-scale AI models\nknown as Foundation Models (FMs), which are more accurate and applicable to a\nwide range of tasks compared to traditional AI, have emerged, and expanded\ntheir application scope in the healthcare field. Numerous FMs have been\ndeveloped in pathology, and there are reported cases of their application in\nvarious tasks, such as disease diagnosis, rare cancer diagnosis, patient\nsurvival prognosis prediction, biomarker expression prediction, and the scoring\nof immunohistochemical expression intensity. However, several challenges remain\nfor the clinical application of FMs, which healthcare professionals, as users,\nmust be aware of. Research is ongoing to address these challenges. In the\nfuture, it is expected that the development of Generalist Medical AI, which\nintegrates pathology FMs with FMs from other medical domains, will progress,\nleading to the effective utilization of AI in real clinical settings to promote\nprecision and personalized medicine.\n","authors":["Mieko Ochi","Daisuke Komura","Shumpei Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2407.21317v2.pdf","comment":"19 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2408.02966v1","updated":"2024-08-06T05:24:06Z","published":"2024-08-06T05:24:06Z","title":"Fast Point Cloud Geometry Compression with Context-based Residual Coding\n  and INR-based Refinement","summary":"  Compressing a set of unordered points is far more challenging than\ncompressing images/videos of regular sample grids, because of the difficulties\nin characterizing neighboring relations in an irregular layout of points. Many\nresearchers resort to voxelization to introduce regularity, but this approach\nsuffers from quantization loss. In this research, we use the KNN method to\ndetermine the neighborhoods of raw surface points. This gives us a means to\ndetermine the spatial context in which the latent features of 3D points are\ncompressed by arithmetic coding. As such, the conditional probability model is\nadaptive to local geometry, leading to significant rate reduction.\nAdditionally, we propose a dual-layer architecture where a non-learning base\nlayer reconstructs the main structures of the point cloud at low complexity,\nwhile a learned refinement layer focuses on preserving fine details. This\ndesign leads to reductions in model complexity and coding latency by two orders\nof magnitude compared to SOTA methods. Moreover, we incorporate an implicit\nneural representation (INR) into the refinement layer, allowing the decoder to\nsample points on the underlying surface at arbitrary densities. This work is\nthe first to effectively exploit content-aware local contexts for compressing\nirregular raw point clouds, achieving high rate-distortion performance, low\ncomplexity, and the ability to function as an arbitrary-scale upsampling\nnetwork simultaneously.\n","authors":["Hao Xu","Xi Zhang","Xiaolin Wu"],"pdf_url":"https://arxiv.org/pdf/2408.02966v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2405.05953v4","updated":"2024-08-06T05:19:06Z","published":"2024-05-09T17:46:22Z","title":"Frame Interpolation with Consecutive Brownian Bridge Diffusion","summary":"  Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a\ndiffusion-based conditional image generation problem, synthesizing the\nintermediate frame given a random noise and neighboring frames. Due to the\nrelatively high resolution of videos, Latent Diffusion Models (LDMs) are\nemployed as the conditional generation model, where the autoencoder compresses\nimages into latent representations for diffusion and then reconstructs images\nfrom these latent representations. Such a formulation poses a crucial\nchallenge: VFI expects that the output is deterministically equal to the ground\ntruth intermediate frame, but LDMs randomly generate a diverse set of different\nimages when the model runs multiple times. The reason for the diverse\ngeneration is that the cumulative variance (variance accumulated at each step\nof generation) of generated latent representations in LDMs is large. This makes\nthe sampling trajectory random, resulting in diverse rather than deterministic\ngenerations. To address this problem, we propose our unique solution: Frame\nInterpolation with Consecutive Brownian Bridge Diffusion. Specifically, we\npropose consecutive Brownian Bridge diffusion that takes a deterministic\ninitial value as input, resulting in a much smaller cumulative variance of\ngenerated latent representations. Our experiments suggest that our method can\nimprove together with the improvement of the autoencoder and achieve\nstate-of-the-art performance in VFI, leaving strong potential for further\nenhancement.\n","authors":["Zonglin Lyu","Ming Li","Jianbo Jiao","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2405.05953v4.pdf","comment":"corrected typo"},{"id":"http://arxiv.org/abs/2408.02957v1","updated":"2024-08-06T04:55:33Z","published":"2024-08-06T04:55:33Z","title":"Online Temporal Action Localization with Memory-Augmented Transformer","summary":"  Online temporal action localization (On-TAL) is the task of identifying\nmultiple action instances given a streaming video. Since existing methods take\nas input only a video segment of fixed size per iteration, they are limited in\nconsidering long-term context and require tuning the segment size carefully. To\novercome these limitations, we propose memory-augmented transformer (MATR).\nMATR utilizes the memory queue that selectively preserves the past segment\nfeatures, allowing to leverage long-term context for inference. We also propose\na novel action localization method that observes the current input segment to\npredict the end time of the ongoing action and accesses the memory queue to\nestimate the start time of the action. Our method outperformed existing methods\non two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the\nonline setting but also some offline TAL methods.\n","authors":["Youngkil Song","Dongkeun Kim","Minsu Cho","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2408.02957v1.pdf","comment":"Accepted to ECCV 2024, Project page:\n  https://cvlab.postech.ac.kr/research/MATR/"},{"id":"http://arxiv.org/abs/2408.02954v1","updated":"2024-08-06T04:44:10Z","published":"2024-08-06T04:44:10Z","title":"WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal\n  Deepfake Detection","summary":"  All current benchmarks for multimodal deepfake detection manipulate entire\nframes using various generation techniques, resulting in oversaturated\ndetection accuracies exceeding 94% at the video-level classification. However,\nthese benchmarks struggle to detect dynamic deepfake attacks with challenging\nframe-by-frame alterations presented in real-world scenarios. To address this\nlimitation, we introduce FakeMix, a novel clip-level evaluation benchmark aimed\nat identifying manipulated segments within both video and audio, providing\ninsight into the origins of deepfakes. Furthermore, we propose novel evaluation\nmetrics, Temporal Accuracy (TA) and Frame-wise Discrimination Metric (FDM), to\nassess the robustness of deepfake detection models. Evaluating state-of-the-art\nmodels against diverse deepfake benchmarks, particularly FakeMix, demonstrates\nthe effectiveness of our approach comprehensively. Specifically, while\nachieving an Average Precision (AP) of 94.2% at the video-level, the evaluation\nof the existing models at the clip-level using the proposed metrics, TA and\nFDM, yielded sharp declines in accuracy to 53.1%, and 52.1%, respectively.\n","authors":["Juho Jung","Sangyoun Lee","Jooeon Kang","Yunjin Na"],"pdf_url":"https://arxiv.org/pdf/2408.02954v1.pdf","comment":"4 pages, 2 figures, 2 tables, Accepted as Oral Presentation at The\n  Trustworthy AI Workshop @ IJCAI 2024"},{"id":"http://arxiv.org/abs/2403.16428v2","updated":"2024-08-06T03:44:00Z","published":"2024-03-25T05:12:21Z","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand\n  Interactions with Objects","summary":"  We interact with the world with our hands and see it through our own\n(egocentric) perspective. A holistic 3Dunderstanding of such interactions from\negocentric views is important for tasks in robotics, AR/VR, action recognition\nand motion generation. Accurately reconstructing such interactions in 3D is\nchallenging due to heavy occlusion, viewpoint bias, camera distortion, and\nmotion blur from the head movement. To this end, we designed the HANDS23\nchallenge based on the AssemblyHands and ARCTIC datasets with carefully\ndesigned training and testing splits. Based on the results of the top submitted\nmethods and more recent baselines on the leaderboards, we perform a thorough\nanalysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates\nthe effectiveness of addressing distortion specific to egocentric cameras,\nadopting high-capacity transformers to learn complex hand-object interactions,\nand fusing predictions from different views. Our study further reveals\nchallenging scenarios intractable with state-of-the-art methods, such as fast\nhand motion, object reconstruction from narrow egocentric views, and close\ncontact between two hands and objects. Our efforts will enrich the community's\nknowledge foundation and facilitate future hand studies on egocentric\nhand-object interactions.\n","authors":["Zicong Fan","Takehiko Ohkawa","Linlin Yang","Nie Lin","Zhishan Zhou","Shihao Zhou","Jiajun Liang","Zhong Gao","Xuanyang Zhang","Xue Zhang","Fei Li","Zheng Liu","Feng Lu","Karim Abou Zeid","Bastian Leibe","Jeongwan On","Seungryul Baek","Aditya Prakash","Saurabh Gupta","Kun He","Yoichi Sato","Otmar Hilliges","Hyung Jin Chang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.16428v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2404.18203v2","updated":"2024-08-06T03:37:31Z","published":"2024-04-28T14:47:09Z","title":"LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM","summary":"  Although large multi-modality models (LMMs) have seen extensive exploration\nand application in various quality assessment studies, their integration into\nPoint Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs'\nexceptional performance and robustness in low-level vision and quality\nassessment tasks, this study aims to investigate the feasibility of imparting\nPCQA knowledge to LMMs through text supervision. To achieve this, we transform\nquality labels into textual descriptions during the fine-tuning phase, enabling\nLMMs to derive quality rating logits from 2D projections of point clouds. To\ncompensate for the loss of perception in the 3D domain, structural features are\nextracted as well. These quality logits and structural features are then\ncombined and regressed into quality scores. Our experimental results affirm the\neffectiveness of our approach, showcasing a novel integration of LMMs into PCQA\nthat enhances model understanding and assessment accuracy. We hope our\ncontributions can inspire subsequent investigations into the fusion of LMMs\nwith PCQA, fostering advancements in 3D visual quality analysis and beyond. The\ncode is available at https://github.com/zzc-1998/LMM-PCQA.\n","authors":["Zicheng Zhang","Haoning Wu","Yingjie Zhou","Chunyi Li","Wei Sun","Chaofeng Chen","Xiongkuo Min","Xiaohong Liu","Weisi Lin","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2404.18203v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12794v2","updated":"2024-08-06T03:28:12Z","published":"2024-04-19T11:17:35Z","title":"MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware\n  State Space Model","summary":"  LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment\nmoving objects in point clouds of the current scan using motion information\nfrom previous scans. Despite the promising results achieved by previous MOS\nmethods, several key issues, such as the weak coupling of temporal and spatial\ninformation, still need further study. In this paper, we propose a novel\nLiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model,\ntermed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue\nBootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial\ninformation in point clouds and alleviate the issue of overlooked temporal\nclues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to\nendow the model with the capacity to understand the temporal correlations of\nthe same object across different time steps. Specifically, MSSM emphasizes the\nmotion states of the same object at different time steps through two distinct\ntemporal modeling and correlation steps. We utilize an improved state space\nmodel to represent these motion differences, significantly modeling the motion\nstates. Finally, extensive experiments on the SemanticKITTI-MOS and KITTI-Road\nbenchmarks demonstrate that the proposed MambaMOS achieves state-of-the-art\nperformance. The source code is publicly available at\nhttps://github.com/Terminal-K/MambaMOS.\n","authors":["Kang Zeng","Hao Shi","Jiacheng Lin","Siyu Li","Jintao Cheng","Kaiwei Wang","Zhiyong Li","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.12794v2.pdf","comment":"Accepted to ACM MM 2024. The source code is publicly available at\n  https://github.com/Terminal-K/MambaMOS"},{"id":"http://arxiv.org/abs/2408.02929v1","updated":"2024-08-06T03:23:42Z","published":"2024-08-06T03:23:42Z","title":"Segmenting Small Stroke Lesions with Novel Labeling Strategies","summary":"  Deep neural networks have demonstrated exceptional efficacy in stroke lesion\nsegmentation. However, the delineation of small lesions, critical for stroke\ndiagnosis, remains a challenge. In this study, we propose two straightforward\nyet powerful approaches that can be seamlessly integrated into a variety of\nnetworks: Multi-Size Labeling (MSL) and Distance-Based Labeling (DBL), with the\naim of enhancing the segmentation accuracy of small lesions. MSL divides lesion\nmasks into various categories based on lesion volume while DBL emphasizes the\nlesion boundaries. Experimental evaluations on the Anatomical Tracings of\nLesions After Stroke (ATLAS) v2.0 dataset showcase that an ensemble of MSL and\nDBL achieves consistently better or equal performance on recall (3.6% and\n3.7%), F1 (2.4% and 1.5%), and Dice scores (1.3% and 0.0%) compared to the\ntop-1 winner of the 2022 MICCAI ATLAS Challenge on both the subset only\ncontaining small lesions and the entire dataset, respectively. Notably, on the\nmini-lesion subset, a single MSL model surpasses the previous best ensemble\nstrategy, with enhancements of 1.0% and 0.3% on F1 and Dice scores,\nrespectively. Our code is available at:\nhttps://github.com/nadluru/StrokeLesSeg.\n","authors":["Liang Shang","Zhengyang Lou","Andrew L. Alexander","Vivek Prabhakaran","William A. Sethares","Veena A. Nair","Nagesh Adluru"],"pdf_url":"https://arxiv.org/pdf/2408.02929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02924v1","updated":"2024-08-06T03:20:10Z","published":"2024-08-06T03:20:10Z","title":"Evaluation of Segment Anything Model 2: The Role of SAM2 in the\n  Underwater Environment","summary":"  With breakthroughs in large-scale modeling, the Segment Anything Model (SAM)\nand its extensions have been attempted for applications in various underwater\nvisualization tasks in marine sciences, and have had a significant impact on\nthe academic community. Recently, Meta has further developed the Segment\nAnything Model 2 (SAM2), which significantly improves running speed and\nsegmentation accuracy compared to its predecessor. This report aims to explore\nthe potential of SAM2 in marine science by evaluating it on the underwater\ninstance segmentation benchmark datasets UIIS and USIS10K. The experiments show\nthat the performance of SAM2 is extremely dependent on the type of\nuser-provided prompts. When using the ground truth bounding box as prompt, SAM2\nperformed excellently in the underwater instance segmentation domain. However,\nwhen running in automatic mode, SAM2's ability with point prompts to sense and\nsegment underwater instances is significantly degraded. It is hoped that this\npaper will inspire researchers to further explore the SAM model family in the\nunderwater domain. The results and evaluation codes in this paper are available\nat https://github.com/LiamLian0727/UnderwaterSAM2Eval.\n","authors":["Shijie Lian","Hua Li"],"pdf_url":"https://arxiv.org/pdf/2408.02924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01351v2","updated":"2024-08-06T02:58:13Z","published":"2023-09-04T04:29:01Z","title":"Adv3D: Generating 3D Adversarial Examples for 3D Object Detection in\n  Driving Scenarios with NeRF","summary":"  Deep neural networks (DNNs) have been proven extremely susceptible to\nadversarial examples, which raises special safety-critical concerns for\nDNN-based autonomous driving stacks (i.e., 3D object detection). Although there\nare extensive works on image-level attacks, most are restricted to 2D pixel\nspaces, and such attacks are not always physically realistic in our 3D world.\nHere we present Adv3D, the first exploration of modeling adversarial examples\nas Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic\nappearances and 3D accurate generation, yielding a more realistic and\nrealizable adversarial example. We train our adversarial NeRF by minimizing the\nsurrounding objects' confidence predicted by 3D detectors on the training set.\nThen we evaluate Adv3D on the unseen validation set and show that it can cause\na large performance reduction when rendering NeRF in any sampled pose. To\ngenerate physically realizable adversarial examples, we propose primitive-aware\nsampling and semantic-guided regularization that enable 3D patch attacks with\ncamouflage adversarial texture. Experimental results demonstrate that the\ntrained adversarial NeRF generalizes well to different poses, scenes, and 3D\ndetectors. Finally, we provide a defense method to our attacks that involves\nadversarial training through data augmentation. Project page:\nhttps://len-li.github.io/adv3d-web\n","authors":["Leheng Li","Qing Lian","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2309.01351v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17379v2","updated":"2024-08-06T02:44:44Z","published":"2024-07-24T15:59:01Z","title":"MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image\n  Relational Association Capabilities in Large Visual Language Models","summary":"  Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process.\n","authors":["Siwei Wu","Kang Zhu","Yu Bai","Yiming Liang","Yizhi Li","Haoning Wu","J. H. Liu","Ruibo Liu","Xingwei Qu","Xuxin Cheng","Ge Zhang","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17379v2.pdf","comment":"VLMs, Multi-Image Association"},{"id":"http://arxiv.org/abs/2303.00952v5","updated":"2024-08-06T02:39:05Z","published":"2023-03-02T04:12:53Z","title":"Towards Activated Muscle Group Estimation in the Wild","summary":"  In this paper, we tackle the new task of video-based Activated Muscle Group\nEstimation (AMGE) aiming at identifying active muscle regions during physical\nactivity in the wild. To this intent, we provide the MuscleMap dataset\nfeaturing >15K video clips with 135 different activities and 20 labeled muscle\ngroups. This dataset opens the vistas to multiple video-based applications in\nsports and rehabilitation medicine under flexible environment constraints. The\nproposed MuscleMap dataset is constructed with YouTube videos, specifically\ntargeting High-Intensity Interval Training (HIIT) physical exercise in the\nwild. To make the AMGE model applicable in real-life situations, it is crucial\nto ensure that the model can generalize well to numerous types of physical\nactivities not present during training and involving new combinations of\nactivated muscles. To achieve this, our benchmark also covers an evaluation\nsetting where the model is exposed to activity types excluded from the training\nset. Our experiments reveal that the generalizability of existing architectures\nadapted for the AMGE task remains a challenge. Therefore, we also propose a new\napproach, TransM3E, which employs a multi-modality feature fusion mechanism\nbetween both the video transformer model and the skeleton-based graph\nconvolution model with novel cross-modal knowledge distillation executed on\nmulti-classification tokens. The proposed method surpasses all popular video\nclassification models when dealing with both, previously seen and new types of\nphysical activities. The database and code can be found at\nhttps://github.com/KPeng9510/MuscleMap.\n","authors":["Kunyu Peng","David Schneider","Alina Roitberg","Kailun Yang","Jiaming Zhang","Chen Deng","Kaiyu Zhang","M. Saquib Sarfraz","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.00952v5.pdf","comment":"Accepted to ACM MM 2024. The database and code can be found at\n  https://github.com/KPeng9510/MuscleMap"},{"id":"http://arxiv.org/abs/2408.02906v1","updated":"2024-08-06T02:38:22Z","published":"2024-08-06T02:38:22Z","title":"Dual-View Pyramid Pooling in Deep Neural Networks for Improved Medical\n  Image Classification and Confidence Calibration","summary":"  Spatial pooling (SP) and cross-channel pooling (CCP) operators have been\napplied to aggregate spatial features and pixel-wise features from feature maps\nin deep neural networks (DNNs), respectively. Their main goal is to reduce\ncomputation and memory overhead without visibly weakening the performance of\nDNNs. However, SP often faces the problem of losing the subtle feature\nrepresentations, while CCP has a high possibility of ignoring salient feature\nrepresentations, which may lead to both miscalibration of confidence issues and\nsuboptimal medical classification results. To address these problems, we\npropose a novel dual-view framework, the first to systematically investigate\nthe relative roles of SP and CCP by analyzing the difference between spatial\nfeatures and pixel-wise features. Based on this framework, we propose a new\npooling method, termed dual-view pyramid pooling (DVPP), to aggregate\nmulti-scale dual-view features. DVPP aims to boost both medical image\nclassification and confidence calibration performance by fully leveraging the\nmerits of SP and CCP operators from a dual-axis perspective. Additionally, we\ndiscuss how to fulfill DVPP with five parameter-free implementations. Extensive\nexperiments on six 2D/3D medical image classification tasks show that our DVPP\nsurpasses state-of-the-art pooling methods in terms of medical image\nclassification results and confidence calibration across different DNNs.\n","authors":["Xiaoqing Zhang","Qiushi Nie","Zunjie Xiao","Jilu Zhao","Xiao Wu","Pengxin Guo","Runzhi Li","Jin Liu","Yanjie Wei","Yi Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02906v1.pdf","comment":"27"},{"id":"http://arxiv.org/abs/2402.13699v4","updated":"2024-08-06T02:32:36Z","published":"2024-02-21T11:00:23Z","title":"Automation of Quantum Dot Measurement Analysis via Explainable Machine\n  Learning","summary":"  The rapid development of quantum dot (QD) devices for quantum computing has\nnecessitated more efficient and automated methods for device characterization\nand tuning. Many of the measurements acquired during the tuning process come in\nthe form of images that need to be properly analyzed to guide the subsequent\ntuning steps. By design, features present in such images capture certain\nbehaviors or states of the measured QD devices. When considered carefully, such\nfeatures can aid the control and calibration of QD devices. An important\nexample of such images are so-called \\textit{triangle plots}, which visually\nrepresent current flow and reveal characteristics important for QD device\ncalibration. While image-based classification tools, such as convolutional\nneural networks (CNNs), can be used to verify whether a given measurement is\n\\textit{good} and thus warrants the initiation of the next phase of tuning,\nthey do not provide any insights into how the device should be adjusted in the\ncase of \\textit{bad} images. This is because CNNs sacrifice prediction and\nmodel intelligibility for high accuracy. To ameliorate this trade-off, a recent\nstudy introduced an image vectorization approach that relies on the Gabor\nwavelet transform [1]. Here we propose an alternative vectorization method that\ninvolves mathematical modeling of synthetic triangles to mimic the experimental\ndata. Using explainable boosting machines, we show that this new method offers\nsuperior explainability of model prediction without sacrificing accuracy. This\nwork demonstrates the feasibility and advantages of applying explainable\nmachine learning techniques to the analysis of quantum dot measurements, paving\nthe way for further advances in automated and transparent QD device tuning.\n","authors":["Daniel Schug","Tyler J. Kovach","M. A. Wolfe","Jared Benson","Sanghyeok Park","J. P. Dodson","J. Corrigan","M. A. Eriksson","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2402.13699v4.pdf","comment":"17 pages, 4 figures, abbreviated version published in Proceedings of\n  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,\n  (Vancouver, Canada)"},{"id":"http://arxiv.org/abs/2408.02904v1","updated":"2024-08-06T02:27:54Z","published":"2024-08-06T02:27:54Z","title":"Enabling Intelligent Traffic Systems: A Deep Learning Method for\n  Accurate Arabic License Plate Recognition","summary":"  This paper introduces a novel two-stage framework for accurate Egyptian\nVehicle License Plate Recognition (EVLPR). The first stage employs image\nprocessing techniques to reliably localize license plates, while the second\nstage utilizes a custom-designed deep learning model for robust Arabic\ncharacter recognition. The proposed system achieves a remarkable 99.3% accuracy\non a diverse dataset, surpassing existing approaches. Its potential\napplications extend to intelligent traffic management, including traffic\nviolation detection and parking optimization. Future research will focus on\nenhancing the system's capabilities through architectural refinements, expanded\ndatasets, and addressing system dependencies.\n","authors":["M. A. Sayedelahl"],"pdf_url":"https://arxiv.org/pdf/2408.02904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02901v1","updated":"2024-08-06T02:15:12Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v1.pdf","comment":"6 pages; library tech report"},{"id":"http://arxiv.org/abs/2408.02900v1","updated":"2024-08-06T02:09:35Z","published":"2024-08-06T02:09:35Z","title":"MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular\n  Annotations for Medicine","summary":"  This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal\ndataset for medicine, covering over 25 million images across 10 modalities,\nwith multigranular annotations for more than 65 diseases. These enriched\nannotations encompass both global textual information, such as disease/lesion\ntype, modality, region-specific descriptions, and inter-regional relationships,\nas well as detailed local annotations for regions of interest (ROIs), including\nbounding boxes, segmentation masks. Unlike existing approach which is limited\nby the availability of image-text pairs, we have developed the first automated\npipeline that scales up multimodal data by generating multigranular visual and\ntexual annotations (in the form of image-ROI-description triplets) without the\nneed for any paired text descriptions. Specifically, data from over 90\ndifferent sources have been collected, preprocessed, and grounded using\ndomain-specific expert models to identify ROIs related to abnormal regions. We\nthen build a comprehensive knowledge base and prompt multimodal large language\nmodels to perform retrieval-augmented generation with the identified ROIs as\nguidance, resulting in multigranular texual descriptions. Compared to existing\ndatasets, MedTrinity-25M provides the most enriched annotations, supporting a\ncomprehensive range of multimodal tasks such as captioning and report\ngeneration, as well as vision-centric tasks like classification and\nsegmentation. Pretraining on MedTrinity-25M, our model achieves\nstate-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodal\nlarge language models and other representative SoTA approaches. This dataset\ncan also be utilized to support large-scale pre-training of multimodal medical\nAI models, contributing to the development of future foundation models in the\nmedical domain.\n","authors":["Yunfei Xie","Ce Zhou","Lang Gao","Juncheng Wu","Xianhang Li","Hong-Yu Zhou","Sheng Liu","Lei Xing","James Zou","Cihang Xie","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02900v1.pdf","comment":"The project page is at https://yunfeixie233.github.io/MedTrinity-25M"},{"id":"http://arxiv.org/abs/2406.14643v2","updated":"2024-08-06T02:04:35Z","published":"2024-06-20T18:07:19Z","title":"Holistic Evaluation for Interleaved Text-and-Image Generation","summary":"  Interleaved text-and-image generation has been an intriguing research\ndirection, where the models are required to generate both images and text\npieces in an arbitrary order. Despite the emerging advancements in interleaved\ngeneration, the progress in its evaluation still significantly lags behind.\nExisting evaluation benchmarks do not support arbitrarily interleaved images\nand text for both inputs and outputs, and they only cover a limited number of\ndomains and use cases. Also, current works predominantly use similarity-based\nmetrics which fall short in assessing the quality in open-ended scenarios. To\nthis end, we introduce InterleavedBench, the first benchmark carefully curated\nfor the evaluation of interleaved text-and-image generation. InterleavedBench\nfeatures a rich array of tasks to cover diverse real-world use cases. In\naddition, we present InterleavedEval, a strong reference-free metric powered by\nGPT-4o to deliver accurate and explainable evaluation. We carefully define five\nessential evaluation aspects for InterleavedEval, including text quality,\nperceptual quality, image coherence, text-image coherence, and helpfulness, to\nensure a comprehensive and fine-grained assessment. Through extensive\nexperiments and rigorous human evaluation, we show that our benchmark and\nmetric can effectively evaluate the existing models with a strong correlation\nwith human judgments surpassing previous reference-based metrics. We also\nprovide substantial findings and insights to foster future research in\ninterleaved generation and its evaluation.\n","authors":["Minqian Liu","Zhiyang Xu","Zihao Lin","Trevor Ashby","Joy Rimchala","Jiaxin Zhang","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2406.14643v2.pdf","comment":"13 pages, 6 figures, 6 tables. Website:\n  https://vt-nlp.github.io/InterleavedEval/. Dataset:\n  https://huggingface.co/mqliu/InterleavedBench"},{"id":"http://arxiv.org/abs/2408.02891v1","updated":"2024-08-06T01:41:40Z","published":"2024-08-06T01:41:40Z","title":"Diverse Generation while Maintaining Semantic Coordination: A\n  Diffusion-Based Data Augmentation Method for Object Detection","summary":"  Recent studies emphasize the crucial role of data augmentation in enhancing\nthe performance of object detection models. However,existing methodologies\noften struggle to effectively harmonize dataset diversity with semantic\ncoordination.To bridge this gap, we introduce an innovative augmentation\ntechnique leveraging pre-trained conditional diffusion models to mediate this\nbalance. Our approach encompasses the development of a Category Affinity\nMatrix, meticulously designed to enhance dataset diversity, and a Surrounding\nRegion Alignment strategy, which ensures the preservation of semantic\ncoordination in the augmented images. Extensive experimental evaluations\nconfirm the efficacy of our method in enriching dataset diversity while\nseamlessly maintaining semantic coordination. Our method yields substantial\naverage improvements of +1.4AP, +0.9AP, and +3.4AP over existing alternatives\non three distinct object detection models, respectively.\n","authors":["Sen Nie","Zhuo Wang","Xinxin Wang","Kun He"],"pdf_url":"https://arxiv.org/pdf/2408.02891v1.pdf","comment":"15 pages, 7 figures, ICPR2024"},{"id":"http://arxiv.org/abs/2408.02888v1","updated":"2024-08-06T01:34:43Z","published":"2024-08-06T01:34:43Z","title":"VizECGNet: Visual ECG Image Network for Cardiovascular Diseases\n  Classification with Multi-Modal Training and Knowledge Distillation","summary":"  An electrocardiogram (ECG) captures the heart's electrical signal to assess\nvarious heart conditions. In practice, ECG data is stored as either digitized\nsignals or printed images. Despite the emergence of numerous deep learning\nmodels for digitized signals, many hospitals prefer image storage due to cost\nconsiderations. Recognizing the unavailability of raw ECG signals in many\nclinical settings, we propose VizECGNet, which uses only printed ECG graphics\nto determine the prognosis of multiple cardiovascular diseases. During\ntraining, cross-modal attention modules (CMAM) are used to integrate\ninformation from two modalities - image and signal, while self-modality\nattention modules (SMAM) capture inherent long-range dependencies in ECG data\nof each modality. Additionally, we utilize knowledge distillation to improve\nthe similarity between two distinct predictions from each modality stream. This\ninnovative multi-modal deep learning architecture enables the utilization of\nonly ECG images during inference. VizECGNet with image input achieves higher\nperformance in precision, recall, and F1-Score compared to signal-based ECG\nclassification models, with improvements of 3.50%, 8.21%, and 7.38%,\nrespectively.\n","authors":["Ju-Hyeon Nam","Seo-Hyung Park","Su Jung Kim","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02888v1.pdf","comment":"Accepted in International Conference on Image Processing (ICIP) 2024"},{"id":"http://arxiv.org/abs/2304.07444v4","updated":"2024-08-06T01:31:28Z","published":"2023-04-15T01:33:14Z","title":"The Art of Camouflage: Few-Shot Learning for Animal Detection and\n  Segmentation","summary":"  Camouflaged object detection and segmentation is a new and challenging\nresearch topic in computer vision. There is a serious issue of lacking data on\nconcealed objects such as camouflaged animals in natural scenes. In this paper,\nwe address the problem of few-shot learning for camouflaged object detection\nand segmentation. To this end, we first collect a new dataset, CAMO-FS, for the\nbenchmark. As camouflaged instances are challenging to recognize due to their\nsimilarity compared to the surroundings, we guide our models to obtain\ncamouflaged features that highly distinguish the instances from the background.\nIn this work, we propose FS-CDIS, a framework to efficiently detect and segment\ncamouflaged instances via two loss functions contributing to the training\nprocess. Firstly, the instance triplet loss with the characteristic of\ndifferentiating the anchor, which is the mean of all camouflaged foreground\npoints, and the background points are employed to work at the instance level.\nSecondly, to consolidate the generalization at the class level, we present\ninstance memory storage with the scope of storing camouflaged features of the\nsame category, allowing the model to capture further class-level information\nduring the learning process. The extensive experiments demonstrated that our\nproposed method achieves state-of-the-art performance on the newly collected\ndataset. Code is available at https://github.com/danhntd/FS-CDIS.\n","authors":["Thanh-Danh Nguyen","Anh-Khoa Nguyen Vu","Nhat-Duy Nguyen","Vinh-Tiep Nguyen","Thanh Duc Ngo","Thanh-Toan Do","Minh-Triet Tran","Tam V. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2304.07444v4.pdf","comment":"IEEE Access 2024"},{"id":"http://arxiv.org/abs/2408.02879v1","updated":"2024-08-06T01:13:09Z","published":"2024-08-06T01:13:09Z","title":"Body of Her: A Preliminary Study on End-to-End Humanoid Agent","summary":"  Interactive virtual humanoid agent is a crucial interface with the physical\nworld. A relatively complete humanoid agent first needs to have face and body,\nthen possess both verbal and non-verbal (such as eye contact, facial\nexpression, lip motion, gesture, and manipulation) abilities, and finally, it\nis capable of real-time duplex communication, e.g., the ability to actively\ninterrupt conversations. Most prior systems typically only consider a subset of\nthese elements, leaving a gap from realistic humanoid agent. In this work, we\npropose a real-time, duplex, interactive end-to-end network capable of modeling\nrealistic agent behaviors, including speech, full-body movements for talking,\nresponding, idling, and manipulation. This system is a multimodal model\nintegrating audio and visual inputs, extended from a pre-trained large language\nmodel (LLM). We collect approximately 200,000 hours of audio, around 130,000\nhours of video data, and about 20,000 alignment samples to build the model. The\nfinal model demonstrates capabilities that are difficult to achieve in previous\nsystems, such as generalized object manipulation. This work performs a\npreliminary exploration of the end-to-end approach in this field, aiming to\ninspire further research towards scaling up.\n","authors":["Tenglong Ao"],"pdf_url":"https://arxiv.org/pdf/2408.02879v1.pdf","comment":"Technical Report v1; Project Page:\n  https://aubrey-ao.github.io/BodyOfHer"},{"id":"http://arxiv.org/abs/2403.09975v2","updated":"2024-08-06T00:28:44Z","published":"2024-03-15T02:42:28Z","title":"Skeleton-Based Human Action Recognition with Noisy Labels","summary":"  Understanding human actions from body poses is critical for assistive robots\nsharing space with humans in order to make informed and safe decisions about\nthe next interaction. However, precise temporal localization and annotation of\nactivity sequences is time-consuming and the resulting labels are often noisy.\nIf not effectively addressed, label noise negatively affects the model's\ntraining, resulting in lower recognition quality. Despite its importance,\naddressing label noise for skeleton-based action recognition has been\noverlooked so far. In this study, we bridge this gap by implementing a\nframework that augments well-established skeleton-based human action\nrecognition methods with label-denoising strategies from various research areas\nto serve as the initial benchmark. Observations reveal that these baselines\nyield only marginal performance when dealing with sparse skeleton data.\nConsequently, we introduce a novel methodology, NoiseEraSAR, which integrates\nglobal sample selection, co-teaching, and Cross-Modal Mixture-of-Experts\n(CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise.\nOur proposed approach demonstrates better performance on the established\nbenchmark, setting new state-of-the-art standards. The source code for this\nstudy is accessible at https://github.com/xuyizdby/NoiseEraSAR.\n","authors":["Yi Xu","Kunyu Peng","Di Wen","Ruiping Liu","Junwei Zheng","Yufan Chen","Jiaming Zhang","Alina Roitberg","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2403.09975v2.pdf","comment":"Accepted to IROS 2024. The source code for this study is accessible\n  at https://github.com/xuyizdby/NoiseEraSAR"},{"id":"http://arxiv.org/abs/2310.02650v3","updated":"2024-08-06T00:08:40Z","published":"2023-10-04T08:18:30Z","title":"Active Visual Localization for Multi-Agent Collaboration: A Data-Driven\n  Approach","summary":"  Rather than having each newly deployed robot create its own map of its\nsurroundings, the growing availability of SLAM-enabled devices provides the\noption of simply localizing in a map of another robot or device. In cases such\nas multi-robot or human-robot collaboration, localizing all agents in the same\nmap is even necessary. However, localizing e.g. a ground robot in the map of a\ndrone or head-mounted MR headset presents unique challenges due to viewpoint\nchanges. This work investigates how active visual localization can be used to\novercome such challenges of viewpoint changes. Specifically, we focus on the\nproblem of selecting the optimal viewpoint at a given location. We compare\nexisting approaches in the literature with additional proposed baselines and\npropose a novel data-driven approach. The result demonstrates the superior\nperformance of the data-driven approach when compared to existing methods, both\nin controlled simulation experiments and real-world deployment.\n","authors":["Matthew Hanlon","Boyang Sun","Marc Pollefeys","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2310.02650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00391v3","updated":"2024-08-06T23:58:07Z","published":"2023-12-31T04:14:43Z","title":"SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with\n  Diffusion-Controllable Adversaries","summary":"  Evaluating the performance of autonomous vehicle planning algorithms\nnecessitates simulating long-tail safety-critical traffic scenarios. However,\ntraditional methods for generating such scenarios often fall short in terms of\ncontrollability and realism; they also neglect the dynamics of agent\ninteractions. To address these limitations, we introduce SAFE-SIM, a novel\ndiffusion-based controllable closed-loop safety-critical simulation framework.\nOur approach yields two distinct advantages: 1) generating realistic long-tail\nsafety-critical scenarios that closely reflect real-world conditions, and 2)\nproviding controllable adversarial behavior for more comprehensive and\ninteractive evaluations. We develop a novel approach to simulate\nsafety-critical scenarios through an adversarial term in the denoising process\nof diffusion models, which allows an adversarial agent to challenge a planner\nwith plausible maneuvers while all agents in the scene exhibit reactive and\nrealistic behaviors. Furthermore, we propose novel guidance objectives and a\npartial diffusion process that enables users to control key aspects of the\nscenarios, such as the collision type and aggressiveness of the adversarial\nagent, while maintaining the realism of the behavior. We validate our framework\nempirically using the nuScenes and nuPlan datasets across multiple planners,\ndemonstrating improvements in both realism and controllability. These findings\naffirm that diffusion models provide a robust and versatile foundation for\nsafety-critical, interactive traffic simulation, extending their utility across\nthe broader autonomous driving landscape. Project website:\nhttps://safe-sim.github.io/.\n","authors":["Wei-Jer Chang","Francesco Pittaluga","Masayoshi Tomizuka","Wei Zhan","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2401.00391v3.pdf","comment":"Accepted by ECCV2024; Project website: https://safe-sim.github.io/"},{"id":"http://arxiv.org/abs/2408.03464v1","updated":"2024-08-06T22:39:34Z","published":"2024-08-06T22:39:34Z","title":"AI Foundation Models in Remote Sensing: A Survey","summary":"  Artificial Intelligence (AI) technologies have profoundly transformed the\nfield of remote sensing, revolutionizing data collection, processing, and\nanalysis. Traditionally reliant on manual interpretation and task-specific\nmodels, remote sensing has been significantly enhanced by the advent of\nfoundation models--large-scale, pre-trained AI models capable of performing a\nwide array of tasks with unprecedented accuracy and efficiency. This paper\nprovides a comprehensive survey of foundation models in the remote sensing\ndomain, covering models released between June 2021 and June 2024. We categorize\nthese models based on their applications in computer vision and domain-specific\ntasks, offering insights into their architectures, pre-training datasets, and\nmethodologies. Through detailed performance comparisons, we highlight emerging\ntrends and the significant advancements achieved by these foundation models.\nAdditionally, we discuss the technical challenges, practical implications, and\nfuture research directions, addressing the need for high-quality data,\ncomputational resources, and improved model generalization. Our research also\nfinds that pre-training methods, particularly self-supervised learning\ntechniques like contrastive learning and masked autoencoders, significantly\nenhance the performance and robustness of foundation models in remote sensing\ntasks such as scene classification, object detection, and other applications.\nThis survey aims to serve as a resource for researchers and practitioners by\nproviding a panorama of advances and promising pathways for continued\ndevelopment and application of foundation models in remote sensing.\n","authors":["Siqi Lu","Junlin Guo","James R Zimmer-Dauphinee","Jordan M Nieusma","Xiao Wang","Parker VanValkenburgh","Steven A Wernke","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2408.03464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19184v2","updated":"2024-08-06T22:36:04Z","published":"2024-07-27T05:52:31Z","title":"Enhancing Tree Type Detection in Forest Fire Risk Assessment:\n  Multi-Stage Approach and Color Encoding with Forest Fire Risk Evaluation\n  Framework for UAV Imagery","summary":"  Forest fires pose a significant threat to ecosystems, economies, and human\nhealth worldwide. Early detection and assessment of forest fires are crucial\nfor effective management and conservation efforts. Unmanned Aerial Vehicles\n(UAVs) equipped with advanced computer vision algorithms offer a promising\nsolution for forest fire detection and assessment. In this paper, we optimize\nan integrated forest fire risk assessment framework using UAVs and multi-stage\nobject detection algorithms. We introduce improvements to our previous\nframework, including the adoption of Faster R-CNN, Grid R-CNN, Sparse R-CNN,\nCascade R-CNN, Dynamic R-CNN, and Libra R-CNN detectors, and explore\noptimizations such as CBAM for attention enhancement, random erasing for\npreprocessing, and different color space representations. We evaluate these\nenhancements through extensive experimentation using aerial image footage from\nvarious regions in British Columbia, Canada. Our findings demonstrate the\neffectiveness of multi-stage detectors and optimizations in improving the\naccuracy of forest fire risk assessment. This research contributes to the\nadvancement of UAV-based forest fire detection and assessment systems,\nenhancing their efficiency and effectiveness in supporting sustainable forest\nmanagement and conservation efforts.\n","authors":["Jinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.19184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12972v2","updated":"2024-08-06T22:28:25Z","published":"2024-01-23T18:58:35Z","title":"On the Efficacy of Text-Based Input Modalities for Action Anticipation","summary":"  Anticipating future actions is a highly challenging task due to the diversity\nand scale of potential future actions; yet, information from different\nmodalities help narrow down plausible action choices. Each modality can provide\ndiverse and often complementary context for the model to learn from. While\nprevious multi-modal methods leverage information from modalities such as video\nand audio, we primarily explore how text descriptions of actions and objects\ncan also lead to more accurate action anticipation by providing additional\ncontextual cues, e.g., about the environment and its contents. We propose a\nMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformer\narchitecture that jointly learns from multi-modal features and text\ndescriptions of actions and objects. We train our model in two stages, where\nthe model first learns to align video clips with descriptions of future\nactions, and is subsequently fine-tuned to predict future actions. Compared to\nexisting methods, M-CAT has the advantage of learning additional context from\ntwo types of text inputs: rich descriptions of future actions during\npre-training, and, text descriptions for detected objects and actions during\nmodality feature fusion. Through extensive experimental evaluation, we\ndemonstrate that our model outperforms previous methods on the EpicKitchens\ndatasets, and show that using simple text descriptions of actions and objects\naid in more effective action anticipation. In addition, we examine the impact\nof object and action information obtained via text, and perform extensive\nablations.\n","authors":["Apoorva Beedu","Karan Samel","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2401.12972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01532v2","updated":"2024-08-06T21:19:20Z","published":"2024-08-02T18:45:01Z","title":"Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and\n  Localization","summary":"  In the digital age, the emergence of deepfakes and synthetic media presents a\nsignificant threat to societal and political integrity. Deepfakes based on\nmulti-modal manipulation, such as audio-visual, are more realistic and pose a\ngreater threat. Current multi-modal deepfake detectors are often based on the\nattention-based fusion of heterogeneous data streams from multiple modalities.\nHowever, the heterogeneous nature of the data (such as audio and visual\nsignals) creates a distributional modality gap and poses a significant\nchallenge in effective fusion and hence multi-modal deepfake detection. In this\npaper, we propose a novel multi-modal attention framework based on recurrent\nneural networks (RNNs) that leverages contextual information for audio-visual\ndeepfake detection. The proposed approach applies attention to multi-modal\nmulti-sequence representations and learns the contributing features among them\nfor deepfake detection and localization. Thorough experimental validations on\naudio-visual deepfake datasets, namely FakeAVCeleb, AV-Deepfake1M, TVIL, and\nLAV-DF datasets, demonstrate the efficacy of our approach. Cross-comparison\nwith the published studies demonstrates superior performance of our approach\nwith an improved accuracy and precision by 3.47% and 2.05% in deepfake\ndetection and localization, respectively. Thus, obtaining state-of-the-art\nperformance. To facilitate reproducibility, the code and the datasets\ninformation is available at https://github.com/vcbsl/audiovisual-deepfake/.\n","authors":["Vinaya Sree Katamneni","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2408.01532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02348v3","updated":"2024-08-06T21:16:43Z","published":"2024-04-02T22:49:25Z","title":"COVID-19 Detection Based on Blood Test Parameters using Various\n  Artificial Intelligence Methods","summary":"  In 2019, the world faced a new challenge: a COVID-19 disease caused by the\nnovel coronavirus, SARS-CoV-2. The virus rapidly spread across the globe,\nleading to a high rate of mortality, which prompted health organizations to\ntake measures to control its transmission. Early disease detection is crucial\nin the treatment process, and computer-based automatic detection systems have\nbeen developed to aid in this effort. These systems often rely on artificial\nintelligence (AI) approaches such as machine learning, neural networks, fuzzy\nsystems, and deep learning to classify diseases. This study aimed to\ndifferentiate COVID-19 patients from others using self-categorizing classifiers\nand employing various AI methods. This study used two datasets: the blood test\nsamples and radiography images. The best results for the blood test samples\nobtained from San Raphael Hospital, which include two classes of individuals,\nthose with COVID-19 and those with non-COVID diseases, were achieved through\nthe use of the Ensemble method (a combination of a neural network and two\nmachines learning methods). The results showed that this approach for COVID-19\ndiagnosis is cost-effective and provides results in a shorter amount of time\nthan other methods. The proposed model achieved an accuracy of 94.09% on the\ndataset used. Secondly, the radiographic images were divided into four classes:\nnormal, viral pneumonia, ground glass opacity, and COVID-19 infection. These\nwere used for segmentation and classification. The lung lobes were extracted\nfrom the images and then categorized into specific classes. We achieved an\naccuracy of 91.1% on the image dataset. Generally, this study highlights the\npotential of AI in detecting and managing COVID-19 and underscores the\nimportance of continued research and development in this field.\n","authors":["Kavian Khanjani","Seyed Rasoul Hosseini","Hamid Taheri","Shahrzad Shashaani","Mohammad Teshnehlab"],"pdf_url":"https://arxiv.org/pdf/2404.02348v3.pdf","comment":"This paper is under review by Int. J. of Computational Science and\n  Engineering"},{"id":"http://arxiv.org/abs/2403.19782v2","updated":"2024-08-06T21:14:43Z","published":"2024-03-28T19:07:26Z","title":"ENet-21: An Optimized light CNN Structure for Lane Detection","summary":"  Lane detection for autonomous vehicles is an important concept, yet it is a\nchallenging issue of driver assistance systems in modern vehicles. The\nemergence of deep learning leads to significant progress in self-driving cars.\nConventional deep learning-based methods handle lane detection problems as a\nbinary segmentation task and determine whether a pixel belongs to a line. These\nmethods rely on the assumption of a fixed number of lanes, which does not\nalways work. This study aims to develop an optimal structure for the lane\ndetection problem, offering a promising solution for driver assistance features\nin modern vehicles by utilizing a machine learning method consisting of binary\nsegmentation and Affinity Fields that can manage varying numbers of lanes and\nlane change scenarios. In this approach, the Convolutional Neural Network\n(CNN), is selected as a feature extractor, and the final output is obtained\nthrough clustering of the semantic segmentation and Affinity Field outputs. Our\nmethod uses less complex CNN architecture than existing ones. Experiments on\nthe TuSimple dataset support the effectiveness of the proposed method.\n","authors":["Seyed Rasoul Hosseini","Hamid Taheri","Mohammad Teshnehlab"],"pdf_url":"https://arxiv.org/pdf/2403.19782v2.pdf","comment":"The paper is under review by Int. J. of Mechatronics and Automation"},{"id":"http://arxiv.org/abs/2407.19305v2","updated":"2024-08-06T21:07:17Z","published":"2024-07-27T17:27:05Z","title":"GP-VLS: A general-purpose vision language model for surgery","summary":"  Surgery requires comprehensive medical knowledge, visual assessment skills,\nand procedural expertise. While recent surgical AI models have focused on\nsolving task-specific problems, there is a need for general-purpose systems\nthat can understand surgical scenes and interact through natural language. This\npaper introduces GP-VLS, a general-purpose vision language model for surgery\nthat integrates medical and surgical knowledge with visual scene understanding.\nFor comprehensively evaluating general-purpose surgical models, we propose\nSurgiQual, which evaluates across medical and surgical knowledge benchmarks as\nwell as surgical vision-language questions. To train GP-VLS, we develop six new\ndatasets spanning medical knowledge, surgical textbooks, and vision-language\npairs for tasks like phase recognition and tool identification. We show that\nGP-VLS significantly outperforms existing open- and closed-source models on\nsurgical vision-language tasks, with 8-21% improvements in accuracy across\nSurgiQual benchmarks. GP-VLS also demonstrates strong performance on medical\nand surgical knowledge tests compared to open-source alternatives. Overall,\nGP-VLS provides an open-source foundation for developing AI assistants to\nsupport surgeons across a wide range of tasks and scenarios. The code and data\nfor this work is publicly available at gpvls-surgery-vlm.github.io.\n","authors":["Samuel Schmidgall","Joseph Cho","Cyril Zakka","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2407.19305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03448v1","updated":"2024-08-06T21:00:02Z","published":"2024-08-06T21:00:02Z","title":"Post-Mortem Human Iris Segmentation Analysis with Deep Learning","summary":"  Iris recognition is widely used in several fields such as mobile phones,\nfinancial transactions, identification cards, airport security, international\nborder control, voter registration for living persons. However, the possibility\nof identifying deceased individuals based on their iris patterns has emerged\nrecently as a supplementary or alternative method valuable in forensic\nanalysis. Simultaneously, it poses numerous new technological challenges and\none of the most challenging among them is the image segmentation stage as\nconventional iris recognition approaches have struggled to reliably execute it.\nThis paper presents and compares Deep Learning (DL) models designed for\nsegmenting iris images collected from the deceased subjects, by training SegNet\nand DeepLabV3+ semantic segmentation methods where using VGG19, ResNet18,\nResNet50, MobileNetv2, Xception, or InceptionResNetv2 as backbones. In this\nstudy, our experiments demonstrate that our proposed method effectively learns\nand identifies specific deformations inherent in post-mortem samples and\nproviding a significant improvement in accuracy. By employing our novel method\nMobileNetv2 as the backbone of DeepLabV3+ and replacing the final layer with a\nhybrid loss function combining Boundary and Dice loss, we achieve Mean\nIntersection over Union of 95.54% on the Warsaw-BioBase-PostMortem-Iris-v1\ndataset. To the best of our knowledge, this study provides the most extensive\nevaluation of DL models for post-mortem iris segmentation.\n","authors":["Afzal Hossain","Tipu Sultan","Stephanie Schuckers"],"pdf_url":"https://arxiv.org/pdf/2408.03448v1.pdf","comment":"submitted to ijcb 2024 special session"},{"id":"http://arxiv.org/abs/2408.03433v1","updated":"2024-08-06T20:19:06Z","published":"2024-08-06T20:19:06Z","title":"Hybrid diffusion models: combining supervised and generative pretraining\n  for label-efficient fine-tuning of segmentation models","summary":"  We are considering in this paper the task of label-efficient fine-tuning of\nsegmentation models: We assume that a large labeled dataset is available and\nallows to train an accurate segmentation model in one domain, and that we have\nto adapt this model on a related domain where only a few samples are available.\nWe observe that this adaptation can be done using two distinct methods: The\nfirst method, supervised pretraining, is simply to take the model trained on\nthe first domain using classical supervised learning, and fine-tune it on the\nsecond domain with the available labeled samples. The second method is to\nperform self-supervised pretraining on the first domain using a generic pretext\ntask in order to get high-quality representations which can then be used to\ntrain a model on the second domain in a label-efficient way. We propose in this\npaper to fuse these two approaches by introducing a new pretext task, which is\nto perform simultaneously image denoising and mask prediction on the first\ndomain. We motivate this choice by showing that in the same way that an image\ndenoiser conditioned on the noise level can be considered as a generative model\nfor the unlabeled image distribution using the theory of diffusion models, a\nmodel trained using this new pretext task can be considered as a generative\nmodel for the joint distribution of images and segmentation masks under the\nassumption that the mapping from images to segmentation masks is deterministic.\nWe then empirically show on several datasets that fine-tuning a model\npretrained using this approach leads to better results than fine-tuning a\nsimilar model trained using either supervised or unsupervised pretraining only.\n","authors":["Bruno Sauvalle","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2408.03433v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2405.14386v2","updated":"2024-08-06T20:14:27Z","published":"2024-05-23T10:04:23Z","title":"Capsule Network Projectors are Equivariant and Invariant Learners","summary":"  Learning invariant representations has been the longstanding approach to\nself-supervised learning. However, recently progress has been made in\npreserving equivariant properties in representations, yet do so with highly\nprescribed architectures. In this work, we propose an invariant-equivariant\nself-supervised architecture that employs Capsule Networks (CapsNets) which\nhave been shown to capture equivariance with respect to novel viewpoints. We\ndemonstrate that the use of CapsNets in equivariant self-supervised\narchitectures achieves improved downstream performance on equivariant tasks\nwith higher efficiency and fewer network parameters. To accommodate the\narchitectural changes of CapsNets, we introduce a new objective function based\non entropy minimisation. This approach which we name CapsIE (Capsule Invariant\nEquivariant Network) achieves state-of-the-art performance across invariant and\nequivariant tasks on the 3DIEBench dataset compared to prior equivariant SSL\nmethods, while outperforming supervised baselines. Our results demonstrate the\nability of CapsNets to learn complex and generalised representations for\nlarge-scale, multi-task datasets compared to previous CapsNet benchmarks. Code\nis available at https://github.com/AberdeenML/CapsIE.\n","authors":["Miles Everett","Aiden Durrant","Mingjun Zhong","Georgios Leontidis"],"pdf_url":"https://arxiv.org/pdf/2405.14386v2.pdf","comment":"17 pages, 7 figures, 10 Tables; code to be released at:\n  https://github.com/AberdeenML/CapsIE V2: corrected typos, added a new Table 3\n  and additional results in Table 1 and Table 2"},{"id":"http://arxiv.org/abs/2309.15329v2","updated":"2024-08-06T19:51:49Z","published":"2023-09-27T00:20:36Z","title":"BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction\n  using Neural Radiance Fields","summary":"  Reconstruction of deformable scenes from endoscopic videos is important for\nmany applications such as intraoperative navigation, surgical visual\nperception, and robotic surgery. It is a foundational requirement for realizing\nautonomous robotic interventions for minimally invasive surgery. However,\nprevious approaches in this domain have been limited by their modular nature\nand are confined to specific camera and scene settings. Our work adopts the\nNeural Radiance Fields (NeRF) approach to learning 3D implicit representations\nof scenes that are both dynamic and deformable over time, and furthermore with\nunknown camera poses. We demonstrate this approach on endoscopic surgical\nscenes from robotic surgery. This work removes the constraints of known camera\nposes and overcomes the drawbacks of the state-of-the-art unstructured dynamic\nscene reconstruction technique, which relies on the static part of the scene\nfor accurate reconstruction. Through several experimental datasets, we\ndemonstrate the versatility of our proposed model to adapt to diverse camera\nand scene settings, and show its promise for both current and future robotic\nsurgical systems.\n","authors":["Shreya Saha","Zekai Liang","Shan Lin","Jingpei Lu","Michael Yip","Sainan Liu"],"pdf_url":"https://arxiv.org/pdf/2309.15329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02138v2","updated":"2024-08-06T19:27:12Z","published":"2024-08-04T20:35:33Z","title":"RICA2: Rubric-Informed, Calibrated Assessment of Actions","summary":"  The ability to quantify how well an action is carried out, also known as\naction quality assessment (AQA), has attracted recent interest in the vision\ncommunity. Unfortunately, prior methods often ignore the score rubric used by\nhuman experts and fall short of quantifying the uncertainty of the model\nprediction. To bridge the gap, we present RICA^2 - a deep probabilistic model\nthat integrates score rubric and accounts for prediction uncertainty for AQA.\nCentral to our method lies in stochastic embeddings of action steps, defined on\na graph structure that encodes the score rubric. The embeddings spread\nprobabilistic density in the latent space and allow our method to represent\nmodel uncertainty. The graph encodes the scoring criteria, based on which the\nquality scores can be decoded. We demonstrate that our method establishes new\nstate of the art on public benchmarks, including FineDiving, MTL-AQA, and\nJIGSAWS, with superior performance in score prediction and uncertainty\ncalibration. Our code is available at https://abrarmajeedi.github.io/rica2_aqa/\n","authors":["Abrar Majeedi","Viswanatha Reddy Gajjala","Satya Sai Srinath Namburi GNVV","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2408.02138v2.pdf","comment":"Accepted at European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2408.02226v2","updated":"2024-08-06T19:12:35Z","published":"2024-08-05T04:10:52Z","title":"ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative\n  Generation","summary":"  In this paper, we propose ProCreate, a simple and easy-to-implement method to\nimprove sample diversity and creativity of diffusion-based image generative\nmodels and to prevent training data reproduction. ProCreate operates on a set\nof reference images and actively propels the generated image embedding away\nfrom the reference embeddings during the generation process. We propose FSCG-8\n(Few-Shot Creative Generation 8), a few-shot creative generation dataset on\neight different categories -- encompassing different concepts, styles, and\nsettings -- in which ProCreate achieves the highest sample diversity and\nfidelity. Furthermore, we show that ProCreate is effective at preventing\nreplicating training data in a large-scale evaluation using training text\nprompts. Code and FSCG-8 are available at\nhttps://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The\nproject page is available at https://procreate-diffusion.github.io.\n","authors":["Jack Lu","Ryan Teehan","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2408.02226v2.pdf","comment":"Accepted to ECCV 2024. Project page:\n  https://procreate-diffusion.github.io"},{"id":"http://arxiv.org/abs/2408.03404v1","updated":"2024-08-06T18:55:31Z","published":"2024-08-06T18:55:31Z","title":"Set2Seq Transformer: Learning Permutation Aware Set Representations of\n  Artistic Sequences","summary":"  We propose Set2Seq Transformer, a novel sequential multiple instance\narchitecture, that learns to rank permutation aware set representations of\nsequences. First, we illustrate that learning temporal position-aware\nrepresentations of discrete timesteps can greatly improve static visual\nmultiple instance learning methods that do not regard temporality and\nconcentrate almost exclusively on visual content analysis. We further\ndemonstrate the significant advantages of end-to-end sequential multiple\ninstance learning, integrating visual content and temporal information in a\nmultimodal manner. As application we focus on fine art analysis related tasks.\nTo that end, we show that our Set2Seq Transformer can leverage visual set and\ntemporal position-aware representations for modelling visual artists' oeuvres\nfor predicting artistic success. Finally, through extensive quantitative and\nqualitative evaluation using a novel dataset, WikiArt-Seq2Rank, and a visual\nlearning-to-rank downstream task, we show that our Set2Seq Transformer captures\nessential temporal information improving the performance of strong static and\nsequential multiple instance learning methods for predicting artistic success.\n","authors":["Athanasios Efthymiou","Stevan Rudinac","Monika Kackovic","Nachoem Wijnberg","Marcel Worring"],"pdf_url":"https://arxiv.org/pdf/2408.03404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00448v2","updated":"2024-08-06T18:52:25Z","published":"2024-06-01T14:10:45Z","title":"Bilateral Guided Radiance Field Processing","summary":"  Neural Radiance Fields (NeRF) achieves unprecedented performance in\nsynthesizing novel view synthesis, utilizing multi-view consistency. When\ncapturing multiple inputs, image signal processing (ISP) in modern cameras will\nindependently enhance them, including exposure adjustment, color correction,\nlocal tone mapping, etc. While these processings greatly improve image quality,\nthey often break the multi-view consistency assumption, leading to \"floaters\"\nin the reconstructed radiance fields. To address this concern without\ncompromising visual aesthetics, we aim to first disentangle the enhancement by\nISP at the NeRF training stage and re-apply user-desired enhancements to the\nreconstructed radiance fields at the finishing stage. Furthermore, to make the\nre-applied enhancements consistent between novel views, we need to perform\nimaging signal processing in 3D space (i.e. \"3D ISP\"). For this goal, we adopt\nthe bilateral grid, a locally-affine model, as a generalized representation of\nISP processing. Specifically, we optimize per-view 3D bilateral grids with\nradiance fields to approximate the effects of camera pipelines for each input\nview. To achieve user-adjustable 3D finishing, we propose to learn a low-rank\n4D bilateral grid from a given single view edit, lifting photo enhancements to\nthe whole 3D scene. We demonstrate our approach can boost the visual quality of\nnovel view synthesis by effectively removing floaters and performing\nenhancements from user retouching. The source code and our data are available\nat: https://bilarfpro.github.io.\n","authors":["Yuehao Wang","Chaoyi Wang","Bingchen Gong","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2406.00448v2.pdf","comment":"SIGGRAPH (ACM TOG), 2024. Project page: https://bilarfpro.github.io"},{"id":"http://arxiv.org/abs/2407.19166v2","updated":"2024-08-06T18:52:04Z","published":"2024-07-27T04:37:16Z","title":"Revisit Self-supervised Depth Estimation with Local\n  Structure-from-Motion","summary":"  Both self-supervised depth estimation and Structure-from-Motion (SfM) recover\nscene depth from RGB videos. Despite sharing a similar objective, the two\napproaches are disconnected. Prior works of self-supervision backpropagate\nlosses defined within immediate neighboring frames. Instead of\nlearning-through-loss, this work proposes an alternative scheme by performing\nlocal SfM. First, with calibrated RGB or RGB-D images, we employ a depth and\ncorrespondence estimator to infer depthmaps and pair-wise correspondence maps.\nThen, a novel bundle-RANSAC-adjustment algorithm jointly optimizes camera poses\nand one depth adjustment for each depthmap. Finally, we fix camera poses and\nemploy a NeRF, however, without a neural network, for dense triangulation and\ngeometric verification. Poses, depth adjustments, and triangulated sparse\ndepths are our outputs. For the first time, we show self-supervision within $5$\nframes already benefits SoTA supervised depth and correspondence models. The\nproject page is held in the link (https://shngjz.github.io/SSfM.github.io/).\n","authors":["Shengjie Zhu","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2407.19166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03393v1","updated":"2024-08-06T18:38:55Z","published":"2024-08-06T18:38:55Z","title":"Biomedical Image Segmentation: A Systematic Literature Review of Deep\n  Learning Based Object Detection Methods","summary":"  Biomedical image segmentation plays a vital role in diagnosis of diseases\nacross various organs. Deep learning-based object detection methods are\ncommonly used for such segmentation. There exists an extensive research in this\ntopic. However, there is no standard review on this topic. Existing surveys\noften lack a standardized approach or focus on broader segmentation techniques.\nIn this paper, we conducted a systematic literature review (SLR), collected and\nanalysed 148 articles that explore deep learning object detection methods for\nbiomedical image segmentation. We critically analyzed these methods, identified\nthe key challenges, and discussed the future directions. From the selected\narticles we extracted the results including the deep learning models, targeted\nimaging modalities, targeted diseases, and the metrics for the analysis of the\nmethods. The results have been presented in tabular and/or charted forms. The\nresults are presented in three major categories including two stage detection\nmodels, one stage detection models and point-based detection models. Each\narticle is individually analyzed along with its pros and cons. Finally, we\ndiscuss open challenges, potential benefits, and future research directions.\nThis SLR aims to provide the research community with a quick yet deeper\nunderstanding of these segmentation models, ultimately facilitating the\ndevelopment of more powerful solutions for biomedical image analysis.\n","authors":["Fazli Wahid","Yingliang Ma","Dawar Khan","Muhammad Aamir","Syed U. K. Bukhari"],"pdf_url":"https://arxiv.org/pdf/2408.03393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11645v2","updated":"2024-08-06T18:31:05Z","published":"2024-06-17T15:28:35Z","title":"SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for\n  Upper-Body Pose Tracking","summary":"  Seams are areas of overlapping fabric formed by stitching two or more pieces\nof fabric together in the cut-and-sew apparel manufacturing process. In\nSeamPose, we repurposed seams as capacitive sensors in a shirt for continuous\nupper-body pose estimation. Compared to previous all-textile motion-capturing\ngarments that place the electrodes on the clothing surface, our solution\nleverages existing seams inside of a shirt by machine-sewing insulated\nconductive threads over the seams. The unique invisibilities and placements of\nthe seams afford the sensing shirt to look and wear similarly as a conventional\nshirt while providing exciting pose-tracking capabilities. To validate this\napproach, we implemented a proof-of-concept untethered shirt with 8 capacitive\nsensing seams. With a 12-participant user study, our customized deep-learning\npipeline accurately estimates the relative (to the pelvis) upper-body 3D joint\npositions with a mean per joint position error (MPJPE) of 6.0 cm. SeamPose\nrepresents a step towards unobtrusive integration of smart clothing for\neveryday pose estimation.\n","authors":["Tianhong Catherine Yu","Manru Mary Zhang","Peter He","Chi-Jung Lee","Cassidy Cheesman","Saif Mahmud","Ruidong Zhang","Franois Guimbretire","Cheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.11645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03388v1","updated":"2024-08-06T18:18:37Z","published":"2024-08-06T18:18:37Z","title":"A Non-negative VAE:the Generalized Gamma Belief Network","summary":"  The gamma belief network (GBN), often regarded as a deep topic model, has\ndemonstrated its potential for uncovering multi-layer interpretable latent\nrepresentations in text data. Its notable capability to acquire interpretable\nlatent factors is partially attributed to sparse and non-negative\ngamma-distributed latent variables. However, the existing GBN and its\nvariations are constrained by the linear generative model, thereby limiting\ntheir expressiveness and applicability. To address this limitation, we\nintroduce the generalized gamma belief network (Generalized GBN) in this paper,\nwhich extends the original linear generative model to a more expressive\nnon-linear generative model. Since the parameters of the Generalized GBN no\nlonger possess an analytic conditional posterior, we further propose an\nupward-downward Weibull inference network to approximate the posterior\ndistribution of the latent variables. The parameters of both the generative\nmodel and the inference network are jointly trained within the variational\ninference framework. Finally, we conduct comprehensive experiments on both\nexpressivity and disentangled representation learning tasks to evaluate the\nperformance of the Generalized GBN against state-of-the-art Gaussian\nvariational autoencoders serving as baselines.\n","authors":["Zhibin Duan","Tiansheng Wen","Muyao Wang","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03356v1","updated":"2024-08-06T10:59:58Z","published":"2024-08-06T10:59:58Z","title":"RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel\n  View Synthesis","summary":"  Differentiable volumetric rendering-based methods made significant progress\nin novel view synthesis. On one hand, innovative methods have replaced the\nNeural Radiance Fields (NeRF) network with locally parameterized structures,\nenabling high-quality renderings in a reasonable time. On the other hand,\napproaches have used differentiable splatting instead of NeRF's ray casting to\noptimize radiance fields rapidly using Gaussian kernels, allowing for fine\nadaptation to the scene. However, differentiable ray casting of irregularly\nspaced kernels has been scarcely explored, while splatting, despite enabling\nfast rendering times, is susceptible to clearly visible artifacts.\n  Our work closes this gap by providing a physically consistent formulation of\nthe emitted radiance c and density {\\sigma}, decomposed with Gaussian functions\nassociated with Spherical Gaussians/Harmonics for all-frequency colorimetric\nrepresentation. We also introduce a method enabling differentiable ray casting\nof irregularly distributed Gaussians using an algorithm that integrates\nradiance fields slab by slab and leverages a BVH structure. This allows our\napproach to finely adapt to the scene while avoiding splatting artifacts. As a\nresult, we achieve superior rendering quality compared to the state-of-the-art\nwhile maintaining reasonable training times and achieving inference speeds of\n25 FPS on the Blender dataset. Project page with videos and code:\nhttps://raygauss.github.io/\n","authors":["Hugo Blanc","Jean-Emmanuel Deschaud","Alexis Paljic"],"pdf_url":"https://arxiv.org/pdf/2408.03356v1.pdf","comment":"Project page with videos and code: https://raygauss.github.io/"},{"id":"http://arxiv.org/abs/2408.03355v1","updated":"2024-08-06T09:16:13Z","published":"2024-08-06T09:16:13Z","title":"FastEdit: Fast Text-Guided Single-Image Editing via Semantic-Aware\n  Diffusion Fine-Tuning","summary":"  Conventional Text-guided single-image editing approaches require a two-step\nprocess, including fine-tuning the target text embedding for over 1K iterations\nand the generative model for another 1.5K iterations. Although it ensures that\nthe resulting image closely aligns with both the input image and the target\ntext, this process often requires 7 minutes per image, posing a challenge for\npractical application due to its time-intensive nature. To address this\nbottleneck, we introduce FastEdit, a fast text-guided single-image editing\nmethod with semantic-aware diffusion fine-tuning, dramatically accelerating the\nediting process to only 17 seconds. FastEdit streamlines the generative model's\nfine-tuning phase, reducing it from 1.5K to a mere 50 iterations. For diffusion\nfine-tuning, we adopt certain time step values based on the semantic\ndiscrepancy between the input image and target text. Furthermore, FastEdit\ncircumvents the initial fine-tuning step by utilizing an image-to-image model\nthat conditions on the feature space, rather than the text embedding space. It\ncan effectively align the target text prompt and input image within the same\nfeature space and save substantial processing time. Additionally, we apply the\nparameter-efficient fine-tuning technique LoRA to U-net. With LoRA, FastEdit\nminimizes the model's trainable parameters to only 0.37\\% of the original size.\nAt the same time, we can achieve comparable editing outcomes with significantly\nreduced computational overhead. We conduct extensive experiments to validate\nthe editing performance of our approach and show promising editing\ncapabilities, including content addition, style transfer, background\nreplacement, and posture manipulation, etc.\n","authors":["Zhi Chen","Zecheng Zhao","Yadan Luo","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.03355v1.pdf","comment":"Technical Report"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2404.03453v3","updated":"2024-08-06T07:53:09Z","published":"2024-04-04T13:57:44Z","title":"Conditioning of Banach Space Valued Gaussian Random Variables: An\n  Approximation Approach Based on Martingales","summary":"  In this paper we investigate the conditional distributions of two Banach\nspace valued, jointly Gaussian random variables. We show that these conditional\ndistributions are again Gaussian and that their means and covariances are\ndetermined by a general finite dimensional approximation scheme based upon a\nmartingale approach. In particular, it turns out that the covariance operators\noccurring in this scheme converge with respect to the nuclear norm and that the\nconditional probabilities converge weakly. Moreover, we discuss in detail, how\nour approximation scheme can be implemented in several classes of important\nBanach spaces such as (reproducing kernel) Hilbert spaces and spaces of\ncontinuous functions. As an example, we then apply our general results to the\ncase of Gaussian processes with continuous paths conditioned to partial but\ninfinite observations of their paths. Here we show that conditioning on\nsufficiently rich, increasing sets of finitely many observations leads to\nconsistent approximations, that is, both the mean and covariance functions\nconverge uniformly and the conditional probabilities converge weakly. Moreover,\nwe discuss how these results improve our understanding of the popular Gaussian\nprocesses for machine learning.\n","authors":["Ingo Steinwart"],"pdf_url":"https://arxiv.org/pdf/2404.03453v3.pdf","comment":"55 pages plus 22 pages of supplemental material"},{"id":"http://arxiv.org/abs/2305.09958v3","updated":"2024-08-06T02:32:05Z","published":"2023-05-17T05:35:49Z","title":"SIGMA: Similarity-based Efficient Global Aggregation for Heterophilous\n  Graph Neural Networks","summary":"  Graph neural networks (GNNs) realize great success in graph learning but\nsuffer from performance loss when meeting heterophily, i.e. neighboring nodes\nare dissimilar, due to their local and uniform aggregation. Existing attempts\nof heterophilous GNNs incorporate long-range or global aggregations to\ndistinguish nodes in the graph. However, these aggregations usually require\niteratively maintaining and updating full-graph information, which limits their\nefficiency when applying to large-scale graphs. In this paper, we propose\nSIGMA, an efficient global heterophilous GNN aggregation integrating the\nstructural similarity measurement SimRank. Our theoretical analysis illustrates\nthat SIGMA inherently captures distant global similarity even under\nheterophily, that conventional approaches can only achieve after iterative\naggregations. Furthermore, it enjoys efficient one-time computation with a\ncomplexity only linear to the node set size $\\mathcal{O}(n)$. Comprehensive\nevaluation demonstrates that SIGMA achieves state-of-the-art performance with\nsuperior aggregation and overall efficiency. Notably, it obtains 5$\\times$\nacceleration on the large-scale heterophily dataset \\emph{pokec} with over 30\nmillion edges compared to the best baseline aggregation.\n","authors":["Haoyu Liu","Ningyi Liao","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2305.09958v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02354v2","updated":"2024-08-06T10:11:28Z","published":"2024-08-05T10:02:29Z","title":"RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders","summary":"  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n","authors":["Danil Gusak","Gleb Mezentsev","Ivan Oseledets","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2408.02354v2.pdf","comment":"5 pages, accepted for CIKM'24"},{"id":"http://arxiv.org/abs/2408.02336v2","updated":"2024-08-06T04:04:23Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2404.07950v3","updated":"2024-08-06T02:42:32Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v3.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2408.03323v1","updated":"2024-08-06T17:58:29Z","published":"2024-08-06T17:58:29Z","title":"ClassiFIM: An Unsupervised Method To Detect Phase Transitions","summary":"  Estimation of the Fisher Information Metric (FIM-estimation) is an important\ntask that arises in unsupervised learning of phase transitions, a problem\nproposed by physicists. This work completes the definition of the task by\ndefining rigorous evaluation metrics distMSE, distMSEPS, and distRE and\nintroduces ClassiFIM, a novel machine learning method designed to solve the\nFIM-estimation task. Unlike existing methods for unsupervised learning of phase\ntransitions, ClassiFIM directly estimates a well-defined quantity (the FIM),\nallowing it to be rigorously compared to any present and future other methods\nthat estimate the same. ClassiFIM transforms a dataset for the FIM-estimation\ntask into a dataset for an auxiliary binary classification task and involves\nselecting and training a model for the latter. We prove that the output of\nClassiFIM approaches the exact FIM in the limit of infinite dataset size and\nunder certain regularity conditions. We implement ClassiFIM on multiple\ndatasets, including datasets describing classical and quantum phase\ntransitions, and find that it achieves a good ground truth approximation with\nmodest computational resources. Furthermore, we independently implement two\nalternative state-of-the-art methods for unsupervised estimation of phase\ntransition locations on the same datasets and find that ClassiFIM predicts such\nlocations at least as well as these other methods. To emphasize the generality\nof our method, we also propose and generate the MNIST-CNN dataset, which\nconsists of the output of CNNs trained on MNIST for different hyperparameter\nchoices. Using ClassiFIM on this dataset suggests there is a phase transition\nin the distribution of image-prediction pairs for CNNs trained on MNIST,\ndemonstrating the broad scope of FIM-estimation beyond physics.\n","authors":["Victor Kasatkin","Evgeny Mozgunov","Nicholas Ezzell","Utkarsh Mishra","Itay Hen","Daniel Lidar"],"pdf_url":"https://arxiv.org/pdf/2408.03323v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.21770v2","updated":"2024-08-06T17:57:41Z","published":"2024-07-31T17:46:51Z","title":"MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware\n  Experts","summary":"  We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.\n","authors":["Xi Victoria Lin","Akshat Shrivastava","Liang Luo","Srinivasan Iyer","Mike Lewis","Gargi Gosh","Luke Zettlemoyer","Armen Aghajanyan"],"pdf_url":"https://arxiv.org/pdf/2407.21770v2.pdf","comment":"v2 -> update related work section"},{"id":"http://arxiv.org/abs/2408.03320v1","updated":"2024-08-06T17:55:58Z","published":"2024-08-06T17:55:58Z","title":"Hedge Fund Portfolio Construction Using PolyModel Theory and\n  iTransformer","summary":"  When constructing portfolios, a key problem is that a lot of financial time\nseries data are sparse, making it challenging to apply machine learning\nmethods. Polymodel theory can solve this issue and demonstrate superiority in\nportfolio construction from various aspects. To implement the PolyModel theory\nfor constructing a hedge fund portfolio, we begin by identifying an asset pool,\nutilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theory\nalso involves choosing a wide-ranging set of risk factors, which includes\nvarious financial indices, currencies, and commodity prices. This comprehensive\nselection mirrors the complexities of the real-world environment. Leveraging on\nthe PolyModel theory, we create quantitative measures such as Long-term Alpha,\nLong-term Ratio, and SVaR. We also use more classical measures like the Sharpe\nratio or Morningstar's MRAR. To enhance the performance of the constructed\nportfolio, we also employ the latest deep learning techniques (iTransformer) to\ncapture the upward trend, while efficiently controlling the downside, using all\nthe features. The iTransformer model is specifically designed to address the\nchallenges in high-dimensional time series forecasting and could largely\nimprove our strategies. More precisely, our strategies achieve better Sharpe\nratio and annualized return. The above process enables us to create multiple\nportfolio strategies aiming for high returns and low risks when compared to\nvarious benchmarks.\n","authors":["Siqiao Zhao","Zhikang Dong","Zeyu Cao","Raphael Douady"],"pdf_url":"https://arxiv.org/pdf/2408.03320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17403v2","updated":"2024-08-06T17:41:52Z","published":"2023-09-29T17:04:06Z","title":"Maximal Volume Matrix Cross Approximation for Image Compression and\n  Least Squares Solution","summary":"  We study the classic matrix cross approximation based on the maximal volume\nsubmatrices. Our main results consist of an improvement of the classic estimate\nfor matrix cross approximation and a greedy approach for finding the maximal\nvolume submatrices. More precisely, we present a new proof of the classic\nestimate of the inequality with an improved constant. Also, we present a family\nof greedy maximal volume algorithms to improve the computational efficiency of\nmatrix cross approximation. The proposed algorithms are shown to have\ntheoretical guarantees of convergence. Finally, we present two applications:\nimage compression and the least squares approximation of continuous functions.\nOur numerical results at the end of the paper demonstrate the effective\nperformance of our approach.\n","authors":["Kenneth Allen","Ming-Jun Lai","Zhaiming Shen"],"pdf_url":"https://arxiv.org/pdf/2309.17403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00035v4","updated":"2024-08-06T17:36:06Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mlanie Ducoffe","Audrey Galametz","Guillaume Povda","Ryma Boumazouza","Nomie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v4.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2408.03314v1","updated":"2024-08-06T17:35:05Z","published":"2024-08-06T17:35:05Z","title":"Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling Model Parameters","summary":"  Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.\n","authors":["Charlie Snell","Jaehoon Lee","Kelvin Xu","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2408.03314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03720v3","updated":"2024-08-06T17:26:55Z","published":"2023-10-05T17:40:09Z","title":"SteP: Stacked LLM Policies for Web Actions","summary":"  Performing tasks on the web presents fundamental challenges to large language\nmodels (LLMs), including combinatorially large open-world tasks and variations\nacross web interfaces. Simply specifying a large prompt to handle all possible\nbehaviors and states is extremely complex, and results in behavior leaks\nbetween unrelated behaviors. Decomposition to distinct policies can address\nthis challenge, but requires carefully handing off control between policies. We\npropose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically\ncompose policies to solve a diverse set of web tasks. SteP defines a Markov\nDecision Process where the state is a stack of policies representing the\ncontrol state, i.e., the chain of policy calls. Unlike traditional methods that\nare restricted to static hierarchies, SteP enables dynamic control that adapts\nto the complexity of the task. We evaluate SteP against multiple baselines and\nweb environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP\nimproves (14.9\\% to 33.5\\%) over SOTA that use GPT-4 policies, while on\nMiniWob++, SteP is competitive with prior works while using significantly less\ndata. Our code and data are available at\nhttps://asappresearch.github.io/webagents-step.\n","authors":["Paloma Sodhi","S. R. K. Branavan","Yoav Artzi","Ryan McDonald"],"pdf_url":"https://arxiv.org/pdf/2310.03720v3.pdf","comment":"Accepted at Conference on Language Modeling (COLM) 2024. 30 pages, 15\n  figures"},{"id":"http://arxiv.org/abs/2408.03307v1","updated":"2024-08-06T17:16:10Z","published":"2024-08-06T17:16:10Z","title":"Pre-training and in-context learning IS Bayesian inference a la De\n  Finetti","summary":"  Accurately gauging uncertainty on the underlying environment is a\nlongstanding goal of intelligent systems. We characterize which latent concepts\npre-trained sequence models are naturally able to reason with. We go back to De\nFinetti's predictive view of Bayesian reasoning: instead of modeling latent\nparameters through priors and likelihoods like topic models do, De Finetti has\nlong advocated for modeling exchangeable (permutation invariant) sequences of\nobservables. According to this view, pre-training autoregressive models\nformulates informed beliefs based on prior observations (\"empirical Bayes\"),\nand forward generation is a simulated instantiation of an environment\n(\"posterior inference\"). This connection allows extending in-context learning\n(ICL) beyond predictive settings, highlighting sequence models' ability to\nperform explicit statistical inference. In particular, we show the sequence\nprediction loss over exchangeable documents controls performance on downstream\ntasks where uncertainty quantification is key. Empirically, we propose and\ndemonstrate several approaches for encoding exchangeability in sequence model\narchitectures: data augmentation, regularization, and causal masking.\n","authors":["Naimeng Ye","Hanming Yang","Andrew Siah","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2408.03307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03304v1","updated":"2024-08-06T17:11:40Z","published":"2024-08-06T17:11:40Z","title":"Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks","summary":"  Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.\n","authors":["Rafael Sterzinger","Christian Stippel","Robert Sablatnig"],"pdf_url":"https://arxiv.org/pdf/2408.03304v1.pdf","comment":"16 pages, accepted at ICPR2024"},{"id":"http://arxiv.org/abs/2304.05339v2","updated":"2024-08-06T17:09:59Z","published":"2023-04-11T16:58:59Z","title":"Deep-learning Assisted Detection and Quantification of (oo)cysts of\n  Giardia and Cryptosporidium on Smartphone Microscopy Images","summary":"  The consumption of microbial-contaminated food and water is responsible for\nthe deaths of millions of people annually. Smartphone-based microscopy systems\nare portable, low-cost, and more accessible alternatives for the detection of\nGiardia and Cryptosporidium than traditional brightfield microscopes. However,\nthe images from smartphone microscopes are noisier and require manual cyst\nidentification by trained technicians, usually unavailable in resource-limited\nsettings. Automatic detection of (oo)cysts using deep-learning-based object\ndetection could offer a solution for this limitation. We evaluate the\nperformance of four state-of-the-art object detectors to detect (oo)cysts of\nGiardia and Cryptosporidium on a custom dataset that includes both smartphone\nand brightfield microscopic images from vegetable samples. Faster RCNN,\nRetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer\n(Deformable DETR) deep-learning models were employed to explore their efficacy\nand limitations. Our results show that while the deep-learning models perform\nbetter with the brightfield microscopy image dataset than the smartphone\nmicroscopy image dataset, the smartphone microscopy predictions are still\ncomparable to the prediction performance of non-experts. Also, we publicly\nrelease brightfield and smartphone microscopy datasets with the benchmark\nresults for the detection of Giardia and Cryptosporidium, independently\ncaptured on reference (or standard lab setting) and vegetable samples. Our code\nand dataset are available at\nhttps://github.com/naamiinepal/smartphone_microscopy and\nhttps://doi.org/10.5281/zenodo.7813183, respectively.\n","authors":["Suprim Nakarmi","Sanam Pudasaini","Safal Thapaliya","Pratima Upretee","Retina Shrestha","Basant Giri","Bhanu Bhakta Neupane","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2304.05339v2.pdf","comment":"21 pages (including supplementary information), 5 figures, 7 tables,\n  Accepted for publication at the Journal of Machine Learning for Biomedical\n  Imaging (MELBA) https://melba-journal.org/2024:014"},{"id":"http://arxiv.org/abs/2408.03290v1","updated":"2024-08-06T16:39:42Z","published":"2024-08-06T16:39:42Z","title":"SARA: Singular-Value Based Adaptive Low-Rank Adaption","summary":"  With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.\n","authors":["Jihao Gu","Shuai Chen","Zelin Wang","Yibo Zhang","Ping Gong"],"pdf_url":"https://arxiv.org/pdf/2408.03290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08871v3","updated":"2024-08-06T16:38:41Z","published":"2024-02-14T00:35:10Z","title":"Position: Topological Deep Learning is the New Frontier for Relational\n  Learning","summary":"  Topological deep learning (TDL) is a rapidly evolving field that uses\ntopological features to understand and design deep learning models. This paper\nposits that TDL is the new frontier for relational learning. TDL may complement\ngraph representation learning and geometric deep learning by incorporating\ntopological concepts, and can thus provide a natural choice for various machine\nlearning settings. To this end, this paper discusses open problems in TDL,\nranging from practical benefits to theoretical foundations. For each problem,\nit outlines potential solutions and future research opportunities. At the same\ntime, this paper serves as an invitation to the scientific community to\nactively participate in TDL research to unlock the potential of this emerging\nfield.\n","authors":["Theodore Papamarkou","Tolga Birdal","Michael Bronstein","Gunnar Carlsson","Justin Curry","Yue Gao","Mustafa Hajij","Roland Kwitt","Pietro Li","Paolo Di Lorenzo","Vasileios Maroulas","Nina Miolane","Farzana Nasrin","Karthikeyan Natesan Ramamurthy","Bastian Rieck","Simone Scardapane","Michael T. Schaub","Petar Velikovi","Bei Wang","Yusu Wang","Guo-Wei Wei","Ghada Zamzmi"],"pdf_url":"https://arxiv.org/pdf/2402.08871v3.pdf","comment":"Proceedings of the 41st International Conference on Machine Learning,\n  Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2402.00809v5","updated":"2024-08-06T16:32:38Z","published":"2024-02-01T17:45:26Z","title":"Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI","summary":"  In the current landscape of deep learning research, there is a predominant\nemphasis on achieving high predictive accuracy in supervised tasks involving\nlarge image and language datasets. However, a broader perspective reveals a\nmultitude of overlooked metrics, tasks, and data types, such as uncertainty,\nactive and continual learning, and scientific data, that demand attention.\nBayesian deep learning (BDL) constitutes a promising avenue, offering\nadvantages across these diverse settings. This paper posits that BDL can\nelevate the capabilities of deep learning. It revisits the strengths of BDL,\nacknowledges existing challenges, and highlights some exciting research avenues\naimed at addressing these obstacles. Looking ahead, the discussion focuses on\npossible ways to combine large-scale foundation models with BDL to unlock their\nfull potential.\n","authors":["Theodore Papamarkou","Maria Skoularidou","Konstantina Palla","Laurence Aitchison","Julyan Arbel","David Dunson","Maurizio Filippone","Vincent Fortuin","Philipp Hennig","Jos Miguel Hernndez-Lobato","Aliaksandr Hubin","Alexander Immer","Theofanis Karaletsos","Mohammad Emtiyaz Khan","Agustinus Kristiadi","Yingzhen Li","Stephan Mandt","Christopher Nemeth","Michael A. Osborne","Tim G. J. Rudner","David Rgamer","Yee Whye Teh","Max Welling","Andrew Gordon Wilson","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.00809v5.pdf","comment":"Proceedings of the 41st International Conference on Machine Learning,\n  Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2408.03274v1","updated":"2024-08-06T16:17:51Z","published":"2024-08-06T16:17:51Z","title":"Compress and Compare: Interactively Evaluating Efficiency and Behavior\n  Across ML Model Compression Experiments","summary":"  To deploy machine learning models on-device, practitioners use compression\nalgorithms to shrink and speed up models while maintaining their high-quality\noutput. A critical aspect of compression in practice is model comparison,\nincluding tracking many compression experiments, identifying subtle changes in\nmodel behavior, and negotiating complex accuracy-efficiency trade-offs.\nHowever, existing compression tools poorly support comparison, leading to\ntedious and, sometimes, incomplete analyses spread across disjoint tools. To\nsupport real-world comparative workflows, we develop an interactive visual\nsystem called Compress and Compare. Within a single interface, Compress and\nCompare surfaces promising compression strategies by visualizing provenance\nrelationships between compressed models and reveals compression-induced\nbehavior changes by comparing models' predictions, weights, and activations. We\ndemonstrate how Compress and Compare supports common compression analysis tasks\nthrough two case studies, debugging failed compression on generative language\nmodels and identifying compression artifacts in image classification models. We\nfurther evaluate Compress and Compare in a user study with eight compression\nexperts, illustrating its potential to provide structure to compression\nworkflows, help practitioners build intuition about compression, and encourage\nthorough analysis of compression's effect on model behavior. Through these\nevaluations, we identify compression-specific challenges that future visual\nanalytics tools should consider and Compress and Compare visualizations that\nmay generalize to broader model comparison tasks.\n","authors":["Angie Boggust","Venkatesh Sivaraman","Yannick Assogba","Donghao Ren","Dominik Moritz","Fred Hohman"],"pdf_url":"https://arxiv.org/pdf/2408.03274v1.pdf","comment":"Accepted to VIS 2024"},{"id":"http://arxiv.org/abs/2312.03179v4","updated":"2024-08-06T16:11:29Z","published":"2023-12-05T23:05:36Z","title":"CaloQVAE : Simulating high-energy particle-calorimeter interactions\n  using hybrid quantum-classical generative models","summary":"  The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.\n","authors":["Sehmimul Hoque","Hao Jia","Abhishek Abhishek","Mojde Fadaie","J. Quetzalcoatl Toledo-Marn","Tiago Vale","Roger G. Melko","Maximilian Swiatlowski","Wojciech T. Fedorko"],"pdf_url":"https://arxiv.org/pdf/2312.03179v4.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.03559v3","updated":"2024-08-06T15:56:31Z","published":"2022-12-07T10:19:39Z","title":"GraphLearner: Graph Node Clustering with Fully Learnable Augmentation","summary":"  Contrastive deep graph clustering (CDGC) leverages the power of contrastive\nlearning to group nodes into different clusters. The quality of contrastive\nsamples is crucial for achieving better performance, making augmentation\ntechniques a key factor in the process. However, the augmentation samples in\nexisting methods are always predefined by human experiences, and agnostic from\nthe downstream task clustering, thus leading to high human resource costs and\npoor performance. To overcome these limitations, we propose a Graph Node\nClustering with Fully Learnable Augmentation, termed GraphLearner. It\nintroduces learnable augmentors to generate high-quality and task-specific\naugmented samples for CDGC. GraphLearner incorporates two learnable augmentors\nspecifically designed for capturing attribute and structural information.\nMoreover, we introduce two refinement matrices, including the high-confidence\npseudo-label matrix and the cross-view sample similarity matrix, to enhance the\nreliability of the learned affinity matrix. During the training procedure, we\nnotice the distinct optimization goals for training learnable augmentors and\ncontrastive learning networks. In other words, we should both guarantee the\nconsistency of the embeddings as well as the diversity of the augmented\nsamples. To address this challenge, we propose an adversarial learning\nmechanism within our method. Besides, we leverage a two-stage training strategy\nto refine the high-confidence matrices. Extensive experimental results on six\nbenchmark datasets validate the effectiveness of GraphLearner.The code and\nappendix of GraphLearner are available at\nhttps://github.com/xihongyang1999/GraphLearner on Github.\n","authors":["Xihong Yang","Erxue Min","Ke Liang","Yue Liu","Siwei Wang","Sihang Zhou","Huijun Wu","Xinwang Liu","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2212.03559v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10799v2","updated":"2024-08-06T15:33:20Z","published":"2024-05-17T14:10:24Z","title":"Training Compute Thresholds: Features and Functions in AI Regulation","summary":"  Regulators in the US and EU are using thresholds based on training\ncompute--the number of computational operations used in training--to identify\ngeneral-purpose artificial intelligence (GPAI) models that may pose risks of\nlarge-scale societal harm. We argue that training compute currently is the most\nsuitable metric to identify GPAI models that deserve regulatory oversight and\nfurther scrutiny. Training compute correlates with model capabilities and\nrisks, is quantifiable, can be measured early in the AI lifecycle, and can be\nverified by external actors, among other advantageous features. These features\nmake compute thresholds considerably more suitable than other proposed metrics\nto serve as an initial filter to trigger additional regulatory requirements and\nscrutiny. However, training compute is an imperfect proxy for risk. As such,\ncompute thresholds should not be used in isolation to determine appropriate\nmitigation measures. Instead, they should be used to detect potentially risky\nGPAI models that warrant regulatory oversight, such as through notification\nrequirements, and further scrutiny, such as via model evaluations and risk\nassessments, the results of which may inform which mitigation measures are\nappropriate. In fact, this appears largely consistent with how compute\nthresholds are used today. As GPAI technology and market structures evolve,\nregulators should update compute thresholds and complement them with other\nmetrics into regulatory review processes.\n","authors":["Lennart Heim","Leonie Koessler"],"pdf_url":"https://arxiv.org/pdf/2405.10799v2.pdf","comment":"v2: Major revision of earlier working paper"},{"id":"http://arxiv.org/abs/2405.06605v3","updated":"2024-08-06T15:20:00Z","published":"2024-05-10T17:12:48Z","title":"Calo-VQ: Vector-Quantized Two-Stage Generative Model in Calorimeter\n  Simulation","summary":"  We introduce a novel machine learning method developed for the fast\nsimulation of calorimeter detector response, adapting vector-quantized\nvariational autoencoder (VQ-VAE). Our model adopts a two-stage generation\nstrategy: initially compressing geometry-aware calorimeter data into a discrete\nlatent space, followed by the application of a sequence model to learn and\ngenerate the latent tokens. Extensive experimentation on the Calo-challenge\ndataset underscores the efficiency of our approach, showcasing a remarkable\nimprovement in the generation speed compared with conventional method by a\nfactor of 2000. Remarkably, our model achieves the generation of calorimeter\nshowers within milliseconds. Furthermore, comprehensive quantitative\nevaluations across various metrics are performed to validate physics\nperformance of generation.\n","authors":["Qibin Liu","Chase Shimmin","Xiulong Liu","Eli Shlizerman","Shu Li","Shih-Chieh Hsu"],"pdf_url":"https://arxiv.org/pdf/2405.06605v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08354v3","updated":"2024-08-06T15:14:36Z","published":"2023-04-17T15:16:10Z","title":"Tool Learning with Foundation Models","summary":"  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n","authors":["Yujia Qin","Shengding Hu","Yankai Lin","Weize Chen","Ning Ding","Ganqu Cui","Zheni Zeng","Yufei Huang","Chaojun Xiao","Chi Han","Yi Ren Fung","Yusheng Su","Huadong Wang","Cheng Qian","Runchu Tian","Kunlun Zhu","Shihao Liang","Xingyu Shen","Bokai Xu","Zhen Zhang","Yining Ye","Bowen Li","Ziwei Tang","Jing Yi","Yuzhang Zhu","Zhenning Dai","Lan Yan","Xin Cong","Yaxi Lu","Weilin Zhao","Yuxiang Huang","Junxi Yan","Xu Han","Xian Sun","Dahai Li","Jason Phang","Cheng Yang","Tongshuang Wu","Heng Ji","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2304.08354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17216v2","updated":"2024-08-06T15:08:44Z","published":"2024-07-24T12:15:59Z","title":"An Adaptive Second-order Method for a Class of Nonconvex Nonsmooth\n  Composite Optimization","summary":"  This paper explores a specific type of nonconvex sparsity-promoting\nregularization problems, namely those involving $\\ell_p$-norm regularization,\nin conjunction with a twice continuously differentiable loss function. We\npropose a novel second-order algorithm designed to effectively address this\nclass of challenging nonconvex and nonsmooth problems, showcasing several\ninnovative features: (i) The use of an alternating strategy to solve a\nreweighted $\\ell_1$ regularized subproblem and the subspace approximate Newton\nstep. (ii) The reweighted $\\ell_1$ regularized subproblem relies on a convex\napproximation to the nonconvex regularization term, enabling a closed-form\nsolution characterized by the soft-thresholding operator. This feature allows\nour method to be applied to various nonconvex regularization problems. (iii)\nOur algorithm ensures that the iterates maintain their sign values and that\nnonzero components are kept away from 0 for a sufficient number of iterations,\neventually transitioning to a perturbed Newton method. (iv) We provide\ntheoretical guarantees of global convergence, local superlinear convergence in\nthe presence of the Kurdyka-\\L ojasiewicz (KL) property, and local quadratic\nconvergence when employing the exact Newton step in our algorithm. We also\nshowcase the effectiveness of our approach through experiments on a diverse set\nof model prediction problems.\n","authors":["Hao Wang","Xiangyu Yang","Yichen Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.17216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16495v2","updated":"2024-08-06T15:03:50Z","published":"2024-04-25T10:40:49Z","title":"T-Explainer: A Model-Agnostic Explainability Framework Based on\n  Gradients","summary":"  The development of machine learning applications has increased significantly\nin recent years, motivated by the remarkable ability of learning-powered\nsystems to discover and generalize intricate patterns hidden in massive\ndatasets. Modern learning models, while powerful, often have a level of\ncomplexity that renders them opaque black boxes, resulting in a notable lack of\ntransparency that hinders our ability to decipher their reasoning. Opacity\nchallenges the interpretability and practical application of machine learning,\nespecially in critical domains where understanding the underlying reasons is\nessential for informed decision-making. Explainable Artificial Intelligence\n(XAI) rises to address that challenge, unraveling the complexity of black boxes\nby providing elucidating explanations. Among the various XAI approaches,\nfeature attribution/importance stands out for its capacity to delineate the\nsignificance of input features in the prediction process. However, most\nexisting attribution methods have limitations, such as instability, when\ndivergent explanations may result from similar or even the same instance. This\nwork introduces T-Explainer, a novel local additive attribution explainer based\non Taylor expansion. It has desirable properties, such as local accuracy and\nconsistency, making T-Explainer stable over multiple runs. We demonstrate\nT-Explainer's effectiveness in quantitative benchmark experiments against\nwell-known attribution methods. Additionally, we provide several tools to\nevaluate and visualize explanations, turning T-Explainer into a comprehensive\nXAI framework.\n","authors":["Evandro S. Ortigossa","Fbio F. Dias","Brian Barr","Claudio T. Silva","Luis Gustavo Nonato"],"pdf_url":"https://arxiv.org/pdf/2404.16495v2.pdf","comment":"16 pages -- 2 figures and 20 tables -- Under review. This work has\n  been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2407.13605v2","updated":"2024-08-06T14:55:04Z","published":"2024-07-18T15:44:23Z","title":"Physics-guided Active Sample Reweighting for Urban Flow Prediction","summary":"  Urban flow prediction is a spatio-temporal modeling task that estimates the\nthroughput of transportation services like buses, taxis, and ride-sharing,\nwhere data-driven models have become the most popular solution in the past\ndecade. Meanwhile, the implicitly learned mapping between historical\nobservations to the prediction targets tend to over-simplify the dynamics of\nreal-world urban flows, leading to suboptimal predictions. Some recent\nspatio-temporal prediction solutions bring remedies with the notion of\nphysics-guided machine learning (PGML), which describes spatio-temporal data\nwith nuanced and principled physics laws, thus enhancing both the prediction\naccuracy and interpretability. However, these spatio-temporal PGML methods are\nbuilt upon a strong assumption that the observed data fully conforms to the\ndifferential equations that define the physical system, which can quickly\nbecome ill-posed in urban flow prediction tasks. The observed urban flow data,\nespecially when sliced into time-dependent snapshots to facilitate predictions,\nis typically incomplete and sparse, and prone to inherent noise incurred in the\ncollection process. As a result, such physical inconsistency between the data\nand PGML model significantly limits the predictive power and robustness of the\nsolution. Moreover, due to the interval-based predictions and intermittent\nnature of data filing in many transportation services, the instantaneous\ndynamics of urban flows can hardly be captured, rendering differential\nequation-based continuous modeling a loose fit for this setting. To overcome\nthe challenges, we develop a discretized physics-guided network (PN), and\npropose a data-aware framework Physics-guided Active Sample Reweighting\n(P-GASR) to enhance PN. Experimental results in four real-world datasets\ndemonstrate that our method achieves state-of-the-art performance with a\ndemonstrable improvement in robustness.\n","authors":["Wei Jiang","Tong Chen","Guanhua Ye","Wentao Zhang","Lizhen Cui","Zi Huang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2407.13605v2.pdf","comment":"This paper is accepted by Proceedings of the 33nd ACM International\n  Conference on Information and Knowledge Management (CIKM '24)"},{"id":"http://arxiv.org/abs/2408.03236v1","updated":"2024-08-06T14:48:34Z","published":"2024-08-06T14:48:34Z","title":"Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding\n  with Extended Degrees of Freedom","summary":"  This paper investigates the problem of direction-of-arrival (DOA) estimation\nusing multiple partially-calibrated sparse subarrays. In particular, we present\nthe Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA\nestimation algorithm to scenarios with partially-calibrated sparse subarrays.\nThe proposed GCA-MUSIC algorithm exploits the difference coarray for each\nsubarray, followed by a specific pseudo-spectrum merging rule that is based on\nthe intersection of the signal subspaces associated to each subarray. This rule\nassumes that there is no a priori knowledge about the cross-covariance between\nsubarrays. In that way, only the second-order statistics of each subarray are\nused to estimate the directions with increased degrees of freedom, i.e., the\nestimation procedure preserves the coarray Multiple Signal Classification and\nsparse arrays properties to estimate more sources than the number of physical\nsensors in each subarray. Numerical simulations show that the proposed\nGCA-MUSIC has better performance than other similar strategies.\n","authors":["W. S. Leite","R. C. de Lamare"],"pdf_url":"https://arxiv.org/pdf/2408.03236v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.03223v1","updated":"2024-08-06T14:36:29Z","published":"2024-08-06T14:36:29Z","title":"Don't Think It Twice: Exploit Shift Invariance for Efficient Online\n  Streaming Inference of CNNs","summary":"  Deep learning time-series processing often relies on convolutional neural\nnetworks with overlapping windows. This overlap allows the network to produce\nan output faster than the window length. However, it introduces additional\ncomputations. This work explores the potential to optimize computational\nefficiency during inference by exploiting convolution's shift-invariance\nproperties to skip the calculation of layer activations between successive\noverlapping windows. Although convolutions are shift-invariant, zero-padding\nand pooling operations, widely used in such networks, are not efficient and\ncomplicate efficient streaming inference. We introduce StreamiNNC, a strategy\nto deploy Convolutional Neural Networks for online streaming inference. We\nexplore the adverse effects of zero padding and pooling on the accuracy of\nstreaming inference, deriving theoretical error upper bounds for pooling during\nstreaming. We address these limitations by proposing signal padding and pooling\nalignment and provide guidelines for designing and deploying models for\nStreamiNNC. We validate our method in simulated data and on three real-world\nbiomedical signal processing applications. StreamiNNC achieves a low deviation\nbetween streaming output and normal inference for all three networks (2.03 -\n3.55% NRMSE). This work demonstrates that it is possible to linearly speed up\nthe inference of streaming CNNs processing overlapping windows, negating the\nadditional computation typically incurred by overlapping windows.\n","authors":["Christodoulos Kechris","Jonathan Dan","Jose Miranda","David Atienza"],"pdf_url":"https://arxiv.org/pdf/2408.03223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09146v4","updated":"2024-08-06T14:30:52Z","published":"2024-02-14T12:55:28Z","title":"ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural\n  Networks","summary":"  In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.\n","authors":["Muhammad Kashif","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2402.09146v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05530v2","updated":"2024-08-06T14:30:31Z","published":"2024-04-08T13:59:02Z","title":"Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a popular method for\naligning Language Models (LM) with human values and preferences. RLHF requires\na large number of preference pairs as training data, which are often used in\nboth the Supervised Fine-Tuning and Reward Model training and therefore\npublicly available datasets are commonly used. In this work, we study to what\nextent a malicious actor can manipulate the LMs generations by poisoning the\npreferences, i.e., injecting poisonous preference pairs into these datasets and\nthe RLHF training process. We propose strategies to build poisonous preference\npairs and test their performance by poisoning two widely used preference\ndatasets. Our results show that preference poisoning is highly effective:\ninjecting a small amount of poisonous data (1-5\\% of the original dataset), we\ncan effectively manipulate the LM to generate a target entity in a target\nsentiment (positive or negative). The findings from our experiments also shed\nlight on strategies to defend against the preference poisoning attack.\n","authors":["Tim Baumgrtner","Yang Gao","Dana Alon","Donald Metzler"],"pdf_url":"https://arxiv.org/pdf/2404.05530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03220v1","updated":"2024-08-06T14:26:09Z","published":"2024-08-06T14:26:09Z","title":"Masked Random Noise for Communication Efficient Federaetd Learning","summary":"  Federated learning is a promising distributed training paradigm that\neffectively safeguards data privacy. However, it may involve significant\ncommunication costs, which hinders training efficiency. In this paper, we aim\nto enhance communication efficiency from a new perspective. Specifically, we\nrequest the distributed clients to find optimal model updates relative to\nglobal model parameters within predefined random noise. For this purpose, we\npropose Federated Masked Random Noise (FedMRN), a novel framework that enables\nclients to learn a 1-bit mask for each model parameter and apply masked random\nnoise (i.e., the Hadamard product of random noise and masks) to represent model\nupdates. To make FedMRN feasible, we propose an advanced mask training\nstrategy, called progressive stochastic masking (PSM). After local training,\neach client only need to transmit local masks and a random seed to the server.\nAdditionally, we provide theoretical guarantees for the convergence of FedMRN\nunder both strongly convex and non-convex assumptions. Extensive experiments\nare conducted on four popular datasets. The results show that FedMRN exhibits\nsuperior convergence speed and test accuracy compared to relevant baselines,\nwhile attaining a similar level of accuracy as FedAvg.\n","authors":["Shiwei Li","Yingyi Cheng","Haozhao Wang","Xing Tang","Shijie Xu","Weihong Luo","Yuhua Li","Dugang Liu","Xiuqiang He","and Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03220v1.pdf","comment":"Accepted by MM 2024"},{"id":"http://arxiv.org/abs/2408.03219v1","updated":"2024-08-06T14:25:23Z","published":"2024-08-06T14:25:23Z","title":"Learning to Learn without Forgetting using Attention","summary":"  Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.\n","authors":["Anna Vettoruzzo","Joaquin Vanschoren","Mohamed-Rafik Bouguelia","Thorsteinn Rgnvaldsson"],"pdf_url":"https://arxiv.org/pdf/2408.03219v1.pdf","comment":"Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),\n  2024"},{"id":"http://arxiv.org/abs/2408.03215v1","updated":"2024-08-06T14:19:06Z","published":"2024-08-06T14:19:06Z","title":"FedBAT: Communication-Efficient Federated Learning via Learnable\n  Binarization","summary":"  Federated learning is a promising distributed machine learning paradigm that\ncan effectively exploit large-scale data without exposing users' privacy.\nHowever, it may incur significant communication overhead, thereby potentially\nimpairing the training efficiency. To address this challenge, numerous studies\nsuggest binarizing the model updates. Nonetheless, traditional methods usually\nbinarize model updates in a post-training manner, resulting in significant\napproximation errors and consequent degradation in model accuracy. To this end,\nwe propose Federated Binarization-Aware Training (FedBAT), a novel framework\nthat directly learns binary model updates during the local training process,\nthus inherently reducing the approximation errors. FedBAT incorporates an\ninnovative binarization operator, along with meticulously designed derivatives\nto facilitate efficient learning. In addition, we establish theoretical\nguarantees regarding the convergence of FedBAT. Extensive experiments are\nconducted on four popular datasets. The results show that FedBAT significantly\naccelerates the convergence and exceeds the accuracy of baselines by up to 9\\%,\neven surpassing that of FedAvg in some cases.\n","authors":["Shiwei Li","Wenchao Xu","Haozhao Wang","Xing Tang","Yining Qi","Shijie Xu","Weihong Luo","Yuhua Li","Xiuqiang He","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03215v1.pdf","comment":"Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2405.11647v3","updated":"2024-08-06T14:12:26Z","published":"2024-05-19T18:57:25Z","title":"Hummer: Towards Limited Competitive Preference Dataset","summary":"  Preference datasets are essential for incorporating human preferences into\npre-trained language models, playing a key role in the success of Reinforcement\nLearning from Human Feedback. However, these datasets often demonstrate\nconflicting alignment objectives, leading to increased vulnerability to\njailbreak attacks and challenges in adapting downstream tasks to prioritize\nspecific alignment objectives without negatively impacting others. In this\nwork, we introduce a novel statistical metric, Alignment Dimension Conflict, to\nquantify the degree of conflict within preference datasets. We then present\n\\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative\npairwise preference datasets with reduced-conflict alignment objectives.\n\\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback\nfrom GPT-4, marking as the first preference dataset aimed at reducing the\ncompetition between alignment objectives. Furthermore, we develop reward\nmodels, HummerRM and HummerRM-F, which employ a hybrid sampling approach to\nbalance diverse alignment objectives effectively. This sampling method\npositions HummerRM as an ideal model for domain-specific further fine-tuning\nand reducing vulnerabilities to attacks.\n","authors":["Li Jiang","Yusen Wu","Junwu Xiong","Jingqing Ruan","Yichuan Ding","Qingpei Guo","Zujie Wen","Jun Zhou","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2405.11647v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03199v1","updated":"2024-08-06T13:58:37Z","published":"2024-08-06T13:58:37Z","title":"Convergence Conditions for Stochastic Line Search Based Optimization of\n  Over-parametrized Models","summary":"  In this paper, we deal with algorithms to solve the finite-sum problems\nrelated to fitting over-parametrized models, that typically satisfy the\ninterpolation condition. In particular, we focus on approaches based on\nstochastic line searches and employing general search directions. We define\nconditions on the sequence of search directions that guarantee finite\ntermination and bounds for the backtracking procedure. Moreover, we shed light\non the additional property of directions needed to prove fast (linear)\nconvergence of the general class of algorithms when applied to PL functions in\nthe interpolation regime. From the point of view of algorithms design, the\nproposed analysis identifies safeguarding conditions that could be employed in\nrelevant algorithmic framework. In particular, it could be of interest to\nintegrate stochastic line searches within momentum, conjugate gradient or\nadaptive preconditioning methods.\n","authors":["Matteo Lapucci","Davide Pucci"],"pdf_url":"https://arxiv.org/pdf/2408.03199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03195v1","updated":"2024-08-06T13:55:51Z","published":"2024-08-06T13:55:51Z","title":"RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning","summary":"  The advent of the \"pre-train, prompt\" paradigm has recently extended its\ngeneralization ability and data efficiency to graph representation learning,\nfollowing its achievements in Natural Language Processing (NLP). Initial graph\nprompt tuning approaches tailored specialized prompting functions for Graph\nNeural Network (GNN) models pre-trained with specific strategies, such as edge\nprediction, thus limiting their applicability. In contrast, another pioneering\nline of research has explored universal prompting via adding prompts to the\ninput graph's feature space, thereby removing the reliance on specific\npre-training strategies. However, the necessity to add feature prompts to all\nnodes remains an open question. Motivated by findings from prompt tuning\nresearch in the NLP domain, which suggest that highly capable pre-trained\nmodels need less conditioning signal to achieve desired behaviors, we advocate\nfor strategically incorporating necessary and lightweight feature prompts to\ncertain graph nodes to enhance downstream task performance. This introduces a\ncombinatorial optimization problem, requiring a policy to decide 1) which nodes\nto prompt and 2) what specific feature prompts to attach. We then address the\nproblem by framing the prompt incorporation process as a sequential\ndecision-making problem and propose our method, RELIEF, which employs\nReinforcement Learning (RL) to optimize it. At each step, the RL agent selects\na node (discrete action) and determines the prompt content (continuous action),\naiming to maximize cumulative performance gain. Extensive experiments on graph\nand node-level tasks with various pre-training strategies in few-shot scenarios\ndemonstrate that our RELIEF outperforms fine-tuning and other prompt-based\napproaches in classification performance and data efficiency.\n","authors":["Jiapeng Zhu","Zichen Ding","Jianxiang Yu","Jiaqi Tan","Xiang Li","Weining Qian"],"pdf_url":"https://arxiv.org/pdf/2408.03195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07186v5","updated":"2024-08-06T13:47:50Z","published":"2023-12-12T11:48:56Z","title":"Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized\n  Least-Squares Algorithm","summary":"  We present the first optimal rates for infinite-dimensional vector-valued\nridge regression on a continuous scale of norms that interpolate between $L_2$\nand the hypothesis space, which we consider as a vector-valued reproducing\nkernel Hilbert space. These rates allow to treat the misspecified case in which\nthe true regression function is not contained in the hypothesis space. We\ncombine standard assumptions on the capacity of the hypothesis space with a\nnovel tensor product construction of vector-valued interpolation spaces in\norder to characterize the smoothness of the regression function. Our upper\nbound not only attains the same rate as real-valued kernel ridge regression,\nbut also removes the assumption that the target regression function is bounded.\nFor the lower bound, we reduce the problem to the scalar setting using a\nprojection argument. We show that these rates are optimal in most cases and\nindependent of the dimension of the output space. We illustrate our results for\nthe special case of vector-valued Sobolev spaces.\n","authors":["Zhu Li","Dimitri Meunier","Mattes Mollenhauer","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2312.07186v5.pdf","comment":"Published JMLR version. arXiv admin note: text overlap with\n  arXiv:2208.01711"},{"id":"http://arxiv.org/abs/2311.15890v3","updated":"2024-08-06T13:47:24Z","published":"2023-11-27T14:56:47Z","title":"Stability-Informed Initialization of Neural Ordinary Differential\n  Equations","summary":"  This paper addresses the training of Neural Ordinary Differential Equations\n(neural ODEs), and in particular explores the interplay between numerical\nintegration techniques, stability regions, step size, and initialization\ntechniques. It is shown how the choice of integration technique implicitly\nregularizes the learned model, and how the solver's corresponding stability\nregion affects training and prediction performance. From this analysis, a\nstability-informed parameter initialization technique is introduced. The\neffectiveness of the initialization method is displayed across several learning\nbenchmarks and industrial applications.\n","authors":["Theodor Westny","Arman Mohammadi","Daniel Jung","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2311.15890v3.pdf","comment":"In Proceedings of the 41 st International Conference on Machine\n  Learning"},{"id":"http://arxiv.org/abs/2408.03178v1","updated":"2024-08-06T13:22:51Z","published":"2024-08-06T13:22:51Z","title":"An Object is Worth 64x64 Pixels: Generating 3D Object via Image\n  Diffusion","summary":"  We introduce a new approach for generating realistic 3D models with UV maps\nthrough a representation termed \"Object Images.\" This approach encapsulates\nsurface geometry, appearance, and patch structures within a 64x64 pixel image,\neffectively converting complex 3D shapes into a more manageable 2D format. By\ndoing so, we address the challenges of both geometric and semantic irregularity\ninherent in polygonal meshes. This method allows us to use image generation\nmodels, such as Diffusion Transformers, directly for 3D shape generation.\nEvaluated on the ABO dataset, our generated shapes with patch structures\nachieve point cloud FID comparable to recent 3D generative models, while\nnaturally supporting PBR material generation.\n","authors":["Xingguang Yan","Han-Hung Lee","Ziyu Wan","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2408.03178v1.pdf","comment":"Project Page: https://omages.github.io/"},{"id":"http://arxiv.org/abs/2408.03172v1","updated":"2024-08-06T13:16:16Z","published":"2024-08-06T13:16:16Z","title":"Leveraging Parameter Efficient Training Methods for Low Resource Text\n  Classification: A Case Study in Marathi","summary":"  With the surge in digital content in low-resource languages, there is an\nescalating demand for advanced Natural Language Processing (NLP) techniques\ntailored to these languages. BERT (Bidirectional Encoder Representations from\nTransformers), serving as the foundational framework for numerous NLP\narchitectures and language models, is increasingly employed for the development\nof low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method\nfor fine-tuning Large Language Models (LLMs) and reducing the training\nparameters to some extent to decrease the computational costs needed for\ntraining the model and achieve results comparable to a fully fine-tuned model.\nIn this work, we present a study of PEFT methods for the Indic low-resource\nlanguage Marathi. We conduct a comprehensive analysis of PEFT methods applied\nto various monolingual and multilingual Marathi BERT models. These approaches\nare evaluated on prominent text classification datasets like MahaSent,\nMahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to\nsignificantly expedite the training speed of the models, addressing a critical\naspect of model development and deployment. In this study, we explore Low-Rank\nAdaptation of Large Language Models (LoRA) and adapter methods for low-resource\ntext classification. We show that these methods are competitive with full\nfine-tuning and can be used without loss in accuracy. This study contributes\nvaluable insights into the effectiveness of Marathi BERT models, offering a\nfoundation for the continued advancement of NLP capabilities in Marathi and\nsimilar Indic languages.\n","authors":["Pranita Deshmukh","Nikita Kulkarni","Sanhita Kulkarni","Kareena Manghani","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2408.03172v1.pdf","comment":"Accepted at I2CT 2024"},{"id":"http://arxiv.org/abs/2408.03156v1","updated":"2024-08-06T12:55:17Z","published":"2024-08-06T12:55:17Z","title":"Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models","summary":"  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n","authors":["Sho Ozaki","Shizuo Kaji","Toshikazu Imae","Kanabu Nawa","Hideomi Yamashita","Keiichi Nakagawa"],"pdf_url":"https://arxiv.org/pdf/2408.03156v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.01661v3","updated":"2024-08-06T12:54:26Z","published":"2024-05-02T18:31:47Z","title":"When a Relation Tells More Than a Concept: Exploring and Evaluating\n  Classifier Decisions with CoReX","summary":"  Explanations for Convolutional Neural Networks (CNNs) based on relevance of\ninput pixels might be too unspecific to evaluate which and how input features\nimpact model decisions. Especially in complex real-world domains like biology,\nthe presence of specific concepts and of relations between concepts might be\ndiscriminating between classes. Pixel relevance is not expressive enough to\nconvey this type of information. In consequence, model evaluation is limited\nand relevant aspects present in the data and influencing the model decisions\nmight be overlooked. This work presents a novel method to explain and evaluate\nCNN models, which uses a concept- and relation-based explainer (CoReX). It\nexplains the predictive behavior of a model on a set of images by masking\n(ir-)relevant concepts from the decision-making process and by constraining\nrelations in a learned interpretable surrogate model. We test our approach with\nseveral image data sets and CNN architectures. Results show that CoReX\nexplanations are faithful to the CNN model in terms of predictive outcomes. We\nfurther demonstrate through a human evaluation that CoReX is a suitable tool\nfor generating combined explanations that help assessing the classification\nquality of CNNs. We further show that CoReX supports the identification and\nre-classification of incorrect or ambiguous classifications.\n","authors":["Bettina Finzel","Patrick Hilme","Johannes Rabold","Ute Schmid"],"pdf_url":"https://arxiv.org/pdf/2405.01661v3.pdf","comment":"preliminary version, submitted to Machine Learning"},{"id":"http://arxiv.org/abs/2408.03152v1","updated":"2024-08-06T12:52:03Z","published":"2024-08-06T12:52:03Z","title":"TSC: A Simple Two-Sided Constraint against Over-Smoothing","summary":"  Graph Convolutional Neural Network (GCN), a widely adopted method for\nanalyzing relational data, enhances node discriminability through the\naggregation of neighboring information. Usually, stacking multiple layers can\nimprove the performance of GCN by leveraging information from high-order\nneighbors. However, the increase of the network depth will induce the\nover-smoothing problem, which can be attributed to the quality and quantity of\nneighbors changing: (a) neighbor quality, node's neighbors become overlapping\nin high order, leading to aggregated information becoming indistinguishable,\n(b) neighbor quantity, the exponentially growing aggregated neighbors submerges\nthe node's initial feature by recursively aggregating operations. Current\nsolutions mainly focus on one of the above causes and seldom consider both at\nonce.\n  Aiming at tackling both causes of over-smoothing in one shot, we introduce a\nsimple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet\npotent techniques: random masking and contrastive constraint. The random\nmasking acts on the representation matrix's columns to regulate the degree of\ninformation aggregation from neighbors, thus preventing the convergence of node\nrepresentations. Meanwhile, the contrastive constraint, applied to the\nrepresentation matrix's rows, enhances the discriminability of the nodes.\nDesigned as a plug-in module, TSC can be easily coupled with GCN or SGC\narchitectures. Experimental analyses on diverse real-world graph datasets\nverify that our approach markedly reduces the convergence of node's\nrepresentation and the performance degradation in deeper GCN.\n","authors":["Furong Peng","Kang Liu","Xuan Lu","Yuhua Qian","Hongren Yan","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03152v1.pdf","comment":"accept by KDD2024"},{"id":"http://arxiv.org/abs/2408.03150v1","updated":"2024-08-06T12:49:33Z","published":"2024-08-06T12:49:33Z","title":"Conditioning LLMs with Emotion in Neural Machine Translation","summary":"  Large Language Models (LLMs) have shown remarkable performance in Natural\nLanguage Processing tasks, including Machine Translation (MT). In this work, we\npropose a novel MT pipeline that integrates emotion information extracted from\na Speech Emotion Recognition (SER) model into LLMs to enhance translation\nquality. We first fine-tune five existing LLMs on the Libri-trans dataset and\nselect the most performant model. Subsequently, we augment LLM prompts with\ndifferent dimensional emotions and train the selected LLM under these different\nconfigurations. Our experiments reveal that integrating emotion information,\nespecially arousal, into LLM prompts leads to notable improvements in\ntranslation quality.\n","authors":["Charles Brazier","Jean-Luc Rouas"],"pdf_url":"https://arxiv.org/pdf/2408.03150v1.pdf","comment":"6 pages, In Proceedings of the 21st International Conference on\n  Spoken Language Translation (IWSLT), Bangkok, Thailand, 2024"},{"id":"http://arxiv.org/abs/2408.03144v1","updated":"2024-08-06T12:39:12Z","published":"2024-08-06T12:39:12Z","title":"Active Learning for Level Set Estimation Using Randomized Straddle\n  Algorithms","summary":"  Level set estimation (LSE), the problem of identifying the set of input\npoints where a function takes value above (or below) a given threshold, is\nimportant in practical applications. When the function is expensive-to-evaluate\nand black-box, the \\textit{straddle} algorithm, which is a representative\nheuristic for LSE based on Gaussian process models, and its extensions having\ntheoretical guarantees have been developed. However, many of existing methods\ninclude a confidence parameter $\\beta^{1/2}_t$ that must be specified by the\nuser, and methods that choose $\\beta^{1/2}_t$ heuristically do not provide\ntheoretical guarantees. In contrast, theoretically guaranteed values of\n$\\beta^{1/2}_t$ need to be increased depending on the number of iterations and\ncandidate points, and are conservative and not good for practical performance.\nIn this study, we propose a novel method, the \\textit{randomized straddle}\nalgorithm, in which $\\beta_t$ in the straddle algorithm is replaced by a random\nsample from the chi-squared distribution with two degrees of freedom. The\nconfidence parameter in the proposed method has the advantages of not needing\nadjustment, not depending on the number of iterations and candidate points, and\nnot being conservative. Furthermore, we show that the proposed method has\ntheoretical guarantees that depend on the sample complexity and the number of\niterations. Finally, we confirm the usefulness of the proposed method through\nnumerical experiments using synthetic and real data.\n","authors":["Yu Inatsu","Shion Takeno","Kentaro Kutsukake","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2408.03144v1.pdf","comment":"21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.00573v2","updated":"2024-08-06T12:36:57Z","published":"2024-08-01T14:06:34Z","title":"Convergence Analysis of Natural Gradient Descent for Over-parameterized\n  Physics-Informed Neural Networks","summary":"  First-order methods, such as gradient descent (GD) and stochastic gradient\ndescent (SGD), have been proven effective in training neural networks. In the\ncontext of over-parameterization, there is a line of work demonstrating that\nrandomly initialized (stochastic) gradient descent converges to a globally\noptimal solution at a linear convergence rate for the quadratic loss function.\nHowever, the learning rate of GD for training two-layer neural networks\nexhibits poor dependence on the sample size and the Gram matrix, leading to a\nslow training process. In this paper, we show that for the $L^2$ regression\nproblems, the learning rate can be improved from $\\mathcal{O}(\\lambda_0/n^2)$\nto $\\mathcal{O}(1/\\|\\bm{H}^{\\infty}\\|_2)$, which implies that GD actually\nenjoys a faster convergence rate. Furthermore, we generalize the method to GD\nin training two-layer Physics-Informed Neural Networks (PINNs), showing a\nsimilar improvement for the learning rate. Although the improved learning rate\nhas a mild dependence on the Gram matrix, we still need to set it small enough\nin practice due to the unknown eigenvalues of the Gram matrix. More\nimportantly, the convergence rate is tied to the least eigenvalue of the Gram\nmatrix, which can lead to slow convergence. In this work, we provide the\nconvergence analysis of natural gradient descent (NGD) in training two-layer\nPINNs, demonstrating that the learning rate can be $\\mathcal{O}(1)$, and at\nthis rate, the convergence rate is independent of the Gram matrix.\n","authors":["Xianliang Xu","Ting Du","Wang Kong","Ye Li","Zhongyi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01294v2","updated":"2024-08-06T12:24:07Z","published":"2024-08-02T14:31:37Z","title":"Feature Clock: High-Dimensional Effects in Two-Dimensional Plots","summary":"  Humans struggle to perceive and interpret high-dimensional data. Therefore,\nhigh-dimensional data are often projected into two dimensions for\nvisualization. Many applications benefit from complex nonlinear dimensionality\nreduction techniques, but the effects of individual high-dimensional features\nare hard to explain in the two-dimensional space. Most visualization solutions\nuse multiple two-dimensional plots, each showing the effect of one\nhigh-dimensional feature in two dimensions; this approach creates a need for a\nvisual inspection of k plots for a k-dimensional input space. Our solution,\nFeature Clock, provides a novel approach that eliminates the need to inspect\nthese k plots to grasp the influence of original features on the data structure\ndepicted in two dimensions. Feature Clock enhances the explainability and\ncompactness of visualizations of embedded data and is available in an\nopen-source Python library.\n","authors":["Olga Ovcharenko","Rita Sevastjanova","Valentina Boeva"],"pdf_url":"https://arxiv.org/pdf/2408.01294v2.pdf","comment":"To be published in IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2407.09064v2","updated":"2024-08-06T11:43:53Z","published":"2024-07-12T07:34:10Z","title":"Multi-Modal Dataset Creation for Federated Learning with DICOM\n  Structured Reports","summary":"  Purpose: Federated training is often hindered by heterogeneous datasets due\nto divergent data storage options, inconsistent naming schemes, varied\nannotation procedures, and disparities in label quality. This is particularly\nevident in the emerging multi-modal learning paradigms, where dataset\nharmonization including a uniform data representation and filtering options are\nof paramount importance.\n  Methods: DICOM structured reports enable the standardized linkage of\narbitrary information beyond the imaging domain and can be used within Python\ndeep learning pipelines with highdicom. Building on this, we developed an open\nplatform for data integration and interactive filtering capabilities that\nsimplifies the process of assembling multi-modal datasets.\n  Results: In this study, we extend our prior work by showing its applicability\nto more and divergent data types, as well as streamlining datasets for\nfederated training within an established consortium of eight university\nhospitals in Germany. We prove its concurrent filtering ability by creating\nharmonized multi-modal datasets across all locations for predicting the outcome\nafter minimally invasive heart valve replacement. The data includes DICOM data\n(i.e. computed tomography images, electrocardiography scans) as well as\nannotations (i.e. calcification segmentations, pointsets and pacemaker\ndependency), and metadata (i.e. prosthesis and diagnoses).\n  Conclusion: Structured reports bridge the traditional gap between imaging\nsystems and information systems. Utilizing the inherent DICOM reference system\narbitrary data types can be queried concurrently to create meaningful cohorts\nfor clinical studies. The graphical interface as well as example structured\nreport templates will be made publicly available.\n","authors":["Malte Tlle","Lukas Burger","Halvar Kelm","Florian Andr","Peter Bannas","Gerhard Diller","Norbert Frey","Philipp Garthe","Stefan Gro","Anja Hennemuth","Lars Kaderali","Nina Krger","Andreas Leha","Simon Martin","Alexander Meyer","Eike Nagel","Stefan Orwat","Clemens Scherer","Moritz Seiffert","Jan Moritz Seliger","Stefan Simm","Tim Friede","Tim Seidler","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2407.09064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11044v2","updated":"2024-08-06T11:29:06Z","published":"2024-01-19T22:11:54Z","title":"Preservation of Feature Stability in Machine Learning Under Data\n  Uncertainty for Decision Support in Critical Domains","summary":"  In a world where Machine Learning (ML) is increasingly deployed to support\ndecision-making in critical domains, providing decision-makers with\nexplainable, stable, and relevant inputs becomes fundamental. Understanding how\nmachine learning works under missing data and how this affects feature\nvariability is paramount. This is even more relevant as machine learning\napproaches focus on standardising decision-making approaches that rely on an\nidealised set of features. However, decision-making in human activities often\nrelies on incomplete data, even in critical domains. This paper addresses this\ngap by conducting a set of experiments using traditional machine learning\nmethods that look for optimal decisions in comparison to a recently deployed\nmachine learning method focused on a classification that is more descriptive\nand mimics human decision making, allowing for the natural integration of\nexplainability. We found that the ML descriptive approach maintains higher\nclassification accuracy while ensuring the stability of feature selection as\ndata incompleteness increases. This suggests that descriptive classification\nmethods can be helpful in uncertain decision-making scenarios.\n","authors":["Karol Capaa","Paulina Tworek","Jose Sousa"],"pdf_url":"https://arxiv.org/pdf/2401.11044v2.pdf","comment":"30 pages, 6 figures, supplementary materials"},{"id":"http://arxiv.org/abs/2407.03234v3","updated":"2024-08-06T11:15:00Z","published":"2024-07-03T16:03:42Z","title":"Self-Evaluation as a Defense Against Adversarial Attacks on LLMs","summary":"  We introduce a defense against adversarial attacks on LLMs utilizing\nself-evaluation. Our method requires no model fine-tuning, instead using\npre-trained models to evaluate the inputs and outputs of a generator model,\nsignificantly reducing the cost of implementation in comparison to other,\nfinetuning-based methods. Our method can significantly reduce the attack\nsuccess rate of attacks on both open and closed-source LLMs, beyond the\nreductions demonstrated by Llama-Guard2 and commonly used content moderation\nAPIs. We present an analysis of the effectiveness of our method, including\nattempts to attack the evaluator in various settings, demonstrating that it is\nalso more resilient to attacks than existing methods. Code and data will be\nmade available at https://github.com/Linlt-leon/self-eval.\n","authors":["Hannah Brown","Leon Lin","Kenji Kawaguchi","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2407.03234v3.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.03099v1","updated":"2024-08-06T11:04:07Z","published":"2024-08-06T11:04:07Z","title":"Topic Modeling with Fine-tuning LLMs and Bag of Sentences","summary":"  Large language models (LLM)'s are increasingly used for topic modeling\noutperforming classical topic models such as LDA. Commonly, pre-trained LLM\nencoders such as BERT are used out-of-the-box despite the fact that fine-tuning\nis known to improve LLMs considerably. The challenge lies in obtaining a\nsuitable (labeled) dataset for fine-tuning. In this paper, we use the recent\nidea to use bag of sentences as the elementary unit in computing topics. In\nturn, we derive an approach FT-Topic to perform unsupervised fine-tuning\nrelying primarily on two steps for constructing a training dataset in an\nautomatic fashion. First, a heuristic method to identifies pairs of sentence\ngroups that are either assumed to be of the same or different topics. Second,\nwe remove sentence pairs that are likely labeled incorrectly. The dataset is\nthen used to fine-tune an encoder LLM, which can be leveraged by any topic\nmodeling approach using embeddings. However, in this work, we demonstrate its\neffectiveness by deriving a novel state-of-the-art topic modeling method called\nSenClu, which achieves fast inference through an expectation-maximization\nalgorithm and hard assignments of sentence groups to a single topic, while\ngiving users the possibility to encode prior knowledge on the topic-document\ndistribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.03099v1.pdf","comment":"This is the submitted journal version of enhanced with the novel\n  fine-tuning part of \"Efficient and Flexible Topic Modeling using Pretrained\n  Embeddings and Bag of Sentences'' which appeared at the International\n  Conference on Agents and Artificial Intelligence(ICAART) in 2024"},{"id":"http://arxiv.org/abs/2211.16237v4","updated":"2024-08-06T10:51:40Z","published":"2022-11-29T14:21:34Z","title":"Closing the gap between SVRG and TD-SVRG with Gradient Splitting","summary":"  Temporal difference (TD) learning is a policy evaluation in reinforcement\nlearning whose performance can be enhanced by variance reduction methods.\nRecently, multiple works have sought to fuse TD learning with Stochastic\nVariance Reduced Gradient (SVRG) method to achieve a geometric rate of\nconvergence. However, the resulting convergence rate is significantly weaker\nthan what is achieved by SVRG in the setting of convex optimization. In this\nwork we utilize a recent interpretation of TD-learning as the splitting of the\ngradient of an appropriately chosen function, thus simplifying the algorithm\nand fusing TD with SVRG. Our main result is a geometric convergence bound with\npredetermined learning rate of $1/8$, which is identical to the convergence\nbound available for SVRG in the convex setting. Our theoretical findings are\nsupported by a set of experiments.\n","authors":["Arsenii Mustafin","Alex Olshevsky","Ioannis Ch. Paschalidis"],"pdf_url":"https://arxiv.org/pdf/2211.16237v4.pdf","comment":"42 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.03093v1","updated":"2024-08-06T10:48:15Z","published":"2024-08-06T10:48:15Z","title":"Learning Provably Robust Policies in Uncertain Parametric Environments","summary":"  We present a data-driven approach for learning MDP policies that are robust\nacross stochastic environments whose transition probabilities are defined by\nparameters with an unknown distribution. We produce probably approximately\ncorrect (PAC) guarantees for the performance of these learned policies in a\nnew, unseen environment over the unknown distribution. Our approach is based on\nfinite samples of the MDP environments, for each of which we build an\napproximation of the model as an interval MDP, by exploring a set of generated\ntrajectories. We use the built approximations to synthesise a single policy\nthat performs well (meets given requirements) across the sampled environments,\nand furthermore bound its risk (of not meeting the given requirements) when\ndeployed in an unseen environment. Our procedure offers a trade-off between the\nguaranteed performance of the learned policy and the risk of not meeting the\nguarantee in an unseen environment. Our approach exploits knowledge of the\nenvironment's state space and graph structure, and we show how additional\nknowledge of its parametric structure can be leveraged to optimize learning and\nto obtain tighter guarantees from less samples. We evaluate our approach on a\ndiverse range of established benchmarks, demonstrating that we can generate\nhighly performing and robust policies, along with guarantees that tightly\nquantify their performance and the associated risk.\n","authors":["Yannik Schnitzer","Alessandro Abate","David Parker"],"pdf_url":"https://arxiv.org/pdf/2408.03093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20800v2","updated":"2024-08-06T10:45:42Z","published":"2024-05-31T14:01:12Z","title":"Shape Constraints in Symbolic Regression using Penalized Least Squares","summary":"  We study the addition of shape constraints (SC) and their consideration\nduring the parameter identification step of symbolic regression (SR). SC serve\nas a means to introduce prior knowledge about the shape of the otherwise\nunknown model function into SR. Unlike previous works that have explored SC in\nSR, we propose minimizing SC violations during parameter identification using\ngradient-based numerical optimization. We test three algorithm variants to\nevaluate their performance in identifying three symbolic expressions from\nsynthetically generated data sets. This paper examines two benchmark scenarios:\none with varying noise levels and another with reduced amounts of training\ndata. The results indicate that incorporating SC into the expression search is\nparticularly beneficial when data is scarce. Compared to using SC only in the\nselection process, our approach of minimizing violations during parameter\nidentification shows a statistically significant benefit in some of our test\ncases, without being significantly worse in any instance.\n","authors":["Viktor Martinek","Julia Reuter","Ophelia Frotscher","Sanaz Mostaghim","Markus Richter","Roland Herzog"],"pdf_url":"https://arxiv.org/pdf/2405.20800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03088v1","updated":"2024-08-06T10:41:46Z","published":"2024-08-06T10:41:46Z","title":"QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction","summary":"  Financial market prediction and optimal trading strategy development remain\nchallenging due to market complexity and volatility. Our research in quantum\nfinance and reinforcement learning for decision-making demonstrates the\napproach of quantum-classical hybrid algorithms to tackling real-world\nfinancial challenges. In this respect, we corroborate the concept with rigorous\nbacktesting and validate the framework's performance under realistic market\nconditions, by including fixed transaction cost per trade. This paper\nintroduces a Quantum Attention Deep Q-Network (QADQN) approach to address these\nchallenges through quantum-enhanced reinforcement learning. Our QADQN\narchitecture uses a variational quantum circuit inside a traditional deep\nQ-learning framework to take advantage of possible quantum advantages in\ndecision-making. We gauge the QADQN agent's performance on historical data from\nmajor market indices, including the S&P 500. We evaluate the agent's learning\nprocess by examining its reward accumulation and the effectiveness of its\nexperience replay mechanism. Our empirical results demonstrate the QADQN's\nsuperior performance, achieving better risk-adjusted returns with Sortino\nratios of 1.28 and 1.19 for non-overlapping and overlapping test periods\nrespectively, indicating effective downside risk management.\n","authors":["Siddhant Dutta","Nouhaila Innan","Alberto Marchisio","Sadok Ben Yahia","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2408.03088v1.pdf","comment":"Accepted at the 2024 IEEE International Conference on Quantum\n  Computing and Engineering (QCE24), QCRL, September 2024"},{"id":"http://arxiv.org/abs/2308.09310v3","updated":"2024-08-06T10:38:22Z","published":"2023-08-18T05:11:50Z","title":"Variance reduction techniques for stochastic proximal point algorithms","summary":"  In the context of finite sums minimization, variance reduction techniques are\nwidely used to improve the performance of state-of-the-art stochastic gradient\nmethods. Their practical impact is clear, as well as their theoretical\nproperties. Stochastic proximal point algorithms have been studied as an\nalternative to stochastic gradient algorithms since they are more stable with\nrespect to the choice of the step size. However, their variance-reduced\nversions are not as well studied as the gradient ones. In this work, we propose\nthe first unified study of variance reduction techniques for stochastic\nproximal point algorithms. We introduce a generic stochastic proximal-based\nalgorithm that can be specified to give the proximal version of SVRG, SAGA, and\nsome of their variants. For this algorithm, in the smooth setting, we provide\nseveral convergence rates for the iterates and the objective function values,\nwhich are faster than those of the vanilla stochastic proximal point algorithm.\nMore specifically, for convex functions, we prove a sublinear convergence rate\nof $O(1/k)$. In addition, under the Polyak-{\\L}ojasiewicz (PL) condition, we\nobtain linear convergence rates. Finally, our numerical experiments demonstrate\nthe advantages of the proximal variance reduction methods over their gradient\ncounterparts in terms of the stability with respect to the choice of the step\nsize in most cases, especially for difficult problems.\n","authors":["Cheik Traor","Vassilis Apidopoulos","Saverio Salzo","Silvia Villa"],"pdf_url":"https://arxiv.org/pdf/2308.09310v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03085v1","updated":"2024-08-06T10:25:02Z","published":"2024-08-06T10:25:02Z","title":"Matrix Multiplication on Quantum Computer","summary":"  This paper introduces an innovative and practical approach to universal\nquantum matrix multiplication. We designed optimized quantum adders and\nmultipliers based on Quantum Fourier Transform (QFT), which significantly\nreduced the number of gates used compared to classical adders and multipliers.\nSubsequently, we construct a basic universal quantum matrix multiplication and\nextend it to the Strassen algorithm. We conduct comparative experiments to\nanalyze the performance of the quantum matrix multiplication and evaluate the\nacceleration provided by the optimized quantum adder and multiplier.\nFurthermore, we investigate the advantages and disadvantages of the quantum\nStrassen algorithm compared to basic quantum matrix multiplication.\n","authors":["Jiaqi Yao","Ding Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03084v1","updated":"2024-08-06T10:24:54Z","published":"2024-08-06T10:24:54Z","title":"Research on Autonomous Driving Decision-making Strategies based Deep\n  Reinforcement Learning","summary":"  The behavior decision-making subsystem is a key component of the autonomous\ndriving system, which reflects the decision-making ability of the vehicle and\nthe driver, and is an important symbol of the high-level intelligence of the\nvehicle. However, the existing rule-based decision-making schemes are limited\nby the prior knowledge of designers, and it is difficult to cope with complex\nand changeable traffic scenarios. In this work, an advanced deep reinforcement\nlearning model is adopted, which can autonomously learn and optimize driving\nstrategies in a complex and changeable traffic environment by modeling the\ndriving decision-making process as a reinforcement learning problem.\nSpecifically, we used Deep Q-Network (DQN) and Proximal Policy Optimization\n(PPO) for comparative experiments. DQN guides the agent to choose the best\naction by approximating the state-action value function, while PPO improves the\ndecision-making quality by optimizing the policy function. We also introduce\nimprovements in the design of the reward function to promote the robustness and\nadaptability of the model in real-world driving situations. Experimental\nresults show that the decision-making strategy based on deep reinforcement\nlearning has better performance than the traditional rule-based method in a\nvariety of driving tasks.\n","authors":["Zixiang Wang","Hao Yan","Changsong Wei","Junyu Wang","Shi Bo","Minheng Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.03084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15325v2","updated":"2024-08-06T10:09:47Z","published":"2023-07-28T06:03:19Z","title":"Equivariance and partial observations in Koopman operator theory for\n  partial differential equations","summary":"  The Koopman operator has become an essential tool for data-driven analysis,\nprediction and control of complex systems. The main reason is the enormous\npotential of identifying linear function space representations of nonlinear\ndynamics from measurements. This equally applies to ordinary, stochastic, and\npartial differential equations (PDEs). Until now, with a few exceptions only,\nthe PDE case is mostly treated rather superficially, and the specific structure\nof the underlying dynamics is largely ignored. In this paper, we show that\nsymmetries in the system dynamics can be carried over to the Koopman operator,\nwhich allows us to massively increase the model efficacy. Moreover, the\nsituation where we only have access to partial observations (i.e.,\nmeasurements, as is very common for experimental data) has not been treated to\nits full extent, either. Moreover, we address the highly-relevant case where we\ncannot measure the full state, such that alternative approaches such as delay\ncoordinates have to be considered. We derive rigorous statements on the\nrequired number of observables in this situation, based on embedding theory. We\npresent numerical evidence using various numerical examples including the wave\nequation and the Kuramoto-Sivashinsky equation.\n","authors":["Sebastian Peitz","Hans Harder","Feliks Nske","Friedrich Philipp","Manuel Schaller","Karl Worthmann"],"pdf_url":"https://arxiv.org/pdf/2307.15325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12465v3","updated":"2024-08-06T09:57:36Z","published":"2024-05-21T02:41:40Z","title":"A finite element-based physics-informed operator learning framework for\n  spatiotemporal partial differential equations on arbitrary domains","summary":"  We propose a novel finite element-based physics-informed operator learning\nframework that allows for predicting spatiotemporal dynamics governed by\npartial differential equations (PDEs). The proposed framework employs a loss\nfunction inspired by the finite element method (FEM) with the implicit Euler\ntime integration scheme. A transient thermal conduction problem is considered\nto benchmark the performance. The proposed operator learning framework takes a\ntemperature field at the current time step as input and predicts a temperature\nfield at the next time step. The Galerkin discretized weak formulation of the\nheat equation is employed to incorporate physics into the loss function, which\nis coined finite operator learning (FOL). Upon training, the networks\nsuccessfully predict the temperature evolution over time for any initial\ntemperature field at high accuracy compared to the FEM solution. The framework\nis also confirmed to be applicable to a heterogeneous thermal conductivity and\narbitrary geometry. The advantages of FOL can be summarized as follows: First,\nthe training is performed in an unsupervised manner, avoiding the need for a\nlarge data set prepared from costly simulations or experiments. Instead, random\ntemperature patterns generated by the Gaussian random process and the Fourier\nseries, combined with constant temperature fields, are used as training data to\ncover possible temperature cases. Second, shape functions and backward\ndifference approximation are exploited for the domain discretization, resulting\nin a purely algebraic equation. This enhances training efficiency, as one\navoids time-consuming automatic differentiation when optimizing weights and\nbiases while accepting possible discretization errors. Finally, thanks to the\ninterpolation power of FEM, any arbitrary geometry can be handled with FOL,\nwhich is crucial to addressing various engineering application scenarios.\n","authors":["Yusuke Yamazaki","Ali Harandi","Mayu Muramatsu","Alexandre Viardin","Markus Apel","Tim Brepols","Stefanie Reese","Shahed Rezaei"],"pdf_url":"https://arxiv.org/pdf/2405.12465v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10107v4","updated":"2024-08-06T08:39:33Z","published":"2024-01-18T16:18:18Z","title":"Comparison analysis between standard polysomnographic data and\n  in-ear-EEG signals: A preliminary study","summary":"  Study Objectives: Polysomnography (PSG) currently serves as the benchmark for\nevaluating sleep disorders. Its discomfort makes long-term monitoring\nunfeasible, leading to bias in sleep quality assessment. Hence, less invasive,\ncost-effective, and portable alternatives need to be explored. One promising\ncontender is the in-ear-EEG sensor. This study aims to establish a methodology\nto assess the similarity between the single-channel in-ear-EEG and standard PSG\nderivations.\n  Methods: The study involves four-hour signals recorded from ten healthy\nsubjects aged 18 to 60 years. Recordings are analyzed following two\ncomplementary approaches: (i) a hypnogram-based analysis aimed at assessing the\nagreement between PSG and in-ear-EEG-derived hypnograms; and (ii) a\nfeature-based analysis based on time- and frequency- domain feature extraction,\nunsupervised feature selection, and definition of Feature-based Similarity\nIndex via Jensen-Shannon Divergence (JSD-FSI).\n  Results: We find large variability between PSG and in-ear-EEG hypnograms\nscored by the same sleep expert according to Cohen's kappa metric, with\nsignificantly greater agreements for PSG scorers than for in-ear-EEG scorers (p\n< 0.001) based on Fleiss' kappa metric. On average, we demonstrate a high\nsimilarity between PSG and in-ear-EEG signals in terms of JSD-FSI (0.79 +/-\n0.06 -awake, 0.77 +/- 0.07 -NREM, and 0.67 +/- 0.10 -REM) and in line with the\nsimilarity values computed independently on standard PSG-channel-combinations.\n  Conclusions: In-ear-EEG is a valuable solution for home-based sleep\nmonitoring, however further studies with a larger and more heterogeneous\ndataset are needed.\n","authors":["Gianpaolo Palo","Luigi Fiorillo","Giuliana Monachino","Michal Bechny","Michel Walti","Elias Meier","Francesca Pentimalli Biscaretti di Ruffia","Mark Melnykowycz","Athina Tzovara","Valentina Agostini","Francesca Dalia Faraci"],"pdf_url":"https://arxiv.org/pdf/2401.10107v4.pdf","comment":"20 figures, 6 tables"},{"id":"http://arxiv.org/abs/2404.14244v2","updated":"2024-08-06T08:11:34Z","published":"2024-04-22T14:57:17Z","title":"AI-Generated Faces in the Real World: A Large-Scale Case Study of\n  Twitter Profile Images","summary":"  Recent advances in the field of generative artificial intelligence (AI) have\nblurred the lines between authentic and machine-generated content, making it\nalmost impossible for humans to distinguish between such media. One notable\nconsequence is the use of AI-generated images for fake profiles on social\nmedia. While several types of disinformation campaigns and similar incidents\nhave been reported in the past, a systematic analysis has been lacking. In this\nwork, we conduct the first large-scale investigation of the prevalence of\nAI-generated profile pictures on Twitter. We tackle the challenges of a\nreal-world measurement study by carefully integrating various data sources and\ndesigning a multi-stage detection pipeline. Our analysis of nearly 15 million\nTwitter profile pictures shows that 0.052% were artificially generated,\nconfirming their notable presence on the platform. We comprehensively examine\nthe characteristics of these accounts and their tweet content, and uncover\npatterns of coordinated inauthentic behavior. The results also reveal several\nmotives, including spamming and political amplification campaigns. Our research\nreaffirms the need for effective detection and mitigation strategies to cope\nwith the potential negative effects of generative AI in the future.\n","authors":["Jonas Ricker","Dennis Assenmacher","Thorsten Holz","Asja Fischer","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2404.14244v2.pdf","comment":"Accepted to RAID 2024"},{"id":"http://arxiv.org/abs/2408.03013v1","updated":"2024-08-06T07:48:51Z","published":"2024-08-06T07:48:51Z","title":"NeurDB: On the Design and Implementation of an AI-powered Autonomous\n  Database","summary":"  Databases are increasingly embracing AI to provide autonomous system\noptimization and intelligent in-database analytics, aiming to relieve end-user\nburdens across various industry sectors. Nonetheless, most existing approaches\nfail to account for the dynamic nature of databases, which renders them\nineffective for real-world applications characterized by evolving data and\nworkloads. This paper introduces NeurDB, an AI-powered autonomous database that\ndeepens the fusion of AI and databases with adaptability to data and workload\ndrift. NeurDB establishes a new in-database AI ecosystem that seamlessly\nintegrates AI workflows within the database. This integration enables efficient\nand effective in-database AI analytics and fast-adaptive learned system\ncomponents. Empirical evaluations demonstrate that NeurDB substantially\noutperforms existing solutions in managing AI analytics tasks, with the\nproposed learned components more effectively handling environmental dynamism\nthan state-of-the-art approaches.\n","authors":["Zhanhao Zhao","Shaofeng Cai","Haotian Gao","Hexiang Pan","Siqi Xiang","Naili Xing","Gang Chen","Beng Chin Ooi","Yanyan Shen","Yuncheng Wu","Meihui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16907v2","updated":"2024-08-06T07:24:35Z","published":"2024-02-25T04:24:28Z","title":"Diffusion Posterior Proximal Sampling for Image Restoration","summary":"  Diffusion models have demonstrated remarkable efficacy in generating\nhigh-quality samples. Existing diffusion-based image restoration algorithms\nexploit pre-trained diffusion models to leverage data priors, yet they still\npreserve elements inherited from the unconditional generation paradigm. These\nstrategies initiate the denoising process with pure white noise and incorporate\nrandom noise at each generative step, leading to over-smoothed results. In this\npaper, we present a refined paradigm for diffusion-based image restoration.\nSpecifically, we opt for a sample consistent with the measurement identity at\neach generative step, exploiting the sampling selection as an avenue for output\nstability and enhancement. The number of candidate samples used for selection\nis adaptively determined based on the signal-to-noise ratio of the timestep.\nAdditionally, we start the restoration process with an initialization combined\nwith the measurement signal, providing supplementary information to better\nalign the generative process. Extensive experimental results and analyses\nvalidate that our proposed method significantly enhances image restoration\nperformance while consuming negligible additional computational resources.\n","authors":["Hongjie Wu","Linchao He","Mingqin Zhang","Dongdong Chen","Kunming Luo","Mengting Luo","Ji-Zhe Zhou","Hu Chen","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2402.16907v2.pdf","comment":"ACM Multimedia 2024 Oral"},{"id":"http://arxiv.org/abs/2408.02998v1","updated":"2024-08-06T07:05:56Z","published":"2024-08-06T07:05:56Z","title":"Federated Learning Architectures: A Performance Evaluation with Crop\n  Yield Prediction Application","summary":"  Federated learning has become an emerging technology for data analysis for\nIoT applications. This paper implements centralized and decentralized federated\nlearning frameworks for crop yield prediction based on Long Short-Term Memory\nNetwork. For centralized federated learning, multiple clients and one server is\nconsidered, where the clients exchange their model updates with the server that\nworks as the aggregator to build the global model. For the decentralized\nframework, a collaborative network is formed among the devices either using\nring topology or using mesh topology. In this network, each device receives\nmodel updates from the neighbour devices, and performs aggregation to build the\nupgraded model. The performance of the centralized and decentralized federated\nlearning frameworks are evaluated in terms of prediction accuracy, precision,\nrecall, F1-Score, and training time. The experimental results present that\n$\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized\nand decentralized federated learning-based frameworks respectively. The results\nalso show that the using centralized federated learning the response time can\nbe reduced by $\\sim$75% than the cloud-only framework. Finally, the future\nresearch directions of the use of federated learning in crop yield prediction\nare explored in this paper.\n","authors":["Anwesha Mukherjee","Rajkumar Buyya"],"pdf_url":"https://arxiv.org/pdf/2408.02998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02987v1","updated":"2024-08-06T06:42:53Z","published":"2024-08-06T06:42:53Z","title":"A Differential Smoothness-based Compact-Dynamic Graph Convolutional\n  Network for Spatiotemporal Signal Recovery","summary":"  High quality spatiotemporal signal is vitally important for real application\nscenarios like energy management, traffic planning and cyber security. Due to\nthe uncontrollable factors like abrupt sensors breakdown or communication\nfault, the spatiotemporal signal collected by sensors is always incomplete. A\ndynamic graph convolutional network (DGCN) is effective for processing\nspatiotemporal signal recovery. However, it adopts a static GCN and a sequence\nneural network to explore the spatial and temporal patterns, separately. Such a\nseparated two-step processing is loose spatiotemporal, thereby failing to\ncapture the complex inner spatiotemporal correlation. To address this issue,\nthis paper proposes a Compact-Dynamic Graph Convolutional Network (CDGCN) for\nspatiotemporal signal recovery with the following two-fold ideas: a) leveraging\nthe tensor M-product to build a unified tensor graph convolution framework,\nwhich considers both spatial and temporal patterns simultaneously; and b)\nconstructing a differential smoothness-based objective function to reduce the\nnoise interference in spatiotemporal signal, thereby further improve the\nrecovery accuracy. Experiments on real-world spatiotemporal datasets\ndemonstrate that the proposed CDGCN significantly outperforms the\nstate-of-the-art models in terms of recovery accuracy.\n","authors":["Pengcheng Gao","Zicheng Gao","Ye Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.02987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01829v2","updated":"2024-08-06T06:30:08Z","published":"2024-08-03T17:43:10Z","title":"Neural Network Emulator for Atmospheric Chemical ODE","summary":"  Modeling atmospheric chemistry is complex and computationally intense. Given\nthe recent success of Deep neural networks in digital signal processing, we\npropose a Neural Network Emulator for fast chemical concentration modeling. We\nconsider atmospheric chemistry as a time-dependent Ordinary Differential\nEquation. To extract the hidden correlations between initial states and future\ntime evolution, we propose ChemNNE, an Attention based Neural Network Emulator\n(NNE) that can model the atmospheric chemistry as a neural ODE process. To\nefficiently simulate the chemical changes, we propose the sinusoidal time\nembedding to estimate the oscillating tendency over time. More importantly, we\nuse the Fourier neural operator to model the ODE process for efficient\ncomputation. We also propose three physical-informed losses to supervise the\ntraining optimization. To evaluate our model, we propose a large-scale chemical\ndataset that can be used for neural network training and evaluation. The\nextensive experiments show that our approach achieves state-of-the-art\nperformance in modeling accuracy and computational speed.\n","authors":["Zhi-Song Liu","Petri Clusius","Michael Boy"],"pdf_url":"https://arxiv.org/pdf/2408.01829v2.pdf","comment":"25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.17963v4","updated":"2024-08-06T06:28:57Z","published":"2023-03-31T11:06:09Z","title":"Learning-Based Optimal Control with Performance Guarantees for Unknown\n  Systems with Latent States","summary":"  As control engineering methods are applied to increasingly complex systems,\ndata-driven approaches for system identification appear as a promising\nalternative to physics-based modeling. While the Bayesian approaches prevalent\nfor safety-critical applications usually rely on the availability of state\nmeasurements, the states of a complex system are often not directly measurable.\nIt may then be necessary to jointly estimate the dynamics and the latent state,\nmaking the quantification of uncertainties and the design of controllers with\nformal performance guarantees considerably more challenging. This paper\nproposes a novel method for the computation of an optimal input trajectory for\nunknown nonlinear systems with latent states based on a combination of particle\nMarkov chain Monte Carlo methods and scenario theory. Probabilistic performance\nguarantees are derived for the resulting input trajectory, and an approach to\nvalidate the performance of arbitrary control laws is presented. The\neffectiveness of the proposed method is demonstrated in a numerical simulation.\n","authors":["Robert Lefringhausen","Supitsana Srithasan","Armin Lederer","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2303.17963v4.pdf","comment":"Accepted version submitted to the 2024 European Control Conference\n  (ECC)"},{"id":"http://arxiv.org/abs/2109.03445v6","updated":"2024-08-06T06:19:46Z","published":"2021-09-08T06:06:28Z","title":"Convergence of Batch Asynchronous Stochastic Approximation With\n  Applications to Reinforcement Learning","summary":"  We begin by briefly surveying some results on the convergence of the\nStochastic Gradient Descent (SGD) Method, proved in a companion paper by the\npresent authors. These results are based on viewing SGD as a version of\nStochastic Approximation (SA). Ever since its introduction in the classic paper\nof Robbins and Monro in 1951, SA has become a standard tool for finding a\nsolution of an equation of the form $f(\\theta) = 0$, when only noisy\nmeasurements of $f(\\cdot)$ are available. In most situations, \\textit{every\ncomponent} of the putative solution $\\theta_t$ is updated at each step $t$. In\nsome applications in Reinforcement Learning (RL), \\textit{only one component}\nof $\\theta_t$ is updated at each $t$. This is known as \\textbf{asynchronous}\nSA. In this paper, we study \\textbf{Block Asynchronous SA (BASA)}, in which, at\neach step $t$, \\textit{some but not necessarily all} components of $\\theta_t$\nare updated. The theory presented here embraces both conventional (synchronous)\nSA as well as asynchronous SA, and all in-between possibilities. We provide\nsufficient conditions for the convergence of BASA, and also prove bounds on the\n\\textit{rate} of convergence of $\\theta_t$ to the solution. For the case of\nconventional SGD, these results reduce to those proved in our companion paper.\nThen we apply these results to the problem of finding a fixed point of a map\nwith only noisy measurements. This problem arises frequently in RL. We prove\nsufficient conditions for convergence as well as estimates for the rate of\nconvergence.\n","authors":["Rajeeva L. Karandikar","M. Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2109.03445v6.pdf","comment":"34 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.02971v1","updated":"2024-08-06T06:00:17Z","published":"2024-08-06T06:00:17Z","title":"Wave Interpolation Neural Operator: Interpolated Prediction of Electric\n  Fields Across Untrained Wavelengths","summary":"  Designing photonic structures requires electromagnetic simulations, which\noften require high computational costs. Researchers have developed surrogate\nsolvers for predicting electric fields to alleviate the computational issues.\nHowever, existing surrogate solvers are limited to performing inference at\nfixed simulation conditions and require retraining for different conditions. To\naddress this, we propose Wave Interpolation Neural Operator (WINO), a novel\nsurrogate solver enabling simulation condition interpolation across a\ncontinuous spectrum of broadband wavelengths. WINO introduces the Fourier Group\nConvolution Shuffling operator and a new conditioning method to efficiently\npredict electric fields from both trained and untrained wavelength data,\nachieving significant improvements in parameter efficiency and spectral\ninterpolation performance. Our model demonstrates approximately 100 times\nfaster performance than traditional finite-difference frequency-domain\nsimulations. Moreover, compared to the state-of-the-art model, we achieve a 74%\nreduction in parameters and 80.5% improvements in prediction accuracy for\nuntrained wavelengths, and 13.2% improvements for trained wavelengths.\n","authors":["Joonhyuk Seo","Chanik Kang","Dongjin Seo","Haejun Chung"],"pdf_url":"https://arxiv.org/pdf/2408.02971v1.pdf","comment":"9 pages, 5 figures, 4 tables / Appendix: 4 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.02965v1","updated":"2024-08-06T05:21:31Z","published":"2024-08-06T05:21:31Z","title":"Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model\n  and Neural Operator","summary":"  Closure models are widely used in simulating complex multiscale dynamical\nsystems such as turbulence and the earth system, for which direct numerical\nsimulation that resolves all scales is often too expensive. For those systems\nwithout a clear scale separation, deterministic and local closure models often\nlack enough generalization capability, which limits their performance in many\nreal-world applications. In this work, we propose a data-driven modeling\nframework for constructing stochastic and non-local closure models via\nconditional diffusion model and neural operator. Specifically, the Fourier\nneural operator is incorporated into a score-based diffusion model, which\nserves as a data-driven stochastic closure model for complex dynamical systems\ngoverned by partial differential equations (PDEs). We also demonstrate how\naccelerated sampling methods can improve the efficiency of the data-driven\nstochastic closure model. The results show that the proposed methodology\nprovides a systematic approach via generative machine learning techniques to\nconstruct data-driven stochastic closure models for multiscale dynamical\nsystems with continuous spatiotemporal fields.\n","authors":["Xinghao Dong","Chuanqi Chen","Jin-Long Wu"],"pdf_url":"https://arxiv.org/pdf/2408.02965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02961v1","updated":"2024-08-06T05:15:57Z","published":"2024-08-06T05:15:57Z","title":"Synaptic Modulation using Interspike Intervals Increases Energy\n  Efficiency of Spiking Neural Networks","summary":"  Despite basic differences between Spiking Neural Networks (SNN) and\nArtificial Neural Networks (ANN), most research on SNNs involve adapting\nANN-based methods for SNNs. Pruning (dropping connections) and quantization\n(reducing precision) are often used to improve energy efficiency of SNNs. These\nmethods are very effective for ANNs whose energy needs are determined by\nsignals transmitted on synapses. However, the event-driven paradigm in SNNs\nimplies that energy is consumed by spikes. In this paper, we propose a new\nsynapse model whose weights are modulated by Interspike Intervals (ISI) i.e.\ntime difference between two spikes. SNNs composed of this synapse model, termed\nISI Modulated SNNs (IMSNN), can use gradient descent to estimate how the ISI of\na neuron changes after updating its synaptic parameters. A higher ISI implies\nfewer spikes and vice-versa. The learning algorithm for IMSNNs exploits this\ninformation to selectively propagate gradients such that learning is achieved\nby increasing the ISIs resulting in a network that generates fewer spikes. The\nperformance of IMSNNs with dense and convolutional layers have been evaluated\nin terms of classification accuracy and the number of spikes using the MNIST\nand FashionMNIST datasets. The performance comparison with conventional SNNs\nshows that IMSNNs exhibit upto 90% reduction in the number of spikes while\nmaintaining similar classification accuracy.\n","authors":["Dylan Adams","Magda Zajaczkowska","Ashiq Anjum","Andrea Soltoggio","Shirin Dora"],"pdf_url":"https://arxiv.org/pdf/2408.02961v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.02950v1","updated":"2024-08-06T04:28:16Z","published":"2024-08-06T04:28:16Z","title":"Kolmogorov-Arnold PointNet: Deep learning for prediction of fluid fields\n  on irregular geometries","summary":"  We present Kolmogorov-Arnold PointNet (KA-PointNet) as a novel supervised\ndeep learning framework for the prediction of incompressible steady-state fluid\nflow fields in irregular domains, where the predicted fields are a function of\nthe geometry of the domains. In KA-PointNet, we implement shared\nKolmogorov-Arnold Networks (KANs) in the segmentation branch of the PointNet\narchitecture. We utilize Jacobi polynomials to construct shared KANs. As a\nbenchmark test case, we consider incompressible laminar steady-state flow over\na cylinder, where the geometry of its cross-section varies over the data set.\nWe investigate the performance of Jacobi polynomials with different degrees as\nwell as special cases of Jacobi polynomials such as Legendre polynomials,\nChebyshev polynomials of the first and second kinds, and Gegenbauer\npolynomials, in terms of the computational cost of training and accuracy of\nprediction of the test set. Additionally, we compare the performance of\nPointNet with shared KANs (i.e., KA-PointNet) and PointNet with shared\nMultilayer Perceptrons (MLPs). It is observed that when the number of trainable\nparameters is approximately equal, PointNet with shared KANs (i.e.,\nKA-PointNet) outperforms PointNet with shared MLPs.\n","authors":["Ali Kashefi"],"pdf_url":"https://arxiv.org/pdf/2408.02950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02338v3","updated":"2024-08-06T04:15:27Z","published":"2024-02-04T04:21:34Z","title":"NetLLM: Adapting Large Language Models for Networking","summary":"  Many networking tasks now employ deep learning (DL) to solve complex\nprediction and optimization problems. However, current design philosophy of\nDL-based algorithms entails intensive engineering overhead due to the manual\ndesign of deep neural networks (DNNs) for different networking tasks. Besides,\nDNNs tend to achieve poor generalization performance on unseen data\ndistributions/environments.\n  Motivated by the recent success of large language models (LLMs), this work\nstudies the LLM adaptation for networking to explore a more sustainable design\nphilosophy. With the powerful pre-trained knowledge, the LLM is promising to\nserve as the foundation model to achieve \"one model for all tasks\" with even\nbetter performance and stronger generalization. In pursuit of this vision, we\npresent NetLLM, the first framework that provides a coherent design to harness\nthe powerful capabilities of LLMs with low efforts to solve networking\nproblems. Specifically, NetLLM empowers the LLM to effectively process\nmultimodal data in networking and efficiently generate task-specific answers.\nBesides, NetLLM drastically reduces the costs of fine-tuning the LLM to acquire\ndomain knowledge for networking. Across three networking-related use cases -\nviewport prediction, adaptive bitrate streaming and cluster job scheduling, we\nshowcase that the NetLLM-adapted LLM significantly outperforms state-of-the-art\nalgorithms.\n","authors":["Duo Wu","Xianda Wang","Yaqi Qiao","Zhi Wang","Junchen Jiang","Shuguang Cui","Fangxin Wang"],"pdf_url":"https://arxiv.org/pdf/2402.02338v3.pdf","comment":"This paper has been accepted by ACM SIGCOMM 2024. DOI:\n  https://doi.org/10.1145/3651890.3672268"},{"id":"http://arxiv.org/abs/2408.02946v1","updated":"2024-08-06T04:14:29Z","published":"2024-08-06T04:14:29Z","title":"Scaling Laws for Data Poisoning in LLMs","summary":"  Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to\ndetect, breaks guardrails, and leads to undesirable and harmful behavior. Given\nthe intense efforts by leading labs to train and deploy increasingly larger and\nmore capable LLMs, it is critical to ask if the risk of data poisoning will be\nnaturally mitigated by scale, or if it is an increasing threat. We consider\nthree threat models by which data poisoning can occur: malicious fine-tuning,\nimperfect data curation, and intentional data contamination. Our experiments\nevaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72\nbillion parameters on three datasets which speak to each of our threat models.\nWe find that larger LLMs are increasingly vulnerable, learning harmful behavior\n-- including sleeper agent behavior -- significantly more quickly than smaller\nLLMs with even minimal data poisoning. These results underscore the need for\nrobust safeguards against data poisoning in larger LLMs.\n","authors":["Dillon Bowen","Brendan Murphy","Will Cai","David Khachaturov","Adam Gleave","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2408.02946v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2312.05356v4","updated":"2024-08-06T03:57:33Z","published":"2023-12-08T20:28:08Z","title":"Neuron Patching: Semantic-based Neuron-level Language Model Repair for\n  Code Generation","summary":"  Large Language Models (LLMs) have already gained widespread adoption in\nsoftware engineering, particularly in code generation tasks. However, updating\nthese models with new knowledge can be prohibitively expensive, yet it is\nessential to maximize their utility, such as implementing a hotfix technique to\naddress urgent or critical LLM errors. In this paper, we propose \\textsc{MENT},\na novel and effective model editing approach to repair LLMs in coding tasks.\n\\textsc{MENT} is effective, efficient, and reliable, capable of correcting a\nneural model by patching just one or two neurons. As pioneering work on\nneuron-level model editing of generative models, we formalize the editing\nprocess and introduce the involved concepts. We also introduce new measures to\nevaluate its generalization ability and establish a benchmark for further\nstudy. Our approach is evaluated on three coding tasks: line-level code\ngeneration, shellcode generation, and intent-to-bash translation. The\nexperimental results demonstrate that the proposed approach significantly\noutperforms the state-of-the-art in both effectiveness and efficiency measures.\nFurthermore, we showcase the applications of \\textsc{MENT} for LLM reasoning in\nsoftware engineering. By editing LLM knowledge, the directly or indirectly\ndependent behaviors of API invocation in the chain-of-thought change\naccordingly. This illustrates the significance of repairing LLMs in the context\nof software engineering.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.05356v4.pdf","comment":"12 pages, 7 figures, 7 tables, under peer-review"},{"id":"http://arxiv.org/abs/2408.02936v1","updated":"2024-08-06T03:42:38Z","published":"2024-08-06T03:42:38Z","title":"Achieving More with Less: A Tensor-Optimization-Powered Ensemble Method","summary":"  Ensemble learning is a method that leverages weak learners to produce a\nstrong learner. However, obtaining a large number of base learners requires\nsubstantial time and computational resources. Therefore, it is meaningful to\nstudy how to achieve the performance typically obtained with many base learners\nusing only a few. We argue that to achieve this, it is essential to enhance\nboth classification performance and generalization ability during the ensemble\nprocess. To increase model accuracy, each weak base learner needs to be more\nefficiently integrated. It is observed that different base learners exhibit\nvarying levels of accuracy in predicting different classes. To capitalize on\nthis, we introduce confidence tensors $\\tilde{\\mathbf{\\Theta}}$ and\n$\\tilde{\\mathbf{\\Theta}}_{rst}$ signifies that the $t$-th base classifier\nassigns the sample to class $r$ while it actually belongs to class $s$. To the\nbest of our knowledge, this is the first time an evaluation of the performance\nof base classifiers across different classes has been proposed. The proposed\nconfidence tensor compensates for the strengths and weaknesses of each base\nclassifier in different classes, enabling the method to achieve superior\nresults with a smaller number of base learners. To enhance generalization\nperformance, we design a smooth and convex objective function that leverages\nthe concept of margin, making the strong learner more discriminative.\nFurthermore, it is proved that in gradient matrix of the loss function, the sum\nof each column's elements is zero, allowing us to solve a constrained\noptimization problem using gradient-based methods. We then compare our\nalgorithm with random forests of ten times the size and other classical methods\nacross numerous datasets, demonstrating the superiority of our approach.\n","authors":["Jinghui Yuan","Weijin Jiang","Zhe Cao","Fangyuan Xie","Rong Wang","Feiping Nie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02932v1","updated":"2024-08-06T03:34:43Z","published":"2024-08-06T03:34:43Z","title":"Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping","summary":"  Clustering is a fundamental task in machine learning and data science, and\nsimilarity graph-based clustering is an important approach within this domain.\nDoubly stochastic symmetric similarity graphs provide numerous benefits for\nclustering problems and downstream tasks, yet learning such graphs remains a\nsignificant challenge. Marcus theorem states that a strictly positive symmetric\nmatrix can be transformed into a doubly stochastic symmetric matrix by diagonal\nmatrices. However, in clustering, learning sparse matrices is crucial for\ncomputational efficiency. We extend Marcus theorem by proposing the Marcus\nmapping, which indicates that certain sparse matrices can also be transformed\ninto doubly stochastic symmetric matrices via diagonal matrices. Additionally,\nwe introduce rank constraints into the clustering problem and propose the\nDoubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus\nMapping (ANCMM). This ensures that the learned graph naturally divides into the\ndesired number of clusters. We validate the effectiveness of our algorithm\nthrough extensive comparisons with state-of-the-art algorithms. Finally, we\nexplore the relationship between the Marcus mapping and optimal transport. We\nprove that the Marcus mapping solves a specific type of optimal transport\nproblem and demonstrate that solving this problem through Marcus mapping is\nmore efficient than directly applying optimal transport methods.\n","authors":["Jinghui Yuan","Chusheng Zeng","Fangyuan Xie","Zhe Cao","Rong Wang","Feiping Nie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.10838v7","updated":"2024-08-06T03:34:27Z","published":"2022-01-26T09:44:13Z","title":"Privacy-Preserving Logistic Regression Training with A Faster Gradient\n  Variant","summary":"  Logistic regression training over encrypted data has been an attractive idea\nto security concerns for years. In this paper, we propose a faster gradient\nvariant called $\\texttt{quadratic gradient}$ for privacy-preserving logistic\nregression training. The core of $\\texttt{quadratic gradient}$ can be seen as\nan extension of the simplified fixed Hessian. We enhance Nesterov's accelerated\ngradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with\n$\\texttt{quadratic gradient}$ and evaluate the enhanced algorithms on several\ndatasets. Experiments show that the enhanced methods have a state-of-the-art\nperformance in convergence speed compared to the raw first-order gradient\nmethods. We then adopt the enhanced NAG method to implement homomorphic\nlogistic regression training, obtaining a comparable result by only $3$\niterations. There is a promising chance that $\\texttt{quadratic gradient}$\ncould be used to enhance other first-order gradient methods for general\nnumerical optimization problems.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2201.10838v7.pdf","comment":"The basic work of this paper, $\\texttt{quadratic gradient}$ and the\n  enhanced full batch NAG, was nearly finished in September 2019. The initial\n  version of this paper was written in April 2020, rejected by ICANN 2020. The\n  enhanced mini-batch NAG was introduced into this paper in September 2020 and\n  later rejected by a special issue on the journal FGCS 2020"},{"id":"http://arxiv.org/abs/2402.12022v2","updated":"2024-08-06T03:34:06Z","published":"2024-02-19T10:31:53Z","title":"Distilling Large Language Models for Text-Attributed Graph Learning","summary":"  Text-Attributed Graphs (TAGs) are graphs of connected textual documents.\nGraph models can efficiently learn TAGs, but their training heavily relies on\nhuman-annotated labels, which are scarce or even unavailable in many\napplications. Large language models (LLMs) have recently demonstrated\nremarkable capabilities in few-shot and zero-shot TAG learning, but they suffer\nfrom scalability, cost, and privacy issues. Therefore, in this work, we focus\non synergizing LLMs and graph models with their complementary strengths by\ndistilling the power of LLMs to a local graph model on TAG learning. To address\nthe inherent gaps between LLMs (generative models for texts) and graph models\n(discriminative models for graphs), we propose first to let LLMs teach an\ninterpreter with rich textual rationale and then let a student model mimic the\ninterpreter's reasoning without LLMs' textual rationale. Extensive experiments\nvalidate the efficacy of our proposed framework.\n","authors":["Bo Pan","Zheng Zhang","Yifei Zhang","Yuntong Hu","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.12022v2.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2404.06675v2","updated":"2024-08-06T03:33:41Z","published":"2024-04-10T01:35:17Z","title":"Toward Cross-Layer Energy Optimizations in AI Systems","summary":"  The \"AI for Science, Energy, and Security\" report from DOE outlines a\nsignificant focus on developing and optimizing artificial intelligence\nworkflows for a foundational impact on a broad range of DOE missions. With the\npervasive usage of artificial intelligence (AI) and machine learning (ML) tools\nand techniques, their energy efficiency is likely to become the gating factor\ntoward adoption. This is because generative AI (GenAI) models are massive\nenergy hogs: for instance, training a 200-billion parameter large language\nmodel (LLM) at Amazon is estimated to have taken 11.9 GWh, which is enough to\npower more than a thousand average U.S. households for a year. Inference\nconsumes even more energy, because a model trained once serve millions. Given\nthis scale, high energy efficiency is key to addressing the power delivery\nproblem of constructing and operating new supercomputers and datacenters\nspecialized for AI workloads. In that regard, we outline software- and\narchitecture-level research challenges and opportunities, setting the stage for\ncreating cross-layer energy optimizations in AI systems.\n","authors":["Jae-Won Chung","Nishil Talati","Mosharaf Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2404.06675v2.pdf","comment":"2024 Energy-Efficient Computing for Science Workshop"},{"id":"http://arxiv.org/abs/2408.02930v1","updated":"2024-08-06T03:26:01Z","published":"2024-08-06T03:26:01Z","title":"The Need for a Big World Simulator: A Scientific Challenge for Continual\n  Learning","summary":"  The \"small agent, big world\" frame offers a conceptual view that motivates\nthe need for continual learning. The idea is that a small agent operating in a\nmuch bigger world cannot store all information that the world has to offer. To\nperform well, the agent must be carefully designed to ingest, retain, and eject\nthe right information. To enable the development of performant continual\nlearning agents, a number of synthetic environments have been proposed.\nHowever, these benchmarks suffer from limitations, including unnatural\ndistribution shifts and a lack of fidelity to the \"small agent, big world\"\nframing. This paper aims to formalize two desiderata for the design of future\nsimulated environments. These two criteria aim to reflect the objectives and\ncomplexity of continual learning in practical settings while enabling rapid\nprototyping of algorithms on a smaller scale.\n","authors":["Saurabh Kumar","Hong Jun Jeon","Alex Lewandowski","Benjamin Van Roy"],"pdf_url":"https://arxiv.org/pdf/2408.02930v1.pdf","comment":"Accepted to the Finding the Frame Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2408.02927v1","updated":"2024-08-06T03:21:13Z","published":"2024-08-06T03:21:13Z","title":"HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy\n  Protection","summary":"  Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.\n","authors":["Yuxin Wang","Duanyu Feng","Yongfu Dai","Zhengyu Chen","Jimin Huang","Sophia Ananiadou","Qianqian Xie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00799v2","updated":"2024-08-06T03:03:16Z","published":"2024-07-22T02:27:57Z","title":"Deep Uncertainty-Based Explore for Index Construction and Retrieval in\n  Recommendation System","summary":"  In recommendation systems, the relevance and novelty of the final results are\nselected through a cascade system of Matching -> Ranking -> Strategy. The\nmatching model serves as the starting point of the pipeline and determines the\nupper bound of the subsequent stages. Balancing the relevance and novelty of\nmatching results is a crucial step in the design and optimization of\nrecommendation systems, contributing significantly to improving recommendation\nquality. However, the typical matching algorithms have not simultaneously\naddressed the relevance and novelty perfectly. One main reason is that deep\nmatching algorithms exhibit significant uncertainty when estimating items in\nthe long tail (e.g., due to insufficient training samples) items.The\nuncertainty not only affects the training of the models but also influences the\nconfidence in the index construction and beam search retrieval process of these\nmodels. This paper proposes the UICR (Uncertainty-based explore for Index\nConstruction and Retrieval) algorithm, which introduces the concept of\nuncertainty modeling in the matching stage and achieves multi-task modeling of\nmodel uncertainty and index uncertainty. The final matching results are\nobtained by combining the relevance score and uncertainty score infered by the\nmodel. Experimental results demonstrate that the UICR improves novelty without\nsacrificing relevance on realworld industrial productive environments and\nmultiple open-source datasets. Remarkably, online A/B test results of display\nadvertising in Shopee demonstrates the effectiveness of the proposed algorithm.\n","authors":["Xin Jiang","Kaiqiang Wang","Yinlong Wang","Fengchang Lv","Taiyang Peng","Shuai Yang","Xianteng Wu","Pengye Zhang","Shuo Yuan","Yifan Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.00799v2.pdf","comment":"accepted by cikm2024"},{"id":"http://arxiv.org/abs/2402.13699v4","updated":"2024-08-06T02:32:36Z","published":"2024-02-21T11:00:23Z","title":"Automation of Quantum Dot Measurement Analysis via Explainable Machine\n  Learning","summary":"  The rapid development of quantum dot (QD) devices for quantum computing has\nnecessitated more efficient and automated methods for device characterization\nand tuning. Many of the measurements acquired during the tuning process come in\nthe form of images that need to be properly analyzed to guide the subsequent\ntuning steps. By design, features present in such images capture certain\nbehaviors or states of the measured QD devices. When considered carefully, such\nfeatures can aid the control and calibration of QD devices. An important\nexample of such images are so-called \\textit{triangle plots}, which visually\nrepresent current flow and reveal characteristics important for QD device\ncalibration. While image-based classification tools, such as convolutional\nneural networks (CNNs), can be used to verify whether a given measurement is\n\\textit{good} and thus warrants the initiation of the next phase of tuning,\nthey do not provide any insights into how the device should be adjusted in the\ncase of \\textit{bad} images. This is because CNNs sacrifice prediction and\nmodel intelligibility for high accuracy. To ameliorate this trade-off, a recent\nstudy introduced an image vectorization approach that relies on the Gabor\nwavelet transform [1]. Here we propose an alternative vectorization method that\ninvolves mathematical modeling of synthetic triangles to mimic the experimental\ndata. Using explainable boosting machines, we show that this new method offers\nsuperior explainability of model prediction without sacrificing accuracy. This\nwork demonstrates the feasibility and advantages of applying explainable\nmachine learning techniques to the analysis of quantum dot measurements, paving\nthe way for further advances in automated and transparent QD device tuning.\n","authors":["Daniel Schug","Tyler J. Kovach","M. A. Wolfe","Jared Benson","Sanghyeok Park","J. P. Dodson","J. Corrigan","M. A. Eriksson","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2402.13699v4.pdf","comment":"17 pages, 4 figures, abbreviated version published in Proceedings of\n  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,\n  (Vancouver, Canada)"},{"id":"http://arxiv.org/abs/2408.02897v1","updated":"2024-08-06T02:06:04Z","published":"2024-08-06T02:06:04Z","title":"A Metric Driven Approach to Mixed Precision Training","summary":"  As deep learning methodologies have developed, it has been generally agreed\nthat increasing neural network size improves model quality. However, this is at\nthe expense of memory and compute requirements, which also need to be\nincreased. Various efficiency techniques have been proposed to rein in hardware\ncosts, one being the use of low precision numerics. Recent accelerators have\nintroduced several different 8-bit data types to help accommodate DNNs in terms\nof numerics. In this paper, we identify a metric driven methodology to aid in\nthe choice of numerics. We demonstrate how such a methodology can help scale\ntraining of a language representation model. The technique can be generalized\nto other model architectures.\n","authors":["Mitchelle Rasquinha","Gil Tabak"],"pdf_url":"https://arxiv.org/pdf/2408.02897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12428v3","updated":"2024-08-06T01:45:44Z","published":"2023-10-19T02:42:20Z","title":"Enhanced Local Explainability and Trust Scores with Random Forest\n  Proximities","summary":"  We initiate a novel approach to explain the predictions and out of sample\nperformance of random forest (RF) regression and classification models by\nexploiting the fact that any RF can be mathematically formulated as an adaptive\nweighted K nearest-neighbors model. Specifically, we employ a recent result\nthat, for both regression and classification tasks, any RF prediction can be\nrewritten exactly as a weighted sum of the training targets, where the weights\nare RF proximities between the corresponding pairs of data points. We show that\nthis linearity facilitates a local notion of explainability of RF predictions\nthat generates attributions for any model prediction across observations in the\ntraining set, and thereby complements established feature-based methods like\nSHAP, which generate attributions for a model prediction across input features.\nWe show how this proximity-based approach to explainability can be used in\nconjunction with SHAP to explain not just the model predictions, but also\nout-of-sample performance, in the sense that proximities furnish a novel means\nof assessing when a given model prediction is more or less likely to be\ncorrect. We demonstrate this approach in the modeling of US corporate bond\nprices and returns in both regression and classification cases.\n","authors":["Joshua Rosaler","Dhruv Desai","Bhaskarjit Sarmah","Dimitrios Vamvourellis","Deran Onay","Dhagash Mehta","Stefano Pasquali"],"pdf_url":"https://arxiv.org/pdf/2310.12428v3.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.19779v2","updated":"2024-08-06T01:25:33Z","published":"2024-05-30T07:44:31Z","title":"Automatic Graph Topology-Aware Transformer","summary":"  Existing efforts are dedicated to designing many topologies and graph-aware\nstrategies for the graph Transformer, which greatly improve the model's\nrepresentation capabilities. However, manually determining the suitable\nTransformer architecture for a specific graph dataset or task requires\nextensive expert knowledge and laborious trials. This paper proposes an\nevolutionary graph Transformer architecture search framework (EGTAS) to\nautomate the construction of strong graph Transformers. We build a\ncomprehensive graph Transformer search space with the micro-level and\nmacro-level designs. EGTAS evolves graph Transformer topologies at the macro\nlevel and graph-aware strategies at the micro level. Furthermore, a surrogate\nmodel based on generic architectural coding is proposed to directly predict the\nperformance of graph Transformers, substantially reducing the evaluation cost\nof evolutionary search. We demonstrate the efficacy of EGTAS across a range of\ngraph-level and node-level tasks, encompassing both small-scale and large-scale\ngraph datasets. Experimental results and ablation studies show that EGTAS can\nconstruct high-performance architectures that rival state-of-the-art manual and\nautomated baselines.\n","authors":["Chao Wang","Jiaxuan Zhao","Lingling Li","Licheng Jiao","Fang Liu","Shuyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2405.19779v2.pdf","comment":"This work has been accepted by IEEE Transactions on Neural Networks\n  and Learning Systems. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2408.02882v1","updated":"2024-08-06T01:20:12Z","published":"2024-08-06T01:20:12Z","title":"Compromising Embodied Agents with Contextual Backdoor Attacks","summary":"  Large language models (LLMs) have transformed the development of embodied\nintelligence. By providing a few contextual demonstrations, developers can\nutilize the extensive internal knowledge of LLMs to effortlessly translate\ncomplex tasks described in abstract language into sequences of code snippets,\nwhich will serve as the execution logic for embodied agents. However, this\npaper uncovers a significant backdoor security threat within this process and\nintroduces a novel method called \\method{}. By poisoning just a few contextual\ndemonstrations, attackers can covertly compromise the contextual environment of\na black-box LLM, prompting it to generate programs with context-dependent\ndefects. These programs appear logically sound but contain defects that can\nactivate and induce unintended behaviors when the operational agent encounters\nspecific triggers in its interactive environment. To compromise the LLM's\ncontextual environment, we employ adversarial in-context generation to optimize\npoisoned demonstrations, where an LLM judge evaluates these poisoned prompts,\nreporting to an additional LLM that iteratively optimizes the demonstration in\na two-player adversarial game using chain-of-thought reasoning. To enable\ncontext-dependent behaviors in downstream agents, we implement a dual-modality\nactivation strategy that controls both the generation and execution of program\ndefects through textual and visual triggers. We expand the scope of our attack\nby developing five program defect modes that compromise key aspects of\nconfidentiality, integrity, and availability in embodied agents. To validate\nthe effectiveness of our approach, we conducted extensive experiments across\nvarious tasks, including robot planning, robot manipulation, and compositional\nvisual reasoning. Additionally, we demonstrate the potential impact of our\napproach by successfully attacking real-world autonomous driving systems.\n","authors":["Aishan Liu","Yuguang Zhou","Xianglong Liu","Tianyuan Zhang","Siyuan Liang","Jiakai Wang","Yanjun Pu","Tianlin Li","Junqi Zhang","Wenbo Zhou","Qing Guo","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2408.02882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01331v2","updated":"2024-08-06T01:10:34Z","published":"2024-08-02T15:29:39Z","title":"UnifiedNN: Efficient Neural Network Training on the Cloud","summary":"  Nowadays, cloud-based services are widely favored over the traditional\napproach of locally training a Neural Network (NN) model. Oftentimes, a cloud\nservice processes multiple requests from users--thus training multiple NN\nmodels concurrently. However, training NN models concurrently is a challenging\nprocess, which typically requires significant amounts of available computing\nresources and takes a long time to complete. In this paper, we present\nUnifiedNN to effectively train multiple NN models concurrently on the cloud.\nUnifiedNN effectively \"combines\" multiple NN models and features several memory\nand time conservation mechanisms to train multiple NN models simultaneously\nwithout impacting the accuracy of the training process. Specifically, UnifiedNN\nmerges multiple NN models and creates a large singular unified model in order\nto efficiently train all models at once. We have implemented a prototype of\nUnifiedNN in PyTorch and we have compared its performance with relevant\nstate-of-the-art frameworks. Our experimental results demonstrate that\nUnifiedNN can reduce memory consumption by up to 53% and training time by up to\n81% when compared with vanilla PyTorch without impacting the model training and\ntesting accuracy. Finally, our results indicate that UnifiedNN can reduce\nmemory consumption by up to 52% and training time by up to 41% when compared to\nstate-of-the-art frameworks when training multiple models concurrently.\n","authors":["Sifat Ut Taki","Arthi Padmanabhan","Spyridon Mastorakis"],"pdf_url":"https://arxiv.org/pdf/2408.01331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07143v2","updated":"2024-08-06T00:51:37Z","published":"2023-10-11T02:36:52Z","title":"Imitation Learning from Purified Demonstrations","summary":"  Imitation learning has emerged as a promising approach for addressing\nsequential decision-making problems, with the assumption that expert\ndemonstrations are optimal. However, in real-world scenarios, most\ndemonstrations are often imperfect, leading to challenges in the effectiveness\nof imitation learning. While existing research has focused on optimizing with\nimperfect demonstrations, the training typically requires a certain proportion\nof optimal demonstrations to guarantee performance. To tackle these problems,\nwe propose to purify the potential noises in imperfect demonstrations first,\nand subsequently conduct imitation learning from these purified demonstrations.\nMotivated by the success of diffusion model, we introduce a two-step\npurification via diffusion process. In the first step, we apply a forward\ndiffusion process to smooth potential noises in imperfect demonstrations by\nintroducing additional noise. Subsequently, a reverse generative process is\nutilized to recover the optimal demonstration from the diffused ones. We\nprovide theoretical evidence supporting our approach, demonstrating that the\ndistance between the purified and optimal demonstration can be bounded.\nEmpirical results on MuJoCo and RoboSuite demonstrate the effectiveness of our\nmethod from different aspects.\n","authors":["Yunke Wang","Minjing Dong","Yukun Zhao","Bo Du","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.07143v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/1912.03573v2","updated":"2024-08-06T00:50:41Z","published":"2019-12-07T23:02:02Z","title":"Deep Variable-Block Chain with Adaptive Variable Selection","summary":"  The architectures of deep neural networks (DNN) rely heavily on the\nunderlying grid structure of variables, for instance, the lattice of pixels in\nan image. For general high dimensional data with variables not associated with\na grid, the multi-layer perceptron and deep belief network are often used.\nHowever, it is frequently observed that those networks do not perform\ncompetitively and they are not helpful for identifying important variables. In\nthis paper, we propose a framework that imposes on blocks of variables a chain\nstructure obtained by step-wise greedy search so that the DNN architecture can\nleverage the constructed grid. We call this new neural network Deep\nVariable-Block Chain (DVC). Because the variable blocks are used for\nclassification in a sequential manner, we further develop the capacity of\nselecting variables adaptively according to a number of regions trained by a\ndecision tree. Our experiments show that DVC outperforms other generic DNNs and\nother strong classifiers. Moreover, DVC can achieve high accuracy at much\nreduced dimensionality and sometimes reveals drastically different sets of\nrelevant variables for different regions.\n","authors":["Lixiang Zhang","Lin Lin","Jia Li"],"pdf_url":"https://arxiv.org/pdf/1912.03573v2.pdf","comment":"24 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.08847v3","updated":"2024-08-06T00:01:01Z","published":"2023-10-13T04:14:51Z","title":"On the Over-Memorization During Natural, Robust and Catastrophic\n  Overfitting","summary":"  Overfitting negatively impacts the generalization ability of deep neural\nnetworks (DNNs) in both natural and adversarial training. Existing methods\nstruggle to consistently address different types of overfitting, typically\ndesigning strategies that focus separately on either natural or adversarial\npatterns. In this work, we adopt a unified perspective by solely focusing on\nnatural patterns to explore different types of overfitting. Specifically, we\nexamine the memorization effect in DNNs and reveal a shared behaviour termed\nover-memorization, which impairs their generalization capacity. This behaviour\nmanifests as DNNs suddenly becoming high-confidence in predicting certain\ntraining patterns and retaining a persistent memory for them. Furthermore, when\nDNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit\nhigh-confidence prediction for the corresponding natural pattern. These\nfindings motivate us to holistically mitigate different types of overfitting by\nhindering the DNNs from over-memorization training patterns. To this end, we\npropose a general framework, Distraction Over-Memorization (DOM), which\nexplicitly prevents over-memorization by either removing or augmenting the\nhigh-confidence natural patterns. Extensive experiments demonstrate the\neffectiveness of our proposed method in mitigating overfitting across various\ntraining paradigms.\n","authors":["Runqi Lin","Chaojian Yu","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08847v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00391v3","updated":"2024-08-06T23:58:07Z","published":"2023-12-31T04:14:43Z","title":"SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with\n  Diffusion-Controllable Adversaries","summary":"  Evaluating the performance of autonomous vehicle planning algorithms\nnecessitates simulating long-tail safety-critical traffic scenarios. However,\ntraditional methods for generating such scenarios often fall short in terms of\ncontrollability and realism; they also neglect the dynamics of agent\ninteractions. To address these limitations, we introduce SAFE-SIM, a novel\ndiffusion-based controllable closed-loop safety-critical simulation framework.\nOur approach yields two distinct advantages: 1) generating realistic long-tail\nsafety-critical scenarios that closely reflect real-world conditions, and 2)\nproviding controllable adversarial behavior for more comprehensive and\ninteractive evaluations. We develop a novel approach to simulate\nsafety-critical scenarios through an adversarial term in the denoising process\nof diffusion models, which allows an adversarial agent to challenge a planner\nwith plausible maneuvers while all agents in the scene exhibit reactive and\nrealistic behaviors. Furthermore, we propose novel guidance objectives and a\npartial diffusion process that enables users to control key aspects of the\nscenarios, such as the collision type and aggressiveness of the adversarial\nagent, while maintaining the realism of the behavior. We validate our framework\nempirically using the nuScenes and nuPlan datasets across multiple planners,\ndemonstrating improvements in both realism and controllability. These findings\naffirm that diffusion models provide a robust and versatile foundation for\nsafety-critical, interactive traffic simulation, extending their utility across\nthe broader autonomous driving landscape. Project website:\nhttps://safe-sim.github.io/.\n","authors":["Wei-Jer Chang","Francesco Pittaluga","Masayoshi Tomizuka","Wei Zhan","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2401.00391v3.pdf","comment":"Accepted by ECCV2024; Project website: https://safe-sim.github.io/"},{"id":"http://arxiv.org/abs/2408.03480v1","updated":"2024-08-06T23:43:03Z","published":"2024-08-06T23:43:03Z","title":"Advancing EEG-Based Gaze Prediction Using Depthwise Separable\n  Convolution and Enhanced Pre-Processing","summary":"  In the field of EEG-based gaze prediction, the application of deep learning\nto interpret complex neural data poses significant challenges. This study\nevaluates the effectiveness of pre-processing techniques and the effect of\nadditional depthwise separable convolution on EEG vision transformers (ViTs) in\na pretrained model architecture. We introduce a novel method, the EEG Deeper\nClustered Vision Transformer (EEG-DCViT), which combines depthwise separable\nconvolutional neural networks (CNNs) with vision transformers, enriched by a\npre-processing strategy involving data clustering. The new approach\ndemonstrates superior performance, establishing a new benchmark with a Root\nMean Square Error (RMSE) of 51.6 mm. This achievement underscores the impact of\npre-processing and model refinement in enhancing EEG-based applications.\n","authors":["Matthew L Key","Tural Mehtiyev","Xiaodong Qu"],"pdf_url":"https://arxiv.org/pdf/2408.03480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03478v1","updated":"2024-08-06T23:34:49Z","published":"2024-08-06T23:34:49Z","title":"Effect of Kernel Size on CNN-Vision-Transformer-Based Gaze Prediction\n  Using Electroencephalography Data","summary":"  In this paper, we present an algorithm of gaze prediction from\nElectroencephalography (EEG) data. EEG-based gaze prediction is a new research\ntopic that can serve as an alternative to traditional video-based eye-tracking.\nCompared to the existing state-of-the-art (SOTA) method, we improved the root\nmean-squared-error of EEG-based gaze prediction to 53.06 millimeters, while\nreducing the training time to less than 33% of its original duration. Our\nsource code can be found at https://github.com/AmCh-Q/CSCI6907Project\n","authors":["Chuhui Qiu","Bugao Liang","Matthew L Key"],"pdf_url":"https://arxiv.org/pdf/2408.03478v1.pdf","comment":"International Conference on Human-Computer Interaction (HCII 2024)"},{"id":"http://arxiv.org/abs/2408.03475v1","updated":"2024-08-06T23:14:39Z","published":"2024-08-06T23:14:39Z","title":"Can LLMs Serve As Time Series Anomaly Detectors?","summary":"  An emerging topic in large language models (LLMs) is their application to\ntime series forecasting, characterizing mainstream and patternable\ncharacteristics of time series. A relevant but rarely explored and more\nchallenging question is whether LLMs can detect and explain time series\nanomalies, a critical task across various real-world applications. In this\npaper, we investigate the capabilities of LLMs, specifically GPT-4 and LLaMA3,\nin detecting and explaining anomalies in time series. Our studies reveal that:\n1) LLMs cannot be directly used for time series anomaly detection. 2) By\ndesigning prompt strategies such as in-context learning and chain-of-thought\nprompting, GPT-4 can detect time series anomalies with results competitive to\nbaseline methods. 3) We propose a synthesized dataset to automatically generate\ntime series anomalies with corresponding explanations. By applying instruction\nfine-tuning on this dataset, LLaMA3 demonstrates improved performance in time\nseries anomaly detection tasks. In summary, our exploration shows the promising\npotential of LLMs as time series anomaly detectors.\n","authors":["Manqing Dong","Hao Huang","Longbing Cao"],"pdf_url":"https://arxiv.org/pdf/2408.03475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08119v3","updated":"2024-08-06T23:09:06Z","published":"2024-01-16T05:23:34Z","title":"SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic\n  Spatio-Temporal Traffic Forecasting","summary":"  Traffic forecasting, a crucial application of spatio-temporal graph (STG)\nlearning, has traditionally relied on deterministic models for accurate point\nestimations. Yet, these models fall short of quantifying future uncertainties.\nRecently, many probabilistic methods, especially variants of diffusion models,\nhave been proposed to fill this gap. However, existing diffusion methods\ntypically deal with individual sensors separately when generating future time\nseries, resulting in limited usage of spatial information in the probabilistic\nlearning process. In this work, we propose SpecSTG, a novel spectral diffusion\nframework, to better leverage spatial dependencies and systematic patterns\ninherent in traffic data. More specifically, our method generates the Fourier\nrepresentation of future time series, transforming the learning process into\nthe spectral domain enriched with spatial information. Additionally, our\napproach incorporates a fast spectral graph convolution designed for Fourier\ninput, alleviating the computational burden associated with existing models.\nCompared with state-of-the-arts, SpecSTG achieves up to 8% improvements on\npoint estimations and up to 0.78% improvements on quantifying future\nuncertainties. Furthermore, SpecSTG's training and validation speed is 3.33X of\nthe most efficient existing diffusion method for STG forecasting. The source\ncode for SpecSTG is available at https://anonymous.4open.science/r/SpecSTG.\n","authors":["Lequan Lin","Dai Shi","Andi Han","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2401.08119v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03472v1","updated":"2024-08-06T23:05:15Z","published":"2024-08-06T23:05:15Z","title":"Integrating HCI Datasets in Project-Based Machine Learning Courses: A\n  College-Level Review and Case Study","summary":"  This study explores the integration of real-world machine learning (ML)\nprojects using human-computer interfaces (HCI) datasets in college-level\ncourses to enhance both teaching and learning experiences. Employing a\ncomprehensive literature review, course websites analysis, and a detailed case\nstudy, the research identifies best practices for incorporating HCI datasets\ninto project-based ML education. Key f indings demonstrate increased student\nengagement, motivation, and skill development through hands-on projects, while\ninstructors benefit from effective tools for teaching complex concepts. The\nstudy also addresses challenges such as data complexity and resource\nallocation, offering recommendations for future improvements. These insights\nprovide a valuable framework for educators aiming to bridge the gap between\n","authors":["Xiaodong Qu","Matthew Key","Eric Luo","Chuhui Qiu"],"pdf_url":"https://arxiv.org/pdf/2408.03472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03464v1","updated":"2024-08-06T22:39:34Z","published":"2024-08-06T22:39:34Z","title":"AI Foundation Models in Remote Sensing: A Survey","summary":"  Artificial Intelligence (AI) technologies have profoundly transformed the\nfield of remote sensing, revolutionizing data collection, processing, and\nanalysis. Traditionally reliant on manual interpretation and task-specific\nmodels, remote sensing has been significantly enhanced by the advent of\nfoundation models--large-scale, pre-trained AI models capable of performing a\nwide array of tasks with unprecedented accuracy and efficiency. This paper\nprovides a comprehensive survey of foundation models in the remote sensing\ndomain, covering models released between June 2021 and June 2024. We categorize\nthese models based on their applications in computer vision and domain-specific\ntasks, offering insights into their architectures, pre-training datasets, and\nmethodologies. Through detailed performance comparisons, we highlight emerging\ntrends and the significant advancements achieved by these foundation models.\nAdditionally, we discuss the technical challenges, practical implications, and\nfuture research directions, addressing the need for high-quality data,\ncomputational resources, and improved model generalization. Our research also\nfinds that pre-training methods, particularly self-supervised learning\ntechniques like contrastive learning and masked autoencoders, significantly\nenhance the performance and robustness of foundation models in remote sensing\ntasks such as scene classification, object detection, and other applications.\nThis survey aims to serve as a resource for researchers and practitioners by\nproviding a panorama of advances and promising pathways for continued\ndevelopment and application of foundation models in remote sensing.\n","authors":["Siqi Lu","Junlin Guo","James R Zimmer-Dauphinee","Jordan M Nieusma","Xiao Wang","Parker VanValkenburgh","Steven A Wernke","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2408.03464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07632v3","updated":"2024-08-06T22:37:21Z","published":"2024-03-12T13:12:24Z","title":"CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs\n  for Reduced hERG Liability","summary":"  The link between in vitro hERG ion channel inhibition and subsequent in vivo\nQT interval prolongation, a critical risk factor for the development of\narrythmias such as Torsade de Pointes, is so well established that in vitro\nhERG activity alone is often sufficient to end the development of an otherwise\npromising drug candidate. It is therefore of tremendous interest to develop\nadvanced methods for identifying hERG-active compounds in the early stages of\ndrug development, as well as for proposing redesigned compounds with reduced\nhERG liability and preserved on-target potency. In this work, we present\nCardioGenAI, a machine learning-based framework for re-engineering both\ndevelopmental and commercially available drugs for reduced hERG activity while\npreserving their pharmacological activity. The framework incorporates novel\nstate-of-the-art discriminative models for predicting hERG channel activity, as\nwell as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to\ntheir potential implications in modulating the arrhythmogenic potential induced\nby hERG channel blockade. We applied the complete framework to pimozide, an\nFDA-approved antipsychotic agent that demonstrates high affinity to the hERG\nchannel, and generated 100 refined candidates. Remarkably, among the candidates\nis fluspirilene, a compound which is of the same class of drugs\n(diphenylmethanes) as pimozide and therefore has similar pharmacological\nactivity, yet exhibits over 700-fold weaker binding to hERG. We envision that\nthis method can effectively be applied to developmental compounds exhibiting\nhERG liabilities to provide a means of rescuing drug development programs that\nhave stalled due to hERG-related safety concerns. We have made all of our\nsoftware open-source to facilitate integration of the CardioGenAI framework for\nmolecular hypothesis generation into drug discovery workflows.\n","authors":["Gregory W. Kyro","Matthew T. Martin","Eric D. Watt","Victor S. Batista"],"pdf_url":"https://arxiv.org/pdf/2403.07632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11704v2","updated":"2024-08-06T22:37:06Z","published":"2024-06-17T16:25:04Z","title":"Nemotron-4 340B Technical Report","summary":"  We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base,\nNemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open\naccess under the NVIDIA Open Model License Agreement, a permissive model\nlicense that allows distribution, modification, and use of the models and its\noutputs. These models perform competitively to open access models on a wide\nrange of evaluation benchmarks, and were sized to fit on a single DGX H100 with\n8 GPUs when deployed in FP8 precision. We believe that the community can\nbenefit from these models in various research studies and commercial\napplications, especially for generating synthetic data to train smaller\nlanguage models. Notably, over 98% of data used in our model alignment process\nis synthetically generated, showcasing the effectiveness of these models in\ngenerating synthetic data. To further support open research and facilitate\nmodel development, we are also open-sourcing the synthetic data generation\npipeline used in our model alignment process.\n","authors":[" Nvidia"," :","Bo Adler","Niket Agarwal","Ashwath Aithal","Dong H. Anh","Pallab Bhattacharya","Annika Brundyn","Jared Casper","Bryan Catanzaro","Sharon Clay","Jonathan Cohen","Sirshak Das","Ayush Dattagupta","Olivier Delalleau","Leon Derczynski","Yi Dong","Daniel Egert","Ellie Evans","Aleksander Ficek","Denys Fridman","Shaona Ghosh","Boris Ginsburg","Igor Gitman","Tomasz Grzegorzek","Robert Hero","Jining Huang","Vibhu Jawa","Joseph Jennings","Aastha Jhunjhunwala","John Kamalu","Sadaf Khan","Oleksii Kuchaiev","Patrick LeGresley","Hui Li","Jiwei Liu","Zihan Liu","Eileen Long","Ameya Sunil Mahabaleshwarkar","Somshubra Majumdar","James Maki","Miguel Martinez","Maer Rodrigues de Melo","Ivan Moshkov","Deepak Narayanan","Sean Narenthiran","Jesus Navarro","Phong Nguyen","Osvald Nitski","Vahid Noroozi","Guruprasad Nutheti","Christopher Parisien","Jupinder Parmar","Mostofa Patwary","Krzysztof Pawelec","Wei Ping","Shrimai Prabhumoye","Rajarshi Roy","Trisha Saar","Vasanth Rao Naik Sabavat","Sanjeev Satheesh","Jane Polak Scowcroft","Jason Sewall","Pavel Shamis","Gerald Shen","Mohammad Shoeybi","Dave Sizer","Misha Smelyanskiy","Felipe Soares","Makesh Narsimhan Sreedhar","Dan Su","Sandeep Subramanian","Shengyang Sun","Shubham Toshniwal","Hao Wang","Zhilin Wang","Jiaxuan You","Jiaqi Zeng","Jimmy Zhang","Jing Zhang","Vivienne Zhang","Yian Zhang","Chen Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.11704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18742v5","updated":"2024-08-06T22:33:26Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15750v2","updated":"2024-08-06T22:29:11Z","published":"2024-05-24T17:47:20Z","title":"Filtered Corpus Training (FiCT) Shows that Language Models can\n  Generalize from Indirect Evidence","summary":"  This paper introduces Filtered Corpus Training, a method that trains language\nmodels (LMs) on corpora with certain linguistic constructions filtered out from\nthe training data, and uses it to measure the ability of LMs to perform\nlinguistic generalization on the basis of indirect evidence. We apply the\nmethod to both LSTM and Transformer LMs (of roughly comparable size),\ndeveloping filtered corpora that target a wide range of linguistic phenomena.\nOur results show that while transformers are better qua LMs (as measured by\nperplexity), both models perform equally and surprisingly well on linguistic\ngeneralization measures, suggesting that they are capable of generalizing from\nindirect evidence.\n","authors":["Abhinav Patil","Jaap Jumelet","Yu Ying Chiu","Andy Lapastora","Peter Shen","Lexie Wang","Clevis Willrich","Shane Steinert-Threlkeld"],"pdf_url":"https://arxiv.org/pdf/2405.15750v2.pdf","comment":"Forthcoming in Transactions of the Association for Computational\n  Linguistics (TACL). This is a pre-MIT Press publication version. For code and\n  trained models, see http://github.com/CLMBRs/corpus-filtering"},{"id":"http://arxiv.org/abs/2401.12972v2","updated":"2024-08-06T22:28:25Z","published":"2024-01-23T18:58:35Z","title":"On the Efficacy of Text-Based Input Modalities for Action Anticipation","summary":"  Anticipating future actions is a highly challenging task due to the diversity\nand scale of potential future actions; yet, information from different\nmodalities help narrow down plausible action choices. Each modality can provide\ndiverse and often complementary context for the model to learn from. While\nprevious multi-modal methods leverage information from modalities such as video\nand audio, we primarily explore how text descriptions of actions and objects\ncan also lead to more accurate action anticipation by providing additional\ncontextual cues, e.g., about the environment and its contents. We propose a\nMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformer\narchitecture that jointly learns from multi-modal features and text\ndescriptions of actions and objects. We train our model in two stages, where\nthe model first learns to align video clips with descriptions of future\nactions, and is subsequently fine-tuned to predict future actions. Compared to\nexisting methods, M-CAT has the advantage of learning additional context from\ntwo types of text inputs: rich descriptions of future actions during\npre-training, and, text descriptions for detected objects and actions during\nmodality feature fusion. Through extensive experimental evaluation, we\ndemonstrate that our model outperforms previous methods on the EpicKitchens\ndatasets, and show that using simple text descriptions of actions and objects\naid in more effective action anticipation. In addition, we examine the impact\nof object and action information obtained via text, and perform extensive\nablations.\n","authors":["Apoorva Beedu","Karan Samel","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2401.12972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03461v1","updated":"2024-08-06T22:14:54Z","published":"2024-08-06T22:14:54Z","title":"When does the mean network capture the topology of a sample of networks?","summary":"  The notion of Fr\\'echet mean (also known as \"barycenter\") network is the\nworkhorse of most machine learning algorithms that require the estimation of a\n\"location\" parameter to analyse network-valued data. In this context, it is\ncritical that the network barycenter inherits the topological structure of the\nnetworks in the training dataset. The metric - which measures the proximity\nbetween networks - controls the structural properties of the barycenter. This\nwork is significant because it provides for the first time analytical estimates\nof the sample Fr\\'echet mean for the stochastic blockmodel, which is at the\ncutting edge of rigorous probabilistic analysis of random networks. We show\nthat the mean network computed with the Hamming distance is unable to capture\nthe topology of the networks in the training sample, whereas the mean network\ncomputed using the effective resistance distance recovers the correct\npartitions and associated edge density. From a practical standpoint, our work\ninforms the choice of metrics in the context where the sample Fr\\'echet mean\nnetwork is used to characterise the topology of networks for network-valued\nmachine learning\n","authors":["Franois G Meyer"],"pdf_url":"https://arxiv.org/pdf/2408.03461v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2408.03459v1","updated":"2024-08-06T22:11:00Z","published":"2024-08-06T22:11:00Z","title":"On the Generalization of Preference Learning with DPO","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09753v2","updated":"2024-08-06T21:54:20Z","published":"2024-04-15T12:54:31Z","title":"Personalized Collaborative Fine-Tuning for On-Device Large Language\n  Models","summary":"  We explore on-device self-supervised collaborative fine-tuning of large\nlanguage models with limited local data availability. Taking inspiration from\nthe collaborative learning community, we introduce three distinct\ntrust-weighted gradient aggregation schemes: weight similarity-based,\nprediction similarity-based and validation performance-based. To minimize\ncommunication overhead, we integrate Low-Rank Adaptation (LoRA) and only\nexchange LoRA weight updates. Our protocols, driven by prediction and\nperformance metrics, surpass both FedAvg and local fine-tuning methods, which\nis particularly evident in realistic scenarios with more diverse local data\ndistributions. The results underscore the effectiveness of our approach in\naddressing heterogeneity and scarcity within local datasets.\n","authors":["Nicolas Wagner","Dongyang Fan","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2404.09753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16266v2","updated":"2024-08-06T21:26:31Z","published":"2024-05-25T15:08:36Z","title":"Deep Reinforcement Learning with Enhanced PPO for Safe Mobile Robot\n  Navigation","summary":"  Collision-free motion is essential for mobile robots. Most approaches to\ncollision-free and efficient navigation with wheeled robots require parameter\ntuning by experts to obtain good navigation behavior. This study investigates\nthe application of deep reinforcement learning to train a mobile robot for\nautonomous navigation in a complex environment. The robot utilizes LiDAR sensor\ndata and a deep neural network to generate control signals guiding it toward a\nspecified target while avoiding obstacles. We employ two reinforcement learning\nalgorithms in the Gazebo simulation environment: Deep Deterministic Policy\nGradient and proximal policy optimization. The study introduces an enhanced\nneural network structure in the Proximal Policy Optimization algorithm to boost\nperformance, accompanied by a well-designed reward function to improve\nalgorithm efficacy. Experimental results conducted in both obstacle and\nobstacle-free environments underscore the effectiveness of the proposed\napproach. This research significantly contributes to the advancement of\nautonomous robotics in complex environments through the application of deep\nreinforcement learning.\n","authors":["Hamid Taheri","Seyed Rasoul Hosseini","Mohammad Ali Nekoui"],"pdf_url":"https://arxiv.org/pdf/2405.16266v2.pdf","comment":"This paper is under review by Int. J. of Intelligent Machines and\n  Robotics"},{"id":"http://arxiv.org/abs/2407.19943v2","updated":"2024-08-06T21:16:16Z","published":"2024-07-29T12:23:59Z","title":"Practical and Robust Safety Guarantees for Advanced Counterfactual\n  Learning to Rank","summary":"  Counterfactual learning to rank (CLTR) can be risky and, in various\ncircumstances, can produce sub-optimal models that hurt performance when\ndeployed. Safe CLTR was introduced to mitigate these risks when using inverse\npropensity scoring to correct for position bias. However, the existing safety\nmeasure for CLTR is not applicable to state-of-the-art CLTR methods, cannot\nhandle trust bias, and relies on specific assumptions about user behavior. Our\ncontributions are two-fold. First, we generalize the existing safe CLTR\napproach to make it applicable to state-of-the-art doubly robust CLTR and trust\nbias. Second, we propose a novel approach, proximal ranking policy optimization\n(PRPO), that provides safety in deployment without assumptions about user\nbehavior. PRPO removes incentives for learning ranking behavior that is too\ndissimilar to a safe ranking model. Thereby, PRPO imposes a limit on how much\nlearned models can degrade performance metrics, without relying on any specific\nuser assumptions. Our experiments show that both our novel safe doubly robust\nmethod and PRPO provide higher performance than the existing safe inverse\npropensity scoring approach. However, in unexpected circumstances, the safe\ndoubly robust approach can become unsafe and bring detrimental performance. In\ncontrast, PRPO always maintains safety, even in maximally adversarial\nsituations. By avoiding assumptions, PRPO is the first method with\nunconditional safety in deployment that translates to robust safety for\nreal-world applications.\n","authors":["Shashank Gupta","Harrie Oosterhuis","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2407.19943v2.pdf","comment":"Accepted as full paper at CIKM 2024"},{"id":"http://arxiv.org/abs/2403.10795v2","updated":"2024-08-06T21:14:23Z","published":"2024-03-16T03:54:38Z","title":"Can Large Language Models Solve Robot Routing?","summary":"  Routing problems are common in mobile robotics, encompassing tasks such as\ninspection, surveillance, and coverage. Depending on the objective and\nconstraints, these problems often reduce to variants of the Traveling Salesman\nProblem (TSP), with solutions traditionally derived by translating high-level\nobjectives into an optimization formulation and using modern solvers to arrive\nat a solution. Here, we explore the potential of Large Language Models (LLMs)\nto replace the entire pipeline from tasks described in natural language to the\ngeneration of robot routes. We systematically investigate the performance of\nLLMs in robot routing by constructing a dataset with 80 unique robot routing\nproblems across 8 variants in both single and multi-robot settings. We evaluate\nLLMs through three frameworks: single attempt, self-debugging, and\nself-debugging with self-verification and various contexts, including\nmathematical formulations, pseudo-code, and related research papers. Our\nfindings reveal that both self-debugging and self-verification enhance success\nrates without significantly lowering the optimality gap. We observe\ncontext-sensitive behavior - providing mathematical formulations as context\ndecreases the optimality gap but significantly decreases success rates and\nproviding pseudo-code and related research papers as context does not\nconsistently improve success rates or decrease the optimality gap. We identify\nkey challenges and propose future directions to enhance LLM performance in\nsolving robot routing problems. Our source code is available on the project\nwebsite: https://sites.google.com/view/words-to-routes/.\n","authors":["Zhehui Huang","Guangyao Shi","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2403.10795v2.pdf","comment":"Submitted to International Symposium of Robotics Research (ISRR 2024)"},{"id":"http://arxiv.org/abs/2407.19305v2","updated":"2024-08-06T21:07:17Z","published":"2024-07-27T17:27:05Z","title":"GP-VLS: A general-purpose vision language model for surgery","summary":"  Surgery requires comprehensive medical knowledge, visual assessment skills,\nand procedural expertise. While recent surgical AI models have focused on\nsolving task-specific problems, there is a need for general-purpose systems\nthat can understand surgical scenes and interact through natural language. This\npaper introduces GP-VLS, a general-purpose vision language model for surgery\nthat integrates medical and surgical knowledge with visual scene understanding.\nFor comprehensively evaluating general-purpose surgical models, we propose\nSurgiQual, which evaluates across medical and surgical knowledge benchmarks as\nwell as surgical vision-language questions. To train GP-VLS, we develop six new\ndatasets spanning medical knowledge, surgical textbooks, and vision-language\npairs for tasks like phase recognition and tool identification. We show that\nGP-VLS significantly outperforms existing open- and closed-source models on\nsurgical vision-language tasks, with 8-21% improvements in accuracy across\nSurgiQual benchmarks. GP-VLS also demonstrates strong performance on medical\nand surgical knowledge tests compared to open-source alternatives. Overall,\nGP-VLS provides an open-source foundation for developing AI assistants to\nsupport surgeons across a wide range of tasks and scenarios. The code and data\nfor this work is publicly available at gpvls-surgery-vlm.github.io.\n","authors":["Samuel Schmidgall","Joseph Cho","Cyril Zakka","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2407.19305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03450v1","updated":"2024-08-06T21:03:16Z","published":"2024-08-06T21:03:16Z","title":"Probabilistic Surrogate Model for Accelerating the Design of Electric\n  Vehicle Battery Enclosures for Crash Performance","summary":"  This paper presents a probabilistic surrogate model for the accelerated\ndesign of electric vehicle battery enclosures with a focus on crash\nperformance. The study integrates high-throughput finite element simulations\nand Gaussian Process Regression to develop a surrogate model that predicts\ncrash parameters with high accuracy while providing uncertainty estimates. The\nmodel was trained using data generated from thermoforming and crash simulations\nover a range of material and process parameters. Validation against new\nsimulation data demonstrated the model's predictive accuracy with mean absolute\npercentage errors within 8.08% for all output variables. Additionally, a Monte\nCarlo uncertainty propagation study revealed the impact of input variability on\noutputs. The results highlight the efficacy of the Gaussian Process Regression\nmodel in capturing complex relationships within the dataset, offering a robust\nand efficient tool for the design optimization of composite battery enclosures.\n","authors":["Shadab Anwar Shaikh","Harish Cherukuri","Kranthi Balusu","Ram Devanathan","Ayoub Soulami"],"pdf_url":"https://arxiv.org/pdf/2408.03450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03449v1","updated":"2024-08-06T21:02:27Z","published":"2024-08-06T21:02:27Z","title":"EEGMobile: Enhancing Speed and Accuracy in EEG-Based Gaze Prediction\n  with Advanced Mobile Architectures","summary":"  Electroencephalography (EEG) analysis is an important domain in the realm of\nBrain-Computer Interface (BCI) research. To ensure BCI devices are capable of\nproviding practical applications in the real world, brain signal processing\ntechniques must be fast, accurate, and resource-conscious to deliver\nlow-latency neural analytics. This study presents a model that leverages a\npre-trained MobileViT alongside Knowledge Distillation (KD) for EEG regression\ntasks. Our results showcase that this model is capable of performing at a level\ncomparable (only 3% lower) to the previous State-Of-The-Art (SOTA) on the\nEEGEyeNet Absolute Position Task while being 33% faster and 60% smaller. Our\nresearch presents a cost-effective model applicable to resource-constrained\ndevices and contributes to expanding future research on lightweight,\nmobile-friendly models for EEG regression.\n","authors":["Teng Liang","Andrews Damoah"],"pdf_url":"https://arxiv.org/pdf/2408.03449v1.pdf","comment":"Accepted HCI International 2024 - Late Breaking Work"},{"id":"http://arxiv.org/abs/2408.03445v1","updated":"2024-08-06T20:53:02Z","published":"2024-08-06T20:53:02Z","title":"Spacecraft inertial parameters estimation using time series clustering\n  and reinforcement learning","summary":"  This paper presents a machine learning approach to estimate the inertial\nparameters of a spacecraft in cases when those change during operations, e.g.\nmultiple deployments of payloads, unfolding of appendages and booms, propellant\nconsumption as well as during in-orbit servicing and active debris removal\noperations. The machine learning approach uses time series clustering together\nwith an optimised actuation sequence generated by reinforcement learning to\nfacilitate distinguishing among different inertial parameter sets. The\nperformance of the proposed strategy is assessed against the case of a\nmulti-satellite deployment system showing that the algorithm is resilient\ntowards common disturbances in such kinds of operations.\n","authors":["Konstantinos Platanitis","Miguel Arana-Catania","Leonardo Capicchiano","Saurabh Upadhyay","Leonard Felicetti"],"pdf_url":"https://arxiv.org/pdf/2408.03445v1.pdf","comment":"6 pages, 3 figures, 1 table. To be presented in ESA - AI for Space\n  (SPAICE)"},{"id":"http://arxiv.org/abs/2408.03441v1","updated":"2024-08-06T20:40:20Z","published":"2024-08-06T20:40:20Z","title":"Simple Perturbations Subvert Ethereum Phishing Transactions Detection:\n  An Empirical Analysis","summary":"  This paper explores the vulnerability of machine learning models,\nspecifically Random Forest, Decision Tree, and K-Nearest Neighbors, to very\nsimple single-feature adversarial attacks in the context of Ethereum fraudulent\ntransaction detection. Through comprehensive experimentation, we investigate\nthe impact of various adversarial attack strategies on model performance\nmetrics, such as accuracy, precision, recall, and F1-score. Our findings,\nhighlighting how prone those techniques are to simple attacks, are alarming,\nand the inconsistency in the attacks' effect on different algorithms promises\nways for attack mitigation. We examine the effectiveness of different\nmitigation strategies, including adversarial training and enhanced feature\nselection, in enhancing model robustness.\n","authors":["Ahod Alghureid","David Mohaisen"],"pdf_url":"https://arxiv.org/pdf/2408.03441v1.pdf","comment":"12 pages, 1 figure, 5 tables, accepted for presentation at WISA 2024"},{"id":"http://arxiv.org/abs/2408.03433v1","updated":"2024-08-06T20:19:06Z","published":"2024-08-06T20:19:06Z","title":"Hybrid diffusion models: combining supervised and generative pretraining\n  for label-efficient fine-tuning of segmentation models","summary":"  We are considering in this paper the task of label-efficient fine-tuning of\nsegmentation models: We assume that a large labeled dataset is available and\nallows to train an accurate segmentation model in one domain, and that we have\nto adapt this model on a related domain where only a few samples are available.\nWe observe that this adaptation can be done using two distinct methods: The\nfirst method, supervised pretraining, is simply to take the model trained on\nthe first domain using classical supervised learning, and fine-tune it on the\nsecond domain with the available labeled samples. The second method is to\nperform self-supervised pretraining on the first domain using a generic pretext\ntask in order to get high-quality representations which can then be used to\ntrain a model on the second domain in a label-efficient way. We propose in this\npaper to fuse these two approaches by introducing a new pretext task, which is\nto perform simultaneously image denoising and mask prediction on the first\ndomain. We motivate this choice by showing that in the same way that an image\ndenoiser conditioned on the noise level can be considered as a generative model\nfor the unlabeled image distribution using the theory of diffusion models, a\nmodel trained using this new pretext task can be considered as a generative\nmodel for the joint distribution of images and segmentation masks under the\nassumption that the mapping from images to segmentation masks is deterministic.\nWe then empirically show on several datasets that fine-tuning a model\npretrained using this approach leads to better results than fine-tuning a\nsimilar model trained using either supervised or unsupervised pretraining only.\n","authors":["Bruno Sauvalle","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2408.03433v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2406.17885v2","updated":"2024-08-06T20:06:11Z","published":"2024-06-25T18:47:50Z","title":"Enabling Regional Explainability by Automatic and Model-agnostic Rule\n  Extraction","summary":"  In Explainable AI, rule extraction translates model knowledge into logical\nrules, such as IF-THEN statements, crucial for understanding patterns learned\nby black-box models. This could significantly aid in fields like disease\ndiagnosis, disease progression estimation, or drug discovery. However, such\napplication domains often contain imbalanced data, with the class of interest\nunderrepresented. Existing methods inevitably compromise the performance of\nrules for the minor class to maximise the overall performance. As the first\nattempt in this field, we propose a model-agnostic approach for extracting\nrules from specific subgroups of data, featuring automatic rule generation for\nnumerical features. This method enhances the regional explainability of machine\nlearning models and offers wider applicability compared to existing methods. We\nadditionally introduce a new method for selecting features to compose rules,\nreducing computational costs in high-dimensional spaces. Experiments across\nvarious datasets and models demonstrate the effectiveness of our methods.\n","authors":["Yu Chen","Tianyu Cui","Alexander Capstick","Nan Fletcher-Loyd","Payam Barnaghi"],"pdf_url":"https://arxiv.org/pdf/2406.17885v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2408.03425v1","updated":"2024-08-06T20:02:57Z","published":"2024-08-06T20:02:57Z","title":"Sequential Conditional Transport on Probabilistic Graphs for\n  Interpretable Counterfactual Fairness","summary":"  In this paper, we link two existing approaches to derive counterfactuals:\nadaptations based on a causal graph, as suggested in Ple\\v{c}ko and Meinshausen\n(2020) and optimal transport, as in De Lara et al. (2024). We extend \"Knothe's\nrearrangement\" Bonnotte (2013) and \"triangular transport\" Zech and Marzouk\n(2022a) to probabilistic graphical models, and use this counterfactual\napproach, referred to as sequential transport, to discuss individual fairness.\nAfter establishing the theoretical foundations of the proposed method, we\ndemonstrate its application through numerical experiments on both synthetic and\nreal datasets.\n","authors":["Agathe Fernandes Machado","Arthur Charpentier","Ewen Gallic"],"pdf_url":"https://arxiv.org/pdf/2408.03425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05707v4","updated":"2024-08-06T19:53:17Z","published":"2023-10-09T13:29:37Z","title":"Guiding Language Model Reasoning with Planning Tokens","summary":"  Large language models (LLMs) have recently attracted considerable interest\nfor their ability to perform complex reasoning tasks, such as chain-of-thought\n(CoT) reasoning. However, most of the existing approaches to enhance this\nability rely heavily on data-driven methods, while neglecting the structural\naspects of the model's reasoning capacity. To encourage a more structural\ngeneration of CoT steps, we propose a hierarchical generation scheme: we let\nthe LM generate a planning token at the start of each reasoning step,\nintuitively serving as a high-level plan of the current step, and add their\nembeddings to the model parameters. Our approach requires a negligible increase\nin trainable parameters (0.001%) and can be applied through either full\nfine-tuning or a more parameter-efficient scheme. We demonstrate our method's\neffectiveness by applying it to three different LLMs, showing notable accuracy\nimprovements across three math word problem datasets and one multihop QA\ndataset with respect to standard fine-tuning baselines.\n","authors":["Xinyi Wang","Lucas Caccia","Oleksiy Ostapenko","Xingdi Yuan","William Yang Wang","Alessandro Sordoni"],"pdf_url":"https://arxiv.org/pdf/2310.05707v4.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.03421v1","updated":"2024-08-06T19:53:00Z","published":"2024-08-06T19:53:00Z","title":"Probabilistic Scores of Classifiers, Calibration is not Enough","summary":"  In binary classification tasks, accurate representation of probabilistic\npredictions is essential for various real-world applications such as predicting\npayment defaults or assessing medical risks. The model must then be\nwell-calibrated to ensure alignment between predicted probabilities and actual\noutcomes. However, when score heterogeneity deviates from the underlying data\nprobability distribution, traditional calibration metrics lose reliability,\nfailing to align score distribution with actual probabilities. In this study,\nwe highlight approaches that prioritize optimizing the alignment between\npredicted scores and true probability distributions over minimizing traditional\nperformance or calibration metrics. When employing tree-based models such as\nRandom Forest and XGBoost, our analysis emphasizes the flexibility these models\noffer in tuning hyperparameters to minimize the Kullback-Leibler (KL)\ndivergence between predicted and true distributions. Through extensive\nempirical analysis across 10 UCI datasets and simulations, we demonstrate that\noptimizing tree-based models based on KL divergence yields superior alignment\nbetween predicted scores and actual probabilities without significant\nperformance loss. In real-world scenarios, the reference probability is\ndetermined a priori as a Beta distribution estimated through maximum\nlikelihood. Conversely, minimizing traditional calibration metrics may lead to\nsuboptimal results, characterized by notable performance declines and inferior\nKL values. Our findings reveal limitations in traditional calibration metrics,\nwhich could undermine the reliability of predictive models for critical\ndecision-making.\n","authors":["Agathe Fernandes Machado","Arthur Charpentier","Emmanuel Flachaire","Ewen Gallic","Franois Hu"],"pdf_url":"https://arxiv.org/pdf/2408.03421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03414v1","updated":"2024-08-06T19:23:42Z","published":"2024-08-06T19:23:42Z","title":"Logistic Regression makes small LLMs strong and explainable\n  \"tens-of-shot\" classifiers","summary":"  For simple classification tasks, we show that users can benefit from the\nadvantages of using small, local, generative language models instead of large\ncommercial models without a trade-off in performance or introducing extra\nlabelling costs. These advantages, including those around privacy,\navailability, cost, and explainability, are important both in commercial\napplications and in the broader democratisation of AI. Through experiments on\n17 sentence classification tasks (2-4 classes), we show that penalised logistic\nregression on the embeddings from a small LLM equals (and usually betters) the\nperformance of a large LLM in the \"tens-of-shot\" regime. This requires no more\nlabelled instances than are needed to validate the performance of the large\nLLM. Finally, we extract stable and sensible explanations for classification\ndecisions.\n","authors":["Marcus Buckmann","Edward Hill"],"pdf_url":"https://arxiv.org/pdf/2408.03414v1.pdf","comment":"41 pages, 24 figures"},{"id":"http://arxiv.org/abs/2408.03413v1","updated":"2024-08-06T19:22:13Z","published":"2024-08-06T19:22:13Z","title":"A TVD neural network closure and application to turbulent combustion","summary":"  Trained neural networks (NN) have attractive features for closing governing\nequations, but in the absence of additional constraints, they can stray from\nphysical reality. A NN formulation is introduced to preclude spurious\noscillations that violate solution boundedness or positivity. It is embedded in\nthe discretized equations as a machine learning closure and strictly\nconstrained, inspired by total variation diminishing (TVD) methods for\nhyperbolic conservation laws. The constraint is exactly enforced during\ngradient-descent training by rescaling the NN parameters, which maps them onto\nan explicit feasible set. Demonstrations show that the constrained NN closure\nmodel usefully recovers linear and nonlinear hyperbolic phenomena and\nanti-diffusion while enforcing the non-oscillatory property. Finally, the model\nis applied to subgrid-scale (SGS) modeling of a turbulent reacting flow, for\nwhich it suppresses spurious oscillations in scalar fields that otherwise\nviolate the solution boundedness. It outperforms a simple penalization of\noscillations in the loss function.\n","authors":["Seung Won Suh","Jonathan F MacArt","Luke N Olson","Jonathan B Freund"],"pdf_url":"https://arxiv.org/pdf/2408.03413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03408v1","updated":"2024-08-06T19:10:25Z","published":"2024-08-06T19:10:25Z","title":"LLM-Aided Compilation for Tensor Accelerators","summary":"  Hardware accelerators, in particular accelerators for tensor processing, have\nmany potential application domains. However, they currently lack the software\ninfrastructure to support the majority of domains outside of deep learning.\nFurthermore, a compiler that can easily be updated to reflect changes at both\napplication and hardware levels would enable more agile development and design\nspace exploration of accelerators, allowing hardware designers to realize\ncloser-to-optimal performance. In this work, we discuss how large language\nmodels (LLMs) could be leveraged to build such a compiler. Specifically, we\ndemonstrate the ability of GPT-4 to achieve high pass rates in translating code\nto the Gemmini accelerator, and prototype a technique for decomposing\ntranslation into smaller, more LLM-friendly steps. Additionally, we propose a\n2-phase workflow for utilizing LLMs to generate hardware-optimized code.\n","authors":["Charles Hong","Sahil Bhatia","Altan Haan","Shengjun Kris Dong","Dima Nikiforov","Alvin Cheung","Yakun Sophia Shao"],"pdf_url":"https://arxiv.org/pdf/2408.03408v1.pdf","comment":"4 page workshop paper"},{"id":"http://arxiv.org/abs/2408.03407v1","updated":"2024-08-06T19:01:47Z","published":"2024-08-06T19:01:47Z","title":"Deep Clustering via Distribution Learning","summary":"  Distribution learning finds probability density functions from a set of data\nsamples, whereas clustering aims to group similar data points to form clusters.\nAlthough there are deep clustering methods that employ distribution learning\nmethods, past work still lacks theoretical analysis regarding the relationship\nbetween clustering and distribution learning. Thus, in this work, we provide a\ntheoretical analysis to guide the optimization of clustering via distribution\nlearning. To achieve better results, we embed deep clustering guided by a\ntheoretical analysis. Furthermore, the distribution learning method cannot\nalways be directly applied to data. To overcome this issue, we introduce a\nclustering-oriented distribution learning method called Monte-Carlo\nMarginalization for Clustering. We integrate Monte-Carlo Marginalization for\nClustering into Deep Clustering, resulting in Deep Clustering via Distribution\nLearning (DCDL). Eventually, the proposed DCDL achieves promising results\ncompared to state-of-the-art methods on popular datasets. Considering a\nclustering task, the new distribution learning method outperforms previous\nmethods as well.\n","authors":["Guanfang Dong","Zijie Tan","Chenqiu Zhao","Anup Basu"],"pdf_url":"https://arxiv.org/pdf/2408.03407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03405v1","updated":"2024-08-06T18:56:29Z","published":"2024-08-06T18:56:29Z","title":"Combining Diverse Information for Coordinated Action: Stochastic Bandit\n  Algorithms for Heterogeneous Agents","summary":"  Stochastic multi-agent multi-armed bandits typically assume that the rewards\nfrom each arm follow a fixed distribution, regardless of which agent pulls the\narm. However, in many real-world settings, rewards can depend on the\nsensitivity of each agent to their environment. In medical screening, disease\ndetection rates can vary by test type; in preference matching, rewards can\ndepend on user preferences; and in environmental sensing, observation quality\ncan vary across sensors. Since past work does not specify how to allocate\nagents of heterogeneous but known sensitivity of these types in a stochastic\nbandit setting, we introduce a UCB-style algorithm, Min-Width, which aggregates\ninformation from diverse agents. In doing so, we address the joint challenges\nof (i) aggregating the rewards, which follow different distributions for each\nagent-arm pair, and (ii) coordinating the assignments of agents to arms.\nMin-Width facilitates efficient collaboration among heterogeneous agents,\nexploiting the known structure in the agents' reward functions to weight their\nrewards accordingly. We analyze the regret of Min-Width and conduct\npseudo-synthetic and fully synthetic experiments to study the performance of\ndifferent levels of information sharing. Our results confirm that the gains to\nmodeling agent heterogeneity tend to be greater when the sensitivities are more\nvaried across agents, while combining more information does not always improve\nperformance.\n","authors":["Lucia Gordon","Esther Rolf","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2408.03405v1.pdf","comment":"19 pages, 6 figures, to be published in ECAI 2024"},{"id":"http://arxiv.org/abs/2408.03404v1","updated":"2024-08-06T18:55:31Z","published":"2024-08-06T18:55:31Z","title":"Set2Seq Transformer: Learning Permutation Aware Set Representations of\n  Artistic Sequences","summary":"  We propose Set2Seq Transformer, a novel sequential multiple instance\narchitecture, that learns to rank permutation aware set representations of\nsequences. First, we illustrate that learning temporal position-aware\nrepresentations of discrete timesteps can greatly improve static visual\nmultiple instance learning methods that do not regard temporality and\nconcentrate almost exclusively on visual content analysis. We further\ndemonstrate the significant advantages of end-to-end sequential multiple\ninstance learning, integrating visual content and temporal information in a\nmultimodal manner. As application we focus on fine art analysis related tasks.\nTo that end, we show that our Set2Seq Transformer can leverage visual set and\ntemporal position-aware representations for modelling visual artists' oeuvres\nfor predicting artistic success. Finally, through extensive quantitative and\nqualitative evaluation using a novel dataset, WikiArt-Seq2Rank, and a visual\nlearning-to-rank downstream task, we show that our Set2Seq Transformer captures\nessential temporal information improving the performance of strong static and\nsequential multiple instance learning methods for predicting artistic success.\n","authors":["Athanasios Efthymiou","Stevan Rudinac","Monika Kackovic","Nachoem Wijnberg","Marcel Worring"],"pdf_url":"https://arxiv.org/pdf/2408.03404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03400v1","updated":"2024-08-06T18:52:17Z","published":"2024-08-06T18:52:17Z","title":"Attacks and Defenses for Generative Diffusion Models: A Comprehensive\n  Survey","summary":"  Diffusion models (DMs) have achieved state-of-the-art performance on various\ngenerative tasks such as image synthesis, text-to-image, and text-guided\nimage-to-image generation. However, the more powerful the DMs, the more harmful\nthey potentially are. Recent studies have shown that DMs are prone to a wide\nrange of attacks, including adversarial attacks, membership inference, backdoor\ninjection, and various multi-modal threats. Since numerous pre-trained DMs are\npublished widely on the Internet, potential threats from these attacks are\nespecially detrimental to the society, making DM-related security a worth\ninvestigating topic. Therefore, in this paper, we conduct a comprehensive\nsurvey on the security aspect of DMs, focusing on various attack and defense\nmethods for DMs. First, we present crucial knowledge of DMs with five main\ntypes of DMs, including denoising diffusion probabilistic models, denoising\ndiffusion implicit models, noise conditioned score networks, stochastic\ndifferential equations, and multi-modal conditional DMs. We further survey a\nvariety of recent studies investigating different types of attacks that exploit\nthe vulnerabilities of DMs. Then, we thoroughly review potential\ncountermeasures to mitigate each of the presented threats. Finally, we discuss\nopen challenges of DM-related security and envision certain research directions\nfor this topic.\n","authors":["Vu Tuan Truong","Luan Ba Dang","Long Bao Le"],"pdf_url":"https://arxiv.org/pdf/2408.03400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03399v1","updated":"2024-08-06T18:52:15Z","published":"2024-08-06T18:52:15Z","title":"RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting\n  Algorithms","summary":"  We introduce the Robustness of Hierarchically Organized Time Series (RHiOTS)\nframework, designed to assess the robustness of hierarchical time series\nforecasting models and algorithms on real-world datasets. Hierarchical time\nseries, where lower-level forecasts must sum to upper-level ones, are prevalent\nin various contexts, such as retail sales across countries. Current empirical\nevaluations of forecasting methods are often limited to a small set of\nbenchmark datasets, offering a narrow view of algorithm behavior. RHiOTS\naddresses this gap by systematically altering existing datasets and modifying\nthe characteristics of individual series and their interrelations. It uses a\nset of parameterizable transformations to simulate those changes in the data\ndistribution. Additionally, RHiOTS incorporates an innovative visualization\ncomponent, turning complex, multidimensional robustness evaluation results into\nintuitive, easily interpretable visuals. This approach allows an in-depth\nanalysis of algorithm and model behavior under diverse conditions. We\nillustrate the use of RHiOTS by analyzing the predictive performance of several\nalgorithms. Our findings show that traditional statistical methods are more\nrobust than state-of-the-art deep learning algorithms, except when the\ntransformation effect is highly disruptive. Furthermore, we found no\nsignificant differences in the robustness of the algorithms when applying\nspecific reconciliation methods, such as MinT. RHiOTS provides researchers with\na comprehensive tool for understanding the nuanced behavior of forecasting\nalgorithms, offering a more reliable basis for selecting the most appropriate\nmethod for a given problem.\n","authors":["Luis Roque","Carlos Soares","Lus Torgo"],"pdf_url":"https://arxiv.org/pdf/2408.03399v1.pdf","comment":"Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining (KDD '24), August 25--29, 2024, Barcelona, Spain"},{"id":"http://arxiv.org/abs/2408.03397v1","updated":"2024-08-06T18:48:01Z","published":"2024-08-06T18:48:01Z","title":"HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for\n  Transformer Acceleration","summary":"  Transformers have revolutionized deep learning and generative modeling to\nenable unprecedented advancements in natural language processing tasks and\nbeyond. However, designing hardware accelerators for executing transformer\nmodels is challenging due to the wide variety of computing kernels involved in\nthe transformer architecture. Existing accelerators are either inadequate to\naccelerate end-to-end transformer models or suffer notable thermal limitations.\nIn this paper, we propose the design of a three-dimensional heterogeneous\narchitecture referred to as HeTraX specifically optimized to accelerate\nend-to-end transformer models. HeTraX employs hardware resources aligned with\nthe computational kernels of transformers and optimizes both performance and\nenergy. Experimental results show that HeTraX outperforms existing\nstate-of-the-art by up to 5.6x in speedup and improves EDP by 14.5x while\nensuring thermally feasibility.\n","authors":["Pratyush Dhingra","Janardhan Rao Doppa","Partha Pratim Pande"],"pdf_url":"https://arxiv.org/pdf/2408.03397v1.pdf","comment":"Presented at ACM/IEEE International Symposium on Low Power\n  Electronics and Design (ISLPED-24)"},{"id":"http://arxiv.org/abs/2408.03388v1","updated":"2024-08-06T18:18:37Z","published":"2024-08-06T18:18:37Z","title":"A Non-negative VAE:the Generalized Gamma Belief Network","summary":"  The gamma belief network (GBN), often regarded as a deep topic model, has\ndemonstrated its potential for uncovering multi-layer interpretable latent\nrepresentations in text data. Its notable capability to acquire interpretable\nlatent factors is partially attributed to sparse and non-negative\ngamma-distributed latent variables. However, the existing GBN and its\nvariations are constrained by the linear generative model, thereby limiting\ntheir expressiveness and applicability. To address this limitation, we\nintroduce the generalized gamma belief network (Generalized GBN) in this paper,\nwhich extends the original linear generative model to a more expressive\nnon-linear generative model. Since the parameters of the Generalized GBN no\nlonger possess an analytic conditional posterior, we further propose an\nupward-downward Weibull inference network to approximate the posterior\ndistribution of the latent variables. The parameters of both the generative\nmodel and the inference network are jointly trained within the variational\ninference framework. Finally, we conduct comprehensive experiments on both\nexpressivity and disentangled representation learning tasks to evaluate the\nperformance of the Generalized GBN against state-of-the-art Gaussian\nvariational autoencoders serving as baselines.\n","authors":["Zhibin Duan","Tiansheng Wen","Muyao Wang","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07346v3","updated":"2024-08-06T18:14:17Z","published":"2024-07-10T03:52:53Z","title":"INSIGHT: Universal Neural Simulator for Analog Circuits Harnessing\n  Autoregressive Transformers","summary":"  Analog front-end design heavily relies on specialized human expertise and\ncostly trial-and-error simulations, which motivated many prior works on analog\ndesign automation. However, efficient and effective exploration of the vast and\ncomplex design space remains constrained by the time-consuming nature of SPICE\nsimulations, making effective design automation a challenging endeavor. In this\npaper, we introduce INSIGHT, a GPU-powered, technology-agnostic, effective\nuniversal neural simulator in the analog front-end design automation loop.\nINSIGHT accurately predicts the performance metrics of analog circuits across\nvarious technologies with just a few microseconds of inference time. Notably,\nits autoregressive capabilities enable INSIGHT to accurately predict\nsimulation-costly critical transient specifications leveraging less expensive\nperformance metric information. The low cost and high fidelity feature make\nINSIGHT a good substitute for standard simulators in analog front-end\noptimization frameworks. INSIGHT is compatible with any optimization framework,\nfacilitating enhanced design space exploration for sample efficiency through\nsophisticated offline learning and adaptation techniques. Our experiments\ndemonstrate that INSIGHT-M, a model-based batch reinforcement learning sizing\nframework with INSIGHT as the accurate surrogate, only requires < 20 real-time\nsimulations with 100-1000x lower simulation costs and significant speedup over\nexisting sizing methods.\n","authors":["Souradip Poddar","Youngmin Oh","Yao Lai","Hanqing Zhu","Bosun Hwang","David Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2407.07346v3.pdf","comment":null}]},"2024-08-04T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.02164v1","updated":"2024-08-04T23:21:46Z","published":"2024-08-04T23:21:46Z","title":"Rethinking Affect Analysis: A Protocol for Ensuring Fairness and\n  Consistency","summary":"  Evaluating affect analysis methods presents challenges due to inconsistencies\nin database partitioning and evaluation protocols, leading to unfair and biased\nresults. Previous studies claim continuous performance improvements, but our\nfindings challenge such assertions. Using these insights, we propose a unified\nprotocol for database partitioning that ensures fairness and comparability. We\nprovide detailed demographic annotations (in terms of race, gender and age),\nevaluation metrics, and a common framework for expression recognition, action\nunit detection and valence-arousal estimation. We also rerun the methods with\nthe new protocol and introduce a new leaderboards to encourage future research\nin affect recognition with a fairer comparison. Our annotations, code, and\npre-trained models are available on\n\\hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.\n","authors":["Guanyu Hu","Dimitrios Kollias","Eleni Papadopoulou","Paraskevi Tzouveli","Jie Wei","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02164v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.06841"},{"id":"http://arxiv.org/abs/2408.02157v1","updated":"2024-08-04T22:23:10Z","published":"2024-08-04T22:23:10Z","title":"PanoFree: Tuning-Free Holistic Multi-view Image Generation with\n  Cross-view Self-Guidance","summary":"  Immersive scene generation, notably panorama creation, benefits significantly\nfrom the adaptation of large pre-trained text-to-image (T2I) models for\nmulti-view image generation. Due to the high cost of acquiring multi-view\nimages, tuning-free generation is preferred. However, existing methods are\neither limited to simple correspondences or require extensive fine-tuning to\ncapture complex ones. We present PanoFree, a novel method for tuning-free\nmulti-view image generation that supports an extensive array of\ncorrespondences. PanoFree sequentially generates multi-view images using\niterative warping and inpainting, addressing the key issues of inconsistency\nand artifacts from error accumulation without the need for fine-tuning. It\nimproves error accumulation by enhancing cross-view awareness and refines the\nwarping and inpainting processes via cross-view guidance, risky area estimation\nand erasing, and symmetric bidirectional guided generation for loop closure,\nalongside guidance-based semantic and density control for scene structure\npreservation. In experiments on Planar, 360{\\deg}, and Full Spherical\nPanoramas, PanoFree demonstrates significant error reduction, improves global\nconsistency, and boosts image quality without extra fine-tuning. Compared to\nexisting methods, PanoFree is up to 5x more efficient in time and 3x more\nefficient in GPU memory usage, and maintains superior diversity of results (2x\nbetter in our user study). PanoFree offers a viable alternative to costly\nfine-tuning or the use of additional pre-trained models. Project website at\nhttps://panofree.github.io/.\n","authors":["Aoming Liu","Zhong Li","Zhang Chen","Nannan Li","Yi Xu","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2408.02157v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.01996v3","updated":"2024-08-04T21:56:57Z","published":"2024-07-02T07:10:10Z","title":"ViG-Bias: Visually Grounded Bias Discovery and Mitigation","summary":"  The proliferation of machine learning models in critical decision making\nprocesses has underscored the need for bias discovery and mitigation\nstrategies. Identifying the reasons behind a biased system is not\nstraightforward, since in many occasions they are associated with hidden\nspurious correlations which are not easy to spot. Standard approaches rely on\nbias audits performed by analyzing model performance in pre-defined subgroups\nof data samples, usually characterized by common attributes like gender or\nethnicity when it comes to people, or other specific attributes defining\nsemantically coherent groups of images. However, it is not always possible to\nknow a-priori the specific attributes defining the failure modes of visual\nrecognition systems. Recent approaches propose to discover these groups by\nleveraging large vision language models, which enable the extraction of\ncross-modal embeddings and the generation of textual descriptions to\ncharacterize the subgroups where a certain model is underperforming. In this\nwork, we argue that incorporating visual explanations (e.g. heatmaps generated\nvia GradCAM or other approaches) can boost the performance of such bias\ndiscovery and mitigation frameworks. To this end, we introduce Visually\nGrounded Bias Discovery and Mitigation (ViG-Bias), a simple yet effective\ntechnique which can be integrated to a variety of existing frameworks to\nimprove both, discovery and mitigation performance. Our comprehensive\nevaluation shows that incorporating visual explanations enhances existing\ntechniques like DOMINO, FACTS and Bias-to-Text, across several challenging\ndatasets, including CelebA, Waterbirds, and NICO++.\n","authors":["Badr-Eddine Marani","Mohamed Hanini","Nihitha Malayarukil","Stergios Christodoulidis","Maria Vakalopoulou","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2407.01996v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02146v1","updated":"2024-08-04T21:09:09Z","published":"2024-08-04T21:09:09Z","title":"Video-based Pedestrian and Vehicle Traffic Analysis During Football\n  Games","summary":"  This paper utilizes video analytics to study pedestrian and vehicle traffic\nbehavior, focusing on analyzing traffic patterns during football gamedays. The\nUniversity of Florida (UF) hosts six to seven home football games on Saturdays\nduring the college football season, attracting significant pedestrian activity.\nThrough video analytics, this study provides valuable insights into the impact\nof these events on traffic volumes and safety at intersections. Comparing\npedestrian and vehicle activities on gamedays versus non-gamedays reveals\ndiffering patterns. For example, pedestrian volume substantially increases\nduring gamedays, which is positively correlated with the probability of the\naway team winning. This correlation is likely because fans of the home team\nenjoy watching difficult games. Win probabilities as an early predictor of\npedestrian volumes at intersections can be a tool to help traffic professionals\nanticipate traffic management needs. Pedestrian-to-vehicle (P2V) conflicts\nnotably increase on gamedays, particularly a few hours before games start.\nAddressing this, a \"Barnes Dance\" movement phase within the intersection is\nrecommended. Law enforcement presence during high-activity gamedays can help\nensure pedestrian compliance and enhance safety. In contrast, we identified\nthat vehicle-to-vehicle (V2V) conflicts generally do not increase on gamedays\nand may even decrease due to heightened driver caution.\n","authors":["Jacques P. Fleischer","Ryan Pallack","Ahan Mishra","Gustavo Riente de Andrade","Subhadipto Poddar","Emmanuel Posadas","Robert Schenck","Tania Banerjee","Anand Rangarajan","Sanjay Ranka"],"pdf_url":"https://arxiv.org/pdf/2408.02146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02140v1","updated":"2024-08-04T20:38:45Z","published":"2024-08-04T20:38:45Z","title":"VidModEx: Interpretable and Efficient Black Box Model Extraction for\n  High-Dimensional Spaces","summary":"  In the domain of black-box model extraction, conventional methods reliant on\nsoft labels or surrogate datasets struggle with scaling to high-dimensional\ninput spaces and managing the complexity of an extensive array of interrelated\nclasses. In this work, we present a novel approach that utilizes SHAP (SHapley\nAdditive exPlanations) to enhance synthetic data generation. SHAP quantifies\nthe individual contributions of each input feature towards the victim model's\noutput, facilitating the optimization of an energy-based GAN towards a\ndesirable output. This method significantly boosts performance, achieving a\n16.45% increase in the accuracy of image classification models and extending to\nvideo classification models with an average improvement of 26.11% and a maximum\nof 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics\n600, and Something-Something V2. We further demonstrate the effectiveness and\npractical utility of our method under various scenarios, including the\navailability of top-k prediction probabilities, top-k prediction labels, and\ntop-1 labels.\n","authors":["Somnath Sendhil Kumar","Yuvaraj Govindarajulu","Pavan Kulkarni","Manojkumar Parmar"],"pdf_url":"https://arxiv.org/pdf/2408.02140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02138v1","updated":"2024-08-04T20:35:33Z","published":"2024-08-04T20:35:33Z","title":"RICA^2: Rubric-Informed, Calibrated Assessment of Actions","summary":"  The ability to quantify how well an action is carried out, also known as\naction quality assessment (AQA), has attracted recent interest in the vision\ncommunity. Unfortunately, prior methods often ignore the score rubric used by\nhuman experts and fall short of quantifying the uncertainty of the model\nprediction. To bridge the gap, we present RICA^2 - a deep probabilistic model\nthat integrates score rubric and accounts for prediction uncertainty for AQA.\nCentral to our method lies in stochastic embeddings of action steps, defined on\na graph structure that encodes the score rubric. The embeddings spread\nprobabilistic density in the latent space and allow our method to represent\nmodel uncertainty. The graph encodes the scoring criteria, based on which the\nquality scores can be decoded. We demonstrate that our method establishes new\nstate of the art on public benchmarks, including FineDiving, MTL-AQA, and\nJIGSAWS, with superior performance in score prediction and uncertainty\ncalibration. Our code is available at https://abrarmajeedi.github.io/rica2_aqa/\n","authors":["Abrar Majeedi","Viswanatha Reddy Gajjala","Satya Sai Srinath Namburi GNVV","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2408.02138v1.pdf","comment":"Accepted at European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2408.02135v1","updated":"2024-08-04T20:12:38Z","published":"2024-08-04T20:12:38Z","title":"A First Look at Chebyshev-Sobolev Series for Digital Ink","summary":"  Considering digital ink as plane curves provides a valuable framework for\nvarious applications, including signature verification, note-taking, and\nmathematical handwriting recognition. These plane curves can be obtained as\nparameterized pairs of approximating truncated series (x(s), y(s)) determined\nby sampled points. Earlier work has found that representing these truncated\nseries (polynomials) in a Legendre or Legendre-Sobolev basis has a number of\ndesirable properties. These include compact data representation, meaningful\nclustering of like symbols in the vector space of polynomial coefficients,\nlinear separability of classes in this space, and highly efficient calculation\nof variation between curves. In this work, we take a first step at examining\nthe use of Chebyshev-Sobolev series for symbol recognition. The early\nindication is that this representation may be superior to Legendre-Sobolev\nrepresentation for some purposes.\n","authors":["Deepak Singh Kalhan","Stephen M. Watt"],"pdf_url":"https://arxiv.org/pdf/2408.02135v1.pdf","comment":"Accepted at MathUI 2024"},{"id":"http://arxiv.org/abs/2408.02123v1","updated":"2024-08-04T19:37:30Z","published":"2024-08-04T19:37:30Z","title":"FovEx: Human-inspired Explanations for Vision Transformers and\n  Convolutional Neural Networks","summary":"  Explainability in artificial intelligence (XAI) remains a crucial aspect for\nfostering trust and understanding in machine learning models. Current visual\nexplanation techniques, such as gradient-based or class-activation-based\nmethods, often exhibit a strong dependence on specific model architectures.\nConversely, perturbation-based methods, despite being model-agnostic, are\ncomputationally expensive as they require evaluating models on a large number\nof forward passes. In this work, we introduce Foveation-based Explanations\n(FovEx), a novel XAI method inspired by human vision. FovEx seamlessly\nintegrates biologically inspired perturbations by iteratively creating foveated\nrenderings of the image and combines them with gradient-based visual\nexplorations to determine locations of interest efficiently. These locations\nare selected to maximize the performance of the model to be explained with\nrespect to the downstream task and then combined to generate an attribution\nmap. We provide a thorough evaluation with qualitative and quantitative\nassessments on established benchmarks. Our method achieves state-of-the-art\nperformance on both transformers (on 4 out of 5 metrics) and convolutional\nmodels (on 3 out of 5 metrics), demonstrating its versatility among various\narchitectures. Furthermore, we show the alignment between the explanation map\nproduced by FovEx and human gaze patterns (+14\\% in NSS compared to RISE,\n+203\\% in NSS compared to GradCAM). This comparison enhances our confidence in\nFovEx's ability to close the interpretation gap between humans and machines.\n","authors":["Mahadev Prasad Panda","Matteo Tiezzi","Martina Vilas","Gemma Roig","Bjoern M. Eskofier","Dario Zanca"],"pdf_url":"https://arxiv.org/pdf/2408.02123v1.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2404.16845v2","updated":"2024-08-04T18:51:59Z","published":"2024-02-14T14:02:04Z","title":"HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring\n  Unconstrained Photo Collections","summary":"  Internet image collections containing photos captured by crowds of\nphotographers show promise for enabling digital exploration of large-scale\ntourist landmarks. However, prior works focus primarily on geometric\nreconstruction and visualization, neglecting the key role of language in\nproviding a semantic interface for navigation and fine-grained understanding.\nIn constrained 3D domains, recent methods have leveraged vision-and-language\nmodels as a strong prior of 2D visual semantics. While these models display an\nexcellent understanding of broad visual semantics, they struggle with\nunconstrained photo collections depicting such tourist landmarks, as they lack\nexpert knowledge of the architectural domain. In this work, we present a\nlocalization system that connects neural representations of scenes depicting\nlarge-scale landmarks with text describing a semantic region within the scene,\nby harnessing the power of SOTA vision-and-language models with adaptations for\nunderstanding landmark scene semantics. To bolster such models with\nfine-grained knowledge, we leverage large-scale Internet data containing images\nof similar landmarks along with weakly-related textual information. Our\napproach is built upon the premise that images physically grounded in space can\nprovide a powerful supervision signal for localizing new concepts, whose\nsemantics may be unlocked from Internet textual metadata with large language\nmodels. We use correspondences between views of scenes to bootstrap spatial\nunderstanding of these semantics, providing guidance for 3D-compatible\nsegmentation that ultimately lifts to a volumetric scene representation. Our\nresults show that HaLo-NeRF can accurately localize a variety of semantic\nconcepts related to architectural landmarks, surpassing the results of other 3D\nmodels as well as strong 2D segmentation baselines. Our project page is at\nhttps://tau-vailab.github.io/HaLo-NeRF/.\n","authors":["Chen Dudai","Morris Alper","Hana Bezalel","Rana Hanocka","Itai Lang","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2404.16845v2.pdf","comment":"Eurographics 2024. Project page:\n  https://tau-vailab.github.io/HaLo-NeRF/"},{"id":"http://arxiv.org/abs/2408.02110v1","updated":"2024-08-04T18:41:35Z","published":"2024-08-04T18:41:35Z","title":"AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction\n  from Sparse Multi-view Videos","summary":"  Despite progress in human motion capture, existing multi-view methods often\nface challenges in estimating the 3D pose and shape of multiple closely\ninteracting people. This difficulty arises from reliance on accurate 2D joint\nestimations, which are hard to obtain due to occlusions and body contact when\npeople are in close interaction. To address this, we propose a novel method\nleveraging the personalized implicit neural avatar of each individual as a\nprior, which significantly improves the robustness and precision of this\nchallenging pose estimation task. Concretely, the avatars are efficiently\nreconstructed via layered volume rendering from sparse multi-view videos. The\nreconstructed avatar prior allows for the direct optimization of 3D poses based\non color and silhouette rendering loss, bypassing the issues associated with\nnoisy 2D detections. To handle interpenetration, we propose a collision loss on\nthe overlapping shape regions of avatars to add penetration constraints.\nMoreover, both 3D poses and avatars are optimized in an alternating manner. Our\nexperimental results demonstrate state-of-the-art performance on several public\ndatasets.\n","authors":["Feichi Lu","Zijian Dong","Jie Song","Otmar Hilliges"],"pdf_url":"https://arxiv.org/pdf/2408.02110v1.pdf","comment":"Project Page: https://feichilu.github.io/AvatarPose/"},{"id":"http://arxiv.org/abs/2408.02100v1","updated":"2024-08-04T17:57:23Z","published":"2024-08-04T17:57:23Z","title":"View-consistent Object Removal in Radiance Fields","summary":"  Radiance Fields (RFs) have emerged as a crucial technology for 3D scene\nrepresentation, enabling the synthesis of novel views with remarkable realism.\nHowever, as RFs become more widely used, the need for effective editing\ntechniques that maintain coherence across different perspectives becomes\nevident. Current methods primarily depend on per-frame 2D image inpainting,\nwhich often fails to maintain consistency across views, thus compromising the\nrealism of edited RF scenes. In this work, we introduce a novel RF editing\npipeline that significantly enhances consistency by requiring the inpainting of\nonly a single reference image. This image is then projected across multiple\nviews using a depth-based approach, effectively reducing the inconsistencies\nobserved with per-frame inpainting. However, projections typically assume\nphotometric consistency across views, which is often impractical in real-world\nsettings. To accommodate realistic variations in lighting and viewpoint, our\npipeline adjusts the appearance of the projected views by generating multiple\ndirectional variants of the inpainted image, thereby adapting to different\nphotometric conditions. Additionally, we present an effective and robust\nmulti-view object segmentation approach as a valuable byproduct of our\npipeline. Extensive experiments demonstrate that our method significantly\nsurpasses existing frameworks in maintaining content consistency across views\nand enhancing visual quality. More results are available at\nhttps://vulab-ai.github.io/View-consistent_Object_Removal_in_Radiance_Fields.\n","authors":["Yiren Lu","Jing Ma","Yu Yin"],"pdf_url":"https://arxiv.org/pdf/2408.02100v1.pdf","comment":"Accepted to ACM Multimedia (MM) 2024. Project website is accessible\n  at\n  https://vulab-ai.github.io/View-consistent_Object_Removal_in_Radiance_Fields"},{"id":"http://arxiv.org/abs/2402.16033v2","updated":"2024-08-04T17:37:02Z","published":"2024-02-25T09:09:30Z","title":"Exploiting Regional Information Transformer for Single Image Deraining","summary":"  Transformer-based Single Image Deraining (SID) methods have achieved\nremarkable success, primarily attributed to their robust capability in\ncapturing long-range interactions. However, we've noticed that current methods\nhandle rain-affected and unaffected regions concurrently, overlooking the\ndisparities between these areas, resulting in confusion between rain streaks\nand background parts, and inabilities to obtain effective interactions,\nultimately resulting in suboptimal deraining outcomes. To address the above\nissue, we introduce the Region Transformer (Regformer), a novel SID method that\nunderlines the importance of independently processing rain-affected and\nunaffected regions while considering their combined impact for high-quality\nimage reconstruction. The crux of our method is the innovative Region\nTransformer Block (RTB), which integrates a Region Masked Attention (RMA)\nmechanism and a Mixed Gate Forward Block (MGFB). Our RTB is used for attention\nselection of rain-affected and unaffected regions and local modeling of mixed\nscales. The RMA generates attention maps tailored to these two regions and\ntheir interactions, enabling our model to capture comprehensive features\nessential for rain removal. To better recover high-frequency textures and\ncapture more local details, we develop the MGFB as a compensation module to\ncomplete local mixed scale modeling. Extensive experiments demonstrate that our\nmodel reaches state-of-the-art performance, significantly improving the image\nderaining quality. Our code and trained models are publicly available.\n","authors":["Baiang Li","Zhao Zhang","Huan Zheng","Xiaogang Xu","Yanyan Wei","Jingyi Zhang","Jicong Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2402.16033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02091v1","updated":"2024-08-04T17:00:37Z","published":"2024-08-04T17:00:37Z","title":"Past Movements-Guided Motion Representation Learning for Human Motion\n  Prediction","summary":"  Human motion prediction based on 3D skeleton is a significant challenge in\ncomputer vision, primarily focusing on the effective representation of motion.\nIn this paper, we propose a self-supervised learning framework designed to\nenhance motion representation. This framework consists of two stages: first,\nthe network is pretrained through the self-reconstruction of past sequences,\nand the guided reconstruction of future sequences based on past movements. We\ndesign a velocity-based mask strategy to focus on the joints with large-scale\nmoving. Subsequently, the pretrained network undergoes finetuning for specific\ntasks. Self-reconstruction, guided by patterns of past motion, substantially\nimproves the model's ability to represent the spatiotemporal relationships\namong joints but also captures the latent relationships between past and future\nsequences. This capability is crucial for motion prediction tasks that solely\ndepend on historical motion data. By employing this straightforward yet\neffective training paradigm, our method outperforms existing\n\\textit{state-of-the-art} methods, reducing the average prediction errors by\n8.8\\% across Human3.6M, 3DPW, and AMASS datasets. The code is available at\nhttps://github.com/JunyuShi02/PMG-MRL.\n","authors":["Junyu Shi","Baoxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02091v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02088v1","updated":"2024-08-04T16:54:49Z","published":"2024-08-04T16:54:49Z","title":"KAN-RCBEVDepth: A multi-modal fusion algorithm in object detection for\n  autonomous driving","summary":"  Accurate 3D object detection in autonomous driving is critical yet\nchallenging due to occlusions, varying object scales, and complex urban\nenvironments. This paper introduces the RCBEV-KAN algorithm, a pioneering\nmethod designed to enhance 3D object detection by fusing multimodal sensor data\nfrom cameras, LiDAR, and millimeter-wave radar. Our innovative Bird's Eye View\n(BEV)-based approach, utilizing a Transformer architecture, significantly\nboosts detection precision and efficiency by seamlessly integrating diverse\ndata sources, improving spatial relationship handling, and optimizing\ncomputational processes. Experimental results show that the RCBEV-KAN model\ndemonstrates superior performance across most detection categories, achieving\nhigher Mean Distance AP (0.389 vs. 0.316, a 23% improvement), better ND Score\n(0.484 vs. 0.415, a 17% improvement), and faster Evaluation Time (71.28s, 8%\nfaster). These results indicate that RCBEV-KAN is more accurate, reliable, and\nefficient, making it ideal for dynamic and challenging autonomous driving\nenvironments.\n","authors":["Zhihao Lai","Chuanhao Liu","Shihui Sheng","Zhiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14584v2","updated":"2024-08-04T16:26:10Z","published":"2024-05-23T13:55:11Z","title":"SE3D: A Framework For Saliency Method Evaluation In 3D Imaging","summary":"  For more than a decade, deep learning models have been dominating in various\n2D imaging tasks. Their application is now extending to 3D imaging, with 3D\nConvolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and\nCT scans, with significant implications for fields such as autonomous driving\nand medical imaging. In these critical settings, explaining the model's\ndecisions is fundamental. Despite recent advances in Explainable Artificial\nIntelligence, however, little effort has been devoted to explaining 3D CNNs,\nand many works explain these models via inadequate extensions of 2D saliency\nmethods.\n  A fundamental limitation to the development of 3D saliency methods is the\nlack of a benchmark to quantitatively assess these on 3D data. To address this\nissue, we propose SE3D: a framework for Saliency method Evaluation in 3D\nimaging. We propose modifications to ShapeNet, ScanNet, and BraTS datasets, and\nevaluation metrics to assess saliency methods for 3D CNNs. We evaluate both\nstate-of-the-art saliency methods designed for 3D data and extensions of\npopular 2D saliency methods to 3D. Our experiments show that 3D saliency\nmethods do not provide explanations of sufficient quality, and that there is\nmargin for future improvements and safer applications of 3D CNNs in critical\nfields.\n","authors":["Mariusz Winiewski","Loris Giulivi","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2405.14584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02079v1","updated":"2024-08-04T16:09:46Z","published":"2024-08-04T16:09:46Z","title":"Improving Neural Surface Reconstruction with Feature Priors from\n  Multi-View Image","summary":"  Recent advancements in Neural Surface Reconstruction (NSR) have significantly\nimproved multi-view reconstruction when coupled with volume rendering. However,\nrelying solely on photometric consistency in image space falls short of\naddressing complexities posed by real-world data, including occlusions and\nnon-Lambertian surfaces. To tackle these challenges, we propose an\ninvestigation into feature-level consistent loss, aiming to harness valuable\nfeature priors from diverse pretext visual tasks and overcome current\nlimitations. It is crucial to note the existing gap in determining the most\neffective pretext visual task for enhancing NSR. In this study, we\ncomprehensively explore multi-view feature priors from seven pretext visual\ntasks, comprising thirteen methods. Our main goal is to strengthen NSR training\nby considering a wide range of possibilities. Additionally, we examine the\nimpact of varying feature resolutions and evaluate both pixel-wise and\npatch-wise consistent losses, providing insights into effective strategies for\nimproving NSR performance. By incorporating pre-trained representations from\nMVSFormer and QuadTree, our approach can generate variations of MVS-NeuS and\nMatch-NeuS, respectively. Our results, analyzed on DTU and EPFL datasets,\nreveal that feature priors from image matching and multi-view stereo outperform\nother pretext tasks. Moreover, we discover that extending patch-wise\nphotometric consistency to the feature level surpasses the performance of\npixel-wise approaches. These findings underscore the effectiveness of these\ntechniques in enhancing NSR outcomes.\n","authors":["Xinlin Ren","Chenjie Cao","Yanwei Fu","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2408.02079v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2408.02078v1","updated":"2024-08-04T16:09:04Z","published":"2024-08-04T16:09:04Z","title":"LDFaceNet: Latent Diffusion-based Network for High-Fidelity Deepfake\n  Generation","summary":"  Over the past decade, there has been tremendous progress in the domain of\nsynthetic media generation. This is mainly due to the powerful methods based on\ngenerative adversarial networks (GANs). Very recently, diffusion probabilistic\nmodels, which are inspired by non-equilibrium thermodynamics, have taken the\nspotlight. In the realm of image generation, diffusion models (DMs) have\nexhibited remarkable proficiency in producing both realistic and heterogeneous\nimagery through their stochastic sampling procedure. This paper proposes a\nnovel facial swapping module, termed as LDFaceNet (Latent Diffusion based Face\nSwapping Network), which is based on a guided latent diffusion model that\nutilizes facial segmentation and facial recognition modules for a conditioned\ndenoising process. The model employs a unique loss function to offer\ndirectional guidance to the diffusion process. Notably, LDFaceNet can\nincorporate supplementary facial guidance for desired outcomes without any\nretraining. To the best of our knowledge, this represents the first application\nof the latent diffusion model in the face-swapping task without prior training.\nThe results of this study demonstrate that the proposed method can generate\nextremely realistic and coherent images by leveraging the potential of the\ndiffusion model for facial swapping, thereby yielding superior visual outcomes\nand greater diversity.\n","authors":["Dwij Mehta","Aditya Mehta","Pratik Narang"],"pdf_url":"https://arxiv.org/pdf/2408.02078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19735v2","updated":"2024-08-04T15:38:41Z","published":"2024-05-30T06:31:03Z","title":"Twin Deformable Point Convolutions for Point Cloud Semantic Segmentation\n  in Remote Sensing Scenes","summary":"  Thanks to the application of deep learning technology in point cloud\nprocessing of the remote sensing field, point cloud segmentation has become a\nresearch hotspot in recent years, which can be applied to real-world 3D, smart\ncities, and other fields. Although existing solutions have made unprecedented\nprogress, they ignore the inherent characteristics of point clouds in remote\nsensing fields that are strictly arranged according to latitude, longitude, and\naltitude, which brings great convenience to the segmentation of point clouds in\nremote sensing fields. To consider this property cleverly, we propose novel\nconvolution operators, termed Twin Deformable point Convolutions (TDConvs),\nwhich aim to achieve adaptive feature learning by learning deformable sampling\npoints in the latitude-longitude plane and altitude direction, respectively.\nFirst, to model the characteristics of the latitude-longitude plane, we propose\na Cylinder-wise Deformable point Convolution (CyDConv) operator, which\ngenerates a two-dimensional cylinder map by constructing a cylinder-like grid\nin the latitude-longitude direction. Furthermore, to better integrate the\nfeatures of the latitude-longitude plane and the spatial geometric features, we\nperform a multi-scale fusion of the extracted latitude-longitude features and\nspatial geometric features, and realize it through the aggregation of adjacent\npoint features of different scales. In addition, a Sphere-wise Deformable point\nConvolution (SpDConv) operator is introduced to adaptively offset the sampling\npoints in three-dimensional space by constructing a sphere grid structure,\naiming at modeling the characteristics in the altitude direction. Experiments\non existing popular benchmarks conclude that our TDConvs achieve the best\nsegmentation performance, surpassing the existing state-of-the-art methods.\n","authors":["Yong-Qiang Mao","Hanbo Bi","Xuexue Li","Kaiqiang Chen","Zhirui Wang","Xian Sun","Kun Fu"],"pdf_url":"https://arxiv.org/pdf/2405.19735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16252v2","updated":"2024-08-04T15:27:28Z","published":"2024-07-23T07:40:41Z","title":"LawLuo: A Chinese Law Firm Co-run by LLM Agents","summary":"  Large Language Models (LLMs) demonstrate substantial potential in delivering\nlegal consultation services to users without a legal background, attributed to\ntheir superior text comprehension and generation capabilities. Nonetheless,\nexisting Chinese legal LLMs limit interaction to a single model-user dialogue,\nunlike the collaborative consultations typical of law firms, where multiple\nstaff members contribute to a single consultation. This limitation prevents an\nauthentic consultation experience. Additionally, extant Chinese legal LLMs\nsuffer from critical limitations: (1) insufficient control over the quality of\ninstruction fine-tuning data; (2) increased model hallucination resulting from\nusers' ambiguous queries; and (3) a reduction in the model's ability to follow\ninstructions over multiple dialogue turns. In response to these challenges, we\npropose a novel legal dialogue framework that leverages the collaborative\ncapabilities of multiple LLM agents, termed LawLuo. This framework encompasses\nfour agents: a receptionist, a lawyer, a secretary, and a boss, each\nresponsible for different functionalities, collaboratively providing a\ncomprehensive legal consultation to users. Additionally, we constructed two\nhigh-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned\nChatGLM-3-6b using these datasets. We propose a legal query clarification\nalgorithm called ToLC. Experimental results demonstrate that LawLuo outperforms\nbaseline LLMs, including GPT-4, across three dimensions: lawyer-like language\nstyle, the usefulness of legal advice, and the accuracy of legal knowledge. Our\ncode and datasets are available at https://github.com/NEFUJing/LawLuo.\n","authors":["Jingyun Sun","Chengxiao Dai","Zhongze Luo","Yangbo Chang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16252v2.pdf","comment":"11 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.02061v1","updated":"2024-08-04T15:20:39Z","published":"2024-08-04T15:20:39Z","title":"ParkingE2E: Camera-based End-to-end Parking Network, from Images to\n  Planning","summary":"  Autonomous parking is a crucial task in the intelligent driving field.\nTraditional parking algorithms are usually implemented using rule-based\nschemes. However, these methods are less effective in complex parking scenarios\ndue to the intricate design of the algorithms. In contrast,\nneural-network-based methods tend to be more intuitive and versatile than the\nrule-based methods. By collecting a large number of expert parking trajectory\ndata and emulating human strategy via learning-based methods, the parking task\ncan be effectively addressed. In this paper, we employ imitation learning to\nperform end-to-end planning from RGB images to path planning by imitating human\ndriving trajectories. The proposed end-to-end approach utilizes a target query\nencoder to fuse images and target features, and a transformer-based decoder to\nautoregressively predict future waypoints. We conducted extensive experiments\nin real-world scenarios, and the results demonstrate that the proposed method\nachieved an average parking success rate of 87.8% across four different\nreal-world garages. Real-vehicle experiments further validate the feasibility\nand effectiveness of the method proposed in this paper.\n","authors":["Changze Li","Ziheng Ji","Zhe Chen","Tong Qin","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02054v1","updated":"2024-08-04T15:01:23Z","published":"2024-08-04T15:01:23Z","title":"Step Saver: Predicting Minimum Denoising Steps for Diffusion Model Image\n  Generation","summary":"  In this paper, we introduce an innovative NLP model specifically fine-tuned\nto determine the minimal number of denoising steps required for any given text\nprompt. This advanced model serves as a real-time tool that recommends the\nideal denoise steps for generating high-quality images efficiently. It is\ndesigned to work seamlessly with the Diffusion model, ensuring that images are\nproduced with superior quality in the shortest possible time. Although our\nexplanation focuses on the DDIM scheduler, the methodology is adaptable and can\nbe applied to various other schedulers like Euler, Euler Ancestral, Heun, DPM2\nKarras, UniPC, and more. This model allows our customers to conserve costly\ncomputing resources by executing the fewest necessary denoising steps to\nachieve optimal quality in the produced images.\n","authors":["Jean Yu","Haim Barad"],"pdf_url":"https://arxiv.org/pdf/2408.02054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02053v1","updated":"2024-08-04T15:01:16Z","published":"2024-08-04T15:01:16Z","title":"PanicleNeRF: low-cost, high-precision in-field phenotypingof rice\n  panicles with smartphone","summary":"  The rice panicle traits significantly influence grain yield, making them a\nprimary target for rice phenotyping studies. However, most existing techniques\nare limited to controlled indoor environments and difficult to capture the rice\npanicle traits under natural growth conditions. Here, we developed PanicleNeRF,\na novel method that enables high-precision and low-cost reconstruction of rice\npanicle three-dimensional (3D) models in the field using smartphone. The\nproposed method combined the large model Segment Anything Model (SAM) and the\nsmall model You Only Look Once version 8 (YOLOv8) to achieve high-precision\nsegmentation of rice panicle images. The NeRF technique was then employed for\n3D reconstruction using the images with 2D segmentation. Finally, the resulting\npoint clouds are processed to successfully extract panicle traits. The results\nshow that PanicleNeRF effectively addressed the 2D image segmentation task,\nachieving a mean F1 Score of 86.9% and a mean Intersection over Union (IoU) of\n79.8%, with nearly double the boundary overlap (BO) performance compared to\nYOLOv8. As for point cloud quality, PanicleNeRF significantly outperformed\ntraditional SfM-MVS (structure-from-motion and multi-view stereo) methods, such\nas COLMAP and Metashape. The panicle length was then accurately extracted with\nthe rRMSE of 2.94% for indica and 1.75% for japonica rice. The panicle volume\nestimated from 3D point clouds strongly correlated with the grain number (R2 =\n0.85 for indica and 0.82 for japonica) and grain mass (0.80 for indica and 0.76\nfor japonica). This method provides a low-cost solution for high-throughput\nin-field phenotyping of rice panicles, accelerating the efficiency of rice\nbreeding.\n","authors":["Xin Yang","Xuqi Lu","Pengyao Xie","Ziyue Guo","Hui Fang","Haowei Fu","Xiaochun Hu","Zhenbiao Sun","Haiyan Cen"],"pdf_url":"https://arxiv.org/pdf/2408.02053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02052v1","updated":"2024-08-04T15:00:22Z","published":"2024-08-04T15:00:22Z","title":"EOL: Transductive Few-Shot Open-Set Recognition by Enhancing Outlier\n  Logits","summary":"  In Few-Shot Learning (FSL), models are trained to recognise unseen objects\nfrom a query set, given a few labelled examples from a support set. In standard\nFSL, models are evaluated on query instances sampled from the same class\ndistribution of the support set. In this work, we explore the more nuanced and\npractical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard\nFSL, OSFSL incorporates unknown classes into the query set, thereby requiring\nthe model not only to classify known classes but also to identify outliers.\nBuilding on the groundwork laid by previous studies, we define a novel\ntransductive inference technique that leverages the InfoMax principle to\nexploit the unlabelled query set. We called our approach the Enhanced Outlier\nLogit (EOL) method. EOL refines class prototype representations through model\ncalibration, effectively balancing the inlier-outlier ratio. This calibration\nenhances pseudo-label accuracy for the query set and improves the optimisation\nobjective within the transductive inference process. We provide a comprehensive\nempirical evaluation demonstrating that EOL consistently surpasses traditional\nmethods, recording performance improvements ranging from approximately $+1.3%$\nto $+6.3%$ across a variety of classification and outlier detection metrics and\nbenchmarks, even in the presence of inlier-outlier imbalance.\n","authors":["Mateusz Ochal","Massimiliano Patacchiola","Malik Boudiaf","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02052v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2408.02049v1","updated":"2024-08-04T14:57:28Z","published":"2024-08-04T14:57:28Z","title":"3D Single-object Tracking in Point Clouds with High Temporal Variation","summary":"  The high temporal variation of the point clouds is the key challenge of 3D\nsingle-object tracking (3D SOT). Existing approaches rely on the assumption\nthat the shape variation of the point clouds and the motion of the objects\nacross neighboring frames are smooth, failing to cope with high temporal\nvariation data. In this paper, we present a novel framework for 3D SOT in point\nclouds with high temporal variation, called HVTrack. HVTrack proposes three\nnovel components to tackle the challenges in the high temporal variation\nscenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud\nshape variations; 2) a Base-Expansion Feature Cross-Attention module to deal\nwith similar object distractions in expanded search areas; 3) a Contextual\nPoint Guided Self-Attention module for suppressing heavy background noise. We\nconstruct a dataset with high temporal variation (KITTI-HV) by setting\ndifferent frame intervals for sampling in the KITTI dataset. On the KITTI-HV\nwith 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker\nCXTracker by 11.3%/15.7% in Success/Precision.\n","authors":["Qiao Wu","Kun Sun","Pei An","Mathieu Salzmann","Yanning Zhang","Jiaqi Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02049v1.pdf","comment":"Accepted by ECCV24"},{"id":"http://arxiv.org/abs/2408.02043v1","updated":"2024-08-04T14:30:14Z","published":"2024-08-04T14:30:14Z","title":"Deep Spectral Methods for Unsupervised Ultrasound Image Interpretation","summary":"  Ultrasound imaging is challenging to interpret due to non-uniform\nintensities, low contrast, and inherent artifacts, necessitating extensive\ntraining for non-specialists. Advanced representation with clear tissue\nstructure separation could greatly assist clinicians in mapping underlying\nanatomy and distinguishing between tissue layers. Decomposing an image into\nsemantically meaningful segments is mainly achieved using supervised\nsegmentation algorithms. Unsupervised methods are beneficial, as acquiring\nlarge labeled datasets is difficult and costly, but despite their advantages,\nthey still need to be explored in ultrasound. This paper proposes a novel\nunsupervised deep learning strategy tailored to ultrasound to obtain easily\ninterpretable tissue separations. We integrate key concepts from unsupervised\ndeep spectral methods, which combine spectral graph theory with deep learning\nmethods. We utilize self-supervised transformer features for spectral\nclustering to generate meaningful segments based on ultrasound-specific metrics\nand shape and positional priors, ensuring semantic consistency across the\ndataset. We evaluate our unsupervised deep learning strategy on three\nultrasound datasets, showcasing qualitative results across anatomical contexts\nwithout label requirements. We also conduct a comparative analysis against\nother clustering algorithms to demonstrate superior segmentation performance,\nboundary preservation, and label consistency.\n","authors":["Oleksandra Tmenova","Yordanka Velikova","Mahdi Saleh","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2408.02043v1.pdf","comment":"Accepted at International Conference on Medical Image Computing and\n  Computer Assisted Intervention, MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.02039v1","updated":"2024-08-04T14:14:54Z","published":"2024-08-04T14:14:54Z","title":"Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly\n  Supervised Semantic Segmentation","summary":"  Recent attention has been devoted to the pursuit of learning semantic\nsegmentation models exclusively from image tags, a paradigm known as\nimage-level Weakly Supervised Semantic Segmentation (WSSS). Existing attempts\nadopt the Class Activation Maps (CAMs) as priors to mine object regions yet\nobserve the imbalanced activation issue, where only the most discriminative\nobject parts are located. In this paper, we argue that the distribution\ndiscrepancy between the discriminative and the non-discriminative parts of\nobjects prevents the model from producing complete and precise pseudo masks as\nground truths. For this purpose, we propose a Pixel-Level Domain Adaptation\n(PLDA) method to encourage the model in learning pixel-wise domain-invariant\nfeatures. Specifically, a multi-head domain classifier trained adversarially\nwith the feature extraction is introduced to promote the emergence of pixel\nfeatures that are invariant with respect to the shift between the source (i.e.,\nthe discriminative object parts) and the target (\\textit{i.e.}, the\nnon-discriminative object parts) domains. In addition, we come up with a\nConfident Pseudo-Supervision strategy to guarantee the discriminative ability\nof each pixel for the segmentation task, which serves as a complement to the\nintra-image domain adversarial training. Our method is conceptually simple,\nintuitive and can be easily integrated into existing WSSS methods. Taking\nseveral strong baseline models as instances, we experimentally demonstrate the\neffectiveness of our approach under a wide range of settings.\n","authors":["Ye Du","Zehua Fu","Qingjie Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02039v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02036v1","updated":"2024-08-04T14:07:14Z","published":"2024-08-04T14:07:14Z","title":"LEGO: Self-Supervised Representation Learning for Scene Text Images","summary":"  In recent years, significant progress has been made in scene text recognition\nby data-driven methods. However, due to the scarcity of annotated real-world\ndata, the training of these methods predominantly relies on synthetic data. The\ndistribution gap between synthetic and real data constrains the further\nperformance improvement of these methods in real-world applications. To tackle\nthis problem, a highly promising approach is to utilize massive amounts of\nunlabeled real data for self-supervised training, which has been widely proven\neffective in many NLP and CV tasks. Nevertheless, generic self-supervised\nmethods are unsuitable for scene text images due to their sequential nature. To\naddress this issue, we propose a Local Explicit and Global Order-aware\nself-supervised representation learning method (LEGO) that accounts for the\ncharacteristics of scene text images. Inspired by the human cognitive process\nof learning words, which involves spelling, reading, and writing, we propose\nthree novel pre-text tasks for LEGO to model sequential, semantic, and\nstructural features, respectively. The entire pre-training process is optimized\nby using a consistent Text Knowledge Codebook. Extensive experiments validate\nthat LEGO outperforms previous scene text self-supervised methods. The\nrecognizer incorporated with our pre-trained model achieves superior or\ncomparable performance compared to state-of-the-art scene text recognition\nmethods on six benchmarks. Furthermore, we demonstrate that LEGO can achieve\nsuperior performance in other text-related tasks.\n","authors":["Yujin Ren","Jiaxin Zhang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2408.02036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02034v1","updated":"2024-08-04T13:55:58Z","published":"2024-08-04T13:55:58Z","title":"Mini-Monkey: Alleviate the Sawtooth Effect by Multi-Scale Adaptive\n  Cropping","summary":"  Recently, there has been significant interest in enhancing the capability of\nmultimodal large language models (MLLMs) to process high-resolution images.\nMost existing methods focus on adopting a cropping strategy to improve the\nability of multimodal large language models to understand image details.\nHowever, this cropping operation inevitably causes the segmentation of objects\nand connected areas, which impairs the MLLM's ability to recognize small or\nirregularly shaped objects or text. This issue is particularly evident in\nlightweight MLLMs. Addressing this issue, we propose Mini-Monkey, a lightweight\nMLLM that incorporates a plug-and-play method called multi-scale adaptive crop\nstrategy (MSAC). Mini-Monkey adaptively generates multi-scale representations,\nallowing it to select non-segmented objects from various scales. To mitigate\nthe computational overhead introduced by MSAC, we propose a Scale Compression\nMechanism (SCM), which effectively compresses image tokens. Mini-Monkey\nachieves state-of-the-art performance among 2B-parameter MLLMs. It not only\ndemonstrates leading performance on a variety of general multimodal\nunderstanding tasks but also shows consistent improvements in document\nunderstanding capabilities. On the OCRBench, Mini-Monkey achieves a score of\n802, outperforming 8B-parameter state-of-the-art model InternVL2-8B. Besides,\nour model and training strategy are very efficient, which can be trained with\nonly eight RTX 3090. The code is available at\nhttps://github.com/Yuliang-Liu/Monkey.\n","authors":["Mingxin Huang","Yuliang Liu","Dingkang Liang","Lianwen Jin","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2408.02034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02033v1","updated":"2024-08-04T13:51:18Z","published":"2024-08-04T13:51:18Z","title":"Enhancing Human Action Recognition and Violence Detection Through Deep\n  Learning Audiovisual Fusion","summary":"  This paper proposes a hybrid fusion-based deep learning approach based on two\ndifferent modalities, audio and video, to improve human activity recognition\nand violence detection in public places. To take advantage of audiovisual\nfusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning\n(HFBDL) are used and compared. Since the objective is to detect and recognize\nhuman violence in public places, Real-life violence situation (RLVS) dataset is\nexpanded and used. Simulating results of HFBDL show 96.67\\% accuracy on\nvalidation data, which is more accurate than the other state-of-the-art methods\non this dataset. To showcase our model's ability in real-world scenarios,\nanother dataset of 54 sounded videos of both violent and non-violent situations\nwas recorded. The model could successfully detect 52 out of 54 videos\ncorrectly. The proposed method shows a promising performance on real scenarios.\nThus, it can be used for human action recognition and violence detection in\npublic places for security purposes.\n","authors":["Pooya Janani","Amirabolfazl Suratgar","Afshin Taghvaeipour"],"pdf_url":"https://arxiv.org/pdf/2408.02033v1.pdf","comment":"This work has been submitted to the IEEE for possible publication, 10\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02032v1","updated":"2024-08-04T13:50:17Z","published":"2024-08-04T13:50:17Z","title":"Self-Introspective Decoding: Alleviating Hallucinations for Large\n  Vision-Language Models","summary":"  While Large Vision-Language Models (LVLMs) have rapidly advanced in recent\nyears, the prevalent issue known as the `hallucination' problem has emerged as\na significant bottleneck, hindering their real-world deployments. Existing\nmethods mitigate this issue mainly from two perspectives: One approach\nleverages extra knowledge like robust instruction tuning LVLMs with curated\ndatasets or employing auxiliary analysis networks, which inevitable incur\nadditional costs. Another approach, known as contrastive decoding, induces\nhallucinations by manually disturbing the vision or instruction raw inputs and\nmitigates them by contrasting the outputs of the disturbed and original LVLMs.\nHowever, these approaches rely on empirical holistic input disturbances and\ndouble the inference cost. To avoid these issues, we propose a simple yet\neffective method named Self-Introspective Decoding (SID). Our empirical\ninvestigation reveals that pretrained LVLMs can introspectively assess the\nimportance of vision tokens based on preceding vision and text (both\ninstruction and generated) tokens. We develop the Context and Text-aware Token\nSelection (CT2S) strategy, which preserves only unimportant vision tokens after\nearly layers of LVLMs to adaptively amplify text-informed hallucination during\nthe auto-regressive decoding. This approach ensures that multimodal knowledge\nabsorbed in the early layers induces multimodal contextual rather than aimless\nhallucinations. Subsequently, the original token logits subtract the amplified\nvision-and-text association hallucinations, guiding LVLMs decoding faithfully.\nExtensive experiments illustrate SID generates less-hallucination and\nhigher-quality texts across various metrics, without extra knowledge and much\nadditional computation burdens.\n","authors":["Fushuo Huo","Wenchao Xu","Zhong Zhang","Haozhao Wang","Zhicheng Chen","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.02032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02024v1","updated":"2024-08-04T13:23:18Z","published":"2024-08-04T13:23:18Z","title":"Faster Diffusion Action Segmentation","summary":"  Temporal Action Segmentation (TAS) is an essential task in video analysis,\naiming to segment and classify continuous frames into distinct action segments.\nHowever, the ambiguous boundaries between actions pose a significant challenge\nfor high-precision segmentation. Recent advances in diffusion models have\ndemonstrated substantial success in TAS tasks due to their stable training\nprocess and high-quality generation capabilities. However, the heavy sampling\nsteps required by diffusion models pose a substantial computational burden,\nlimiting their practicality in real-time applications. Additionally, most\nrelated works utilize Transformer-based encoder architectures. Although these\narchitectures excel at capturing long-range dependencies, they incur high\ncomputational costs and face feature-smoothing issues when processing long\nvideo sequences. To address these challenges, we propose EffiDiffAct, an\nefficient and high-performance TAS algorithm. Specifically, we develop a\nlightweight temporal feature encoder that reduces computational overhead and\nmitigates the rank collapse phenomenon associated with traditional\nself-attention mechanisms. Furthermore, we introduce an adaptive skip strategy\nthat allows for dynamic adjustment of timestep lengths based on computed\nsimilarity metrics during inference, thereby further enhancing computational\nefficiency. Comprehensive experiments on the 50Salads, Breakfast, and GTEA\ndatasets demonstrated the effectiveness of the proposed algorithm.\n","authors":["Shuaibing Wang","Shunli Wang","Mingcheng Li","Dingkang Yang","Haopeng Kuang","Ziyun Qian","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02024v1.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.02018v1","updated":"2024-08-04T13:09:06Z","published":"2024-08-04T13:09:06Z","title":"Individualized multi-horizon MRI trajectory prediction for Alzheimer's\n  Disease","summary":"  Neurodegeneration as measured through magnetic resonance imaging (MRI) is\nrecognized as a potential biomarker for diagnosing Alzheimer's disease (AD),\nbut is generally considered less specific than amyloid or tau based biomarkers.\nDue to a large amount of variability in brain anatomy between different\nindividuals, we hypothesize that leveraging MRI time series can help improve\nspecificity, by treating each patient as their own baseline. Here we turn to\nconditional variational autoencoders to generate individualized MRI predictions\ngiven the subject's age, disease status and one previous scan. Using serial\nimaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a\nnovel architecture to build a latent space distribution which can be sampled\nfrom to generate future predictions of changing anatomy. This enables us to\nextrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated\nthe model on a held-out set from ADNI and an independent dataset (from Open\nAccess Series of Imaging Studies). By comparing to several alternatives, we\nshow that our model produces more individualized images with higher resolution.\nFurther, if an individual already has a follow-up MRI, we demonstrate a usage\nof our model to compute a likelihood ratio classifier for disease status. In\npractice, the model may be able to assist in early diagnosis of AD and provide\na counterfactual baseline trajectory for treatment effect estimation.\nFurthermore, it generates a synthetic dataset that can potentially be used for\ndownstream tasks such as anomaly detection and classification.\n","authors":["Rosemary He","Gabriella Ang","Daniel Tward"],"pdf_url":"https://arxiv.org/pdf/2408.02018v1.pdf","comment":"MICCAI 2024 LDTM workshop"},{"id":"http://arxiv.org/abs/2408.02014v1","updated":"2024-08-04T12:52:44Z","published":"2024-08-04T12:52:44Z","title":"Unsupervised Representation Learning by Balanced Self Attention Matching","summary":"  Many leading self-supervised methods for unsupervised representation\nlearning, in particular those for embedding image features, are built on\nvariants of the instance discrimination task, whose optimization is known to be\nprone to instabilities that can lead to feature collapse. Different techniques\nhave been devised to circumvent this issue, including the use of negative pairs\nwith different contrastive losses, the use of external memory banks, and\nbreaking of symmetry by using separate encoding networks with possibly\ndifferent structures. Our method, termed BAM, rather than directly matching\nfeatures of different views (augmentations) of input images, is based on\nmatching their self-attention vectors, which are the distributions of\nsimilarities to the entire set of augmented images of a batch. We obtain rich\nrepresentations and avoid feature collapse by minimizing a loss that matches\nthese distributions to their globally balanced and entropy regularized version,\nwhich is obtained through a simple self-optimal-transport computation. We\nablate and verify our method through a wide set of experiments that show\ncompetitive performance with leading methods on both semi-supervised and\ntransfer-learning benchmarks. Our implementation and pre-trained models are\navailable at github.com/DanielShalam/BAM .\n","authors":["Daniel Shalam","Simon Korman"],"pdf_url":"https://arxiv.org/pdf/2408.02014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02012v1","updated":"2024-08-04T12:48:20Z","published":"2024-08-04T12:48:20Z","title":"Decision Support System to triage of liver trauma","summary":"  Trauma significantly impacts global health, accounting for over 5 million\ndeaths annually, which is comparable to mortality rates from diseases such as\ntuberculosis, AIDS, and malaria. In Iran, the financial repercussions of road\ntraffic accidents represent approximately 2% of the nation's Gross National\nProduct each year. Bleeding is the leading cause of mortality in trauma\npatients within the first 24 hours following an injury, making rapid diagnosis\nand assessment of severity crucial. Trauma patients require comprehensive scans\nof all organs, generating a large volume of data. Evaluating CT images for the\nentire body is time-consuming and requires significant expertise, underscoring\nthe need for efficient time management in diagnosis. Efficient diagnostic\nprocesses can significantly reduce treatment costs and decrease the likelihood\nof secondary complications. In this context, the development of a reliable\nDecision Support System (DSS) for trauma triage, particularly focused on the\nabdominal area, is vital. This paper presents a novel method for detecting\nliver bleeding and lacerations using CT scans, utilising the GAN Pix2Pix\ntranslation model. The effectiveness of the method is quantified by Dice score\nmetrics, with the model achieving an accuracy of 97% for liver bleeding and 93%\nfor liver laceration detection. These results represent a notable improvement\nover current state-of-the-art technologies. The system's design integrates\nseamlessly with existing medical imaging technologies, making it a practical\naddition to emergency medical services. This research underscores the potential\nof advanced image translation models like GAN Pix2Pix in improving the\nprecision and speed of medical diagnostics in critical care scenarios.\n","authors":["Ali Jamali","Azadeh Nazemi","Ashkan Sami","Rosemina Bahrololoom","Shahram Paydar","Alireza Shakibafar"],"pdf_url":"https://arxiv.org/pdf/2408.02012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02001v1","updated":"2024-08-04T11:59:09Z","published":"2024-08-04T11:59:09Z","title":"AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and\n  Accurate Diagnosis","summary":"  The integration of vision-language models such as CLIP and Concept Bottleneck\nModels (CBMs) offers a promising approach to explaining deep neural network\n(DNN) decisions using concepts understandable by humans, addressing the\nblack-box concern of DNNs. While CLIP provides both explainability and\nzero-shot classification capability, its pre-training on generic image and text\ndata may limit its classification accuracy and applicability to medical image\ndiagnostic tasks, creating a transfer learning problem. To maintain\nexplainability and address transfer learning needs, CBM methods commonly design\npost-processing modules after the bottleneck module. However, this way has been\nineffective. This paper takes an unconventional approach by re-examining the\nCBM framework through the lens of its geometrical representation as a simple\nlinear classification system. The analysis uncovers that post-CBM fine-tuning\nmodules merely rescale and shift the classification outcome of the system,\nfailing to fully leverage the system's learning potential. We introduce an\nadaptive module strategically positioned between CLIP and CBM to bridge the gap\nbetween source and downstream domains. This simple yet effective approach\nenhances classification performance while preserving the explainability\nafforded by the framework. Our work offers a comprehensive solution that\nencompasses the entire process, from concept discovery to model training,\nproviding a holistic recipe for leveraging the strengths of GPT, CLIP, and CBM.\n","authors":["Townim F. Chowdhury","Vu Minh Hieu Phan","Kewen Liao","Minh-Son To","Yutong Xie","Anton van den Hengel","Johan W. Verjans","Zhibin Liao"],"pdf_url":"https://arxiv.org/pdf/2408.02001v1.pdf","comment":"Accepted at MICCAI 2024, the 27th International Conference on Medical\n  Image Computing and Computer Assisted Intervention"},{"id":"http://arxiv.org/abs/2408.01998v1","updated":"2024-08-04T11:51:00Z","published":"2024-08-04T11:51:00Z","title":"What Happens Without Background? Constructing Foreground-Only Data for\n  Fine-Grained Tasks","summary":"  Fine-grained recognition, a pivotal task in visual signal processing, aims to\ndistinguish between similar subclasses based on discriminative information\npresent in samples. However, prevailing methods often erroneously focus on\nbackground areas, neglecting the capture of genuinely effective discriminative\ninformation from the subject, thus impeding practical application. To\nfacilitate research into the impact of background noise on models and enhance\ntheir ability to concentrate on the subject's discriminative features, we\npropose an engineered pipeline that leverages the capabilities of SAM and Detic\nto create fine-grained datasets with only foreground subjects, devoid of\nbackground. Extensive cross-experiments validate this approach as a\npreprocessing step prior to training, enhancing algorithmic performance and\nholding potential for further modal expansion of the data.\n","authors":["Yuetian Wang","Wenjin Hou","Qinmu Peng","Xinge You"],"pdf_url":"https://arxiv.org/pdf/2408.01998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01986v1","updated":"2024-08-04T10:54:36Z","published":"2024-08-04T10:54:36Z","title":"DeMansia: Mamba Never Forgets Any Tokens","summary":"  This paper examines the mathematical foundations of transformer\narchitectures, highlighting their limitations particularly in handling long\nsequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM),\nand LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia\nintegrates state space models with token labeling techniques to enhance\nperformance in image classification tasks, efficiently addressing the\ncomputational challenges posed by traditional transformers. The architecture,\nbenchmark, and comparisons with contemporary models demonstrate DeMansia's\neffectiveness. The implementation of this paper is available on GitHub at\nhttps://github.com/catalpaaa/DeMansia\n","authors":["Ricky Fang"],"pdf_url":"https://arxiv.org/pdf/2408.01986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13355v2","updated":"2024-08-04T10:31:41Z","published":"2023-11-22T12:47:12Z","title":"Unified Classification and Rejection: A One-versus-All Framework","summary":"  Classifying patterns of known classes and rejecting ambiguous and novel (also\ncalled as out-of-distribution (OOD)) inputs are involved in open world pattern\nrecognition. Deep neural network models usually excel in closed-set\nclassification while performs poorly in rejecting OOD inputs. To tackle this\nproblem, numerous methods have been designed to perform open set recognition\n(OSR) or OOD rejection/detection tasks. Previous methods mostly take\npost-training score transformation or hybrid models to ensure low scores on OOD\ninputs while separating known classes. In this paper, we attempt to build a\nunified framework for building open set classifiers for both classification and\nOOD rejection. We formulate the open set recognition of $ K $-known-class as a\n$ (K+1) $-class classification problem with model trained on known-class\nsamples only. By decomposing the $ K $-class problem into $ K $ one-versus-all\n(OVA) binary classification tasks and binding some parameters, we show that\ncombining the scores of OVA classifiers can give $ (K+1) $-class posterior\nprobabilities, which enables classification and OOD rejection in a unified\nframework. To maintain the closed-set classification accuracy of the OVA\ntrained classifier, we propose a hybrid training strategy combining OVA loss\nand multi-class cross-entropy loss. We implement the OVA framework and hybrid\ntraining strategy on the recently proposed convolutional prototype network and\nprototype classifier on vision transformer (ViT) backbone. Experiments on\npopular OSR and OOD detection datasets demonstrate that the proposed framework,\nusing a single multi-class classifier, yields competitive performance in\nclosed-set classification, OOD detection, and misclassification detection.\n","authors":["Zhen Cheng","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.13355v2.pdf","comment":"Published in Machine Intelligence Research\n  (https://link.springer.com/article/10.1007/s11633-024-1514-4)"},{"id":"http://arxiv.org/abs/2304.08386v3","updated":"2024-08-04T10:25:50Z","published":"2023-04-17T15:54:10Z","title":"Progressive Visual Prompt Learning with Contrastive Feature Re-formation","summary":"  Prompt learning has been designed as an alternative to fine-tuning for\nadapting Vision-language (V-L) models to the downstream tasks. Previous works\nmainly focus on text prompt while visual prompt works are limited for V-L\nmodels. The existing visual prompt methods endure either mediocre performance\nor unstable training process, indicating the difficulty of visual prompt\nlearning. In this paper, we propose a new Progressive Visual Prompt (ProVP)\nstructure to strengthen the interactions among prompts of different layers.\nMore importantly, our ProVP could effectively propagate the image embeddings to\ndeep layers and behave partially similar to an instance adaptive prompt method.\nTo alleviate generalization deterioration, we further propose a new contrastive\nfeature re-formation, which prevents the serious deviation of the prompted\nvisual feature from the fixed CLIP visual feature distribution. Combining both,\nour method (ProVP-Ref) is evaluated on 11 image benchmark datasets and achieves\n7/11 state-of-theart results on both few-shot and base-to-novel settings. To\nthe best of our knowledge, we are the first to demonstrate the superior\nperformance of visual prompts in V-L models to previous prompt-based methods in\ndownstream tasks. Meanwhile, it implies that our ProVP-Ref shows the best\ncapability to adapt and to generalize.\n","authors":["Chen Xu","Yuhan Zhu","Haocheng Shen","Boheng Chen","Yixuan Liao","Xiaoxin Chen","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2304.08386v3.pdf","comment":"IJCV 2024 Accepted"},{"id":"http://arxiv.org/abs/2408.01978v1","updated":"2024-08-04T09:53:50Z","published":"2024-08-04T09:53:50Z","title":"AdvQDet: Detecting Query-Based Adversarial Attacks with Adversarial\n  Contrastive Prompt Tuning","summary":"  Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks\neven under a black-box setting where the adversary can only query the model.\nParticularly, query-based black-box adversarial attacks estimate adversarial\ngradients based on the returned probability vectors of the target model for a\nsequence of queries. During this process, the queries made to the target model\nare intermediate adversarial examples crafted at the previous attack step,\nwhich share high similarities in the pixel space. Motivated by this\nobservation, stateful detection methods have been proposed to detect and reject\nquery-based attacks. While demonstrating promising results, these methods\neither have been evaded by more advanced attacks or suffer from low efficiency\nin terms of the number of shots (queries) required to detect different attacks.\nArguably, the key challenge here is to assign high similarity scores for any\ntwo intermediate adversarial examples perturbed from the same clean image. To\naddress this challenge, we propose a novel Adversarial Contrastive Prompt\nTuning (ACPT) method to robustly fine-tune the CLIP image encoder to extract\nsimilar embeddings for any two intermediate adversarial queries. With ACPT, we\nfurther introduce a detection framework AdvQDet that can detect 7\nstate-of-the-art query-based attacks with $>99\\%$ detection rate within 5\nshots. We also show that ACPT is robust to 3 types of adaptive attacks. Code is\navailable at https://github.com/xinwong/AdvQDet.\n","authors":["Xin Wang","Kai Chen","Xingjun Ma","Zhineng Chen","Jingjing Chen","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.01978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01977v1","updated":"2024-08-04T09:51:14Z","published":"2024-08-04T09:51:14Z","title":"Label Augmentation for Neural Networks Robustness","summary":"  Out-of-distribution generalization can be categorized into two types: common\nperturbations arising from natural variations in the real world and adversarial\nperturbations that are intentionally crafted to deceive neural networks. While\ndeep neural networks excel in accuracy under the assumption of identical\ndistributions between training and test data, they often encounter\nout-of-distribution scenarios resulting in a significant decline in accuracy.\nData augmentation methods can effectively enhance robustness against common\ncorruptions, but they typically fall short in improving robustness against\nadversarial perturbations. In this study, we develop Label Augmentation (LA),\nwhich enhances robustness against both common and intentional perturbations and\nimproves uncertainty estimation. Our findings indicate a Clean error rate\nimprovement of up to 23.29% when employing LA in comparisons to the baseline.\nAdditionally, it enhances robustness under common corruptions benchmark by up\nto 24.23%. When tested against FGSM and PGD attacks, improvements in\nadversarial robustness are noticeable, with enhancements of up to 53.18% for\nFGSM and 24.46% for PGD attacks.\n","authors":["Fatemeh Amerehi","Patrick Healy"],"pdf_url":"https://arxiv.org/pdf/2408.01977v1.pdf","comment":"21 pages, 4 figures, Published at 3rd Conference on Lifelong Learning\n  Agents (CoLLAs), 2024"},{"id":"http://arxiv.org/abs/2408.01976v1","updated":"2024-08-04T09:44:47Z","published":"2024-08-04T09:44:47Z","title":"Single-Point Supervised High-Resolution Dynamic Network for Infrared\n  Small Target Detection","summary":"  Infrared small target detection (IRSTD) tasks are extremely challenging for\ntwo main reasons: 1) it is difficult to obtain accurate labelling information\nthat is critical to existing methods, and 2) infrared (IR) small target\ninformation is easily lost in deep networks. To address these issues, we\npropose a single-point supervised high-resolution dynamic network (SSHD-Net).\nIn contrast to existing methods, we achieve state-of-the-art (SOTA) detection\nperformance using only single-point supervision. Specifically, we first design\na high-resolution cross-feature extraction module (HCEM), that achieves\nbi-directional feature interaction through stepped feature cascade channels\n(SFCC). It balances network depth and feature resolution to maintain deep IR\nsmall-target information. Secondly, the effective integration of global and\nlocal features is achieved through the dynamic coordinate fusion module (DCFM),\nwhich enhances the anti-interference ability in complex backgrounds. In\naddition, we introduce the high-resolution multilevel residual module (HMRM) to\nenhance the semantic information extraction capability. Finally, we design the\nadaptive target localization detection head (ATLDH) to improve detection\naccuracy. Experiments on the publicly available datasets NUDT-SIRST and\nIRSTD-1k demonstrate the effectiveness of our method. Compared to other SOTA\nmethods, our method can achieve better detection performance with only a single\npoint of supervision.\n","authors":["Jing Wu","Rixiang Ni","Feng Huang","Zhaobing Qiu","Liqiong Chen","Changhai Luo","Yunxiang Li","Youli Li"],"pdf_url":"https://arxiv.org/pdf/2408.01976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01970v1","updated":"2024-08-04T09:09:35Z","published":"2024-08-04T09:09:35Z","title":"SR-CIS: Self-Reflective Incremental System with Decoupled Memory and\n  Reasoning","summary":"  The ability of humans to rapidly learn new knowledge while retaining old\nmemories poses a significant challenge for current deep learning models. To\nhandle this challenge, we draw inspiration from human memory and learning\nmechanisms and propose the Self-Reflective Complementary Incremental System\n(SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and\nComplementary Memory Module (CMM), SR-CIS features a small model for fast\ninference and a large model for slow deliberation in CIM, enabled by the\nConfidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient\ncollaboration. CMM consists of task-specific Short-Term Memory (STM) region and\na universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank\nAdaptive (LoRA) and corresponding prototype weights and biases, it instantiates\nexternal storage for parameter and representation memory, thus deconstructing\nthe memory module from the inference module. By storing textual descriptions of\nimages during training and combining them with the Scenario Replay Module (SRM)\npost-training for memory combination, along with periodic short-to-long-term\nmemory restructuring, SR-CIS achieves stable incremental memory with limited\nstorage requirements. Balancing model plasticity and memory stability under\nconstraints of limited storage and low data resources, SR-CIS surpasses\nexisting competitive baselines on multiple standard and few-shot incremental\nlearning benchmarks.\n","authors":["Biqing Qi","Junqi Gao","Xinquan Chen","Dong Li","Weinan Zhang","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.01970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13309v2","updated":"2024-08-04T08:40:36Z","published":"2024-07-18T09:13:08Z","title":"Exposure Completing for Temporally Consistent Neural High Dynamic Range\n  Video Rendering","summary":"  High dynamic range (HDR) video rendering from low dynamic range (LDR) videos\nwhere frames are of alternate exposure encounters significant challenges, due\nto the exposure change and absence at each time stamp. The exposure change and\nabsence make existing methods generate flickering HDR results. In this paper,\nwe propose a novel paradigm to render HDR frames via completing the absent\nexposure information, hence the exposure information is complete and\nconsistent. Our approach involves interpolating neighbor LDR frames in the time\ndimension to reconstruct LDR frames for the absent exposures. Combining the\ninterpolated and given LDR frames, the complete set of exposure information is\navailable at each time stamp. This benefits the fusing process for HDR results,\nreducing noise and ghosting artifacts therefore improving temporal consistency.\nExtensive experimental evaluations on standard benchmarks demonstrate that our\nmethod achieves state-of-the-art performance, highlighting the importance of\nabsent exposure completing in HDR video rendering. The code is available at\nhttps://github.com/cuijiahao666/NECHDR.\n","authors":["Jiahao Cui","Wei Jiang","Zhan Peng","Zhiyu Pan","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2407.13309v2.pdf","comment":"9 pages, 6 figures, accepted by ACM-MM 2024 (poster)"},{"id":"http://arxiv.org/abs/2408.01960v1","updated":"2024-08-04T08:33:44Z","published":"2024-08-04T08:33:44Z","title":"AnomalySD: Few-Shot Multi-Class Anomaly Detection with Stable Diffusion\n  Model","summary":"  Anomaly detection is a critical task in industrial manufacturing, aiming to\nidentify defective parts of products. Most industrial anomaly detection methods\nassume the availability of sufficient normal data for training. This assumption\nmay not hold true due to the cost of labeling or data privacy policies.\nAdditionally, mainstream methods require training bespoke models for different\nobjects, which incurs heavy costs and lacks flexibility in practice. To address\nthese issues, we seek help from Stable Diffusion (SD) model due to its\ncapability of zero/few-shot inpainting, which can be leveraged to inpaint\nanomalous regions as normal. In this paper, a few-shot multi-class anomaly\ndetection framework that adopts Stable Diffusion model is proposed, named\nAnomalySD. To adapt SD to anomaly detection task, we design different\nhierarchical text descriptions and the foreground mask mechanism for\nfine-tuning SD. In the inference stage, to accurately mask anomalous regions\nfor inpainting, we propose multi-scale mask strategy and prototype-guided mask\nstrategy to handle diverse anomalous regions. Hierarchical text prompts are\nalso utilized to guide the process of inpainting in the inference stage. The\nanomaly score is estimated based on inpainting result of all masks. Extensive\nexperiments on the MVTec-AD and VisA datasets demonstrate the superiority of\nour approach. We achieved anomaly classification and segmentation results of\n93.6%/94.8% AUROC on the MVTec-AD dataset and 86.1%/96.5% AUROC on the VisA\ndataset under multi-class and one-shot settings.\n","authors":["Zhenyu Yan","Qingqing Fang","Wenxi Lv","Qinliang Su"],"pdf_url":"https://arxiv.org/pdf/2408.01960v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.06863v4","updated":"2024-08-04T08:28:25Z","published":"2024-07-09T13:50:43Z","title":"Beyond Aesthetics: Cultural Competence in Text-to-Image Models","summary":"  Text-to-Image (T2I) models are being increasingly adopted in diverse global\ncommunities where they create visual representations of their unique cultures.\nCurrent T2I benchmarks primarily focus on faithfulness, aesthetics, and realism\nof generated images, overlooking the critical dimension of cultural competence.\nIn this work, we introduce a framework to evaluate cultural competence of T2I\nmodels along two crucial dimensions: cultural awareness and cultural diversity,\nand present a scalable approach using a combination of structured knowledge\nbases and large language models to build a large dataset of cultural artifacts\nto enable this evaluation. In particular, we apply this approach to build CUBE\n(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to\nevaluate cultural competence of T2I models. CUBE covers cultural artifacts\nassociated with 8 countries across different geo-cultural regions and along 3\nconcepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of\nhigh-quality prompts that enable the evaluation of cultural awareness, and 2)\nCUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to\nevaluate cultural diversity. We also introduce cultural diversity as a novel\nT2I evaluation component, leveraging quality-weighted Vendi score. Our\nevaluations reveal significant gaps in the cultural awareness of existing\nmodels across countries and provide valuable insights into the cultural\ndiversity of T2I outputs for under-specified prompts. Our methodology is\nextendable to other cultural regions and concepts, and can facilitate the\ndevelopment of T2I models that better cater to the global population.\n","authors":["Nithish Kannen","Arif Ahmad","Marco Andreetto","Vinodkumar Prabhakaran","Utsav Prabhu","Adji Bousso Dieng","Pushpak Bhattacharyya","Shachi Dave"],"pdf_url":"https://arxiv.org/pdf/2407.06863v4.pdf","comment":"30 pages, 10 figures, preprint"},{"id":"http://arxiv.org/abs/2408.01959v1","updated":"2024-08-04T08:26:58Z","published":"2024-08-04T08:26:58Z","title":"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in\n  Vision-Language AI","summary":"  Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.\n","authors":["Robert Wolfe","Aayushi Dangol","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01959v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.01953v1","updated":"2024-08-04T07:59:17Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v1.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2408.01952v1","updated":"2024-08-04T07:48:12Z","published":"2024-08-04T07:48:12Z","title":"CACE-Net: Co-guidance Attention and Contrastive Enhancement for\n  Effective Audio-Visual Event Localization","summary":"  The audio-visual event localization task requires identifying concurrent\nvisual and auditory events from unconstrained videos within a network model,\nlocating them, and classifying their category. The efficient extraction and\nintegration of audio and visual modal information have always been challenging\nin this field. In this paper, we introduce CACE-Net, which differs from most\nexisting methods that solely use audio signals to guide visual information. We\npropose an audio-visual co-guidance attention mechanism that allows for\nadaptive bi-directional cross-modal attentional guidance between audio and\nvisual information, thus reducing inconsistencies between modalities. Moreover,\nwe have observed that existing methods have difficulty distinguishing between\nsimilar background and event and lack the fine-grained features for event\nclassification. Consequently, we employ background-event contrast enhancement\nto increase the discrimination of fused feature and fine-tuned pre-trained\nmodel to extract more refined and discernible features from complex multimodal\ninputs. Specifically, we have enhanced the model's ability to discern subtle\ndifferences between event and background and improved the accuracy of event\nclassification in our model. Experiments on the AVE dataset demonstrate that\nCACE-Net sets a new benchmark in the audio-visual event localization task,\nproving the effectiveness of our proposed methods in handling complex\nmultimodal learning and event localization in unconstrained videos. Code is\navailable at https://github.com/Brain-Cog-Lab/CACE-Net.\n","authors":["Xiang He","Xiangxi Liu","Yang Li","Dongcheng Zhao","Guobin Shen","Qingqun Kong","Xin Yang","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.01952v1.pdf","comment":"Accepted by ACM MM 2024. Code is available at this\n  https://github.com/Brain-Cog-Lab/CACE-Net"},{"id":"http://arxiv.org/abs/2408.01946v1","updated":"2024-08-04T07:12:59Z","published":"2024-08-04T07:12:59Z","title":"Masked Angle-Aware Autoencoder for Remote Sensing Images","summary":"  To overcome the inherent domain gap between remote sensing (RS) images and\nnatural images, some self-supervised representation learning methods have made\npromising progress. However, they have overlooked the diverse angles present in\nRS objects. This paper proposes the Masked Angle-Aware Autoencoder (MA3E) to\nperceive and learn angles during pre-training. We design a \\textit{scaling\ncenter crop} operation to create the rotated crop with random orientation on\neach original image, introducing the explicit angle variation. MA3E inputs this\ncomposite image while reconstruct the original image, aiming to effectively\nlearn rotation-invariant representations by restoring the angle variation\nintroduced on the rotated crop. To avoid biases caused by directly\nreconstructing the rotated crop, we propose an Optimal Transport (OT) loss that\nautomatically assigns similar original image patches to each rotated crop patch\nfor reconstruction. MA3E demonstrates more competitive performance than\nexisting pre-training methods on seven different RS image datasets in three\ndownstream tasks.\n","authors":["Zhihao Li","Biao Hou","Siteng Ma","Zitong Wu","Xianpeng Guo","Bo Ren","Licheng Jiao"],"pdf_url":"https://arxiv.org/pdf/2408.01946v1.pdf","comment":"This paper has been accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13200v2","updated":"2024-08-04T07:06:04Z","published":"2024-07-18T06:32:45Z","title":"Adapt PointFormer: 3D Point Cloud Analysis via Adapting 2D Visual\n  Transformers","summary":"  Pre-trained large-scale models have exhibited remarkable efficacy in computer\nvision, particularly for 2D image analysis. However, when it comes to 3D point\nclouds, the constrained accessibility of data, in contrast to the vast\nrepositories of images, poses a challenge for the development of 3D pre-trained\nmodels. This paper therefore attempts to directly leverage pre-trained models\nwith 2D prior knowledge to accomplish the tasks for 3D point cloud analysis.\nAccordingly, we propose the Adaptive PointFormer (APF), which fine-tunes\npre-trained 2D models with only a modest number of parameters to directly\nprocess point clouds, obviating the need for mapping to images. Specifically,\nwe convert raw point clouds into point embeddings for aligning dimensions with\nimage tokens. Given the inherent disorder in point clouds, in contrast to the\nstructured nature of images, we then sequence the point embeddings to optimize\nthe utilization of 2D attention priors. To calibrate attention across 3D and 2D\ndomains and reduce computational overhead, a trainable PointFormer with a\nlimited number of parameters is subsequently concatenated to a frozen\npre-trained image model. Extensive experiments on various benchmarks\ndemonstrate the effectiveness of the proposed APF. The source code and more\ndetails are available at https://vcc.tech/research/2024/PointFormer.\n","authors":["Mengke Li","Da Li","Guoqing Yang","Yiu-ming Cheung","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2407.13200v2.pdf","comment":"ECAI 2024 main conference paper"},{"id":"http://arxiv.org/abs/2408.01945v1","updated":"2024-08-04T07:06:04Z","published":"2024-08-04T07:06:04Z","title":"Generalized Maximum Likelihood Estimation for Perspective-n-Point\n  Problem","summary":"  The Perspective-n-Point (PnP) problem has been widely studied in the\nliterature and applied in various vision-based pose estimation scenarios.\nHowever, existing methods ignore the anisotropy uncertainty of observations, as\ndemonstrated in several real-world datasets in this paper. This oversight may\nlead to suboptimal and inaccurate estimation, particularly in the presence of\nnoisy observations. To this end, we propose a generalized maximum likelihood\nPnP solver, named GMLPnP, that minimizes the determinant criterion by iterating\nthe GLS procedure to estimate the pose and uncertainty simultaneously. Further,\nthe proposed method is decoupled from the camera model. Results of synthetic\nand real experiments show that our method achieves better accuracy in common\npose estimation scenarios, GMLPnP improves rotation/translation accuracy by\n4.7%/2.0% on TUM-RGBD and 18.6%/18.4% on KITTI-360 dataset compared to the best\nbaseline. It is more accurate under very noisy observations in a vision-based\nUAV localization task, outperforming the best baseline by 34.4% in translation\nestimation accuracy.\n","authors":["Tian Zhan","Chunfeng Xu","Cheng Zhang","Ke Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.01945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01944v1","updated":"2024-08-04T07:04:59Z","published":"2024-08-04T07:04:59Z","title":"RobNODDI: Robust NODDI Parameter Estimation with Adaptive Sampling under\n  Continuous Representation","summary":"  Neurite Orientation Dispersion and Density Imaging (NODDI) is an important\nimaging technology used to evaluate the microstructure of brain tissue, which\nis of great significance for the discovery and treatment of various\nneurological diseases. Current deep learning-based methods perform parameter\nestimation through diffusion magnetic resonance imaging (dMRI) with a small\nnumber of diffusion gradients. These methods speed up parameter estimation and\nimprove accuracy. However, the diffusion directions used by most existing deep\nlearning models during testing needs to be strictly consistent with the\ndiffusion directions during training. This results in poor generalization and\nrobustness of deep learning models in dMRI parameter estimation. In this work,\nwe verify for the first time that the parameter estimation performance of\ncurrent mainstream methods will significantly decrease when the testing\ndiffusion directions and the training diffusion directions are inconsistent. A\nrobust NODDI parameter estimation method with adaptive sampling under\ncontinuous representation (RobNODDI) is proposed. Furthermore, long short-term\nmemory (LSTM) units and fully connected layers are selected to learn continuous\nrepresentation signals. To this end, we use a total of 100 subjects to conduct\nexperiments based on the Human Connectome Project (HCP) dataset, of which 60\nare used for training, 20 are used for validation, and 20 are used for testing.\nThe test results indicate that RobNODDI improves the generalization performance\nand robustness of the deep learning model, enhancing the stability and\nflexibility of deep learning NODDI parameter estimatimation applications.\n","authors":["Taohui Xiao","Jian Cheng","Wenxin Fan","Jing Yang","Cheng Li","Enqing Dong","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01942v1","updated":"2024-08-04T06:34:24Z","published":"2024-08-04T06:34:24Z","title":"Visual Grounding for Object-Level Generalization in Reinforcement\n  Learning","summary":"  Generalization is a pivotal challenge for agents following natural language\ninstructions. To approach this goal, we leverage a vision-language model (VLM)\nfor visual grounding and transfer its vision-language knowledge into\nreinforcement learning (RL) for object-centric tasks, which makes the agent\ncapable of zero-shot generalization to unseen objects and instructions. By\nvisual grounding, we obtain an object-grounded confidence map for the target\nobject indicated in the instruction. Based on this map, we introduce two routes\nto transfer VLM knowledge into RL. Firstly, we propose an object-grounded\nintrinsic reward function derived from the confidence map to more effectively\nguide the agent towards the target object. Secondly, the confidence map offers\na more unified, accessible task representation for the agent's policy, compared\nto language embeddings. This enables the agent to process unseen objects and\ninstructions through comprehensible visual confidence maps, facilitating\nzero-shot object-level generalization. Single-task experiments prove that our\nintrinsic reward significantly improves performance on challenging skill\nlearning. In multi-task experiments, through testing on tasks beyond the\ntraining set, we show that the agent, when provided with the confidence map as\nthe task representation, possesses better generalization capabilities than\nlanguage-based conditioning. The code is available at\nhttps://github.com/PKU-RL/COPL.\n","authors":["Haobin Jiang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01942v1.pdf","comment":"35 pages, 14 figures, 17 tables"},{"id":"http://arxiv.org/abs/2406.16464v3","updated":"2024-08-04T05:42:58Z","published":"2024-06-24T09:13:42Z","title":"InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection","summary":"  The prevalence of sarcasm in social media, conveyed through text-image\ncombinations, presents significant challenges for sentiment analysis and\nintention mining. Existing multi-modal sarcasm detection methods have been\nproven to overestimate performance, as they struggle to effectively capture the\nintricate sarcastic cues that arise from the interaction between an image and\ntext. To address these issues, we propose InterCLIP-MEP, a novel framework for\nmulti-modal sarcasm detection. Specifically, we introduce an Interactive CLIP\n(InterCLIP) as the backbone to extract text-image representations, enhancing\nthem by embedding cross-modality information directly within each encoder,\nthereby improving the representations to capture text-image interactions\nbetter. Furthermore, an efficient training strategy is designed to adapt\nInterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic,\nfixed-length dual-channel memory to store historical knowledge of valuable test\nsamples during inference. It then leverages this memory as a non-parametric\nclassifier to derive the final prediction, offering a more robust recognition\nof multi-modal sarcasm. Experiments demonstrate that InterCLIP-MEP achieves\nstate-of-the-art performance on the MMSD2.0 benchmark, with an accuracy\nimprovement of 1.08% and an F1 score improvement of 1.51% over the previous\nbest method. Code and data are available at\nhttps://github.com/CoderChen01/InterCLIP-MEP.\n","authors":["Junjie Chen","Hang Yu","Weidong Liu","Subin Huang","Sanmin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.16464v3.pdf","comment":"9 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.04071v4","updated":"2024-08-04T05:21:19Z","published":"2024-01-08T18:18:02Z","title":"Fun with Flags: Robust Principal Directions via Flag Manifolds","summary":"  Principal component analysis (PCA), along with its extensions to manifolds\nand outlier contaminated data, have been indispensable in computer vision and\nmachine learning. In this work, we present a unifying formalism for PCA and its\nvariants, and introduce a framework based on the flags of linear subspaces, ie\na hierarchy of nested linear subspaces of increasing dimension, which not only\nallows for a common implementation but also yields novel variants, not explored\npreviously. We begin by generalizing traditional PCA methods that either\nmaximize variance or minimize reconstruction error. We expand these\ninterpretations to develop a wide array of new dimensionality reduction\nalgorithms by accounting for outliers and the data manifold. To devise a common\ncomputational approach, we recast robust and dual forms of PCA as optimization\nproblems on flag manifolds. We then integrate tangent space approximations of\nprincipal geodesic analysis (tangent-PCA) into this flag-based framework,\ncreating novel robust and dual geodesic PCA variations. The remarkable\nflexibility offered by the 'flagification' introduced here enables even more\nalgorithmic variants identified by specific flag types. Last but not least, we\npropose an effective convergent solver for these flag-formulations employing\nthe Stiefel manifold. Our empirical results on both real-world and synthetic\nscenarios, demonstrate the superiority of our novel algorithms, especially in\nterms of robustness to outliers on manifolds.\n","authors":["Nathan Mankovich","Gustau Camps-Valls","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2401.04071v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01929v1","updated":"2024-08-04T04:55:10Z","published":"2024-08-04T04:55:10Z","title":"Advancing H&E-to-IHC Stain Translation in Breast Cancer: A\n  Multi-Magnification and Attention-Based Approach","summary":"  Breast cancer presents a significant healthcare challenge globally, demanding\nprecise diagnostics and effective treatment strategies, where histopathological\nexamination of Hematoxylin and Eosin (H&E) stained tissue sections plays a\ncentral role. Despite its importance, evaluating specific biomarkers like Human\nEpidermal Growth Factor Receptor 2 (HER2) for personalized treatment remains\nconstrained by the resource-intensive nature of Immunohistochemistry (IHC).\nRecent strides in deep learning, particularly in image-to-image translation,\noffer promise in synthesizing IHC-HER2 slides from H\\&E stained slides.\nHowever, existing methodologies encounter challenges, including managing\nmultiple magnifications in pathology images and insufficient focus on crucial\ninformation during translation. To address these issues, we propose a novel\nmodel integrating attention mechanisms and multi-magnification information\nprocessing. Our model employs a multi-magnification processing strategy to\nextract and utilize information from various magnifications within pathology\nimages, facilitating robust image translation. Additionally, an attention\nmodule within the generative network prioritizes critical information for image\ndistribution translation while minimizing less pertinent details. Rigorous\ntesting on a publicly available breast cancer dataset demonstrates superior\nperformance compared to existing methods, establishing our model as a\nstate-of-the-art solution in advancing pathology image translation from H&E to\nIHC staining.\n","authors":["Linhao Qu","Chengsheng Zhang","Guihui Li","Haiyong Zheng","Chen Peng","Wei He"],"pdf_url":"https://arxiv.org/pdf/2408.01929v1.pdf","comment":"Accepted by IEEE CIS-RAM 2024 Invited Session Oral"},{"id":"http://arxiv.org/abs/2304.09854v4","updated":"2024-08-04T04:30:45Z","published":"2023-04-19T17:59:02Z","title":"Transformer-Based Visual Segmentation: A Survey","summary":"  Visual segmentation seeks to partition images, video frames, or point clouds\ninto multiple segments or groups. This technique has numerous real-world\napplications, such as autonomous driving, image editing, robot sensing, and\nmedical analysis. Over the past decade, deep learning-based methods have made\nremarkable strides in this area. Recently, transformers, a type of neural\nnetwork based on self-attention originally designed for natural language\nprocessing, have considerably surpassed previous convolutional or recurrent\napproaches in various vision processing tasks. Specifically, vision\ntransformers offer robust, unified, and even simpler solutions for various\nsegmentation tasks. This survey provides a thorough overview of\ntransformer-based visual segmentation, summarizing recent advancements. We\nfirst review the background, encompassing problem definitions, datasets, and\nprior convolutional methods. Next, we summarize a meta-architecture that\nunifies all recent transformer-based approaches. Based on this\nmeta-architecture, we examine various method designs, including modifications\nto the meta-architecture and associated applications. We also present several\nclosely related settings, including 3D point cloud segmentation, foundation\nmodel tuning, domain-aware segmentation, efficient segmentation, and medical\nsegmentation. Additionally, we compile and re-evaluate the reviewed methods on\nseveral well-established datasets. Finally, we identify open challenges in this\nfield and propose directions for future research. The project page can be found\nat https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also\ncontinually monitor developments in this rapidly evolving field.\n","authors":["Xiangtai Li","Henghui Ding","Haobo Yuan","Wenwei Zhang","Jiangmiao Pang","Guangliang Cheng","Kai Chen","Ziwei Liu","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2304.09854v4.pdf","comment":"Accepted by IEEE T-PAMI. Project page:\n  https://github.com/lxtGH/Awesome-Segmentation-With-Transformer"},{"id":"http://arxiv.org/abs/2407.18559v2","updated":"2024-08-04T04:08:59Z","published":"2024-07-26T07:16:52Z","title":"VSSD: Vision Mamba with Non-Causal State Space Duality","summary":"  Vision transformers have significantly advanced the field of computer vision,\noffering robust modeling capabilities and global receptive field. However,\ntheir high computational demands limit their applicability in processing long\nsequences. To tackle this issue, State Space Models (SSMs) have gained\nprominence in vision tasks as they offer linear computational complexity.\nRecently, State Space Duality (SSD), an improved variant of SSMs, was\nintroduced in Mamba2 to enhance model performance and efficiency. However, the\ninherent causal nature of SSD/SSMs restricts their applications in non-causal\nvision tasks. To address this limitation, we introduce Visual State Space\nDuality (VSSD) model, which has a non-causal format of SSD. Specifically, we\npropose to discard the magnitude of interactions between the hidden state and\ntokens while preserving their relative weights, which relieves the dependencies\nof token contribution on previous tokens. Together with the involvement of\nmulti-scan strategies, we show that the scanning results can be integrated to\nachieve non-causality, which not only improves the performance of SSD in vision\ntasks but also enhances its efficiency. We conduct extensive experiments on\nvarious benchmarks including image classification, detection, and segmentation,\nwhere VSSD surpasses existing state-of-the-art SSM-based models. Code and\nweights are available at \\url{https://github.com/YuHengsss/VSSD}.\n","authors":["Yuheng Shi","Minjing Dong","Mingjia Li","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2407.18559v2.pdf","comment":"16 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2408.01920v1","updated":"2024-08-04T04:08:21Z","published":"2024-08-04T04:08:21Z","title":"Self-Supervised Pretrained Models and Latent Feature Distribution\n  Optimization","summary":"  In the face of complex natural images, existing deep clustering algorithms\nfall significantly short in terms of clustering accuracy when compared to\nsupervised classification methods, making them less practical. This paper\nintroduces an image clustering algorithm based on self-supervised pretrained\nmodels and latent feature distribution optimization, substantially enhancing\nclustering performance. It is found that: (1) For complex natural images, we\neffectively enhance the discriminative power of latent features by leveraging\nself-supervised pretrained models and their fine-tuning, resulting in improved\nclustering performance. (2) In the latent feature space, by searching for\nk-nearest neighbor images for each training sample and shortening the distance\nbetween the training sample and its nearest neighbor, the discriminative power\nof latent features can be further enhanced, and clustering performance can be\nimproved. (3) In the latent feature space, reducing the distance between sample\nfeatures and the nearest predefined cluster centroids can optimize the\ndistribution of latent features, therefore further improving clustering\nperformance. Through experiments on multiple datasets, our approach outperforms\nthe latest clustering algorithms and achieves state-of-the-art clustering\nresults. When the number of categories in the datasets is small, such as\nCIFAR-10 and STL-10, and there are significant differences between categories,\nour clustering algorithm has similar accuracy to supervised methods without\nusing pretrained models, slightly lower than supervised methods using\npre-trained models. The code linked algorithm is\nhttps://github.com/LihengHu/ICBPL.\n","authors":["Qiuyu Zhu","Liheng Hu","Sijin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21136v2","updated":"2024-08-04T03:32:03Z","published":"2024-07-30T18:57:06Z","title":"Adding Multimodal Controls to Whole-body Human Motion Generation","summary":"  Whole-body multimodal motion generation, controlled by text, speech, or\nmusic, has numerous applications including video generation and character\nanimation. However, employing a unified model to accomplish various generation\ntasks with different condition modalities presents two main challenges: motion\ndistribution drifts across different generation scenarios and the complex\noptimization of mixed conditions with varying granularity. Furthermore,\ninconsistent motion formats in existing datasets further hinder effective\nmultimodal motion generation. In this paper, we propose ControlMM, a unified\nframework to Control whole-body Multimodal Motion generation in a plug-and-play\nmanner. To effectively learn and transfer motion knowledge across different\nmotion distributions, we propose ControlMM-Attn, for parallel modeling of\nstatic and dynamic human topology graphs. To handle conditions with varying\ngranularity, ControlMM employs a coarse-to-fine training strategy, including\nstage-1 text-to-motion pre-training for semantic generation and stage-2\nmultimodal control adaptation for conditions of varying low-level granularity.\nTo address existing benchmarks' varying motion format limitations, we introduce\nControlMM-Bench, the first publicly available multimodal whole-body human\nmotion generation benchmark based on the unified whole-body SMPL-X format.\nExtensive experiments show that ControlMM achieves state-of-the-art performance\nacross various standard motion generation tasks. Our website is at\nhttps://yxbian23.github.io/ControlMM.\n","authors":["Yuxuan Bian","Ailing Zeng","Xuan Ju","Xian Liu","Zhaoyang Zhang","Wei Liu","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2407.21136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01897v1","updated":"2024-08-04T01:44:44Z","published":"2024-08-04T01:44:44Z","title":"CAF-YOLO: A Robust Framework for Multi-Scale Lesion Detection in\n  Biomedical Imagery","summary":"  Object detection is of paramount importance in biomedical image analysis,\nparticularly for lesion identification. While current methodologies are\nproficient in identifying and pinpointing lesions, they often lack the\nprecision needed to detect minute biomedical entities (e.g., abnormal cells,\nlung nodules smaller than 3 mm), which are critical in blood and lung\npathology. To address this challenge, we propose CAF-YOLO, based on the YOLOv8\narchitecture, a nimble yet robust method for medical object detection that\nleverages the strengths of convolutional neural networks (CNNs) and\ntransformers. To overcome the limitation of convolutional kernels, which have a\nconstrained capacity to interact with distant information, we introduce an\nattention and convolution fusion module (ACFM). This module enhances the\nmodeling of both global and local features, enabling the capture of long-term\nfeature dependencies and spatial autocorrelation. Additionally, to improve the\nrestricted single-scale feature aggregation inherent in feed-forward networks\n(FFN) within transformer architectures, we design a multi-scale neural network\n(MSNN). This network improves multi-scale information aggregation by extracting\nfeatures across diverse scales. Experimental evaluations on widely used\ndatasets, such as BCCD and LUNA16, validate the rationale and efficacy of\nCAF-YOLO. This methodology excels in detecting and precisely locating diverse\nand intricate micro-lesions within biomedical imagery. Our codes are available\nat https://github.com/xiaochen925/CAF-YOLO.\n","authors":["Zilin Chen","Shengnan Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01895v1","updated":"2024-08-04T01:34:22Z","published":"2024-08-04T01:34:22Z","title":"Computational Trichromacy Reconstruction: Empowering the Color-Vision\n  Deficient to Recognize Colors Using Augmented Reality","summary":"  We propose an assistive technology that helps individuals with Color Vision\nDeficiencies (CVD) to recognize/name colors. A dichromat's color perception is\na reduced two-dimensional (2D) subset of a normal trichromat's three\ndimensional color (3D) perception, leading to confusion when visual stimuli\nthat appear identical to the dichromat are referred to by different color\nnames. Using our proposed system, CVD individuals can interactively induce\ndistinct perceptual changes to originally confusing colors via a computational\ncolor space transformation. By combining their original 2D precepts for colors\nwith the discriminative changes, a three dimensional color space is\nreconstructed, where the dichromat can learn to resolve color name confusions\nand accurately recognize colors. Our system is implemented as an Augmented\nReality (AR) interface on smartphones, where users interactively control the\nrotation through swipe gestures and observe the induced color shifts in the\ncamera view or in a displayed image. Through psychophysical experiments and a\nlongitudinal user study, we demonstrate that such rotational color shifts have\ndiscriminative power (initially confusing colors become distinct under\nrotation) and exhibit structured perceptual shifts dichromats can learn with\nmodest training. The AR App is also evaluated in two real-world scenarios\n(building with lego blocks and interpreting artistic works); users all report\npositive experience in using the App to recognize object colors that they\notherwise could not.\n","authors":["Yuhao Zhu","Ethan Chen","Colin Hascup","Yukang Yan","Gaurav Charma"],"pdf_url":"https://arxiv.org/pdf/2408.01895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15953v2","updated":"2024-08-04T00:25:10Z","published":"2024-05-24T21:46:52Z","title":"Activator: GLU Activation Function as the Core Component of a Vision\n  Transformer","summary":"  Transformer architecture currently represents the main driver behind many\nsuccesses in a variety of tasks addressed by deep learning, especially the\nrecent advances in natural language processing (NLP) culminating with large\nlanguage models (LLM). In addition, transformer architecture has found a wide\nspread of interest from computer vision (CV) researchers and practitioners,\nallowing for many advancements in vision-related tasks and opening the door for\nmulti-task and multi-modal deep learning architectures that share the same\nprinciple of operation. One drawback to these architectures is their reliance\non the scaled dot product attention mechanism with the softmax activation\nfunction, which is computationally expensive and requires large compute\ncapabilities both for training and inference. This paper investigates\nsubstituting the attention mechanism usually adopted for transformer\narchitecture with an architecture incorporating gated linear unit (GLU)\nactivation within a multi-layer perceptron (MLP) structure in conjunction with\nthe default MLP incorporated in the traditional transformer design. Another\nstep forward taken by this paper is to eliminate the second non-gated MLP to\nfurther reduce the computational cost. Experimental assessments conducted by\nthis research show that both proposed modifications and reductions offer\ncompetitive performance in relation to baseline architectures, in support of\nthe aims of this work in establishing a more efficient yet capable alternative\nto the traditional attention mechanism as the core component in designing\ntransformer architectures.\n","authors":["Abdullah Nazhat Abdullah","Tarkan Aydin"],"pdf_url":"https://arxiv.org/pdf/2405.15953v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2403.02411"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2209.03885v4","updated":"2024-08-04T23:45:40Z","published":"2022-09-08T15:41:31Z","title":"A Framework for Evaluating Privacy-Utility Trade-off in Vertical\n  Federated Learning","summary":"  Federated learning (FL) has emerged as a practical solution to tackle data\nsilo issues without compromising user privacy. One of its variants, vertical\nfederated learning (VFL), has recently gained increasing attention as the VFL\nmatches the enterprises' demands of leveraging more valuable features to build\nbetter machine learning models while preserving user privacy. Current works in\nVFL concentrate on developing a specific protection or attack mechanism for a\nparticular VFL algorithm. In this work, we propose an evaluation framework that\nformulates the privacy-utility evaluation problem. We then use this framework\nas a guide to comprehensively evaluate a broad range of protection mechanisms\nagainst most of the state-of-the-art privacy attacks for three widely deployed\nVFL algorithms. These evaluations may help FL practitioners select appropriate\nprotection mechanisms given specific requirements. Our evaluation results\ndemonstrate that: the model inversion and most of the label inference attacks\ncan be thwarted by existing protection mechanisms; the model completion (MC)\nattack is difficult to be prevented, which calls for more advanced MC-targeted\nprotection mechanisms. Based on our evaluation results, we offer concrete\nadvice on improving the privacy-preserving capability of VFL systems. The code\nis available at https://github.com/yankang18/Attack-Defense-VFL\n","authors":["Yan Kang","Jiahuan Luo","Yuanqin He","Xiaojin Zhang","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2209.03885v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02165v1","updated":"2024-08-04T23:23:48Z","published":"2024-08-04T23:23:48Z","title":"SelfBC: Self Behavior Cloning for Offline Reinforcement Learning","summary":"  Policy constraint methods in offline reinforcement learning employ additional\nregularization techniques to constrain the discrepancy between the learned\npolicy and the offline dataset. However, these methods tend to result in overly\nconservative policies that resemble the behavior policy, thus limiting their\nperformance. We investigate this limitation and attribute it to the static\nnature of traditional constraints. In this paper, we propose a novel dynamic\npolicy constraint that restricts the learned policy on the samples generated by\nthe exponential moving average of previously learned policies. By integrating\nthis self-constraint mechanism into off-policy methods, our method facilitates\nthe learning of non-conservative policies while avoiding policy collapse in the\noffline setting. Theoretical results show that our approach results in a nearly\nmonotonically improved reference policy. Extensive experiments on the D4RL\nMuJoCo domain demonstrate that our proposed method achieves state-of-the-art\nperformance among the policy constraint methods.\n","authors":["Shirong Liu","Chenjia Bai","Zixian Guo","Hao Zhang","Gaurav Sharma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02161v1","updated":"2024-08-04T23:05:42Z","published":"2024-08-04T23:05:42Z","title":"Distilling Machine Learning's Added Value: Pareto Fronts in Atmospheric\n  Applications","summary":"  While the added value of machine learning (ML) for weather and climate\napplications is measurable, explaining it remains challenging, especially for\nlarge deep learning models. Inspired by climate model hierarchies, we propose\nthat a full hierarchy of Pareto-optimal models, defined within an appropriately\ndetermined error-complexity plane, can guide model development and help\nunderstand the models' added value. We demonstrate the use of Pareto fronts in\natmospheric physics through three sample applications, with hierarchies ranging\nfrom semi-empirical models with minimal tunable parameters (simplest) to deep\nlearning algorithms (most complex). First, in cloud cover parameterization, we\nfind that neural networks identify nonlinear relationships between cloud cover\nand its thermodynamic environment, and assimilate previously neglected features\nsuch as vertical gradients in relative humidity that improve the representation\nof low cloud cover. This added value is condensed into a ten-parameter equation\nthat rivals the performance of deep learning models. Second, we establish a ML\nmodel hierarchy for emulating shortwave radiative transfer, distilling the\nimportance of bidirectional vertical connectivity for accurately representing\nabsorption and scattering, especially for multiple cloud layers. Third, we\nemphasize the importance of convective organization information when modeling\nthe relationship between tropical precipitation and its surrounding\nenvironment. We discuss the added value of temporal memory when high-resolution\nspatial information is unavailable, with implications for precipitation\nparameterization. Therefore, by comparing data-driven models directly with\nexisting schemes using Pareto optimality, we promote process understanding by\nhierarchically unveiling system complexity, with the hope of improving the\ntrustworthiness of ML models in atmospheric applications.\n","authors":["Tom Beucler","Arthur Grundner","Sara Shamekh","Peter Ukkonen","Matthew Chantry","Ryan Lagerquist"],"pdf_url":"https://arxiv.org/pdf/2408.02161v1.pdf","comment":"18 pages, 4 figures, submitted to AMS Artificial Intelligence for the\n  Earth Systems (AIES)"},{"id":"http://arxiv.org/abs/2307.01357v3","updated":"2024-08-04T22:31:59Z","published":"2023-07-03T21:13:40Z","title":"Adaptive Principal Component Regression with Applications to Panel Data","summary":"  Principal component regression (PCR) is a popular technique for fixed-design\nerror-in-variables regression, a generalization of the linear regression\nsetting in which the observed covariates are corrupted with random noise. We\nprovide the first time-uniform finite sample guarantees for (regularized) PCR\nwhenever data is collected adaptively. Since the proof techniques for analyzing\nPCR in the fixed design setting do not readily extend to the online setting,\nour results rely on adapting tools from modern martingale concentration to the\nerror-in-variables setting. We demonstrate the usefulness of our bounds by\napplying them to the domain of panel data, a ubiquitous setting in econometrics\nand statistics. As our first application, we provide a framework for experiment\ndesign in panel data settings when interventions are assigned adaptively. Our\nframework may be thought of as a generalization of the synthetic control and\nsynthetic interventions frameworks, where data is collected via an adaptive\nintervention assignment policy. Our second application is a procedure for\nlearning such an intervention assignment policy in a setting where units arrive\nsequentially to be treated. In addition to providing theoretical performance\nguarantees (as measured by regret), we show that our method empirically\noutperforms a baseline which does not leverage error-in-variables regression.\n","authors":["Anish Agarwal","Keegan Harris","Justin Whitehouse","Zhiwei Steven Wu"],"pdf_url":"https://arxiv.org/pdf/2307.01357v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02159v1","updated":"2024-08-04T22:26:34Z","published":"2024-08-04T22:26:34Z","title":"SPINEX-TimeSeries: Similarity-based Predictions with Explainable\n  Neighbors Exploration for Time Series and Forecasting Problems","summary":"  This paper introduces a new addition to the SPINEX (Similarity-based\nPredictions with Explainable Neighbors Exploration) family, tailored\nspecifically for time series and forecasting analysis. This new algorithm\nleverages the concept of similarity and higher-order temporal interactions\nacross multiple time scales to enhance predictive accuracy and interpretability\nin forecasting. To evaluate the effectiveness of SPINEX, we present\ncomprehensive benchmarking experiments comparing it against 18 algorithms and\nacross 49 synthetic and real datasets characterized by varying trends,\nseasonality, and noise levels. Our performance assessment focused on\nforecasting accuracy and computational efficiency. Our findings reveal that\nSPINEX consistently ranks among the top 5 performers in forecasting precision\nand has a superior ability to handle complex temporal dynamics compared to\ncommonly adopted algorithms. Moreover, the algorithm's explainability features,\nPareto efficiency, and medium complexity (on the order of O(log n)) are\ndemonstrated through detailed visualizations to enhance the prediction and\ndecision-making process. We note that integrating similarity-based concepts\nopens new avenues for research in predictive analytics, promising more accurate\nand transparent decision making.\n","authors":["Ahmed Z Naser","MZ Naser"],"pdf_url":"https://arxiv.org/pdf/2408.02159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21772v2","updated":"2024-08-04T22:13:39Z","published":"2024-07-31T17:48:14Z","title":"ShieldGemma: Generative AI Content Moderation Based on Gemma","summary":"  We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers.\n","authors":["Wenjun Zeng","Yuchi Liu","Ryan Mullins","Ludovic Peran","Joe Fernandez","Hamza Harkous","Karthik Narasimhan","Drew Proud","Piyush Kumar","Bhaktipriya Radharapu","Olivia Sturman","Oscar Wahltinez"],"pdf_url":"https://arxiv.org/pdf/2407.21772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18426v2","updated":"2024-08-04T22:13:16Z","published":"2024-07-25T23:04:37Z","title":"Diffusion-based subsurface multiphysics monitoring and forecasting","summary":"  Carbon capture and storage (CCS) plays a crucial role in mitigating\ngreenhouse gas emissions, particularly from industrial outputs. Using seismic\nmonitoring can aid in an accurate and robust monitoring system to ensure the\neffectiveness of CCS and mitigate associated risks. However, conventional\nseismic wave equation-based approaches are computationally demanding, which\nhinders real-time applications. In addition to efficiency, forecasting and\nuncertainty analysis are not easy to handle using such\nnumerical-simulation-based approaches. To this end, we propose a novel\nsubsurface multiphysics monitoring and forecasting framework utilizing video\ndiffusion models. This approach can generate high-quality representations of\nCO$2$ evolution and associated changes in subsurface elastic properties. With\nreconstruction guidance, forecasting and inversion can be achieved conditioned\non historical frames and/or observational data. Meanwhile, due to the\ngenerative nature of the approach, we can quantify uncertainty in the\nprediction. Tests based on the Compass model show that the proposed method\nsuccessfully captured the inherently complex physical phenomena associated with\nCO$_2$ monitoring, and it can predict and invert the subsurface elastic\nproperties and CO$_2$ saturation with consistency in their evolution.\n","authors":["Xinquan Huang","Fu Wang","Tariq Alkhalifah"],"pdf_url":"https://arxiv.org/pdf/2407.18426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02153v1","updated":"2024-08-04T22:13:14Z","published":"2024-08-04T22:13:14Z","title":"ARVO: Atlas of Reproducible Vulnerabilities for Open Source Software","summary":"  High-quality datasets of real-world vulnerabilities are enormously valuable\nfor downstream research in software security, but existing datasets are\ntypically small, require extensive manual effort to update, and are missing\ncrucial features that such research needs. In this paper, we introduce ARVO: an\nAtlas of Reproducible Vulnerabilities in Open-source software. By sourcing\nvulnerabilities from C/C++ projects that Google's OSS-Fuzz discovered and\nimplementing a reliable re-compilation system, we successfully reproduce more\nthan 5,000 memory vulnerabilities across over 250 projects, each with a\ntriggering input, the canonical developer-written patch for fixing the\nvulnerability, and the ability to automatically rebuild the project from source\nand run it at its vulnerable and patched revisions. Moreover, our dataset can\nbe automatically updated as OSS-Fuzz finds new vulnerabilities, allowing it to\ngrow over time. We provide a thorough characterization of the ARVO dataset,\nshow that it can locate fixes more accurately than Google's own OSV\nreproduction effort, and demonstrate its value for future research through two\ncase studies: firstly evaluating real-world LLM-based vulnerability repair, and\nsecondly identifying over 300 falsely patched (still-active) zero-day\nvulnerabilities from projects improperly labeled by OSS-Fuzz.\n","authors":["Xiang Mei","Pulkit Singh Singaria","Jordi Del Castillo","Haoran Xi"," Abdelouahab"," Benchikh","Tiffany Bao","Ruoyu Wang","Yan Shoshitaishvili","Adam Doup","Hammond Pearce","Brendan Dolan-Gavitt"],"pdf_url":"https://arxiv.org/pdf/2408.02153v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02152v1","updated":"2024-08-04T22:00:34Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"  Existing generative retrieval (GR) approaches rely on training-based\nindexing, i.e., fine-tuning a model to memorise the associations between a\nquery and the document identifier (docid) of a relevant document.\nTraining-based indexing has three limitations: high training overhead,\nunder-utilization of the pre-trained knowledge of large language models (LLMs),\nand challenges in adapting to a dynamic document corpus. To address the above\nissues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).\nIt has a novel few-shot indexing process, where we prompt an LLM to generate\ndocids for all documents in a corpus, ultimately creating a docid bank for the\nentire corpus. During retrieval, we feed a query to the same LLM and constrain\nit to generate a docid within the docid bank created during indexing, and then\nmap the generated docid back to its corresponding document. Few-Shot GR relies\nsolely on prompting an LLM without requiring any training, making it more\nefficient. Moreover, we devise few-shot indexing with one-to-many mapping to\nfurther enhance Few-Shot GR. Experiments show that Few-Shot GR achieves\nsuperior performance to state-of-the-art GR methods that require heavy\ntraining.\n","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2408.02152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16056v2","updated":"2024-08-04T20:45:16Z","published":"2023-05-25T13:38:53Z","title":"Markov Decision Processes under External Temporal Processes","summary":"  Most reinforcement learning algorithms treat the context under which they\noperate as a stationary, isolated, and undisturbed environment. However, in\nreal world applications, environments constantly change due to a variety of\nexternal events. To address this problem, we study Markov Decision Processes\n(MDP) under the influence of an external temporal process. We formalize this\nnotion and discuss conditions under which the problem becomes tractable with\nsuitable solutions. We propose a policy iteration algorithm to solve this\nproblem and theoretically analyze its performance. We derive results on the\nsample complexity of the algorithm and study its dependency on the extent of\nnon-stationarity of the environment. We then conduct experiments to illustrate\nour results in a classic control environment.\n","authors":["Ranga Shaarad Ayyagari","Ambedkar Dukkipati"],"pdf_url":"https://arxiv.org/pdf/2305.16056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02140v1","updated":"2024-08-04T20:38:45Z","published":"2024-08-04T20:38:45Z","title":"VidModEx: Interpretable and Efficient Black Box Model Extraction for\n  High-Dimensional Spaces","summary":"  In the domain of black-box model extraction, conventional methods reliant on\nsoft labels or surrogate datasets struggle with scaling to high-dimensional\ninput spaces and managing the complexity of an extensive array of interrelated\nclasses. In this work, we present a novel approach that utilizes SHAP (SHapley\nAdditive exPlanations) to enhance synthetic data generation. SHAP quantifies\nthe individual contributions of each input feature towards the victim model's\noutput, facilitating the optimization of an energy-based GAN towards a\ndesirable output. This method significantly boosts performance, achieving a\n16.45% increase in the accuracy of image classification models and extending to\nvideo classification models with an average improvement of 26.11% and a maximum\nof 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics\n600, and Something-Something V2. We further demonstrate the effectiveness and\npractical utility of our method under various scenarios, including the\navailability of top-k prediction probabilities, top-k prediction labels, and\ntop-1 labels.\n","authors":["Somnath Sendhil Kumar","Yuvaraj Govindarajulu","Pavan Kulkarni","Manojkumar Parmar"],"pdf_url":"https://arxiv.org/pdf/2408.02140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02131v1","updated":"2024-08-04T20:02:07Z","published":"2024-08-04T20:02:07Z","title":"Model Hijacking Attack in Federated Learning","summary":"  Machine learning (ML), driven by prominent paradigms such as centralized and\nfederated learning, has made significant progress in various critical\napplications ranging from autonomous driving to face recognition. However, its\nremarkable success has been accompanied by various attacks. Recently, the model\nhijacking attack has shown that ML models can be hijacked to execute tasks\ndifferent from their original tasks, which increases both accountability and\nparasitic computational risks. Nevertheless, thus far, this attack has only\nfocused on centralized learning. In this work, we broaden the scope of this\nattack to the federated learning domain, where multiple clients collaboratively\ntrain a global model without sharing their data. Specifically, we present\nHijackFL, the first-of-its-kind hijacking attack against the global model in\nfederated learning. The adversary aims to force the global model to perform a\ndifferent task (called hijacking task) from its original task without the\nserver or benign client noticing. To accomplish this, unlike existing methods\nthat use data poisoning to modify the target model's parameters, HijackFL\nsearches for pixel-level perturbations based on their local model (without\nmodifications) to align hijacking samples with the original ones in the feature\nspace. When performing the hijacking task, the adversary applies these cloaks\nto the hijacking samples, compelling the global model to identify them as\noriginal samples and predict them accordingly. We conduct extensive experiments\non four benchmark datasets and three popular models. Empirical results\ndemonstrate that its attack performance outperforms baselines. We further\ninvestigate the factors that affect its performance and discuss possible\ndefenses to mitigate its impact.\n","authors":["Zheng Li","Siyuan Wu","Ruichuan Chen","Paarijaat Aditya","Istemi Ekin Akkus","Manohar Vanga","Min Zhang","Hao Li","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02123v1","updated":"2024-08-04T19:37:30Z","published":"2024-08-04T19:37:30Z","title":"FovEx: Human-inspired Explanations for Vision Transformers and\n  Convolutional Neural Networks","summary":"  Explainability in artificial intelligence (XAI) remains a crucial aspect for\nfostering trust and understanding in machine learning models. Current visual\nexplanation techniques, such as gradient-based or class-activation-based\nmethods, often exhibit a strong dependence on specific model architectures.\nConversely, perturbation-based methods, despite being model-agnostic, are\ncomputationally expensive as they require evaluating models on a large number\nof forward passes. In this work, we introduce Foveation-based Explanations\n(FovEx), a novel XAI method inspired by human vision. FovEx seamlessly\nintegrates biologically inspired perturbations by iteratively creating foveated\nrenderings of the image and combines them with gradient-based visual\nexplorations to determine locations of interest efficiently. These locations\nare selected to maximize the performance of the model to be explained with\nrespect to the downstream task and then combined to generate an attribution\nmap. We provide a thorough evaluation with qualitative and quantitative\nassessments on established benchmarks. Our method achieves state-of-the-art\nperformance on both transformers (on 4 out of 5 metrics) and convolutional\nmodels (on 3 out of 5 metrics), demonstrating its versatility among various\narchitectures. Furthermore, we show the alignment between the explanation map\nproduced by FovEx and human gaze patterns (+14\\% in NSS compared to RISE,\n+203\\% in NSS compared to GradCAM). This comparison enhances our confidence in\nFovEx's ability to close the interpretation gap between humans and machines.\n","authors":["Mahadev Prasad Panda","Matteo Tiezzi","Martina Vilas","Gemma Roig","Bjoern M. Eskofier","Dario Zanca"],"pdf_url":"https://arxiv.org/pdf/2408.02123v1.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2408.02117v1","updated":"2024-08-04T19:14:36Z","published":"2024-08-04T19:14:36Z","title":"Value-Based Rationales Improve Social Experience: A Multiagent\n  Simulation Study","summary":"  We propose Exanna, a framework to realize agents that incorporate values in\ndecision making. An Exannaagent considers the values of itself and others when\nproviding rationales for its actions and evaluating the rationales provided by\nothers. Via multiagent simulation, we demonstrate that considering values in\ndecision making and producing rationales, especially for norm-deviating\nactions, leads to (1) higher conflict resolution, (2) better social experience,\n(3) higher privacy, and (4) higher flexibility.\n","authors":["Sz-Ting Tzeng","Nirav Ajmeri","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2408.02117v1.pdf","comment":"13 pages, 13 figures, 13 tables (and supplementary material with\n  reproducibility and additional results), accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2408.02114v1","updated":"2024-08-04T18:57:21Z","published":"2024-08-04T18:57:21Z","title":"Recent Advances in Multi-Choice Machine Reading Comprehension: A Survey\n  on Methods and Datasets","summary":"  This paper provides a thorough examination of recent developments in the\nfield of multi-choice Machine Reading Comprehension (MRC). Focused on benchmark\ndatasets, methodologies, challenges, and future trajectories, our goal is to\noffer researchers a comprehensive overview of the current landscape in\nmulti-choice MRC. The analysis delves into 30 existing cloze-style and\nmultiple-choice MRC benchmark datasets, employing a refined classification\nmethod based on attributes such as corpus style, domain, complexity, context\nstyle, question style, and answer style. This classification system enhances\nour understanding of each dataset's diverse attributes and categorizes them\nbased on their complexity. Furthermore, the paper categorizes recent\nmethodologies into Fine-tuned and Prompt-tuned methods. Fine-tuned methods\ninvolve adapting pre-trained language models (PLMs) to a specific task through\nretraining on domain-specific datasets, while prompt-tuned methods use prompts\nto guide PLM response generation, presenting potential applications in\nzero-shot or few-shot learning scenarios. By contributing to ongoing\ndiscussions, inspiring future research directions, and fostering innovations,\nthis paper aims to propel multi-choice MRC towards new frontiers of\nachievement.\n","authors":["Shima Foolad","Kourosh Kiani","Razieh Rastgoo"],"pdf_url":"https://arxiv.org/pdf/2408.02114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02111v1","updated":"2024-08-04T18:47:55Z","published":"2024-08-04T18:47:55Z","title":"Understanding Deep Learning via Notions of Rank","summary":"  Despite the extreme popularity of deep learning in science and industry, its\nformal understanding is limited. This thesis puts forth notions of rank as key\nfor developing a theory of deep learning, focusing on the fundamental aspects\nof generalization and expressiveness. In particular, we establish that\ngradient-based training can induce an implicit regularization towards low rank\nfor several neural network architectures, and demonstrate empirically that this\nphenomenon may facilitate an explanation of generalization over natural data\n(e.g., audio, images, and text). Then, we characterize the ability of graph\nneural networks to model interactions via a notion of rank, which is commonly\nused for quantifying entanglement in quantum physics. A central tool underlying\nthese results is a connection between neural networks and tensor\nfactorizations. Practical implications of our theory for designing explicit\nregularization schemes and data preprocessing algorithms are presented.\n","authors":["Noam Razin"],"pdf_url":"https://arxiv.org/pdf/2408.02111v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2405.01013v2","updated":"2024-08-04T18:09:39Z","published":"2024-05-02T05:29:22Z","title":"Non-clairvoyant Scheduling with Partial Predictions","summary":"  The non-clairvoyant scheduling problem has gained new interest within\nlearning-augmented algorithms, where the decision-maker is equipped with\npredictions without any quality guarantees. In practical settings, access to\npredictions may be reduced to specific instances, due to cost or data\nlimitations. Our investigation focuses on scenarios where predictions for only\n$B$ job sizes out of $n$ are available to the algorithm. We first establish\nnear-optimal lower bounds and algorithms in the case of perfect predictions.\nSubsequently, we present a learning-augmented algorithm satisfying the\nrobustness, consistency, and smoothness criteria, and revealing a novel\ntradeoff between consistency and smoothness inherent in the scenario with a\nrestricted number of predictions.\n","authors":["Ziyad Benomar","Vianney Perchet"],"pdf_url":"https://arxiv.org/pdf/2405.01013v2.pdf","comment":"Accepted as a conference paper at ICML 2024"},{"id":"http://arxiv.org/abs/2402.12668v2","updated":"2024-08-04T18:07:51Z","published":"2024-02-20T02:36:26Z","title":"Randomization Can Reduce Both Bias and Variance: A Case Study in Random\n  Forests","summary":"  We study the often overlooked phenomenon, first noted in\n\\cite{breiman2001random}, that random forests appear to reduce bias compared to\nbagging. Motivated by an interesting paper by \\cite{mentch2020randomization},\nwhere the authors argue that random forests reduce effective degrees of freedom\nand only outperform bagging ensembles in low signal-to-noise ratio (SNR)\nsettings, we explore how random forests can uncover patterns in the data missed\nby bagging. We empirically demonstrate that in the presence of such patterns,\nrandom forests reduce bias along with variance and increasingly outperform\nbagging ensembles when SNR is high. Our observations offer insights into the\nreal-world success of random forests across a range of SNRs and enhance our\nunderstanding of the difference between random forests and bagging ensembles\nwith respect to the randomization injected into each split. Our investigations\nalso yield practical insights into the importance of tuning $mtry$ in random\nforests.\n","authors":["Brian Liu","Rahul Mazumder"],"pdf_url":"https://arxiv.org/pdf/2402.12668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00927v2","updated":"2024-08-04T18:04:50Z","published":"2024-07-01T03:17:34Z","title":"Learnability of Parameter-Bounded Bayes Nets","summary":"  Bayes nets are extensively used in practice to efficiently represent joint\nprobability distributions over a set of random variables and capture dependency\nrelations. In a seminal paper, Chickering et al. (JMLR 2004) showed that given\na distribution $\\mathbb{P}$, that is defined as the marginal distribution of a\nBayes net, it is $\\mathsf{NP}$-hard to decide whether there is a\nparameter-bounded Bayes net that represents $\\mathbb{P}$. They called this\nproblem LEARN. In this work, we extend the $\\mathsf{NP}$-hardness result of\nLEARN and prove the $\\mathsf{NP}$-hardness of a promise search variant of\nLEARN, whereby the Bayes net in question is guaranteed to exist and one is\nasked to find such a Bayes net. We complement our hardness result with a\npositive result about the sample complexity that is sufficient to recover a\nparameter-bounded Bayes net that is close (in TV distance) to a given\ndistribution $\\mathbb{P}$, that is represented by some parameter-bounded Bayes\nnet, generalizing a degree-bounded sample complexity result of Brustle et al.\n(EC 2020).\n","authors":["Arnab Bhattacharyya","Davin Choo","Sutanu Gayen","Dimitrios Myrisiotis"],"pdf_url":"https://arxiv.org/pdf/2407.00927v2.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.14302v2","updated":"2024-08-04T17:48:33Z","published":"2023-12-21T21:22:41Z","title":"Exploiting Novel GPT-4 APIs","summary":"  Language model attacks typically assume one of two extreme threat models:\nfull white-box access to model weights, or black-box access limited to a text\ngeneration API. However, real-world APIs are often more flexible than just text\ngeneration: these APIs expose \"gray-box\" access leading to new threat vectors.\nTo explore this, we red-team three new functionalities exposed in the GPT-4\nAPIs: fine-tuning, function calling and knowledge retrieval. We find that\nfine-tuning a model on as few as 15 harmful examples or 100 benign examples can\nremove core safeguards from GPT-4, enabling a range of harmful outputs.\nFurthermore, we find that GPT-4 Assistants readily divulge the function call\nschema and can be made to execute arbitrary function calls. Finally, we find\nthat knowledge retrieval can be hijacked by injecting instructions into\nretrieval documents. These vulnerabilities highlight that any additions to the\nfunctionality exposed by an API can create new vulnerabilities.\n","authors":["Kellin Pelrine","Mohammad Taufeeque","Micha Zajc","Euan McLean","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2312.14302v2.pdf","comment":"10 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2307.00575v2","updated":"2024-08-04T16:28:21Z","published":"2023-07-02T13:59:47Z","title":"Mode-wise Principal Subspace Pursuit and Matrix Spiked Covariance Model","summary":"  This paper introduces a novel framework called Mode-wise Principal Subspace\nPursuit (MOP-UP) to extract hidden variations in both the row and column\ndimensions for matrix data. To enhance the understanding of the framework, we\nintroduce a class of matrix-variate spiked covariance models that serve as\ninspiration for the development of the MOP-UP algorithm. The MOP-UP algorithm\nconsists of two steps: Average Subspace Capture (ASC) and Alternating\nProjection (AP). These steps are specifically designed to capture the row-wise\nand column-wise dimension-reduced subspaces which contain the most informative\nfeatures of the data. ASC utilizes a novel average projection operator as\ninitialization and achieves exact recovery in the noiseless setting. We analyze\nthe convergence and non-asymptotic error bounds of MOP-UP, introducing a\nblockwise matrix eigenvalue perturbation bound that proves the desired bound,\nwhere classic perturbation bounds fail. The effectiveness and practical merits\nof the proposed framework are demonstrated through experiments on both\nsimulated and real datasets. Lastly, we discuss generalizations of our approach\nto higher-order data.\n","authors":["Runshi Tang","Ming Yuan","Anru R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.00575v2.pdf","comment":"Journal of the Royal Statistical Society, Series B, to appear"},{"id":"http://arxiv.org/abs/2404.14462v3","updated":"2024-08-04T16:24:15Z","published":"2024-04-22T06:19:46Z","title":"Towards smaller, faster decoder-only transformers: Architectural\n  variants and their implications","summary":"  In recent times, the research on Large Language Models (LLMs) has grown\nexponentially, predominantly focusing on models underpinned by the transformer\narchitecture, as established by [1], and further developed through the\ndecoder-only variations by [2]. Contemporary efforts in this field primarily\naim to enhance model capabilities by scaling up both the architecture and data\nvolumes utilized during training. However, the exploration into reduce these\nmodel sizes while preserving their efficacy remains scant. In this study, we\nintroduce three modifications to the decoder-only transformer architecture,\nnamely ParallelGPT (pgpt), LinearGPT (lgpt), and ConvGPT (cgpt). These variants\ndemonstrate comparable performance to the conventional architecture in language\ngeneration, yet benefit from reduced model sizes and faster training processes.\nWe open-source the model weights and the complete codebase for these\nimplementation for further research.\n","authors":["Sathya Krishnan Suresh","Shunmugapriya P"],"pdf_url":"https://arxiv.org/pdf/2404.14462v3.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.02461v2","updated":"2024-08-04T15:41:23Z","published":"2024-07-02T17:40:06Z","title":"Decentralized Intelligence Network (DIN)","summary":"  Decentralized Intelligence Network (DIN) is a theoretical framework\naddressing data fragmentation and siloing challenges, enabling scalable AI\nthrough data sovereignty. It facilitates effective AI utilization within\nsovereign networks by overcoming barriers to accessing diverse data sources,\nleveraging: 1) personal data stores to ensure data sovereignty, where data\nremains securely within Participants' control; 2) a scalable federated learning\nprotocol implemented on a public blockchain for decentralized AI training,\nwhere only model parameter updates are shared, keeping data within the personal\ndata stores; and 3) a scalable, trustless cryptographic rewards mechanism on a\npublic blockchain to incentivize participation and ensure fair reward\ndistribution through a decentralized auditing protocol. This approach\nguarantees that no entity can prevent or control access to training data or\ninfluence financial benefits, as coordination and reward distribution are\nmanaged on the public blockchain with an immutable record. The framework\nsupports effective AI training by allowing Participants to maintain control\nover their data, benefit financially, and contribute to a decentralized,\nscalable ecosystem that leverages collective AI to develop beneficial\nalgorithms.\n","authors":["Abraham Nash"],"pdf_url":"https://arxiv.org/pdf/2407.02461v2.pdf","comment":"14 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.02065v1","updated":"2024-08-04T15:30:15Z","published":"2024-08-04T15:30:15Z","title":"A Multi-class Ride-hailing Service Subsidy System Utilizing Deep Causal\n  Networks","summary":"  In the ride-hailing industry, subsidies are predominantly employed to\nincentivize consumers to place more orders, thereby fostering market growth.\nCausal inference techniques are employed to estimate the consumer elasticity\nwith different subsidy levels. However, the presence of confounding effects\nposes challenges in achieving an unbiased estimate of the uplift effect. We\nintroduce a consumer subsidizing system to capture relationships between\nsubsidy propensity and the treatment effect, which proves effective while\nmaintaining a lightweight online environment.\n","authors":["Zhe Yu","Chi Xia","Shaosheng Cao","Lin Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02063v3","updated":"2024-08-04T15:16:27Z","published":"2024-05-03T12:48:21Z","title":"Few-sample Variational Inference of Bayesian Neural Networks with\n  Arbitrary Nonlinearities","summary":"  Bayesian Neural Networks (BNNs) extend traditional neural networks to provide\nuncertainties associated with their outputs. On the forward pass through a BNN,\npredictions (and their uncertainties) are made either by Monte Carlo sampling\nnetwork weights from the learned posterior or by analytically propagating\nstatistical moments through the network. Though flexible, Monte Carlo sampling\nis computationally expensive and can be infeasible or impractical under\nresource constraints or for large networks. While moment propagation can\nameliorate the computational costs of BNN inference, it can be difficult or\nimpossible for networks with arbitrary nonlinearities, thereby restricting the\npossible set of network layers permitted with such a scheme. In this work, we\ndemonstrate a simple yet effective approach for propagating statistical moments\nthrough arbitrary nonlinearities with only 3 deterministic samples, enabling\nfew-sample variational inference of BNNs without restricting the set of network\nlayers used. Furthermore, we leverage this approach to demonstrate a novel\nnonlinear activation function that we use to inject physics-informed prior\ninformation into output nodes of a BNN.\n","authors":["David J. Schodt"],"pdf_url":"https://arxiv.org/pdf/2405.02063v3.pdf","comment":"Comment 1: Fixed plot markers in figure 6 to match legend and to\n  improve grayscale appearance"},{"id":"http://arxiv.org/abs/2408.02056v1","updated":"2024-08-04T15:07:44Z","published":"2024-08-04T15:07:44Z","title":"MedSyn: LLM-based Synthetic Medical Text Generation Framework","summary":"  Generating synthetic text addresses the challenge of data availability in\nprivacy-sensitive domains such as healthcare. This study explores the\napplicability of synthetic data in real-world medical settings. We introduce\nMedSyn, a novel medical text generation framework that integrates large\nlanguage models with a Medical Knowledge Graph (MKG). We use MKG to sample\nprior medical information for the prompt and generate synthetic clinical notes\nwith GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data\nthrough application in the ICD code prediction task. Our research indicates\nthat synthetic data can increase the classification accuracy of vital and\nchallenging codes by up to 17.8% compared to settings without synthetic data.\nFurthermore, to provide new data for further research in the healthcare domain,\nwe present the largest open-source synthetic dataset of clinical notes for the\nRussian language, comprising over 41k samples covering 219 ICD-10 codes.\n","authors":["Gleb Kumichev","Pavel Blinov","Yulia Kuzkina","Vasily Goncharov","Galina Zubkova","Nikolai Zenovkin","Aleksei Goncharov","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2408.02056v1.pdf","comment":"16 pages, accepted to ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2408.02052v1","updated":"2024-08-04T15:00:22Z","published":"2024-08-04T15:00:22Z","title":"EOL: Transductive Few-Shot Open-Set Recognition by Enhancing Outlier\n  Logits","summary":"  In Few-Shot Learning (FSL), models are trained to recognise unseen objects\nfrom a query set, given a few labelled examples from a support set. In standard\nFSL, models are evaluated on query instances sampled from the same class\ndistribution of the support set. In this work, we explore the more nuanced and\npractical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard\nFSL, OSFSL incorporates unknown classes into the query set, thereby requiring\nthe model not only to classify known classes but also to identify outliers.\nBuilding on the groundwork laid by previous studies, we define a novel\ntransductive inference technique that leverages the InfoMax principle to\nexploit the unlabelled query set. We called our approach the Enhanced Outlier\nLogit (EOL) method. EOL refines class prototype representations through model\ncalibration, effectively balancing the inlier-outlier ratio. This calibration\nenhances pseudo-label accuracy for the query set and improves the optimisation\nobjective within the transductive inference process. We provide a comprehensive\nempirical evaluation demonstrating that EOL consistently surpasses traditional\nmethods, recording performance improvements ranging from approximately $+1.3%$\nto $+6.3%$ across a variety of classification and outlier detection metrics and\nbenchmarks, even in the presence of inlier-outlier imbalance.\n","authors":["Mateusz Ochal","Massimiliano Patacchiola","Malik Boudiaf","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02052v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2408.02050v1","updated":"2024-08-04T14:57:44Z","published":"2024-08-04T14:57:44Z","title":"Recovering the state and dynamics of autonomous system with partial\n  states solution using neural networks","summary":"  In this paper we explore the performance of deep hidden physics model (M.\nRaissi 2018) for autonomous system, this systems do not explicitly depend on\ntime. The dynamics of states are dependent on states itself. Such systems can\nbe found in nature and have applications\n  in modeling chemical concentrations, population dynamics, n-body problems in\nphysics etc. In this work we are going to see how we can obtain dynamics of\nstates based on solution of limited partial states. The proposed method can\nfind the state and dynamics of which the data is provided in the training,\nalthough we do not claim to accurately find the solution of states whose data\nis not utilized while training.\n","authors":["Vijay Kag"],"pdf_url":"https://arxiv.org/pdf/2408.02050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07769v3","updated":"2024-08-04T14:46:14Z","published":"2024-01-15T15:27:24Z","title":"Deep Evolutional Instant Interest Network for CTR Prediction in\n  Trigger-Induced Recommendation","summary":"  The recommendation has been playing a key role in many industries, e.g.,\ne-commerce, streaming media, social media, etc. Recently, a new recommendation\nscenario, called Trigger-Induced Recommendation (TIR), where users are able to\nexplicitly express their instant interests via trigger items, is emerging as an\nessential role in many e-commerce platforms, e.g., Alibaba.com and Amazon.\nWithout explicitly modeling the user's instant interest, traditional\nrecommendation methods usually obtain sub-optimal results in TIR. Even though\nthere are a few methods considering the trigger and target items simultaneously\nto solve this problem, they still haven't taken into account temporal\ninformation of user behaviors, the dynamic change of user instant interest when\nthe user scrolls down and the interactions between the trigger and target\nitems. To tackle these problems, we propose a novel method -- Deep Evolutional\nInstant Interest Network (DEI2N), for click-through rate prediction in TIR\nscenarios. Specifically, we design a User Instant Interest Modeling Layer to\npredict the dynamic change of the intensity of instant interest when the user\nscrolls down. Temporal information is utilized in user behavior modeling.\nMoreover, an Interaction Layer is introduced to learn better interactions\nbetween the trigger and target items. We evaluate our method on several offline\nand real-world industrial datasets. Experimental results show that our proposed\nDEI2N outperforms state-of-the-art baselines. In addition, online A/B testing\ndemonstrates the superiority over the existing baseline in real-world\nproduction environments.\n","authors":["Zhibo Xiao","Luwei Yang","Tao Zhang","Wen Jiang","Wei Ning","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2401.07769v3.pdf","comment":"7 pages, 6 figures, accepted by the 17th ACM International Conference\n  on Web Search and Data Mining(WSDM'2024)"},{"id":"http://arxiv.org/abs/2408.02045v1","updated":"2024-08-04T14:45:26Z","published":"2024-08-04T14:45:26Z","title":"DNA-SE: Towards Deep Neural-Nets Assisted Semiparametric Estimation","summary":"  Semiparametric statistics play a pivotal role in a wide range of domains,\nincluding but not limited to missing data, causal inference, and transfer\nlearning, to name a few. In many settings, semiparametric theory leads to\n(nearly) statistically optimal procedures that yet involve numerically solving\nFredholm integral equations of the second kind. Traditional numerical methods,\nsuch as polynomial or spline approximations, are difficult to scale to\nmulti-dimensional problems. Alternatively, statisticians may choose to\napproximate the original integral equations by ones with closed-form solutions,\nresulting in computationally more efficient, but statistically suboptimal or\neven incorrect procedures. To bridge this gap, we propose a novel framework by\nformulating the semiparametric estimation problem as a bi-level optimization\nproblem; and then we develop a scalable algorithm called Deep Neural-Nets\nAssisted Semiparametric Estimation (DNA-SE) by leveraging the universal\napproximation property of Deep Neural-Nets (DNN) to streamline semiparametric\nprocedures. Through extensive numerical experiments and a real data analysis,\nwe demonstrate the numerical and statistical advantages of $\\dnase$ over\ntraditional methods. To the best of our knowledge, we are the first to bring\nDNN into semiparametric statistics as a numerical solver of integral equations\nin our proposed general framework.\n","authors":["Qinshuo Liu","Zixin Wang","Xi-An Li","Xinyao Ji","Lei Zhang","Lin Liu","Zhonghua Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02045v1.pdf","comment":"semiparametric statistics, missing data, causal inference, Fredholm\n  integral equations of the second kind, bi-level optimization, deep learning,\n  AI for science"},{"id":"http://arxiv.org/abs/2408.02033v1","updated":"2024-08-04T13:51:18Z","published":"2024-08-04T13:51:18Z","title":"Enhancing Human Action Recognition and Violence Detection Through Deep\n  Learning Audiovisual Fusion","summary":"  This paper proposes a hybrid fusion-based deep learning approach based on two\ndifferent modalities, audio and video, to improve human activity recognition\nand violence detection in public places. To take advantage of audiovisual\nfusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning\n(HFBDL) are used and compared. Since the objective is to detect and recognize\nhuman violence in public places, Real-life violence situation (RLVS) dataset is\nexpanded and used. Simulating results of HFBDL show 96.67\\% accuracy on\nvalidation data, which is more accurate than the other state-of-the-art methods\non this dataset. To showcase our model's ability in real-world scenarios,\nanother dataset of 54 sounded videos of both violent and non-violent situations\nwas recorded. The model could successfully detect 52 out of 54 videos\ncorrectly. The proposed method shows a promising performance on real scenarios.\nThus, it can be used for human action recognition and violence detection in\npublic places for security purposes.\n","authors":["Pooya Janani","Amirabolfazl Suratgar","Afshin Taghvaeipour"],"pdf_url":"https://arxiv.org/pdf/2408.02033v1.pdf","comment":"This work has been submitted to the IEEE for possible publication, 10\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02022v1","updated":"2024-08-04T13:19:45Z","published":"2024-08-04T13:19:45Z","title":"Scenario-based Thermal Management Parametrization Through Deep\n  Reinforcement Learning","summary":"  The thermal system of battery electric vehicles demands advanced control. Its\nthermal management needs to effectively control active components across\nvarying operating conditions. While robust control function parametrization is\nrequired, current methodologies show significant drawbacks. They consume\nconsiderable time, human effort, and extensive real-world testing.\nConsequently, there is a need for innovative and intelligent solutions that are\ncapable of autonomously parametrizing embedded controllers. Addressing this\nissue, our paper introduces a learning-based tuning approach. We propose a\nmethodology that benefits from automated scenario generation for increased\nrobustness across vehicle usage scenarios. Our deep reinforcement learning\nagent processes the tuning task context and incorporates an image-based\ninterpretation of embedded parameter sets. We demonstrate its applicability to\na valve controller parametrization task and verify it in real-world vehicle\ntesting. The results highlight the competitive performance to baseline methods.\nThis novel approach contributes to the shift towards virtual development of\nthermal management functions, with promising potential of large-scale parameter\ntuning in the automotive industry.\n","authors":["Thomas Rudolf","Philip Muhl","Sren Hohmann","Lutz Eckstein"],"pdf_url":"https://arxiv.org/pdf/2408.02022v1.pdf","comment":"8 pages, 7 figures, 2 tables, 1 algorithm, 10 equations, conference"},{"id":"http://arxiv.org/abs/2408.02019v1","updated":"2024-08-04T13:11:49Z","published":"2024-08-04T13:11:49Z","title":"Personalized Federated Learning on Heterogeneous and Long-Tailed Data\n  via Expert Collaborative Learning","summary":"  Personalized Federated Learning (PFL) aims to acquire customized models for\neach client without disclosing raw data by leveraging the collective knowledge\nof distributed clients. However, the data collected in real-world scenarios is\nlikely to follow a long-tailed distribution. For example, in the medical\ndomain, it is more common for the number of general health notes to be much\nlarger than those specifically relatedto certain diseases. The presence of\nlong-tailed data can significantly degrade the performance of PFL models.\nAdditionally, due to the diverse environments in which each client operates,\ndata heterogeneity is also a classic challenge in federated learning. In this\npaper, we explore the joint problem of global long-tailed distribution and data\nheterogeneity in PFL and propose a method called Expert Collaborative Learning\n(ECL) to tackle this problem. Specifically, each client has multiple experts,\nand each expert has a different training subset, which ensures that each class,\nespecially the minority classes, receives sufficient training. Multiple experts\ncollaborate synergistically to produce the final prediction output. Without\nspecial bells and whistles, the vanilla ECL outperforms other state-of-the-art\nPFL methods on several benchmark datasets under different degrees of data\nheterogeneity and long-tailed distribution.\n","authors":["Fengling Lv","Xinyi Shang","Yang Zhou","Yiqun Zhang","Mengke Li","Yang Lu"],"pdf_url":"https://arxiv.org/pdf/2408.02019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02014v1","updated":"2024-08-04T12:52:44Z","published":"2024-08-04T12:52:44Z","title":"Unsupervised Representation Learning by Balanced Self Attention Matching","summary":"  Many leading self-supervised methods for unsupervised representation\nlearning, in particular those for embedding image features, are built on\nvariants of the instance discrimination task, whose optimization is known to be\nprone to instabilities that can lead to feature collapse. Different techniques\nhave been devised to circumvent this issue, including the use of negative pairs\nwith different contrastive losses, the use of external memory banks, and\nbreaking of symmetry by using separate encoding networks with possibly\ndifferent structures. Our method, termed BAM, rather than directly matching\nfeatures of different views (augmentations) of input images, is based on\nmatching their self-attention vectors, which are the distributions of\nsimilarities to the entire set of augmented images of a batch. We obtain rich\nrepresentations and avoid feature collapse by minimizing a loss that matches\nthese distributions to their globally balanced and entropy regularized version,\nwhich is obtained through a simple self-optimal-transport computation. We\nablate and verify our method through a wide set of experiments that show\ncompetitive performance with leading methods on both semi-supervised and\ntransfer-learning benchmarks. Our implementation and pre-trained models are\navailable at github.com/DanielShalam/BAM .\n","authors":["Daniel Shalam","Simon Korman"],"pdf_url":"https://arxiv.org/pdf/2408.02014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01249v2","updated":"2024-08-04T12:34:29Z","published":"2023-09-03T19:24:34Z","title":"Large AI Model Empowered Multimodal Semantic Communications","summary":"  Multimodal signals, including text, audio, image, and video, can be\nintegrated into Semantic Communication (SC) systems to provide an immersive\nexperience with low latency and high quality at the semantic level. However,\nthe multimodal SC has several challenges, including data heterogeneity,\nsemantic ambiguity, and signal distortion during transmission. Recent\nadvancements in large AI models, particularly in the Multimodal Language Model\n(MLM) and Large Language Model (LLM), offer potential solutions for addressing\nthese issues. To this end, we propose a Large AI Model-based Multimodal SC\n(LAM-MSC) framework, where we first present the MLM-based Multimodal Alignment\n(MMA) that utilizes the MLM to enable the transformation between multimodal and\nunimodal data while preserving semantic consistency. Then, a personalized\nLLM-based Knowledge Base (LKB) is proposed, which allows users to perform\npersonalized semantic extraction or recovery through the LLM. This effectively\naddresses the semantic ambiguity. Finally, we apply the Conditional Generative\nadversarial network-based channel Estimation (CGE) for estimating the wireless\nchannel state information. This approach effectively mitigates the impact of\nfading channels in SC. Finally, we conduct simulations that demonstrate the\nsuperior performance of the LAM-MSC framework.\n","authors":["Feibo Jiang","Li Dong","Yubo Peng","Kezhi Wang","Kun Yang","Cunhua Pan","Xiaohu You"],"pdf_url":"https://arxiv.org/pdf/2309.01249v2.pdf","comment":"Accepted by IEEE CM"},{"id":"http://arxiv.org/abs/2312.16019v3","updated":"2024-08-04T12:12:36Z","published":"2023-12-26T12:18:31Z","title":"Robust Survival Analysis with Adversarial Regularization","summary":"  Survival Analysis (SA) models the time until an event occurs, with\napplications in fields like medicine, defense, finance, and aerospace. Recent\nwork shows that Neural Networks (NNs) can capture complex relationships in SA.\nHowever, dataset uncertainties (e.g., noisy measurements, human error) can\ndegrade model performance. To address this, we leverage NN verification\nadvances to create algorithms for robust, fully-parametric survival models. We\nintroduce a robust loss function and use CROWN-IBP regularization to handle\ncomputational challenges in the Min-Max problem. Evaluating our approach on\nSurvSet datasets, we find that our Survival Analysis with Adversarial\nRegularization (SAWAR) method consistently outperforms baselines under various\nperturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier\nScore (IBS), and Concordance Index (CI). This demonstrates that adversarial\nregularization enhances SA performance and calibration, mitigating data\nuncertainty and improving generalization across diverse datasets up to 150%\nacross all perturbation magnitudes.\n","authors":["Michael Potter","Stefano Maxenti","Michael Everett"],"pdf_url":"https://arxiv.org/pdf/2312.16019v3.pdf","comment":"12 pages, 2 figures, submission to IEEE Journal of Biomedical and\n  Health Informatics"},{"id":"http://arxiv.org/abs/2408.01993v1","updated":"2024-08-04T11:25:07Z","published":"2024-08-04T11:25:07Z","title":"Towards Automatic Hands-on-Keyboard Attack Detection Using LLMs in EDR\n  Solutions","summary":"  Endpoint Detection and Remediation (EDR) platforms are essential for\nidentifying and responding to cyber threats. This study presents a novel\napproach using Large Language Models (LLMs) to detect Hands-on-Keyboard (HOK)\ncyberattacks. Our method involves converting endpoint activity data into\nnarrative forms that LLMs can analyze to distinguish between normal operations\nand potential HOK attacks. We address the challenges of interpreting endpoint\ndata by segmenting narratives into windows and employing a dual training\nstrategy. The results demonstrate that LLM-based models have the potential to\noutperform traditional machine learning methods, offering a promising direction\nfor enhancing EDR capabilities and apply LLMs in cybersecurity.\n","authors":["Amit Portnoy","Ehud Azikri","Shay Kels"],"pdf_url":"https://arxiv.org/pdf/2408.01993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01988v1","updated":"2024-08-04T11:00:43Z","published":"2024-08-04T11:00:43Z","title":"MetaWearS: A Shortcut in Wearable Systems Lifecycle with Only a Few\n  Shots","summary":"  Wearable systems provide continuous health monitoring and can lead to early\ndetection of potential health issues. However, the lifecycle of wearable\nsystems faces several challenges. First, effective model training for new\nwearable devices requires substantial labeled data from various subjects\ncollected directly by the wearable. Second, subsequent model updates require\nfurther extensive labeled data for retraining. Finally, frequent model updating\non the wearable device can decrease the battery life in long-term data\nmonitoring. Addressing these challenges, in this paper, we propose MetaWearS, a\nmeta-learning method to reduce the amount of initial data collection required.\nMoreover, our approach incorporates a prototypical updating mechanism,\nsimplifying the update process by modifying the class prototype rather than\nretraining the entire model. We explore the performance of MetaWearS in two\ncase studies, namely, the detection of epileptic seizures and the detection of\natrial fibrillation. We show that by fine-tuning with just a few samples, we\nachieve 70% and 82% AUC for the detection of epileptic seizures and the\ndetection of atrial fibrillation, respectively. Compared to a conventional\napproach, our proposed method performs better with up to 45% AUC. Furthermore,\nupdating the model with only 16 minutes of additional labeled data increases\nthe AUC by up to 5.3%. Finally, MetaWearS reduces the energy consumption for\nmodel updates by 456x and 418x for epileptic seizure and AF detection,\nrespectively.\n","authors":["Alireza Amirshahi","Maedeh H. Toosi","Siamak Mohammadi","Stefano Albini","Pasquale Davide Schiavone","Giovanni Ansaloni","Amir Aminifar","David Atienza"],"pdf_url":"https://arxiv.org/pdf/2408.01988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.08442v2","updated":"2024-08-04T10:42:42Z","published":"2022-06-16T20:48:19Z","title":"A Look at Value-Based Decision-Time vs. Background Planning Methods\n  Across Different Settings","summary":"  In model-based reinforcement learning (RL), an agent can leverage a learned\nmodel to improve its way of behaving in different ways. Two of the prevalent\nways to do this are through decision-time and background planning methods. In\nthis study, we are interested in understanding how the value-based versions of\nthese two planning methods will compare against each other across different\nsettings. Towards this goal, we first consider the simplest instantiations of\nvalue-based decision-time and background planning methods and provide\ntheoretical results on which one will perform better in the regular RL and\ntransfer learning settings. Then, we consider the modern instantiations of them\nand provide hypotheses on which one will perform better in the same settings.\nFinally, we perform illustrative experiments to validate these theoretical\nresults and hypotheses. Overall, our findings suggest that even though\nvalue-based versions of the two planning methods perform on par in their\nsimplest instantiations, the modern instantiations of value-based decision-time\nplanning methods can perform on par or better than the modern instantiations of\nvalue-based background planning methods in both the regular RL and transfer\nlearning settings.\n","authors":["Safa Alver","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2206.08442v2.pdf","comment":"Accepted to EWRL 2024"},{"id":"http://arxiv.org/abs/2311.13355v2","updated":"2024-08-04T10:31:41Z","published":"2023-11-22T12:47:12Z","title":"Unified Classification and Rejection: A One-versus-All Framework","summary":"  Classifying patterns of known classes and rejecting ambiguous and novel (also\ncalled as out-of-distribution (OOD)) inputs are involved in open world pattern\nrecognition. Deep neural network models usually excel in closed-set\nclassification while performs poorly in rejecting OOD inputs. To tackle this\nproblem, numerous methods have been designed to perform open set recognition\n(OSR) or OOD rejection/detection tasks. Previous methods mostly take\npost-training score transformation or hybrid models to ensure low scores on OOD\ninputs while separating known classes. In this paper, we attempt to build a\nunified framework for building open set classifiers for both classification and\nOOD rejection. We formulate the open set recognition of $ K $-known-class as a\n$ (K+1) $-class classification problem with model trained on known-class\nsamples only. By decomposing the $ K $-class problem into $ K $ one-versus-all\n(OVA) binary classification tasks and binding some parameters, we show that\ncombining the scores of OVA classifiers can give $ (K+1) $-class posterior\nprobabilities, which enables classification and OOD rejection in a unified\nframework. To maintain the closed-set classification accuracy of the OVA\ntrained classifier, we propose a hybrid training strategy combining OVA loss\nand multi-class cross-entropy loss. We implement the OVA framework and hybrid\ntraining strategy on the recently proposed convolutional prototype network and\nprototype classifier on vision transformer (ViT) backbone. Experiments on\npopular OSR and OOD detection datasets demonstrate that the proposed framework,\nusing a single multi-class classifier, yields competitive performance in\nclosed-set classification, OOD detection, and misclassification detection.\n","authors":["Zhen Cheng","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.13355v2.pdf","comment":"Published in Machine Intelligence Research\n  (https://link.springer.com/article/10.1007/s11633-024-1514-4)"},{"id":"http://arxiv.org/abs/2408.01981v1","updated":"2024-08-04T10:16:11Z","published":"2024-08-04T10:16:11Z","title":"Multiview learning with twin parametric margin SVM","summary":"  Multiview learning (MVL) seeks to leverage the benefits of diverse\nperspectives to complement each other, effectively extracting and utilizing the\nlatent information within the dataset. Several twin support vector\nmachine-based MVL (MvTSVM) models have been introduced and demonstrated\noutstanding performance in various learning tasks. However, MvTSVM-based models\nface significant challenges in the form of computational complexity due to four\nmatrix inversions, the need to reformulate optimization problems in order to\nemploy kernel-generated surfaces for handling non-linear cases, and the\nconstraint of uniform noise assumption in the training data. Particularly in\ncases where the data possesses a heteroscedastic error structure, these\nchallenges become even more pronounced. In view of the aforementioned\nchallenges, we propose multiview twin parametric margin support vector machine\n(MvTPMSVM). MvTPMSVM constructs parametric hyperplanes with the goal of\nmaximizing the parametric margin between two classes, aiming to regulate and\nmanage the impact of the heteroscedastic noise structure existing within the\ndata. The proposed MvTPMSVM model avoids the explicit computation of matrix\ninversions in the dual formulation, leading to enhanced computational\nefficiency. We perform an extensive assessment of the MvTPMSVM model using\nbenchmark datasets such as UCI, KEEL, synthetic, and Animals with Attributes\n(AwA). Our experimental results, coupled with rigorous statistical analyses,\nconfirm the superior generalization capabilities of the proposed MvTPMSVM model\ncompared to the baseline models. The source code of the proposed MvTPMSVM model\nis available at \\url{https://github.com/mtanveer1/MvTPMSVM}.\n","authors":["A. Quadir","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2408.01981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01979v1","updated":"2024-08-04T09:53:57Z","published":"2024-08-04T09:53:57Z","title":"Shaping Rewards, Shaping Routes: On Multi-Agent Deep Q-Networks for\n  Routing in Satellite Constellation Networks","summary":"  Effective routing in satellite mega-constellations has become crucial to\nfacilitate the handling of increasing traffic loads, more complex network\narchitectures, as well as the integration into 6G networks. To enhance\nadaptability as well as robustness to unpredictable traffic demands, and to\nsolve dynamic routing environments efficiently, machine learning-based\nsolutions are being considered. For network control problems, such as\noptimizing packet forwarding decisions according to Quality of Service\nrequirements and maintaining network stability, deep reinforcement learning\ntechniques have demonstrated promising results. For this reason, we investigate\nthe viability of multi-agent deep Q-networks for routing in satellite\nconstellation networks. We focus specifically on reward shaping and quantifying\ntraining convergence for joint optimization of latency and load balancing in\nstatic and dynamic scenarios. To address identified drawbacks, we propose a\nnovel hybrid solution based on centralized learning and decentralized control.\n","authors":["Manuel M. H. Roth","Anupama Hegde","Thomas Delamotte","Andreas Knopp"],"pdf_url":"https://arxiv.org/pdf/2408.01979v1.pdf","comment":"5 pages, 5 figures, to be published in proceedings of European Space\n  Agency SPAICE Conference 2024, https://spaice.esa.int/"},{"id":"http://arxiv.org/abs/2408.01972v1","updated":"2024-08-04T09:26:00Z","published":"2024-08-04T09:26:00Z","title":"RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning","summary":"  In this paper, we propose an off-policy deep reinforcement learning (DRL)\nmethod utilizing the average reward criterion. While most existing DRL methods\nemploy the discounted reward criterion, this can potentially lead to a\ndiscrepancy between the training objective and performance metrics in\ncontinuing tasks, making the average reward criterion a recommended\nalternative. We introduce RVI-SAC, an extension of the state-of-the-art\noff-policy DRL method, Soft Actor-Critic (SAC), to the average reward\ncriterion. Our proposal consists of (1) Critic updates based on RVI Q-learning,\n(2) Actor updates introduced by the average reward soft policy improvement\ntheorem, and (3) automatic adjustment of Reset Cost enabling the average reward\nreinforcement learning to be applied to tasks with termination. We apply our\nmethod to the Gymnasium's Mujoco tasks, a subset of locomotion tasks, and\ndemonstrate that RVI-SAC shows competitive performance compared to existing\nmethods.\n","authors":["Yukinari Hisaki","Isao Ono"],"pdf_url":"https://arxiv.org/pdf/2408.01972v1.pdf","comment":"Accepted at ICML 2024; Code:\n  https://github.com/yhisaki/average-reward-drl"},{"id":"http://arxiv.org/abs/2408.01967v1","updated":"2024-08-04T09:08:55Z","published":"2024-08-04T09:08:55Z","title":"A multi-task deep learning approach for lane-level pavement performance\n  prediction with segment-level data","summary":"  The elaborate pavement performance prediction is an important premise of\nimplementing preventive maintenance. Our survey reveals that in practice, the\npavement performance is usually measured at segment-level, where an unique\nperformance value is obtained for all lanes within one segment of 1km length.\nIt still lacks more elaborate performance analysis at lane-level due to costly\ndata collection and difficulty in prediction modeling. Therefore, this study\ndeveloped a multi-task deep learning approach to predict the lane-level\npavement performance with a large amount of historical segment-level\nperformance measurement data. The unified prediction framework can effectively\naddress inherent correlation and differences across lanes. In specific, the\nprediction framework firstly employed an Long Short-Term Memory (LSTM) layer to\ncapture the segment-level pavement deterioration pattern. Then multiple\ntask-specific LSTM layers were designed based on number of lanes to capture\nlane-level differences in pavement performance. Finally, we concatenated\nmultiple task-specific LSTM outputs with auxiliary features for further\ntraining and obtained the lane-level predictions after fully connected layer.\nThe aforementioned prediction framework was validated with a real case in\nChina. It revealed a better model performance regardless of one-way 2-lane,\n3-lane, and 4-lane scenarios, all lower than 10% in terms of mean absolute\npercentage error. The proposed prediction framework also outperforms other\nensemble learning and shallow machine learning methods in almost every lane.\n","authors":["Bo Wang","Wenbo Zhang","Yunpeng LI"],"pdf_url":"https://arxiv.org/pdf/2408.01967v1.pdf","comment":"24 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.18569v3","updated":"2024-08-04T09:01:00Z","published":"2024-07-26T07:51:11Z","title":"PP-TIL: Personalized Planning for Autonomous Driving with Instance-based\n  Transfer Imitation Learning","summary":"  Personalized motion planning holds significant importance within urban\nautomated driving, catering to the unique requirements of individual users.\nNevertheless, prior endeavors have frequently encountered difficulties in\nsimultaneously addressing two crucial aspects: personalized planning within\nintricate urban settings and enhancing planning performance through data\nutilization. The challenge arises from the expensive and limited nature of user\ndata, coupled with the scene state space tending towards infinity. These\nfactors contribute to overfitting and poor generalization problems during model\ntraining. Henceforth, we propose an instance-based transfer imitation learning\napproach. This method facilitates knowledge transfer from extensive expert\ndomain data to the user domain, presenting a fundamental resolution to these\nissues. We initially train a pre-trained model using large-scale expert data.\nSubsequently, during the fine-tuning phase, we feed the batch data, which\ncomprises expert and user data. Employing the inverse reinforcement learning\ntechnique, we extract the style feature distribution from user demonstrations,\nconstructing the regularization term for the approximation of user style. In\nour experiments, we conducted extensive evaluations of the proposed method.\nCompared to the baseline methods, our approach mitigates the overfitting issue\ncaused by sparse user data. Furthermore, we discovered that integrating the\ndriving model with a differentiable nonlinear optimizer as a safety protection\nlayer for end-to-end personalized fine-tuning results in superior planning\nperformance.\n","authors":["Fangze Lin","Ying He","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2407.18569v3.pdf","comment":"IROS 2024 Accepted"},{"id":"http://arxiv.org/abs/2306.04802v4","updated":"2024-08-04T08:53:23Z","published":"2023-06-07T21:51:56Z","title":"A Review on Knowledge Graphs for Healthcare: Resources, Applications,\n  and Promises","summary":"  Healthcare knowledge graphs (HKGs) are valuable tools for organizing\nbiomedical concepts and their relationships with interpretable structures. The\nrecent advent of large language models (LLMs) has paved the way for building\nmore comprehensive and accurate HKGs. This, in turn, can improve the\nreliability of generated content and enable better evaluation of LLMs. However,\nthe challenges of HKGs such as regarding data heterogeneity and limited\ncoverage are not fully understood, highlighting the need for detailed reviews.\nThis work provides the first comprehensive review of HKGs. It summarizes the\npipeline and key techniques for HKG construction, as well as the common\nutilization approaches, i.e., model-free and model-based. The existing HKG\nresources are also organized based on the data types they capture and\napplication domains they cover, along with relevant statistical information\n(Resource available at\nhttps://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase). At the\napplication level, we delve into the successful integration of HKGs across\nvarious health domains, ranging from fine-grained basic science research to\nhigh-level clinical decision support and public health. Lastly, the paper\nhighlights the opportunities for HKGs in the era of LLMs. This work aims to\nserve as a valuable resource for understanding the potential and opportunities\nof HKG in health research.\n","authors":["Carl Yang","Hejie Cui","Jiaying Lu","Shiyu Wang","Ran Xu","Wenjing Ma","Yue Yu","Shaojun Yu","Xuan Kan","Chen Ling","Tianfan Fu","Liang Zhao","Joyce Ho","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2306.04802v4.pdf","comment":"21 pages, preprint submitted to ACM"},{"id":"http://arxiv.org/abs/2408.01964v1","updated":"2024-08-04T08:44:00Z","published":"2024-08-04T08:44:00Z","title":"Top K Enhanced Reinforcement Learning Attacks on Heterogeneous Graph\n  Node Classification","summary":"  Graph Neural Networks (GNNs) have attracted substantial interest due to their\nexceptional performance on graph-based data. However, their robustness,\nespecially on heterogeneous graphs, remains underexplored, particularly against\nadversarial attacks. This paper proposes HeteroKRLAttack, a targeted evasion\nblack-box attack method for heterogeneous graphs. By integrating reinforcement\nlearning with a Top-K algorithm to reduce the action space, our method\nefficiently identifies effective attack strategies to disrupt node\nclassification tasks. We validate the effectiveness of HeteroKRLAttack through\nexperiments on multiple heterogeneous graph datasets, showing significant\nreductions in classification accuracy compared to baseline methods. An ablation\nstudy underscores the critical role of the Top-K algorithm in enhancing attack\nperformance. Our findings highlight potential vulnerabilities in current models\nand provide guidance for future defense strategies against adversarial attacks\non heterogeneous graphs.\n","authors":["Honglin Gao","Gaoxi Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.01964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16364v2","updated":"2024-08-04T08:36:08Z","published":"2024-02-26T07:33:28Z","title":"Where Do We Go from Here? Multi-scale Allocentric Relational Inference\n  from Natural Spatial Descriptions","summary":"  When communicating routes in natural language, the concept of acquired\nspatial knowledge is crucial for geographic information retrieval (GIR) and in\nspatial cognitive research. However, NLP navigation studies often overlook the\nimpact of such acquired knowledge on textual descriptions. Current navigation\nstudies concentrate on egocentric local descriptions (e.g., `it will be on your\nright') that require reasoning over the agent's local perception. These\ninstructions are typically given as a sequence of steps, with each action-step\nexplicitly mentioning and being followed by a landmark that the agent can use\nto verify they are on the right path (e.g., `turn right and then you will\nsee...'). In contrast, descriptions based on knowledge acquired through a map\nprovide a complete view of the environment and capture its overall structure.\nThese instructions (e.g., `it is south of Central Park and a block north of a\npolice station') are typically non-sequential, contain allocentric relations,\nwith multiple spatial relations and implicit actions, without any explicit\nverification. This paper introduces the Rendezvous (RVS) task and dataset,\nwhich includes 10,404 examples of English geospatial instructions for reaching\na target location using map-knowledge. Our analysis reveals that RVS exhibits a\nricher use of spatial allocentric relations, and requires resolving more\nspatial relations simultaneously compared to previous text-based navigation\nbenchmarks.\n","authors":["Tzuf Paz-Argaman","Sayali Kulkarni","John Palowitch","Jason Baldridge","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2402.16364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01961v1","updated":"2024-08-04T08:35:02Z","published":"2024-08-04T08:35:02Z","title":"Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study","summary":"  Popular and news media often portray teenagers with sensationalism, as both a\nrisk to society and at risk from society. As AI begins to absorb some of the\nepistemic functions of traditional media, we study how teenagers in two\ncountries speaking two languages: 1) are depicted by AI, and 2) how they would\nprefer to be depicted. Specifically, we study the biases about teenagers\nlearned by static word embeddings (SWEs) and generative language models (GLMs),\ncomparing these with the perspectives of adolescents living in the U.S. and\nNepal. We find English-language SWEs associate teenagers with societal\nproblems, and more than 50% of the 1,000 words most associated with teenagers\nin the pretrained GloVe SWE reflect such problems. Given prompts about\nteenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss\nsocietal problems, most commonly violence, but also drug use, mental illness,\nand sexual taboo. Nepali models, while not free of such associations, are less\ndominated by social problems. Data from workshops with N=13 U.S. adolescents\nand N=18 Nepalese adolescents show that AI presentations are disconnected from\nteenage life, which revolves around activities like school and friendship.\nParticipant ratings of how well 20 trait words describe teens are decorrelated\nfrom SWE associations, with Pearson's r=.02, n.s. in English FastText and\nr=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in\nGloVe. U.S. participants suggested AI could fairly present teens by\nhighlighting diversity, while Nepalese participants centered positivity.\nParticipants were optimistic that, if it learned from adolescents, rather than\nmedia sources, AI could help mitigate stereotypes. Our work offers an\nunderstanding of the ways SWEs and GLMs misrepresent a developmentally\nvulnerable group and provides a template for less sensationalized\ncharacterization.\n","authors":["Robert Wolfe","Aayushi Dangol","Bill Howe","Alexis Hiniker"],"pdf_url":"https://arxiv.org/pdf/2408.01961v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.01959v1","updated":"2024-08-04T08:26:58Z","published":"2024-08-04T08:26:58Z","title":"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in\n  Vision-Language AI","summary":"  Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.\n","authors":["Robert Wolfe","Aayushi Dangol","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01959v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.01953v1","updated":"2024-08-04T07:59:17Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v1.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2407.10768v5","updated":"2024-08-04T07:53:03Z","published":"2024-07-15T14:50:15Z","title":"ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time\n  Series Forecasting","summary":"  Long time series forecasting aims to utilize historical information to\nforecast future states over extended horizons. Traditional RNN-based series\nforecasting methods struggle to effectively address long-term dependencies and\ngradient issues in long time series problems. Recently, SegRNN has emerged as a\nleading RNN-based model tailored for long-term series forecasting,\ndemonstrating state-of-the-art performance while maintaining a streamlined\narchitecture through innovative segmentation and parallel decoding techniques.\nNevertheless, SegRNN has several limitations: its fixed segmentation disrupts\ndata continuity and fails to effectively leverage information across different\nsegments, the segmentation strategy employed by SegRNN does not fundamentally\naddress the issue of information loss within the recurrent structure. To\naddress these issues, we propose the ISMRNN method with three key enhancements:\nwe introduce an implicit segmentation structure to decompose the time series\nand map it to segmented hidden states, resulting in denser information exchange\nduring the segmentation phase. Additionally, we incorporate residual structures\nin the encoding layer to mitigate information loss within the recurrent\nstructure. To extract information more effectively, we further integrate the\nMamba architecture to enhance time series information extraction. Experiments\non several real-world long time series forecasting datasets demonstrate that\nour model surpasses the performance of current state-of-the-art models.\n","authors":["GaoXiang Zhao","Li Zhou","XiaoQiang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10768v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11060v2","updated":"2024-08-04T05:59:13Z","published":"2023-10-17T08:06:08Z","title":"Privacy-Preserving Graph Embedding based on Local Differential Privacy","summary":"  Graph embedding has become a powerful tool for learning latent\nrepresentations of nodes in a graph. Despite its superior performance in\nvarious graph-based machine learning tasks, serious privacy concerns arise when\nthe graph data contains personal or sensitive information. To address this\nissue, we investigate and develop graph embedding algorithms that satisfy local\ndifferential privacy (LDP). We introduce a novel privacy-preserving graph\nembedding framework, named PrivGE, to protect node data privacy. Specifically,\nwe propose an LDP mechanism to obfuscate node data and utilize personalized\nPageRank as the proximity measure to learn node representations. Furthermore,\nwe provide a theoretical analysis of the privacy guarantees and utility offered\nby the PrivGE framework. Extensive experiments on several real-world graph\ndatasets demonstrate that PrivGE achieves an optimal balance between privacy\nand utility, and significantly outperforms existing methods in node\nclassification and link prediction tasks.\n","authors":["Zening Li","Rong-Hua Li","Meihao Liao","Fusheng Jin","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2310.11060v2.pdf","comment":"to be published in CIKM 2024"},{"id":"http://arxiv.org/abs/2407.15100v2","updated":"2024-08-04T05:55:40Z","published":"2024-07-21T09:32:34Z","title":"A General Framework for Data-Use Auditing of ML Models","summary":"  Auditing the use of data in training machine-learning (ML) models is an\nincreasingly pressing challenge, as myriad ML practitioners routinely leverage\nthe effort of content creators to train models without their permission. In\nthis paper, we propose a general method to audit an ML model for the use of a\ndata-owner's data in training, without prior knowledge of the ML task for which\nthe data might be used. Our method leverages any existing black-box membership\ninference method, together with a sequential hypothesis test of our own design,\nto detect data use with a quantifiable, tunable false-detection rate. We show\nthe effectiveness of our proposed framework by applying it to audit data use in\ntwo types of ML models, namely image classifiers and foundation models.\n","authors":["Zonghao Huang","Neil Zhenqiang Gong","Michael K. Reiter"],"pdf_url":"https://arxiv.org/pdf/2407.15100v2.pdf","comment":"The full paper of \"A General Framework for Data-Use Auditing of ML\n  Models\" accepted by CCS 2024"},{"id":"http://arxiv.org/abs/2407.21530v2","updated":"2024-08-04T05:53:25Z","published":"2024-07-31T11:26:57Z","title":"Data Contamination Report from the 2024 CONDA Shared Task","summary":"  The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.\n","authors":["Oscar Sainz","Iker Garca-Ferrero","Alon Jacovi","Jon Ander Campos","Yanai Elazar","Eneko Agirre","Yoav Goldberg","Wei-Lin Chen","Jenny Chim","Leshem Choshen","Luca D'Amico-Wong","Melissa Dell","Run-Ze Fan","Shahriar Golchin","Yucheng Li","Pengfei Liu","Bhavish Pahwa","Ameya Prabhu","Suryansh Sharma","Emily Silcock","Kateryna Solonko","David Stap","Mihai Surdeanu","Yu-Min Tseng","Vishaal Udandarao","Zengzhi Wang","Ruijie Xu","Jinglin Yang"],"pdf_url":"https://arxiv.org/pdf/2407.21530v2.pdf","comment":"https://huggingface.co/spaces/CONDA-Workshop/Data-Contamination-Database"}]},"2024-08-07T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.11652v5","updated":"2024-08-07T05:14:24Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v5.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.02085v3","updated":"2024-08-07T06:04:31Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v3.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.02922v2","updated":"2024-08-07T06:44:24Z","published":"2024-08-06T03:15:18Z","title":"Pose Magic: Efficient and Temporally Consistent Human Pose Estimation\n  with a Hybrid Mamba-GCN Network","summary":"  Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are\nprimarily based on Transformers. However, existing Transformer-based 3D HPE\nbackbones often encounter a trade-off between accuracy and computational\nefficiency. To resolve the above dilemma, in this work, we leverage recent\nadvances in state space models and utilize Mamba for high-quality and efficient\nlong-range modeling. Nonetheless, Mamba still faces challenges in precisely\nexploiting local dependencies between joints. To address these issues, we\npropose a new attention-free hybrid spatiotemporal architecture named Hybrid\nMamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN\nby capturing relationships between neighboring joints, thus producing new\nrepresentations to complement Mamba's outputs. By adaptively fusing\nrepresentations from Mamba and GCN, Pose Magic demonstrates superior capability\nin learning the underlying 3D structure. To meet the requirements of real-time\ninference, we also provide a fully causal version. Extensive experiments show\nthat Pose Magic achieves new SOTA results ($\\downarrow 0.9 mm$) while saving\n$74.1\\%$ FLOPs. In addition, Pose Magic exhibits optimal motion consistency and\nthe ability to generalize to unseen sequence lengths.\n","authors":["Xinyi Zhang","Qiqi Bao","Qinpeng Cui","Wenming Yang","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2408.02922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03940v1","updated":"2024-08-07T17:59:40Z","published":"2024-08-07T17:59:40Z","title":"How Well Can Vision Language Models See Image Details?","summary":"  Large Language Model-based Vision-Language Models (LLM-based VLMs) have\ndemonstrated impressive results in various vision-language understanding tasks.\nHowever, how well these VLMs can see image detail beyond the semantic level\nremains unclear. In our study, we introduce a pixel value prediction task (PVP)\nto explore \"How Well Can Vision Language Models See Image Details?\" and to\nassist VLMs in perceiving more details. Typically, these models comprise a\nfrozen CLIP visual encoder, a large language model, and a connecting module.\nAfter fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to\npredict precise pixel values by only fine-tuning the connection module and LLM;\nand 2) prediction precision is significantly improved when the vision encoder\nis also adapted. Additionally, our research reveals that incorporating pixel\nvalue prediction as one of the VLM pre-training tasks and vision encoder\nadaptation markedly boosts VLM performance on downstream image-language\nunderstanding tasks requiring detailed image perception, such as referring\nimage segmentation (with an average +10.19 cIoU improvement) and video game\ndecision making (with average score improvements of +80.34 and +70.54 on two\ngames, respectively).\n","authors":["Chenhui Gou","Abdulwahab Felemban","Faizan Farooq Khan","Deyao Zhu","Jianfei Cai","Hamid Rezatofighi","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2408.03940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03923v1","updated":"2024-08-07T17:30:59Z","published":"2024-08-07T17:30:59Z","title":"Fast Sprite Decomposition from Animated Graphics","summary":"  This paper presents an approach to decomposing animated graphics into\nsprites, a set of basic elements or layers. Our approach builds on the\noptimization of sprite parameters to fit the raster video. For efficiency, we\nassume static textures for sprites to reduce the search space while preventing\nartifacts using a texture prior model. To further speed up the optimization, we\nintroduce the initialization of the sprite parameters utilizing a pre-trained\nvideo object segmentation model and user input of single frame annotations. For\nour study, we construct the Crello Animation dataset from an online design\nservice and define quantitative metrics to measure the quality of the extracted\nsprites. Experiments show that our method significantly outperforms baselines\nfor similar decomposition tasks in terms of the quality/efficiency tradeoff.\n","authors":["Tomoyuki Suzuki","Kotaro Kikuchi","Kota Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2408.03923v1.pdf","comment":"To be published ECCV 2024, project page:\n  https://cyberagentailab.github.io/sprite-decompose/"},{"id":"http://arxiv.org/abs/2405.19450v2","updated":"2024-08-07T17:30:16Z","published":"2024-05-29T18:58:59Z","title":"FourierMamba: Fourier Learning Integration with State Space Models for\n  Image Deraining","summary":"  Image deraining aims to remove rain streaks from rainy images and restore\nclear backgrounds. Currently, some research that employs the Fourier transform\nhas proved to be effective for image deraining, due to it acting as an\neffective frequency prior for capturing rain streaks. However, despite there\nexists dependency of low frequency and high frequency in images, these\nFourier-based methods rarely exploit the correlation of different frequencies\nfor conjuncting their learning procedures, limiting the full utilization of\nfrequency information for image deraining. Alternatively, the recently emerged\nMamba technique depicts its effectiveness and efficiency for modeling\ncorrelation in various domains (e.g., spatial, temporal), and we argue that\nintroducing Mamba into its unexplored Fourier spaces to correlate different\nfrequencies would help improve image deraining. This motivates us to propose a\nnew framework termed FourierMamba, which performs image deraining with Mamba in\nthe Fourier space. Owning to the unique arrangement of frequency orders in\nFourier space, the core of FourierMamba lies in the scanning encoding of\ndifferent frequencies, where the low-high frequency order formats exhibit\ndifferently in the spatial dimension (unarranged in axis) and channel dimension\n(arranged in axis). Therefore, we design FourierMamba that correlates Fourier\nspace information in the spatial and channel dimensions with distinct designs.\nSpecifically, in the spatial dimension Fourier space, we introduce the zigzag\ncoding to scan the frequencies to rearrange the orders from low to high\nfrequencies, thereby orderly correlating the connections between frequencies;\nin the channel dimension Fourier space with arranged orders of frequencies in\naxis, we can directly use Mamba to perform frequency correlation and improve\nthe channel information representation.\n","authors":["Dong Li","Yidi Liu","Xueyang Fu","Senyan Xu","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2405.19450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03922v1","updated":"2024-08-07T17:29:19Z","published":"2024-08-07T17:29:19Z","title":"FMiFood: Multi-modal Contrastive Learning for Food Image Classification","summary":"  Food image classification is the fundamental step in image-based dietary\nassessment, which aims to estimate participants' nutrient intake from eating\noccasion images. A common challenge of food images is the intra-class diversity\nand inter-class similarity, which can significantly hinder classification\nperformance. To address this issue, we introduce a novel multi-modal\ncontrastive learning framework called FMiFood, which learns more discriminative\nfeatures by integrating additional contextual information, such as food\ncategory text descriptions, to enhance classification accuracy. Specifically,\nwe propose a flexible matching technique that improves the similarity matching\nbetween text and image embeddings to focus on multiple key information.\nFurthermore, we incorporate the classification objectives into the framework\nand explore the use of GPT-4 to enrich the text descriptions and provide more\ndetailed context. Our method demonstrates improved performance on both the\nUPMC-101 and VFN datasets compared to existing methods.\n","authors":["Xinyue Pan","Jiangpeng He","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.03922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03913v1","updated":"2024-08-07T17:19:15Z","published":"2024-08-07T17:19:15Z","title":"AdapMTL: Adaptive Pruning Framework for Multitask Learning Model","summary":"  In the domain of multimedia and multimodal processing, the efficient handling\nof diverse data streams such as images, video, and sensor data is paramount.\nModel compression and multitask learning (MTL) are crucial in this field,\noffering the potential to address the resource-intensive demands of processing\nand interpreting multiple forms of media simultaneously. However, effectively\ncompressing a multitask model presents significant challenges due to the\ncomplexities of balancing sparsity allocation and accuracy performance across\nmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive\npruning framework for MTL models. AdapMTL leverages multiple learnable soft\nthresholds independently assigned to the shared backbone and the task-specific\nheads to capture the nuances in different components' sensitivity to pruning.\nDuring training, it co-optimizes the soft thresholds and MTL model weights to\nautomatically determine the suitable sparsity level at each component to\nachieve both high task accuracy and high overall sparsity. It further\nincorporates an adaptive weighting mechanism that dynamically adjusts the\nimportance of task-specific losses based on each task's robustness to pruning.\nWe demonstrate the effectiveness of AdapMTL through comprehensive experiments\non popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different\narchitectures, showcasing superior performance compared to state-of-the-art\npruning methods.\n","authors":["Mingcan Xiang","Steven Jiaxun Tang","Qizheng Yang","Hui Guan","Tongping Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03913v1.pdf","comment":"13 pages, 9 figures, Published at ACM Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.03904v1","updated":"2024-08-07T17:08:46Z","published":"2024-08-07T17:08:46Z","title":"Lightweight Video Denoising Using a Classic Bayesian Backbone","summary":"  In recent years, state-of-the-art image and video denoising networks have\nbecome increasingly large, requiring millions of trainable parameters to\nachieve best-in-class performance. Improved denoising quality has come at the\ncost of denoising speed, where modern transformer networks are far slower to\nrun than smaller denoising networks such as FastDVDnet and classic Bayesian\ndenoisers such as the Wiener filter.\n  In this paper, we implement a hybrid Wiener filter which leverages small\nancillary networks to increase the original denoiser performance, while\nretaining fast denoising speeds. These networks are used to refine the Wiener\ncoring estimate, optimise windowing functions and estimate the unknown noise\nprofile. Using these methods, we outperform several popular denoisers and\nremain within 0.2 dB, on average, of the popular VRT transformer. Our method\nwas found to be over x10 faster than the transformer method, with a far lower\nparameter cost.\n","authors":["Clment Bled","Franois Piti"],"pdf_url":"https://arxiv.org/pdf/2408.03904v1.pdf","comment":"Paper accepted to ICME 2024"},{"id":"http://arxiv.org/abs/2403.15098v3","updated":"2024-08-07T17:03:30Z","published":"2024-03-22T10:36:50Z","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","summary":"  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, e.g., in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\nhttps://github.com/vita-epfl/UniTraj\n","authors":["Lan Feng","Mohammadhossein Bahari","Kaouther Messaoud Ben Amor","loi Zablocki","Matthieu Cord","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.15098v3.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2403.11956v5","updated":"2024-08-07T17:02:00Z","published":"2024-03-18T16:52:49Z","title":"Subjective-Aligned Dataset and Metric for Text-to-Video Quality\n  Assessment","summary":"  With the rapid development of generative models, Artificial\nIntelligence-Generated Contents (AIGC) have exponentially increased in daily\nlives. Among them, Text-to-Video (T2V) generation has received widespread\nattention. Though many T2V models have been released for generating high\nperceptual quality videos, there is still lack of a method to evaluate the\nquality of these videos quantitatively. To solve this issue, we establish the\nlargest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The\ndataset is composed of 10,000 videos generated by 9 different T2V models. We\nalso conduct a subjective study to obtain each video's corresponding mean\nopinion score. Based on T2VQA-DB, we propose a novel transformer-based model\nfor subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model\nextracts features from text-video alignment and video fidelity perspectives,\nthen it leverages the ability of a large language model to give the prediction\nscore. Experimental results show that T2VQA outperforms existing T2V metrics\nand SOTA video quality assessment models. Quantitative analysis indicates that\nT2VQA is capable of giving subjective-align predictions, validating its\neffectiveness. The dataset and code will be released at\nhttps://github.com/QMME/T2VQA.\n","authors":["Tengchuan Kou","Xiaohong Liu","Zicheng Zhang","Chunyi Li","Haoning Wu","Xiongkuo Min","Guangtao Zhai","Ning Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11956v5.pdf","comment":"Accepted by ACMMM 24"},{"id":"http://arxiv.org/abs/2408.03888v1","updated":"2024-08-07T16:39:16Z","published":"2024-08-07T16:39:16Z","title":"Dual-Modeling Decouple Distillation for Unsupervised Anomaly Detection","summary":"  Knowledge distillation based on student-teacher network is one of the\nmainstream solution paradigms for the challenging unsupervised Anomaly\nDetection task, utilizing the difference in representation capabilities of the\nteacher and student networks to implement anomaly localization. However,\nover-generalization of the student network to the teacher network may lead to\nnegligible differences in representation capabilities of anomaly, thus\naffecting the detection effectiveness. Existing methods address the possible\nover-generalization by using differentiated students and teachers from the\nstructural perspective or explicitly expanding distilled information from the\ncontent perspective, which inevitably result in an increased likelihood of\nunderfitting of the student network and poor anomaly detection capabilities in\nanomaly center or edge. In this paper, we propose Dual-Modeling Decouple\nDistillation (DMDD) for the unsupervised anomaly detection. In DMDD, a Decouple\nStudent-Teacher Network is proposed to decouple the initial student features\ninto normality and abnormality features. We further introduce Dual-Modeling\nDistillation based on normal-anomaly image pairs, fitting normality features of\nanomalous image and the teacher features of the corresponding normal image,\nwidening the distance between abnormality features and the teacher features in\nanomalous regions. Synthesizing these two distillation ideas, we achieve\nanomaly detection which focuses on both edge and center of anomaly. Finally, a\nMulti-perception Segmentation Network is proposed to achieve focused anomaly\nmap fusion based on multiple attention. Experimental results on MVTec AD show\nthat DMDD surpasses SOTA localization performance of previous knowledge\ndistillation-based methods, reaching 98.85% on pixel-level AUC and 96.13% on\nPRO.\n","authors":["Xinyue Liu","Jianyuan Wang","Biao Leng","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03888v1.pdf","comment":"10 pages, 8 figures, Accepted to ACM MM '24"},{"id":"http://arxiv.org/abs/2408.03885v1","updated":"2024-08-07T16:34:32Z","published":"2024-08-07T16:34:32Z","title":"Global-Local Progressive Integration Network for Blind Image Quality\n  Assessment","summary":"  Vision transformers (ViTs) excel in computer vision for modeling long-term\ndependencies, yet face two key challenges for image quality assessment (IQA):\ndiscarding fine details during patch embedding, and requiring extensive\ntraining data due to lack of inductive biases. In this study, we propose a\nGlobal-Local progressive INTegration network for IQA, called GlintIQA, to\naddress these issues through three key components: 1) Hybrid feature extraction\ncombines ViT-based global feature extractor (VGFE) and convolutional neural\nnetworks (CNNs)-based local feature extractor (CLFE) to capture global\ncoarse-grained features and local fine-grained features, respectively. The\nincorporation of CNNs mitigates the patch-level information loss and inductive\nbias constraints inherent to ViT architectures. 2) Progressive feature\nintegration leverages diverse kernel sizes in embedding to spatially align\ncoarse- and fine-grained features, and progressively aggregate these features\nby interactively stacking channel-wise attention and spatial enhancement\nmodules to build effective quality-aware representations. 3) Content\nsimilarity-based labeling approach is proposed that automatically assigns\nquality labels to images with diverse content based on subjective quality\nscores. This addresses the scarcity of labeled training data in synthetic\ndatasets and bolsters model generalization. The experimental results\ndemonstrate the efficacy of our approach, yielding 5.04% average SROCC gains on\ncross-authentic dataset evaluations. Moreover, our model and its counterpart\npre-trained on the proposed dataset respectively exhibited 5.40% and 13.23%\nimprovements on across-synthetic datasets evaluation. The codes and proposed\ndataset will be released at https://github.com/XiaoqiWang/GlintIQA.\n","authors":["Xiaoqi Wang","Yun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00920v3","updated":"2024-08-07T16:30:56Z","published":"2023-10-02T06:17:24Z","title":"Every Dataset Counts: Scaling up Monocular 3D Object Detection with\n  Joint Datasets Training","summary":"  Monocular 3D object detection plays a crucial role in autonomous driving.\nHowever, existing monocular 3D detection algorithms depend on 3D labels derived\nfrom LiDAR measurements, which are costly to acquire for new datasets and\nchallenging to deploy in novel environments. Specifically, this study\ninvestigates the pipeline for training a monocular 3D object detection model on\na diverse collection of 3D and 2D datasets. The proposed framework comprises\nthree components: (1) a robust monocular 3D model capable of functioning across\nvarious camera settings, (2) a selective-training strategy to accommodate\ndatasets with differing class annotations, and (3) a pseudo 3D training\napproach using 2D labels to enhance detection performance in scenes containing\nonly 2D labels. With this framework, we could train models on a joint set of\nvarious open 3D/2D datasets to obtain models with significantly stronger\ngeneralization capability and enhanced performance on new dataset with only 2D\nlabels. We conduct extensive experiments on\nKITTI/nuScenes/ONCE/Cityscapes/BDD100K datasets to demonstrate the scaling\nability of the proposed method.\n","authors":["Fulong Ma","Xiaoyang Yan","Guoyang Zhao","Xiaojie Xu","Yuxuan Liu","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2310.00920v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03867v1","updated":"2024-08-07T16:16:31Z","published":"2024-08-07T16:16:31Z","title":"Surgformer: Surgical Transformer with Hierarchical Temporal Attention\n  for Surgical Phase Recognition","summary":"  Existing state-of-the-art methods for surgical phase recognition either rely\non the extraction of spatial-temporal features at a short-range temporal\nresolution or adopt the sequential extraction of the spatial and temporal\nfeatures across the entire temporal resolution. However, these methods have\nlimitations in modeling spatial-temporal dependency and addressing\nspatial-temporal redundancy: 1) These methods fail to effectively model\nspatial-temporal dependency, due to the lack of long-range information or joint\nspatial-temporal modeling. 2) These methods utilize dense spatial features\nacross the entire temporal resolution, resulting in significant\nspatial-temporal redundancy. In this paper, we propose the Surgical Transformer\n(Surgformer) to address the issues of spatial-temporal modeling and redundancy\nin an end-to-end manner, which employs divided spatial-temporal attention and\ntakes a limited set of sparse frames as input. Moreover, we propose a novel\nHierarchical Temporal Attention (HTA) to capture both global and local\ninformation within varied temporal resolutions from a target frame-centric\nperspective. Distinct from conventional temporal attention that primarily\nemphasizes dense long-range similarity, HTA not only captures long-term\ninformation but also considers local latent consistency among informative\nframes. HTA then employs pyramid feature aggregation to effectively utilize\ntemporal information across diverse temporal resolutions, thereby enhancing the\noverall temporal representation. Extensive experiments on two challenging\nbenchmark datasets verify that our proposed Surgformer performs favorably\nagainst the state-of-the-art methods. The code is released at\nhttps://github.com/isyangshu/Surgformer.\n","authors":["Shu Yang","Luyang Luo","Qiong Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15636v3","updated":"2024-08-07T16:07:05Z","published":"2024-05-24T15:22:58Z","title":"Visualize and Paint GAN Activations","summary":"  We investigate how generated structures of GANs correlate with their\nactivations in hidden layers, with the purpose of better understanding the\ninner workings of those models and being able to paint structures with\nunconditionally trained GANs. This gives us more control over the generated\nimages, allowing to generate them from a semantic segmentation map while not\nrequiring such a segmentation in the training data. To this end we introduce\nthe concept of tileable features, allowing us to identify activations that work\nwell for painting.\n","authors":["Rudolf Herdt","Peter Maass"],"pdf_url":"https://arxiv.org/pdf/2405.15636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03842v1","updated":"2024-08-07T15:35:25Z","published":"2024-08-07T15:35:25Z","title":"Bi-Level Spatial and Channel-aware Transformer for Learned Image\n  Compression","summary":"  Recent advancements in learned image compression (LIC) methods have\ndemonstrated superior performance over traditional hand-crafted codecs. These\nlearning-based methods often employ convolutional neural networks (CNNs) or\nTransformer-based architectures. However, these nonlinear approaches frequently\noverlook the frequency characteristics of images, which limits their\ncompression efficiency. To address this issue, we propose a novel\nTransformer-based image compression method that enhances the transformation\nstage by considering frequency components within the feature map. Our method\nintegrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),\nwhere a spatial-based branch independently handles high and low frequencies at\nthe attention layer, and a Channel-aware Self-Attention (CaSA) module captures\ninformation across channels, significantly improving compression performance.\nAdditionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)\nwithin the Transformer block to enhance the extraction of diverse and rich\ninformation, which is crucial for effective compression. These innovations\ncollectively improve the transformation's ability to project data into a more\ndecorrelated latent space, thereby boosting overall compression efficiency.\nExperimental results demonstrate that our framework surpasses state-of-the-art\nLIC methods in rate-distortion performance.\n","authors":["Hamidreza Soltani","Erfan Ghasemi"],"pdf_url":"https://arxiv.org/pdf/2408.03842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03838v1","updated":"2024-08-07T15:24:25Z","published":"2024-08-07T15:24:25Z","title":"Using a Distance Sensor to Detect Deviations in a Planar Surface","summary":"  We investigate methods for determining if a planar surface contains geometric\ndeviations (e.g., protrusions, objects, divots, or cliffs) using only an\ninstantaneous measurement from a miniature optical time-of-flight sensor. The\nkey to our method is to utilize the entirety of information encoded in raw\ntime-of-flight data captured by off-the-shelf distance sensors. We provide an\nanalysis of the problem in which we identify the key ambiguity between geometry\nand surface photometrics. To overcome this challenging ambiguity, we fit a\nGaussian mixture model to a small dataset of planar surface measurements. This\nmodel implicitly captures the expected geometry and distribution of\nphotometrics of the planar surface and is used to identify measurements that\nare likely to contain deviations. We characterize our method on a variety of\nsurfaces and planar deviations across a range of scenarios. We find that our\nmethod utilizing raw time-of-flight data outperforms baselines which use only\nderived distance estimates. We build an example application in which our method\nenables mobile robot obstacle and cliff avoidance over a wide field-of-view.\n","authors":["Carter Sifferman","William Sun","Mohit Gupta","Michael Gleicher"],"pdf_url":"https://arxiv.org/pdf/2408.03838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03834v1","updated":"2024-08-07T15:17:51Z","published":"2024-08-07T15:17:51Z","title":"Target Prompting for Information Extraction with Vision Language Model","summary":"  The recent trend in the Large Vision and Language model has brought a new\nchange in how information extraction systems are built. VLMs have set a new\nbenchmark with their State-of-the-art techniques in understanding documents and\nbuilding question-answering systems across various industries. They are\nsignificantly better at generating text from document images and providing\naccurate answers to questions. However, there are still some challenges in\neffectively utilizing these models to build a precise conversational system.\nGeneral prompting techniques used with large language models are often not\nsuitable for these specially designed vision language models. The output\ngenerated by such generic input prompts is ordinary and may contain information\ngaps when compared with the actual content of the document. To obtain more\naccurate and specific answers, a well-targeted prompt is required by the vision\nlanguage model, along with the document image. In this paper, a technique is\ndiscussed called Target prompting, which focuses on explicitly targeting parts\nof document images and generating related answers from those specific regions\nonly. The paper also covers the evaluation of response for each prompting\ntechnique using different user queries and input prompts.\n","authors":["Dipankar Medhi"],"pdf_url":"https://arxiv.org/pdf/2408.03834v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.19588v2","updated":"2024-08-07T15:11:01Z","published":"2024-03-28T17:12:39Z","title":"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs","summary":"  This paper revives Densely Connected Convolutional Networks (DenseNets) and\nreveals the underrated effectiveness over predominant ResNet-style\narchitectures. We believe DenseNets' potential was overlooked due to untouched\ntraining methods and traditional design elements not fully revealing their\ncapabilities. Our pilot study shows dense connections through concatenation are\nstrong, demonstrating that DenseNets can be revitalized to compete with modern\narchitectures. We methodically refine suboptimal components - architectural\nadjustments, block redesign, and improved training recipes towards widening\nDenseNets and boosting memory efficiency while keeping concatenation shortcuts.\nOur models, employing simple architectural elements, ultimately surpass Swin\nTransformer, ConvNeXt, and DeiT-III - key architectures in the residual\nlearning lineage. Furthermore, our models exhibit near state-of-the-art\nperformance on ImageNet-1K, competing with the very recent models and\ndownstream tasks, ADE20k semantic segmentation, and COCO object\ndetection/instance segmentation. Finally, we provide empirical analyses that\nuncover the merits of the concatenation over additive shortcuts, steering a\nrenewed preference towards DenseNet-style designs. Our code is available at\nhttps://github.com/naver-ai/rdnet.\n","authors":["Donghyun Kim","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19588v2.pdf","comment":"ECCV 2024. Code at https://github.com/naver-ai/rdnet"},{"id":"http://arxiv.org/abs/2401.00763v2","updated":"2024-08-07T15:10:15Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v2.pdf","comment":"ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2408.03825v1","updated":"2024-08-07T15:01:08Z","published":"2024-08-07T15:01:08Z","title":"Towards Real-Time Gaussian Splatting: Accelerating 3DGS through\n  Photometric SLAM","summary":"  Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous\nLocalization and Mapping (VSLAM) demonstrate the generation of high-quality\nvolumetric reconstructions from monocular video streams. However, despite these\npromising advancements, current 3DGS integrations have reduced tracking\nperformance and lower operating speeds compared to traditional VSLAM. To\naddress these issues, we propose integrating 3DGS with Direct Sparse Odometry,\na monocular photometric SLAM system. We have done preliminary experiments\nshowing that using Direct Sparse Odometry point cloud outputs, as opposed to\nstandard structure-from-motion methods, significantly shortens the training\ntime needed to achieve high-quality renders. Reducing 3DGS training time\nenables the development of 3DGS-integrated SLAM systems that operate in\nreal-time on mobile hardware. These promising initial findings suggest further\nexploration is warranted in combining traditional VSLAM systems with 3DGS.\n","authors":["Yan Song Hu","Dayou Mao","Yuhao Chen","John Zelek"],"pdf_url":"https://arxiv.org/pdf/2408.03825v1.pdf","comment":"This extended abstract has been submitted to be presented at an IEEE\n  conference. It will be made available online by IEEE but will not be\n  published in IEEE Xplore. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2408.03822v1","updated":"2024-08-07T14:56:34Z","published":"2024-08-07T14:56:34Z","title":"Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields","summary":"  3D Gaussian splatting (3DGS) has recently emerged as an alternative\nrepresentation that leverages a 3D Gaussian-based representation and introduces\nan approximated volumetric rendering, achieving very fast rendering speed and\npromising image quality. Furthermore, subsequent studies have successfully\nextended 3DGS to dynamic 3D scenes, demonstrating its wide range of\napplications. However, a significant drawback arises as 3DGS and its following\nmethods entail a substantial number of Gaussians to maintain the high fidelity\nof the rendered images, which requires a large amount of memory and storage. To\naddress this critical issue, we place a specific emphasis on two key\nobjectives: reducing the number of Gaussian points without sacrificing\nperformance and compressing the Gaussian attributes, such as view-dependent\ncolor and covariance. To this end, we propose a learnable mask strategy that\nsignificantly reduces the number of Gaussians while preserving high\nperformance. In addition, we propose a compact but effective representation of\nview-dependent color by employing a grid-based neural field rather than relying\non spherical harmonics. Finally, we learn codebooks to compactly represent the\ngeometric and temporal attributes by residual vector quantization. With model\ncompression techniques such as quantization and entropy coding, we consistently\nshow over 25x reduced storage and enhanced rendering speed compared to 3DGS for\nstatic scenes, while maintaining the quality of the scene representation. For\ndynamic scenes, our approach achieves more than 12x storage efficiency and\nretains a high-quality reconstruction compared to the existing state-of-the-art\nmethods. Our work provides a comprehensive framework for 3D scene\nrepresentation, achieving high performance, fast training, compactness, and\nreal-time rendering. Our project page is available at\nhttps://maincold2.github.io/c3dgs/.\n","authors":["Joo Chan Lee","Daniel Rho","Xiangyu Sun","Jong Hwan Ko","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2408.03822v1.pdf","comment":"Project page: https://maincold2.github.io/c3dgs/"},{"id":"http://arxiv.org/abs/2407.18520v2","updated":"2024-08-07T14:33:14Z","published":"2024-07-26T05:29:24Z","title":"Text-Region Matching for Multi-Label Image Recognition with Missing\n  Labels","summary":"  Recently, large-scale visual language pre-trained (VLP) models have\ndemonstrated impressive performance across various downstream tasks. Motivated\nby these advancements, pioneering efforts have emerged in multi-label image\nrecognition with missing labels, leveraging VLP prompt-tuning technology.\nHowever, they usually cannot match text and vision features well, due to\ncomplicated semantics gaps and missing labels in a multi-label image. To tackle\nthis challenge, we propose \\textbf{T}ext-\\textbf{R}egion \\textbf{M}atching for\noptimizing \\textbf{M}ulti-\\textbf{L}abel prompt tuning, namely TRM-ML, a novel\nmethod for enhancing meaningful cross-modal matching. Compared to existing\nmethods, we advocate exploring the information of category-aware regions rather\nthan the entire image or pixels, which contributes to bridging the semantic gap\nbetween textual and visual representations in a one-to-one matching manner.\nConcurrently, we further introduce multimodal contrastive learning to narrow\nthe semantic gap between textual and visual modalities and establish\nintra-class and inter-class relationships. Additionally, to deal with missing\nlabels, we propose a multimodal category prototype that leverages intra- and\ninter-category semantic relationships to estimate unknown labels, facilitating\npseudo-label generation. Extensive experiments on the MS-COCO, PASCAL VOC,\nVisual Genome, NUS-WIDE, and CUB-200-211 benchmark datasets demonstrate that\nour proposed framework outperforms the state-of-the-art methods by a\nsignificant margin. Our code is available\nhere\\href{https://github.com/yu-gi-oh-leilei/TRM-ML}{\\raisebox{-1pt}{\\faGithub}}.\n","authors":["Leilei Ma","Hongxing Xie","Lei Wang","Yanping Fu","Dengdi Sun","Haifeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.18520v2.pdf","comment":"Accepted to ACM International Conference on Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.03790v1","updated":"2024-08-07T14:14:53Z","published":"2024-08-07T14:14:53Z","title":"Vision-Language Guidance for LiDAR-based Unsupervised 3D Object\n  Detection","summary":"  Accurate 3D object detection in LiDAR point clouds is crucial for autonomous\ndriving systems. To achieve state-of-the-art performance, the supervised\ntraining of detectors requires large amounts of human-annotated data, which is\nexpensive to obtain and restricted to predefined object categories. To mitigate\nmanual labeling efforts, recent unsupervised object detection approaches\ngenerate class-agnostic pseudo-labels for moving objects, subsequently serving\nas supervision signal to bootstrap a detector. Despite promising results, these\napproaches do not provide class labels or generalize well to static objects.\nFurthermore, they are mostly restricted to data containing multiple drives from\nthe same scene or images from a precisely calibrated and synchronized camera\nsetup. To overcome these limitations, we propose a vision-language-guided\nunsupervised 3D detection approach that operates exclusively on LiDAR point\nclouds. We transfer CLIP knowledge to classify point clusters of static and\nmoving objects, which we discover by exploiting the inherent spatio-temporal\ninformation of LiDAR point clouds for clustering, tracking, as well as box and\nlabel refinement. Our approach outperforms state-of-the-art unsupervised 3D\nobject detectors on the Waymo Open Dataset ($+23~\\text{AP}_{3D}$) and Argoverse\n2 ($+7.9~\\text{AP}_{3D}$) and provides class labels not solely based on object\nsize assumptions, marking a significant advancement in the field.\n","authors":["Christian Fruhwirth-Reisinger","Wei Lin","Duan Mali","Horst Bischof","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2408.03790v1.pdf","comment":"Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2408.03789v1","updated":"2024-08-07T14:14:05Z","published":"2024-08-07T14:14:05Z","title":"Counterfactuals and Uncertainty-Based Explainable Paradigm for the\n  Automated Detection and Segmentation of Renal Cysts in Computed Tomography\n  Images: A Multi-Center Study","summary":"  Routine computed tomography (CT) scans often detect a wide range of renal\ncysts, some of which may be malignant. Early and precise localization of these\ncysts can significantly aid quantitative image analysis. Current segmentation\nmethods, however, do not offer sufficient interpretability at the feature and\npixel levels, emphasizing the necessity for an explainable framework that can\ndetect and rectify model inaccuracies. We developed an interpretable\nsegmentation framework and validated it on a multi-centric dataset. A\nVariational Autoencoder Generative Adversarial Network (VAE-GAN) was employed\nto learn the latent representation of 3D input patches and reconstruct input\nimages. Modifications in the latent representation using the gradient of the\nsegmentation model generated counterfactual explanations for varying dice\nsimilarity coefficients (DSC). Radiomics features extracted from these\ncounterfactual images, using a ground truth cyst mask, were analyzed to\ndetermine their correlation with segmentation performance. The DSCs for the\noriginal and VAE-GAN reconstructed images for counterfactual image generation\nshowed no significant differences. Counterfactual explanations highlighted how\nvariations in cyst image features influence segmentation outcomes and showed\nmodel discrepancies. Radiomics features correlating positively and negatively\nwith dice scores were identified. The uncertainty of the predicted segmentation\nmasks was estimated using posterior sampling of the weight space. The\ncombination of counterfactual explanations and uncertainty maps provided a\ndeeper understanding of the image features within the segmented renal cysts\nthat lead to high uncertainty. The proposed segmentation framework not only\nachieved high segmentation accuracy but also increased interpretability\nregarding how image features impact segmentation performance.\n","authors":["Zohaib Salahuddin","Abdalla Ibrahim","Sheng Kuang","Yousif Widaatalla","Razvan L. Miclea","Oliver Morin","Spencer Behr","Marnix P. M. Kop","Tom Marcelissen","Patricia Zondervan","Auke Jager","Philippe Lambin","Henry C Woodruff"],"pdf_url":"https://arxiv.org/pdf/2408.03789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06646v2","updated":"2024-08-07T14:06:30Z","published":"2024-03-20T05:52:11Z","title":"Diffusion-based Human Motion Style Transfer with Semantic Guidance","summary":"  3D Human motion style transfer is a fundamental problem in computer graphic\nand animation processing. Existing AdaIN- based methods necessitate datasets\nwith balanced style distribution and content/style labels to train the\nclustered latent space. However, we may encounter a single unseen style example\nin practical scenarios, but not in sufficient quantity to constitute a style\ncluster for AdaIN-based methods. Therefore, in this paper, we propose a novel\ntwo-stage framework for few-shot style transfer learning based on the diffusion\nmodel. Specifically, in the first stage, we pre-train a diffusion-based\ntext-to-motion model as a generative prior so that it can cope with various\ncontent motion inputs. In the second stage, based on the single style example,\nwe fine-tune the pre-trained diffusion model in a few-shot manner to make it\ncapable of style transfer. The key idea is regarding the reverse process of\ndiffusion as a motion-style translation process since the motion styles can be\nviewed as special motion variations. During the fine-tuning for style transfer,\na simple yet effective semantic-guided style transfer loss coordinated with\nstyle example reconstruction loss is introduced to supervise the style transfer\nin CLIP semantic space. The qualitative and quantitative evaluations\ndemonstrate that our method can achieve state-of-the-art performance and has\npractical applications.\n","authors":["Lei Hu","Zihao Zhang","Yongjing Ye","Yiwen Xu","Shihong Xia"],"pdf_url":"https://arxiv.org/pdf/2405.06646v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.05771v2","updated":"2024-08-07T14:01:34Z","published":"2024-07-08T09:27:34Z","title":"Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction","summary":"  Inverse rendering methods have achieved remarkable performance in\nreconstructing high-fidelity 3D objects with disentangled geometries,\nmaterials, and environmental light. However, they still face huge challenges in\nreflective surface reconstruction. Although recent methods model the light\ntrace to learn specularity, the ignorance of indirect illumination makes it\nhard to handle inter-reflections among multiple smooth objects. In this work,\nwe propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which\ncomprehensively computes the environmental illumination and meanwhile considers\nthe reflective light from object surfaces. To address the computation challenge\nas the times of Monte Carlo sampling grow, we propose a specularity-adaptive\nsampling strategy, significantly reducing the computational complexity. Besides\nthe computational resource, higher geometry accuracy is also required because\ngeometric errors accumulate multiple times. Therefore, we further introduce a\nreflection-aware surface model to initialize the geometry and refine it during\ninverse rendering. We construct a challenging dataset containing scenes with\nmultiple objects and inter-reflections. Experiments show that our method\noutperforms other inverse rendering methods on various object groups. We also\nshow downstream applications, e.g., relighting and material editing, to\nillustrate the disentanglement ability of our method.\n","authors":["Tengjie Zhu","Zhuo Chen","Jingnan Gao","Yichao Yan","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.05771v2.pdf","comment":"10 pages,6 figures,NeurIPS 2024 Submitted"},{"id":"http://arxiv.org/abs/2407.15793v2","updated":"2024-08-07T13:59:46Z","published":"2024-07-22T16:51:28Z","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning","summary":"  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Emanuele Frascaroli","Aniello Panariello","Pietro Buzzega","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.15793v2.pdf","comment":"15 pages, 1 figure. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2408.03771v1","updated":"2024-08-07T13:47:32Z","published":"2024-08-07T13:47:32Z","title":"Methodological Explainability Evaluation of an Interpretable Deep\n  Learning Model for Post-Hepatectomy Liver Failure Prediction Incorporating\n  Counterfactual Explanations and Layerwise Relevance Propagation: A\n  Prospective In Silico Trial","summary":"  Artificial intelligence (AI)-based decision support systems have demonstrated\nvalue in predicting post-hepatectomy liver failure (PHLF) in hepatocellular\ncarcinoma (HCC). However, they often lack transparency, and the impact of model\nexplanations on clinicians' decisions has not been thoroughly evaluated.\nBuilding on prior research, we developed a variational autoencoder-multilayer\nperceptron (VAE-MLP) model for preoperative PHLF prediction. This model\nintegrated counterfactuals and layerwise relevance propagation (LRP) to provide\ninsights into its decision-making mechanism. Additionally, we proposed a\nmethodological framework for evaluating the explainability of AI systems. This\nframework includes qualitative and quantitative assessments of explanations\nagainst recognized biomarkers, usability evaluations, and an in silico clinical\ntrial. Our evaluations demonstrated that the model's explanation correlated\nwith established biomarkers and exhibited high usability at both the case and\nsystem levels. Furthermore, results from the three-track in silico clinical\ntrial showed that clinicians' prediction accuracy and confidence increased when\nAI explanations were provided.\n","authors":["Xian Zhong","Zohaib Salahuddin","Yi Chen","Henry C Woodruff","Haiyi Long","Jianyun Peng","Nuwan Udawatte","Roberto Casale","Ayoub Mokhtari","Xiaoer Zhang","Jiayao Huang","Qingyu Wu","Li Tan","Lili Chen","Dongming Li","Xiaoyan Xie","Manxia Lin","Philippe Lambin"],"pdf_url":"https://arxiv.org/pdf/2408.03771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10428v5","updated":"2024-08-07T13:44:06Z","published":"2023-03-18T14:46:44Z","title":"RCA: Region Conditioned Adaptation for Visual Abductive Reasoning","summary":"  Visual abductive reasoning aims to make likely explanations for visual\nobservations. We propose a simple yet effective Region Conditioned Adaptation,\na hybrid parameter-efficient fine-tuning method that equips the frozen CLIP\nwith the ability to infer explanations from local visual cues. We encode\n``local hints'' and ``global contexts'' into visual prompts of the CLIP model\nseparately at fine and coarse-grained levels. Adapters are used for fine-tuning\nCLIP models for downstream tasks and we design a new attention adapter, that\ndirectly steers the focus of the attention map with trainable query and key\nprojections of a frozen CLIP model. Finally, we train our new model with a\nmodified contrastive loss to regress the visual feature simultaneously toward\nfeatures of literal description and plausible explanations. The loss enables\nCLIP to maintain both perception and reasoning abilities. Experiments on the\nSherlock visual abductive reasoning benchmark show that the RCA significantly\noutstands previous SOTAs, ranking the \\nth{1} on the leaderboards (e.g., Human\nAcc: RCA 31.74 \\textit{vs} CPT-CLIP 29.58, higher =better). We also validate\nthe RCA is generalizable to local perception benchmarks like RefCOCO. We\nopen-source our project at\n\\textit{\\color{magenta}{\\url{https://github.com/LUNAProject22/RPA}}}.\n","authors":["Hao Zhang","Yeo Keat Ee","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2303.10428v5.pdf","comment":"13 pages, 11 figures, ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2406.18944v3","updated":"2024-08-07T13:37:41Z","published":"2024-06-27T07:14:14Z","title":"Investigating and Defending Shortcut Learning in Personalized Diffusion\n  Models","summary":"  Personalized diffusion models have gained popularity for adapting pre-trained\ntext-to-image models to generate images of specific topics with minimal\ntraining data. However, these models are vulnerable to minor adversarial\nperturbations, leading to degraded performance on corrupted datasets. Such\nvulnerabilities are further exploited to craft protective perturbations on\nsensitive images like portraits that prevent unauthorized generation. In\nresponse, diffusion-based purification methods have been proposed to remove\nthese perturbations and retain generation performance. However, existing works\nturn to over-purifying the images, which causes information loss. In this\npaper, we take a closer look at the fine-tuning process of personalized\ndiffusion models through the lens of shortcut learning. And we propose a\nhypothesis explaining the manipulation mechanisms of existing perturbation\nmethods, demonstrating that perturbed images significantly deviate from their\noriginal prompts in the CLIP-based latent space. This misalignment during\nfine-tuning causes models to associate noisy patterns with identifiers,\nresulting in performance degradation. Based on these insights, we introduce a\nsystematic approach to maintain training performance through purification. Our\nmethod first purifies the images to realign them with their original semantic\nmeanings in latent space. Then, we introduce contrastive learning with negative\ntokens to decouple the learning of clean identities from noisy patterns, which\nshows a strong potential capacity against adaptive perturbation. Our study\nuncovers shortcut learning vulnerabilities in personalized diffusion models and\nprovides a firm evaluation framework for future protective perturbation\nresearch. Code is available at https://github.com/liuyixin-louis/DiffShortcut.\n","authors":["Yixin Liu","Ruoxi Chen","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18944v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.03761v1","updated":"2024-08-07T13:30:58Z","published":"2024-08-07T13:30:58Z","title":"MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video","summary":"  We present the first automated multimodal summary generation system,\nMMSummary, for medical imaging video, particularly with a focus on fetal\nultrasound analysis. Imitating the examination process performed by a human\nsonographer, MMSummary is designed as a three-stage pipeline, progressing from\nkeyframe detection to keyframe captioning and finally anatomy segmentation and\nmeasurement. In the keyframe detection stage, an innovative automated workflow\nis proposed to progressively select a concise set of keyframes, preserving\nsufficient video information without redundancy. Subsequently, we adapt a large\nlanguage model to generate meaningful captions for fetal ultrasound keyframes\nin the keyframe captioning stage. If a keyframe is captioned as fetal biometry,\nthe segmentation and measurement stage estimates biometric parameters by\nsegmenting the region of interest according to the textual prior. The MMSummary\nsystem provides comprehensive summaries for fetal ultrasound examinations and\nbased on reported experiments is estimated to reduce scanning time by\napproximately 31.5%, thereby suggesting the potential to enhance clinical\nworkflow efficiency.\n","authors":["Xiaoqing Guo","Qianhui Men","J. Alison Noble"],"pdf_url":"https://arxiv.org/pdf/2408.03761v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.01953v2","updated":"2024-08-07T13:24:38Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v2.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2404.11317v2","updated":"2024-08-07T13:20:30Z","published":"2024-04-17T12:30:54Z","title":"Improving Composed Image Retrieval via Contrastive Learning with Scaling\n  Positives and Negatives","summary":"  The Composed Image Retrieval (CIR) task aims to retrieve target images using\na composed query consisting of a reference image and a modified text. Advanced\nmethods often utilize contrastive learning as the optimization objective, which\nbenefits from adequate positive and negative examples. However, the triplet for\nCIR incurs high manual annotation costs, resulting in limited positive\nexamples. Furthermore, existing methods commonly use in-batch negative\nsampling, which reduces the negative number available for the model. To address\nthe problem of lack of positives, we propose a data generation method by\nleveraging a multi-modal large language model to construct triplets for CIR. To\nintroduce more negatives during fine-tuning, we design a two-stage fine-tuning\nframework for CIR, whose second stage introduces plenty of static\nrepresentations of negatives to optimize the representation space rapidly. The\nabove two improvements can be effectively stacked and designed to be\nplug-and-play, easily applied to existing CIR models without changing their\noriginal architectures. Extensive experiments and ablation analysis demonstrate\nthat our method effectively scales positives and negatives and achieves\nstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, our\nmethod also performs well in zero-shot composed image retrieval, providing a\nnew CIR solution for the low-resources scenario. Our code and data are released\nat https://github.com/BUAADreamer/SPN4CIR.\n","authors":["Zhangchi Feng","Richong Zhang","Zhijie Nie"],"pdf_url":"https://arxiv.org/pdf/2404.11317v2.pdf","comment":"Accepted to ACM MM 2024 Regular Papers"},{"id":"http://arxiv.org/abs/2408.03753v1","updated":"2024-08-07T13:06:29Z","published":"2024-08-07T13:06:29Z","title":"3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting","summary":"  The use of 3D Gaussians as representation of radiance fields has enabled high\nquality novel view synthesis at real-time rendering speed. However, the choice\nof optimising the outgoing radiance of each Gaussian independently as spherical\nharmonics results in unsatisfactory view dependent effects. In response to\nthese limitations, our work, Factorised Tensorial Illumination for 3D Gaussian\nSplatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering\nquality. Instead of optimising a single outgoing radiance parameter, 3iGS\nenhances 3DGS view-dependent effects by expressing the outgoing radiance as a\nfunction of a local illumination field and Bidirectional Reflectance\nDistribution Function (BRDF) features. We optimise a continuous incident\nillumination field through a Tensorial Factorisation representation, while\nseparately fine-tuning the BRDF features of each 3D Gaussian relative to this\nillumination field. Our methodology significantly enhances the rendering\nquality of specular view-dependent effects of 3DGS, while maintaining rapid\ntraining and rendering speeds.\n","authors":["Zhe Jun Tang","Tat-Jen Cham"],"pdf_url":"https://arxiv.org/pdf/2408.03753v1.pdf","comment":"The 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03748v1","updated":"2024-08-07T13:01:10Z","published":"2024-08-07T13:01:10Z","title":"Data Generation Scheme for Thermal Modality with Edge-Guided Adversarial\n  Conditional Diffusion Model","summary":"  In challenging low light and adverse weather conditions,thermal vision\nalgorithms,especially object detection,have exhibited remarkable\npotential,contrasting with the frequent struggles encountered by visible vision\nalgorithms. Nevertheless,the efficacy of thermal vision algorithms driven by\ndeep learning models remains constrained by the paucity of available training\ndata samples. To this end,this paper introduces a novel approach termed the\nedge guided conditional diffusion model. This framework aims to produce\nmeticulously aligned pseudo thermal images at the pixel level,leveraging edge\ninformation extracted from visible images. By utilizing edges as contextual\ncues from the visible domain,the diffusion model achieves meticulous control\nover the delineation of objects within the generated images. To alleviate the\nimpacts of those visible-specific edge information that should not appear in\nthe thermal domain,a two-stage modality adversarial training strategy is\nproposed to filter them out from the generated images by differentiating the\nvisible and thermal modality. Extensive experiments on LLVIP demonstrate ECDM s\nsuperiority over existing state-of-the-art approaches in terms of image\ngeneration quality.\n","authors":["Guoqing Zhu","Honghu Pan","Qiang Wang","Chao Tian","Chao Yang","Zhenyu He"],"pdf_url":"https://arxiv.org/pdf/2408.03748v1.pdf","comment":"accepted by ACM MM 2024/ACM MM24"},{"id":"http://arxiv.org/abs/2305.18381v4","updated":"2024-08-07T12:59:31Z","published":"2023-05-28T06:53:41Z","title":"Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation","summary":"  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n","authors":["Yue Xu","Yong-Lu Li","Kaitong Cui","Ziyu Wang","Cewu Lu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2305.18381v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03745v1","updated":"2024-08-07T12:58:39Z","published":"2024-08-07T12:58:39Z","title":"Intuitionistic Fuzzy Cognitive Maps for Interpretable Image\n  Classification","summary":"  The interpretability of machine learning models is critical, as users may be\nreluctant to rely on their inferences. Intuitionistic FCMs (iFCMs) have been\nproposed as an extension of FCMs offering a natural mechanism to assess the\nquality of their output through the estimation of hesitancy, a concept\nresembling to human hesitation in decision making. To address the challenge of\ninterpretable image classification, this paper introduces a novel framework,\nnamed Interpretable Intuitionistic FCM (I2FCM) which is domain-independent,\nsimple to implement, and can be applied on Convolutional Neural Network (CNN)\nmodels, rendering them interpretable. To the best of our knowledge this is the\nfirst time iFCMs are applied for image classification. Further novel\ncontributions include: a feature extraction process focusing on the most\ninformative image regions; a learning algorithm for data-driven determination\nof the intuitionistic fuzzy interconnections of the iFCM; an inherently\ninterpretable classification approach based on image contents. In the context\nof image classification, hesitancy is considered as a degree of inconfidence\nwith which an image is categorized to a class. The constructed iFCM model\ndistinguishes the most representative image semantics and analyses them\nutilizing cause-and-effect relations. The effectiveness of the introduced\nframework is evaluated on publicly available datasets, and the experimental\nresults confirm that it can provide enhanced classification performance, while\nproviding interpretable inferences.\n","authors":["Georgia Sovatzidi","Michael D. Vasilakakis","Dimitris K. Iakovidis"],"pdf_url":"https://arxiv.org/pdf/2408.03745v1.pdf","comment":"This work has been submitted for possible journal publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03735v1","updated":"2024-08-07T12:42:09Z","published":"2024-08-07T12:42:09Z","title":"Advancing Multimodal Large Language Models with Quantization-Aware Scale\n  Learning for Efficient Adaptation","summary":"  This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.\n","authors":["Jingjing Xie","Yuxin Zhang","Mingbao Lin","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03735v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.03734v1","updated":"2024-08-07T12:42:06Z","published":"2024-08-07T12:42:06Z","title":"Soft-Hard Attention U-Net Model and Benchmark Dataset for Multiscale\n  Image Shadow Removal","summary":"  Effective shadow removal is pivotal in enhancing the visual quality of images\nin various applications, ranging from computer vision to digital photography.\nDuring the last decades physics and machine learning -based methodologies have\nbeen proposed; however, most of them have limited capacity in capturing complex\nshadow patterns due to restrictive model assumptions, neglecting the fact that\nshadows usually appear at different scales. Also, current datasets used for\nbenchmarking shadow removal are composed of a limited number of images with\nsimple scenes containing mainly uniform shadows cast by single objects, whereas\nonly a few of them include both manual shadow annotations and paired\nshadow-free images. Aiming to address all these limitations in the context of\nnatural scene imaging, including urban environments with complex scenes, the\ncontribution of this study is twofold: a) it proposes a novel deep learning\narchitecture, named Soft-Hard Attention U-net (SHAU), focusing on multiscale\nshadow removal; b) it provides a novel synthetic dataset, named Multiscale\nShadow Removal Dataset (MSRD), containing complex shadow patterns of multiple\nscales, aiming to serve as a privacy-preserving dataset for a more\ncomprehensive benchmarking of future shadow removal methodologies. Key\narchitectural components of SHAU are the soft and hard attention modules, which\nalong with multiscale feature extraction blocks enable effective shadow removal\nof different scales and intensities. The results demonstrate the effectiveness\nof SHAU over the relevant state-of-the-art shadow removal methods across\nvarious benchmark datasets, improving the Peak Signal-to-Noise Ratio and Root\nMean Square Error for the shadow area by 25.1% and 61.3%, respectively.\n","authors":["Eirini Cholopoulou","Dimitrios E. Diamantis","Dimitra-Christina C. Koutsiou","Dimitris K. Iakovidis"],"pdf_url":"https://arxiv.org/pdf/2408.03734v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03717v1","updated":"2024-08-07T12:10:32Z","published":"2024-08-07T12:10:32Z","title":"Pick of the Bunch: Detecting Infrared Small Targets Beyond Hit-Miss\n  Trade-Offs via Selective Rank-Aware Attention","summary":"  Infrared small target detection faces the inherent challenge of precisely\nlocalizing dim targets amidst complex background clutter. Traditional\napproaches struggle to balance detection precision and false alarm rates. To\nbreak this dilemma, we propose SeRankDet, a deep network that achieves high\naccuracy beyond the conventional hit-miss trade-off, by following the ``Pick of\nthe Bunch'' principle. At its core lies our Selective Rank-Aware Attention\n(SeRank) module, employing a non-linear Top-K selection process that preserves\nthe most salient responses, preventing target signal dilution while maintaining\nconstant complexity. Furthermore, we replace the static concatenation typical\nin U-Net structures with our Large Selective Feature Fusion (LSFF) module, a\ndynamic fusion strategy that empowers SeRankDet with adaptive feature\nintegration, enhancing its ability to discriminate true targets from false\nalarms. The network's discernment is further refined by our Dilated Difference\nConvolution (DDC) module, which merges differential convolution aimed at\namplifying subtle target characteristics with dilated convolution to expand the\nreceptive field, thereby substantially improving target-background separation.\nDespite its lightweight architecture, the proposed SeRankDet sets new\nbenchmarks in state-of-the-art performance across multiple public datasets. The\ncode is available at https://github.com/GrokCV/SeRankDet.\n","authors":["Yimian Dai","Peiwen Pan","Yulei Qian","Yuxuan Li","Xiang Li","Jian Yang","Huan Wan"],"pdf_url":"https://arxiv.org/pdf/2408.03717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01334v2","updated":"2024-08-07T12:01:58Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v2.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2305.12661v4","updated":"2024-08-07T11:37:02Z","published":"2023-05-22T03:04:22Z","title":"Semantic-guided modeling of spatial relation and object co-occurrence\n  for indoor scene recognition","summary":"  Exploring the semantic context in scene images is essential for indoor scene\nrecognition. However, due to the diverse intra-class spatial layouts and the\ncoexisting inter-class objects, modeling contextual relationships to adapt\nvarious image characteristics is a great challenge. Existing contextual\nmodeling methods for scene recognition exhibit two limitations: 1) They\ntypically model only one type of spatial relationship (order or metric) among\nobjects within scenes, with limited exploration of diverse spatial layouts. 2)\nThey often overlook the differences in coexisting objects across different\nscenes, suppressing scene recognition performance. To overcome these\nlimitations, we propose SpaCoNet, which simultaneously models Spatial relation\nand Co-occurrence of objects guided by semantic segmentation. Firstly, the\nSemantic Spatial Relation Module (SSRM) is constructed to model scene spatial\nfeatures. With the help of semantic segmentation, this module decouples spatial\ninformation from the scene image and thoroughly explores all spatial\nrelationships among objects in an end-to-end manner, thereby obtaining\nsemantic-based spatial features. Secondly, both spatial features from the SSRM\nand deep features from the Image Feature Extraction Module are allocated to\neach object, so as to distinguish the coexisting object across different\nscenes. Finally, utilizing the discriminative features above, we design a\nGlobal-Local Dependency Module to explore the long-range co-occurrence among\nobjects, and further generate a semantic-guided feature representation for\nindoor scene recognition. Experimental results on three widely used scene\ndatasets demonstrate the effectiveness and generality of the proposed method.\n","authors":["Chuanxin Song","Hanbo Wu","Xin Ma"],"pdf_url":"https://arxiv.org/pdf/2305.12661v4.pdf","comment":"Under second review at Expert Systems with Applications"},{"id":"http://arxiv.org/abs/2408.03703v1","updated":"2024-08-07T11:33:46Z","published":"2024-08-07T11:33:46Z","title":"CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications","summary":"  Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we construct a novel additive\nsimilarity function following this paradigm and present an efficient\nimplementation named Convolutional Additive Token Mixer (CATM). This\nsimplification leads to a significant reduction in computational overhead. We\nevaluate CAS-ViT across a variety of vision tasks, including image\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Our experiments, conducted on GPUs, ONNX, and iPhones,\ndemonstrate that CAS-ViT achieves a competitive performance when compared to\nother state-of-the-art backbones, establishing it as a viable option for\nefficient mobile vision applications. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}\n","authors":["Tianfang Zhang","Lei Li","Yang Zhou","Wentao Liu","Chen Qian","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05278v2","updated":"2024-08-07T11:33:21Z","published":"2024-07-07T06:36:09Z","title":"HyperKAN: Kolmogorov-Arnold Networks make Hyperspectral Image\n  Classificators Smarter","summary":"  In traditional neural network architectures, a multilayer perceptron (MLP) is\ntypically employed as a classification block following the feature extraction\nstage. However, the Kolmogorov-Arnold Network (KAN) presents a promising\nalternative to MLP, offering the potential to enhance prediction accuracy. In\nthis paper, we propose the replacement of linear and convolutional layers of\ntraditional networks with KAN-based counterparts. These modifications allowed\nus to significantly increase the per-pixel classification accuracy for\nhyperspectral remote-sensing images. We modified seven different neural network\narchitectures for hyperspectral image classification and observed a substantial\nimprovement in the classification accuracy across all the networks. The\narchitectures considered in the paper include baseline MLP, state-of-the-art 1D\n(1DCNN) and 3D convolutional (two different 3DCNN, NM3DCNN), and transformer\n(SSFTT) architectures, as well as newly proposed M1DCNN. The greatest effect\nwas achieved for convolutional networks working exclusively on spectral data,\nand the best classification quality was achieved using a KAN-based transformer\narchitecture. All the experiments were conducted using seven openly available\nhyperspectral datasets. Our code is available at\nhttps://github.com/f-neumann77/HyperKAN.\n","authors":["Valeriy Lobanov","Nikita Firsov","Evgeny Myasnikov","Roman Khabibullin","Artem Nikonorov"],"pdf_url":"https://arxiv.org/pdf/2407.05278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10271v2","updated":"2024-08-07T11:32:19Z","published":"2023-12-16T00:23:21Z","title":"Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse\n  Training Data","summary":"  Deep learning based methods for image reconstruction are state-of-the-art for\na variety of imaging tasks. However, neural networks often perform worse if the\ntraining data differs significantly from the data they are applied to. For\nexample, a model trained for accelerated magnetic resonance imaging (MRI) on\none scanner performs worse on another scanner. In this work, we investigate the\nimpact of the training data on a model's performance and robustness for\naccelerated MRI. We find that models trained on the combination of various data\ndistributions, such as those obtained from different MRI scanners and\nanatomies, exhibit robustness equal or superior to models trained on the best\nsingle distribution for a specific target distribution. Thus training on such\ndiverse data tends to improve robustness. Furthermore, training on such a\ndiverse dataset does not compromise in-distribution performance, i.e., a model\ntrained on diverse data yields in-distribution performance at least as good as\nmodels trained on the more narrow individual distributions. Our results suggest\nthat training a model for imaging on a variety of distributions tends to yield\na more effective and robust model than maintaining separate models for\nindividual distributions.\n","authors":["Kang Lin","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2312.10271v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.03695v1","updated":"2024-08-07T11:20:37Z","published":"2024-08-07T11:20:37Z","title":"Openstory++: A Large-scale Dataset and Benchmark for Instance-aware\n  Open-domain Visual Storytelling","summary":"  Recent image generation models excel at creating high-quality images from\nbrief captions. However, they fail to maintain consistency of multiple\ninstances across images when encountering lengthy contexts. This inconsistency\nis largely due to in existing training datasets the absence of granular\ninstance feature labeling in existing training datasets. To tackle these\nissues, we introduce Openstory++, a large-scale dataset combining additional\ninstance-level annotations with both images and text. Furthermore, we develop a\ntraining methodology that emphasizes entity-centric image-text generation,\nensuring that the models learn to effectively interweave visual and textual\ninformation. Specifically, Openstory++ streamlines the process of keyframe\nextraction from open-domain videos, employing vision-language models to\ngenerate captions that are then polished by a large language model for\nnarrative continuity. It surpasses previous datasets by offering a more\nexpansive open-domain resource, which incorporates automated captioning,\nhigh-resolution imagery tailored for instance count, and extensive frame\nsequences for temporal consistency. Additionally, we present Cohere-Bench, a\npioneering benchmark framework for evaluating the image generation tasks when\nlong multimodal context is provided, including the ability to keep the\nbackground, style, instances in the given context coherent. Compared to\nexisting benchmarks, our work fills critical gaps in multi-modal generation,\npropelling the development of models that can adeptly generate and interpret\ncomplex narratives in open-domain environments. Experiments conducted within\nCohere-Bench confirm the superiority of Openstory++ in nurturing high-quality\nvisual storytelling models, enhancing their ability to address open-domain\ngeneration tasks. More details can be found at https://openstorypp.github.io/\n","authors":["Zilyu Ye","Jinxiu Liu","Ruotian Peng","Jinjin Cao","Zhiyang Chen","Yiyang Zhang","Ziwei Xuan","Mingyuan Zhou","Xiaoqian Shen","Mohamed Elhoseiny","Qi Liu","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2408.03695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12670v3","updated":"2024-08-07T10:45:03Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots hold the promise of enabling natural human-robot\ninteraction through lifelike facial expressions. However, generating realistic,\nspeech-synchronized robot expressions poses significant challenges due to the\ncomplexities of facial biomechanics and the need for responsive motion\nsynthesis. This paper introduces a novel, skinning-centric approach to drive\nanimatronic robot facial expressions from speech input. At its core, the\nproposed approach employs linear blend skinning (LBS) as a unifying\nrepresentation, guiding innovations in both embodiment design and motion\nsynthesis. LBS informs the actuation topology, facilitates human expression\nretargeting, and enables efficient speech-driven facial motion generation. This\napproach demonstrates the capability to produce highly realistic facial\nexpressions on an animatronic face in real-time at over 4000 fps on a single\nNvidia RTX 4090, significantly advancing robots' ability to replicate nuanced\nhuman expressions for natural interaction. To foster further research and\ndevelopment in this field, the code has been made publicly available at:\n\\url{https://github.com/library87/OpenRoboExp}.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v3.pdf","comment":"8 pages, 6 figures, accepted to IROS 2024. For associated project\n  page, see https://library87.github.io/animatronic-face-iros24"},{"id":"http://arxiv.org/abs/2408.03677v1","updated":"2024-08-07T10:36:26Z","published":"2024-08-07T10:36:26Z","title":"L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection","summary":"  LiDAR-based vision systems are integral for 3D object detection, which is\ncrucial for autonomous navigation. However, they suffer from performance\ndegradation in adverse weather conditions due to the quality deterioration of\nLiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is\nexpected to solve this problem. However, the fusion of LiDAR and 4D radar is\nchallenging because they differ significantly in terms of data quality and the\ndegree of degradation in adverse weather. To address these issues, we introduce\nL4DR, a weather-robust 3D object detection method that effectively achieves\nLiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and\nForeground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is\nthe first exploration of the complementarity of early fusion between LiDAR and\n4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )\nparallel feature extraction backbone coupled with a Multi-Scale Gated Fusion\n(MSGF) module to counteract the varying degrees of sensor degradation under\nadverse weather conditions. Experimental evaluation on a VoD dataset with\nsimulated fog proves that L4DR is more adaptable to changing weather\nconditions. It delivers a significant performance increase under different fog\nlevels, improving the 3D mAP by up to 18.17% over the traditional LiDAR-only\napproach. Moreover, the results on the K-Radar dataset validate the consistent\nperformance improvement of L4DR in real-world adverse weather conditions.\n","authors":["Xun Huang","Ziyu Xu","Hai Wu","Jinlong Wang","Qiming Xia","Yan Xia","Jonathan Li","Kyle Gao","Chenglu Wen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14068v2","updated":"2024-08-07T10:28:18Z","published":"2024-06-20T07:44:56Z","title":"Classifying Dry Eye Disease Patients from Healthy Controls Using Machine\n  Learning and Metabolomics Data","summary":"  Dry eye disease is a common disorder of the ocular surface, leading patients\nto seek eye care. Clinical signs and symptoms are currently used to diagnose\ndry eye disease. Metabolomics, a method for analyzing biological systems, has\nbeen found helpful in identifying distinct metabolites in patients and in\ndetecting metabolic profiles that may indicate dry eye disease at early stages.\nIn this study, we explored using machine learning and metabolomics information\nto identify which cataract patients suffered from dry eye disease. As there is\nno one-size-fits-all machine learning model for metabolomics data, choosing the\nmost suitable model can significantly affect the quality of predictions and\nsubsequent metabolomics analyses. To address this challenge, we conducted a\ncomparative analysis of nine machine learning models on three metabolomics data\nsets from cataract patients with and without dry eye disease. The models were\nevaluated and optimized using nested k-fold cross-validation. To assess the\nperformance of these models, we selected a set of suitable evaluation metrics\ntailored to the data set's challenges. The logistic regression model overall\nperformed the best, achieving the highest area under the curve score of 0.8378,\nbalanced accuracy of 0.735, Matthew's correlation coefficient of 0.5147, an\nF1-score of 0.8513, and a specificity of 0.5667. Additionally, following the\nlogistic regression, the XGBoost and Random Forest models also demonstrated\ngood performance.\n","authors":["Sajad Amouei Sheshkal","Morten Gundersen","Michael Alexander Riegler","ygunn Aass Utheim","Kjell Gunnar Gundersen","Hugo Lewi Hammer"],"pdf_url":"https://arxiv.org/pdf/2406.14068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01126v2","updated":"2024-08-07T10:25:08Z","published":"2024-08-02T09:07:31Z","title":"IG-SLAM: Instant Gaussian SLAM","summary":"  3D Gaussian Splatting has recently shown promising results as an alternative\nscene representation in SLAM systems to neural implicit representations.\nHowever, current methods either lack dense depth maps to supervise the mapping\nprocess or detailed training designs that consider the scale of the\nenvironment. To address these drawbacks, we present IG-SLAM, a dense RGB-only\nSLAM system that employs robust Dense-SLAM methods for tracking and combines\nthem with Gaussian Splatting. A 3D map of the environment is constructed using\naccurate pose and dense depth provided by tracking. Additionally, we utilize\ndepth uncertainty in map optimization to improve 3D reconstruction. Our decay\nstrategy in map optimization enhances convergence and allows the system to run\nat 10 fps in a single process. We demonstrate competitive performance with\nstate-of-the-art RGB-only SLAM systems while achieving faster operation speeds.\nWe present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC\ndatasets. The system achieves photo-realistic 3D reconstruction in large-scale\nsequences, particularly in the EuRoC dataset.\n","authors":["F. Aykut Sarikamis","A. Aydin Alatan"],"pdf_url":"https://arxiv.org/pdf/2408.01126v2.pdf","comment":"8 pages, 3 page ref, 5 figures"},{"id":"http://arxiv.org/abs/2408.03663v1","updated":"2024-08-07T10:04:04Z","published":"2024-08-07T10:04:04Z","title":"Designing Extremely Memory-Efficient CNNs for On-device Vision Tasks","summary":"  In this paper, we introduce a memory-efficient CNN (convolutional neural\nnetwork), which enables resource-constrained low-end embedded and IoT devices\nto perform on-device vision tasks, such as image classification and object\ndetection, using extremely low memory, i.e., only 63 KB on ImageNet\nclassification. Based on the bottleneck block of MobileNet, we propose three\ndesign principles that significantly curtail the peak memory usage of a CNN so\nthat it can fit the limited KB memory of the low-end device. First, 'input\nsegmentation' divides an input image into a set of patches, including the\ncentral patch overlapped with the others, reducing the size (and memory\nrequirement) of a large input image. Second, 'patch tunneling' builds\nindependent tunnel-like paths consisting of multiple bottleneck blocks per\npatch, penetrating through the entire model from an input patch to the last\nlayer of the network, maintaining lightweight memory usage throughout the whole\nnetwork. Lastly, 'bottleneck reordering' rearranges the execution order of\nconvolution operations inside the bottleneck block such that the memory usage\nremains constant regardless of the size of the convolution output channels. The\nexperiment result shows that the proposed network classifies ImageNet with\nextremely low memory (i.e., 63 KB) while achieving competitive top-1 accuracy\n(i.e., 61.58\\%). To the best of our knowledge, the memory usage of the proposed\nnetwork is far smaller than state-of-the-art memory-efficient networks, i.e.,\nup to 89x and 3.1x smaller than MobileNet (i.e., 5.6 MB) and MCUNet (i.e., 196\nKB), respectively.\n","authors":["Jaewook Lee","Yoel Park","Seulki Lee"],"pdf_url":"https://arxiv.org/pdf/2408.03663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03657v1","updated":"2024-08-07T09:52:30Z","published":"2024-08-07T09:52:30Z","title":"PHOCUS: Physics-Based Deconvolution for Ultrasound Resolution\n  Enhancement","summary":"  Ultrasound is widely used in medical diagnostics allowing for accessible and\npowerful imaging but suffers from resolution limitations due to diffraction and\nthe finite aperture of the imaging system, which restricts diagnostic use. The\nimpulse function of an ultrasound imaging system is called the point spread\nfunction (PSF), which is convolved with the spatial distribution of reflectors\nin the image formation process. Recovering high-resolution reflector\ndistributions by removing image distortions induced by the convolution process\nimproves image clarity and detail. Conventionally, deconvolution techniques\nattempt to rectify the imaging system's dependent PSF, working directly on the\nradio-frequency (RF) data. However, RF data is often not readily accessible.\nTherefore, we introduce a physics-based deconvolution process using a modeled\nPSF, working directly on the more commonly available B-mode images. By\nleveraging Implicit Neural Representations (INRs), we learn a continuous\nmapping from spatial locations to their respective echogenicity values,\neffectively compensating for the discretized image space. Our contribution\nconsists of a novel methodology for retrieving a continuous echogenicity map\ndirectly from a B-mode image through a differentiable physics-based rendering\npipeline for ultrasound resolution enhancement. We qualitatively and\nquantitatively evaluate our approach on synthetic data, demonstrating\nimprovements over traditional methods in metrics such as PSNR and SSIM.\nFurthermore, we show qualitative enhancements on an ultrasound phantom and an\nin-vivo acquisition of a carotid artery.\n","authors":["Felix Duelmer","Walter Simson","Mohammad Farid Azampour","Magdalena Wysocki","Angelos Karlas","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2408.03657v1.pdf","comment":"Accepted at the Workshop of Advances in Simplifying Medical\n  Ultrasound at MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.03654v1","updated":"2024-08-07T09:40:26Z","published":"2024-08-07T09:40:26Z","title":"Unsupervised Detection of Fetal Brain Anomalies using Denoising\n  Diffusion Models","summary":"  Congenital malformations of the brain are among the most common fetal\nabnormalities that impact fetal development. Previous anomaly detection methods\non ultrasound images are based on supervised learning, rely on manual\nannotations, and risk missing underrepresented categories. In this work, we\nframe fetal brain anomaly detection as an unsupervised task using diffusion\nmodels. To this end, we employ an inpainting-based Noise Agnostic Anomaly\nDetection approach that identifies the abnormality using\ndiffusion-reconstructed fetal brain images from multiple noise levels. Our\napproach only requires normal fetal brain ultrasound images for training,\naddressing the limited availability of abnormal data. Our experiments on a\nreal-world clinical dataset show the potential of using unsupervised methods\nfor fetal brain anomaly detection. Additionally, we comprehensively evaluate\nhow different noise types affect diffusion models in the fetal anomaly\ndetection domain.\n","authors":["Markus Ditlev Sjgren Olsen","Jakob Ambsdorf","Manxi Lin","Caroline Takse-Vester","Morten Bo Sndergaard Svendsen","Anders Nymark Christensen","Mads Nielsen","Martin Grnnebk Tolsgaard","Aasa Feragen","Paraskevas Pegios"],"pdf_url":"https://arxiv.org/pdf/2408.03654v1.pdf","comment":"Accepted at ASMUS@MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.00354v2","updated":"2024-08-07T09:36:55Z","published":"2024-05-01T07:16:03Z","title":"CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with\n  Perturbation Strategies and Knowledge Distillation","summary":"  Semi-supervised learning for medical image segmentation presents a unique\nchallenge of efficiently using limited labeled data while leveraging abundant\nunlabeled data. Despite advancements, existing methods often do not fully\nexploit the potential of the unlabeled data for enhancing model robustness and\naccuracy. In this paper, we introduce CrossMatch, a novel framework that\nintegrates knowledge distillation with dual perturbation strategies-image-level\nand feature-level-to improve the model's learning from both labeled and\nunlabeled data. CrossMatch employs multiple encoders and decoders to generate\ndiverse data streams, which undergo self-knowledge distillation to enhance\nconsistency and reliability of predictions across varied perturbations. Our\nmethod significantly surpasses other state-of-the-art techniques in standard\nbenchmarks by effectively minimizing the gap between training on labeled and\nunlabeled data and improving edge accuracy and generalization in medical image\nsegmentation. The efficacy of CrossMatch is demonstrated through extensive\nexperimental validations, showing remarkable performance improvements without\nincreasing computational costs. Code for this implementation is made available\nat https://github.com/AiEson/CrossMatch.git.\n","authors":["Bin Zhao","Chunshi Wang","Shuxue Ding"],"pdf_url":"https://arxiv.org/pdf/2405.00354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15838v2","updated":"2024-08-07T09:34:25Z","published":"2024-07-22T17:55:22Z","title":"MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with\n  Extensive Diversity","summary":"  Despite the effectiveness of vision-language supervised fine-tuning in\nenhancing the performance of Vision Large Language Models (VLLMs). However,\nexisting visual instruction tuning datasets include the following limitations:\n(1) Instruction annotation quality: despite existing VLLMs exhibiting strong\nperformance, instructions generated by those advanced VLLMs may still suffer\nfrom inaccuracies, such as hallucinations. (2) Instructions and image\ndiversity: the limited range of instruction types and the lack of diversity in\nimage data may impact the model's ability to generate diversified and closer to\nreal-world scenarios outputs. To address these challenges, we construct a\nhigh-quality, diverse visual instruction tuning dataset MMInstruct, which\nconsists of 973K instructions from 24 domains. There are four instruction\ntypes: Judgement, Multiple-Choice, Long Visual Question Answering and Short\nVisual Question Answering. To construct MMInstruct, we propose an instruction\ngeneration data engine that leverages GPT-4V, GPT-3.5, and manual correction.\nOur instruction generation engine enables semi-automatic, low-cost, and\nmulti-domain instruction generation at 1/6 the cost of manual construction.\nThrough extensive experiment validation and ablation experiments, we\ndemonstrate that MMInstruct could significantly improve the performance of\nVLLMs, e.g., the model fine-tuning on MMInstruct achieves new state-of-the-art\nperformance on 10 out of 12 benchmarks. The code and data shall be available at\nhttps://github.com/yuecao0119/MMInstruct.\n","authors":["Yangzhou Liu","Yue Cao","Zhangwei Gao","Weiyun Wang","Zhe Chen","Wenhai Wang","Hao Tian","Lewei Lu","Xizhou Zhu","Tong Lu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2407.15838v2.pdf","comment":"18 pages, 8 figures, technical report"},{"id":"http://arxiv.org/abs/2408.03651v1","updated":"2024-08-07T09:30:51Z","published":"2024-08-07T09:30:51Z","title":"SAM2-PATH: A better segment anything model for semantic segmentation in\n  digital pathology","summary":"  The semantic segmentation task in pathology plays an indispensable role in\nassisting physicians in determining the condition of tissue lesions. Foundation\nmodels, such as the SAM (Segment Anything Model) and SAM2, exhibit exceptional\nperformance in instance segmentation within everyday natural scenes. SAM-PATH\nhas also achieved impressive results in semantic segmentation within the field\nof pathology. However, in computational pathology, the models mentioned above\nstill have the following limitations. The pre-trained encoder models suffer\nfrom a scarcity of pathology image data; SAM and SAM2 are not suitable for\nsemantic segmentation. In this paper, we have designed a trainable\nKolmogorov-Arnold Networks(KAN) classification module within the SAM2 workflow,\nand we have introduced the largest pretrained vision encoder for histopathology\n(UNI) to date. Our proposed framework, SAM2-PATH, augments SAM2's capability to\nperform semantic segmentation in digital pathology autonomously, eliminating\nthe need for human provided input prompts. The experimental results demonstrate\nthat, after fine-tuning the KAN classification module and decoder, Our dataset\nhas achieved competitive results on publicly available pathology data. The code\nhas been open-sourced and can be found at the following address:\nhttps://github.com/simzhangbest/SAM2PATH.\n","authors":["Mingya Zhang","Liang Wang","Limei Gu","Zhao Li","Yaohui Wang","Tingshen Ling","Xianping Tao"],"pdf_url":"https://arxiv.org/pdf/2408.03651v1.pdf","comment":"6 pages , 3 figures"},{"id":"http://arxiv.org/abs/2408.02164v2","updated":"2024-08-07T09:23:36Z","published":"2024-08-04T23:21:46Z","title":"Rethinking Affect Analysis: A Protocol for Ensuring Fairness and\n  Consistency","summary":"  Evaluating affect analysis methods presents challenges due to inconsistencies\nin database partitioning and evaluation protocols, leading to unfair and biased\nresults. Previous studies claim continuous performance improvements, but our\nfindings challenge such assertions. Using these insights, we propose a unified\nprotocol for database partitioning that ensures fairness and comparability. We\nprovide detailed demographic annotations (in terms of race, gender and age),\nevaluation metrics, and a common framework for expression recognition, action\nunit detection and valence-arousal estimation. We also rerun the methods with\nthe new protocol and introduce a new leaderboards to encourage future research\nin affect recognition with a fairer comparison. Our annotations, code, and\npre-trained models are available on\n\\hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.\n","authors":["Guanyu Hu","Dimitrios Kollias","Eleni Papadopoulou","Paraskevi Tzouveli","Jie Wei","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02164v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.06841"},{"id":"http://arxiv.org/abs/2405.10885v2","updated":"2024-08-07T09:11:38Z","published":"2024-05-17T16:22:52Z","title":"FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth\n  Estimation","summary":"  Most existing methods often rely on complex models to predict scene depth\nwith high accuracy, resulting in slow inference that is not conducive to\ndeployment. To better balance precision and speed, we first designed SmallDepth\nbased on sparsity. Second, to enhance the feature representation ability of\nSmallDepth during training under the condition of equal complexity during\ninference, we propose an equivalent transformation module(ETM). Third, to\nimprove the ability of each layer in the case of a fixed SmallDepth to perceive\ndifferent context information and improve the robustness of SmallDepth to the\nleft-right direction and illumination changes, we propose pyramid loss. Fourth,\nto further improve the accuracy of SmallDepth, we utilized the proposed\nfunction approximation loss (APX) to transfer knowledge in the pretrained\nHQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in\nsome regions, to SmallDepth. Extensive experiments demonstrate that each\nproposed component improves the precision of SmallDepth without changing the\ncomplexity of SmallDepth during inference, and the developed approach achieves\nstate-of-the-art results on KITTI at an inference speed of more than 500 frames\nper second and with approximately 2 M parameters. The code and models will be\npublicly available at https://github.com/fwucas/FA-Depth.\n","authors":["Fei Wang","Jun Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.10885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14973v2","updated":"2024-08-07T08:53:25Z","published":"2024-03-22T06:04:11Z","title":"Pose-Aware Self-Supervised Learning with Viewpoint Trajectory\n  Regularization","summary":"  Learning visual features from unlabeled images has proven successful for\nsemantic categorization, often by mapping different $views$ of the same object\nto the same feature to achieve recognition invariance. However, visual\nrecognition involves not only identifying $what$ an object is but also\nunderstanding $how$ it is presented. For example, seeing a car from the side\nversus head-on is crucial for deciding whether to stay put or jump out of the\nway. While unsupervised feature learning for downstream viewpoint reasoning is\nimportant, it remains under-explored, partly due to the lack of a standardized\nevaluation method and benchmarks.\n  We introduce a new dataset of adjacent image triplets obtained from a\nviewpoint trajectory, without any semantic or pose labels. We benchmark both\nsemantic classification and pose estimation accuracies on the same visual\nfeature. Additionally, we propose a viewpoint trajectory regularization loss\nfor learning features from unlabeled image triplets. Our experiments\ndemonstrate that this approach helps develop a visual representation that\nencodes object identity and organizes objects by their poses, retaining\nsemantic classification accuracy while achieving emergent global pose awareness\nand better generalization to novel objects. Our dataset and code are available\nat http://pwang.pw/trajSSL/.\n","authors":["Jiayun Wang","Yubei Chen","Stella X. Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14973v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03637v1","updated":"2024-08-07T08:52:21Z","published":"2024-08-07T08:52:21Z","title":"TALE: Training-free Cross-domain Image Composition via Adaptive Latent\n  Manipulation and Energy-guided Optimization","summary":"  We present TALE, a novel training-free framework harnessing the generative\ncapabilities of text-to-image diffusion models to address the cross-domain\nimage composition task that focuses on flawlessly incorporating user-specified\nobjects into a designated visual contexts regardless of domain disparity.\nPrevious methods often involve either training auxiliary networks or finetuning\ndiffusion models on customized datasets, which are expensive and may undermine\nthe robust textual and visual priors of pre-trained diffusion models. Some\nrecent works attempt to break the barrier by proposing training-free\nworkarounds that rely on manipulating attention maps to tame the denoising\nprocess implicitly. However, composing via attention maps does not necessarily\nyield desired compositional outcomes. These approaches could only retain some\nsemantic information and usually fall short in preserving identity\ncharacteristics of input objects or exhibit limited background-object style\nadaptation in generated images. In contrast, TALE is a novel method that\noperates directly on latent space to provide explicit and effective guidance\nfor the composition process to resolve these problems. Specifically, we equip\nTALE with two mechanisms dubbed Adaptive Latent Manipulation and Energy-guided\nLatent Optimization. The former formulates noisy latents conducive to\ninitiating and steering the composition process by directly leveraging\nbackground and foreground latents at corresponding timesteps, and the latter\nexploits designated energy functions to further optimize intermediate latents\nconforming to specific conditions that complement the former to generate\ndesired final results. Our experiments demonstrate that TALE surpasses prior\nbaselines and attains state-of-the-art performance in image-guided composition\nacross various photorealistic and artistic domains.\n","authors":["Kien T. Pham","Jingye Chen","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03637v1.pdf","comment":"The 32nd ACM Multimedia Conference (MM '24)"},{"id":"http://arxiv.org/abs/2408.03632v1","updated":"2024-08-07T08:43:58Z","published":"2024-08-07T08:43:58Z","title":"Concept Conductor: Orchestrating Multiple Personalized Concepts in\n  Text-to-Image Synthesis","summary":"  The customization of text-to-image models has seen significant advancements,\nyet generating multiple personalized concepts remains a challenging task.\nCurrent methods struggle with attribute leakage and layout confusion when\nhandling multiple concepts, leading to reduced concept fidelity and semantic\nconsistency. In this work, we introduce a novel training-free framework,\nConcept Conductor, designed to ensure visual fidelity and correct layout in\nmulti-concept customization. Concept Conductor isolates the sampling processes\nof multiple custom models to prevent attribute leakage between different\nconcepts and corrects erroneous layouts through self-attention-based spatial\nguidance. Additionally, we present a concept injection technique that employs\nshape-aware masks to specify the generation area for each concept. This\ntechnique injects the structure and appearance of personalized concepts through\nfeature fusion in the attention layers, ensuring harmony in the final image.\nExtensive qualitative and quantitative experiments demonstrate that Concept\nConductor can consistently generate composite images with accurate layouts\nwhile preserving the visual details of each concept. Compared to existing\nbaselines, Concept Conductor shows significant performance improvements. Our\nmethod supports the combination of any number of concepts and maintains high\nfidelity even when dealing with visually similar concepts. The code and models\nare available at https://github.com/Nihukat/Concept-Conductor.\n","authors":["Zebin Yao","Fangxiang Feng","Ruifan Li","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03632v1.pdf","comment":"Github Page: https://github.com/Nihukat/Concept-Conductor"},{"id":"http://arxiv.org/abs/2408.03627v1","updated":"2024-08-07T08:39:33Z","published":"2024-08-07T08:39:33Z","title":"Weakly Contrastive Learning via Batch Instance Discrimination and\n  Feature Clustering for Small Sample SAR ATR","summary":"  In recent years, impressive performance of deep learning technology has been\nrecognized in Synthetic Aperture Radar (SAR) Automatic Target Recognition\n(ATR). Since a large amount of annotated data is required in this technique, it\nposes a trenchant challenge to the issue of obtaining a high recognition rate\nthrough less labeled data. To overcome this problem, inspired by the\ncontrastive learning, we proposed a novel framework named Batch Instance\nDiscrimination and Feature Clustering (BIDFC). In this framework, different\nfrom that of the objective of general contrastive learning methods, embedding\ndistance between samples should be moderate because of the high similarity\nbetween samples in the SAR images. Consequently, our flexible framework is\nequipped with adjustable distance between embedding, which we term as weakly\ncontrastive learning. Technically, instance labels are assigned to the\nunlabeled data in per batch and random augmentation and training are performed\nfew times on these augmented data. Meanwhile, a novel Dynamic-Weighted Variance\nloss (DWV loss) function is also posed to cluster the embedding of enhanced\nversions for each sample. Experimental results on the moving and stationary\ntarget acquisition and recognition (MSTAR) database indicate a 91.25%\nclassification accuracy of our method fine-tuned on only 3.13% training data.\nEven though a linear evaluation is performed on the same training data, the\naccuracy can still reach 90.13%. We also verified the effectiveness of BIDFC in\nOpenSarShip database, indicating that our method can be generalized to other\ndatasets. Our code is avaliable at:\nhttps://github.com/Wenlve-Zhou/BIDFC-master.\n","authors":["Yikui Zhai","Wenlve Zhou","Bing Sun","Jingwen Li","Qirui Ke","Zilu Ying","Junying Gan","Chaoyun Mai","Ruggero Donida Labati","Vincenzo Piuri","Fabio Scotti"],"pdf_url":"https://arxiv.org/pdf/2408.03627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03624v1","updated":"2024-08-07T08:34:48Z","published":"2024-08-07T08:34:48Z","title":"AgentsCoMerge: Large Language Model Empowered Collaborative Decision\n  Making for Ramp Merging","summary":"  Ramp merging is one of the bottlenecks in traffic systems, which commonly\ncause traffic congestion, accidents, and severe carbon emissions. In order to\naddress this essential issue and enhance the safety and efficiency of connected\nand autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel\ncollaborative decision-making framework, named AgentsCoMerge, to leverage large\nlanguage models (LLMs). Specifically, we first design a scene observation and\nunderstanding module to allow an agent to capture the traffic environment. Then\nwe propose a hierarchical planning module to enable the agent to make decisions\nand plan trajectories based on the observation and the agent's own state. In\naddition, in order to facilitate collaboration among multiple agents, we\nintroduce a communication module to enable the surrounding agents to exchange\nnecessary information and coordinate their actions. Finally, we develop a\nreinforcement reflection guided training paradigm to further enhance the\ndecision-making capability of the framework. Extensive experiments are\nconducted to evaluate the performance of our proposed method, demonstrating its\nsuperior efficiency and effectiveness for multi-agent collaborative\ndecision-making under various ramp merging scenarios.\n","authors":["Senkang Hu","Zhengru Fang","Zihan Fang","Yiqin Deng","Xianhao Chen","Yuguang Fang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2408.03624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09914v4","updated":"2024-08-07T08:20:43Z","published":"2023-04-19T18:32:49Z","title":"The Face of Populism: Examining Differences in Facial Emotional\n  Expressions of Political Leaders Using Machine Learning","summary":"  Populist rhetoric employed on online media is characterized as deeply\nimpassioned and often imbued with strong emotions. The aim of this paper is to\nempirically investigate the differences in affective nonverbal communication of\npolitical leaders. We use a deep-learning approach to process a sample of 220\nYouTube videos of political leaders from 15 different countries, analyze their\nfacial expressions of emotion and then examine differences in average emotion\nscores representing the relative presence of 6 emotional states (anger,\ndisgust, fear, happiness, sadness, and surprise) and a neutral expression for\neach frame of the YouTube video. Based on a sample of manually coded images, we\nfind that this deep-learning approach has 53-60\\% agreement with human labels.\nWe observe statistically significant differences in the average score of\nnegative emotions between groups of leaders with varying degrees of populist\nrhetoric.\n","authors":["Sara Major","Aleksandar Tomaevi"],"pdf_url":"https://arxiv.org/pdf/2304.09914v4.pdf","comment":"Version 4.0: Annotation study added, supplementary information\n  extended"},{"id":"http://arxiv.org/abs/2408.03616v1","updated":"2024-08-07T08:17:34Z","published":"2024-08-07T08:17:34Z","title":"Distillation Learning Guided by Image Reconstruction for One-Shot\n  Medical Image Segmentation","summary":"  Traditional one-shot medical image segmentation (MIS) methods use\nregistration networks to propagate labels from a reference atlas or rely on\ncomprehensive sampling strategies to generate synthetic labeled data for\ntraining. However, these methods often struggle with registration errors and\nlow-quality synthetic images, leading to poor performance and generalization.\nTo overcome this, we introduce a novel one-shot MIS framework based on\nknowledge distillation, which allows the network to directly 'see' real images\nthrough a distillation process guided by image reconstruction. It focuses on\nanatomical structures in a single labeled image and a few unlabeled ones. A\nregistration-based data augmentation network creates realistic, labeled\nsamples, while a feature distillation module helps the student network learn\nsegmentation from these samples, guided by the teacher network. During\ninference, the streamlined student network accurately segments new images.\nEvaluations on three public datasets (OASIS for T1 brain MRI, BCV for abdomen\nCT, and VerSe for vertebrae CT) show superior segmentation performance and\ngeneralization across different medical image datasets and modalities compared\nto leading methods. Our code is available at\nhttps://github.com/NoviceFodder/OS-MedSeg.\n","authors":["Feng Zhou","Yanjie Zhou","Longjie Wang","Yun Peng","David E. Carlson","Liyun Tu"],"pdf_url":"https://arxiv.org/pdf/2408.03616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03612v1","updated":"2024-08-07T08:08:08Z","published":"2024-08-07T08:08:08Z","title":"JARViS: Detecting Actions in Video Using Unified Actor-Scene Context\n  Relation Modeling","summary":"  Video action detection (VAD) is a formidable vision task that involves the\nlocalization and classification of actions within the spatial and temporal\ndimensions of a video clip. Among the myriad VAD architectures, two-stage VAD\nmethods utilize a pre-trained person detector to extract the region of interest\nfeatures, subsequently employing these features for action detection. However,\nthe performance of two-stage VAD methods has been limited as they depend solely\non localized actor features to infer action semantics. In this study, we\npropose a new two-stage VAD framework called Joint Actor-scene context Relation\nmodeling based on Visual Semantics (JARViS), which effectively consolidates\ncross-modal action semantics distributed globally across spatial and temporal\ndimensions using Transformer attention. JARViS employs a person detector to\nproduce densely sampled actor features from a keyframe. Concurrently, it uses a\nvideo backbone to create spatio-temporal scene features from a video clip.\nFinally, the fine-grained interactions between actors and scenes are modeled\nthrough a Unified Action-Scene Context Transformer to directly output the final\nset of actions in parallel. Our experimental results demonstrate that JARViS\noutperforms existing methods by significant margins and achieves\nstate-of-the-art performance on three popular VAD datasets, including AVA,\nUCF101-24, and JHMDB51-21.\n","authors":["Seok Hwan Lee","Taein Son","Soo Won Seo","Jisong Kim","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2408.03612v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.03608v1","updated":"2024-08-07T07:54:19Z","published":"2024-08-07T07:54:19Z","title":"InPer: Whole-Process Domain Generalization via Causal Intervention and\n  Perturbation","summary":"  Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer.\n","authors":["Luyao Tang","Yuxuan Yuan","Chaoqi Chen","Xinghao Ding","Yue Huang"],"pdf_url":"https://arxiv.org/pdf/2408.03608v1.pdf","comment":"Accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2408.03598v1","updated":"2024-08-07T07:35:17Z","published":"2024-08-07T07:35:17Z","title":"PRISM: PRogressive dependency maxImization for Scale-invariant image\n  Matching","summary":"  Image matching aims at identifying corresponding points between a pair of\nimages. Currently, detector-free methods have shown impressive performance in\nchallenging scenarios, thanks to their capability of generating dense matches\nand global receptive field. However, performing feature interaction and\nproposing matches across the entire image is unnecessary, because not all image\nregions contribute to the matching process. Interacting and matching in\nunmatchable areas can introduce errors, reducing matching accuracy and\nefficiency. Meanwhile, the scale discrepancy issue still troubles existing\nmethods. To address above issues, we propose PRogressive dependency\nmaxImization for Scale-invariant image Matching (PRISM), which jointly prunes\nirrelevant patch features and tackles the scale discrepancy. To do this, we\nfirstly present a Multi-scale Pruning Module (MPM) to adaptively prune\nirrelevant features by maximizing the dependency between the two feature sets.\nMoreover, we design the Scale-Aware Dynamic Pruning Attention (SADPA) to\naggregate information from different scales via a hierarchical design. Our\nmethod's superior matching performance and generalization capability are\nconfirmed by leading accuracy across various evaluation benchmarks and\ndownstream tasks. The code is publicly available at\nhttps://github.com/Master-cai/PRISM.\n","authors":["Xudong Cai","Yongcai Wang","Lun Luo","Minhang Wang","Deying Li","Jintao Xu","Weihao Gu","Rui Ai"],"pdf_url":"https://arxiv.org/pdf/2408.03598v1.pdf","comment":"15 pages, 8 figures, ACM MM 2024. Supplementary materials are\n  included"},{"id":"http://arxiv.org/abs/2405.19722v2","updated":"2024-08-07T07:28:02Z","published":"2024-05-30T06:07:57Z","title":"QClusformer: A Quantum Transformer-based Framework for Unsupervised\n  Visual Clustering","summary":"  Unsupervised vision clustering, a cornerstone in computer vision, has been\nstudied for decades, yielding significant outcomes across numerous vision\ntasks. However, these algorithms involve substantial computational demands when\nconfronted with vast amounts of unlabeled data. Conversely, quantum computing\nholds promise in expediting unsupervised algorithms when handling large-scale\ndatabases. In this study, we introduce QClusformer, a pioneering\nTransformer-based framework leveraging quantum machines to tackle unsupervised\nvision clustering challenges. Specifically, we design the Transformer\narchitecture, including the self-attention module and transformer blocks, from\na quantum perspective to enable execution on quantum hardware. In addition, we\npresent QClusformer, a variant based on the Transformer architecture, tailored\nfor unsupervised vision clustering tasks. By integrating these elements into an\nend-to-end framework, QClusformer consistently outperforms previous methods\nrunning on classical computers. Empirical evaluations across diverse\nbenchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior\nperformance of QClusformer compared to state-of-the-art methods.\n","authors":["Xuan-Bac Nguyen","Hoang-Quan Nguyen","Samuel Yen-Chi Chen","Samee U. Khan","Hugh Churchill","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2405.19722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03596v1","updated":"2024-08-07T07:24:15Z","published":"2024-08-07T07:24:15Z","title":"Hierarchical Quantum Control Gates for Functional MRI Understanding","summary":"  Quantum computing has emerged as a powerful tool for solving complex problems\nintractable for classical computers, particularly in popular fields such as\ncryptography, optimization, and neurocomputing. In this paper, we present a new\nquantum-based approach named the Hierarchical Quantum Control Gates (HQCG)\nmethod for efficient understanding of Functional Magnetic Resonance Imaging\n(fMRI) data. This approach includes two novel modules: the Local Quantum\nControl Gate (LQCG) and the Global Quantum Control Gate (GQCG), which are\ndesigned to extract local and global features of fMRI signals, respectively.\nOur method operates end-to-end on a quantum machine, leveraging quantum\nmechanics to learn patterns within extremely high-dimensional fMRI signals,\nsuch as 30,000 samples which is a challenge for classical computers. Empirical\nresults demonstrate that our approach significantly outperforms classical\nmethods. Additionally, we found that the proposed quantum model is more stable\nand less prone to overfitting than the classical methods.\n","authors":["Xuan-Bac Nguyen","Hoang-Quan Nguyen","Hugh Churchill","Samee U. Khan","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2408.03596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03592v1","updated":"2024-08-07T07:12:52Z","published":"2024-08-07T07:12:52Z","title":"HistoSPACE: Histology-Inspired Spatial Transcriptome Prediction And\n  Characterization Engine","summary":"  Spatial transcriptomics (ST) enables the visualization of gene expression\nwithin the context of tissue morphology. This emerging discipline has the\npotential to serve as a foundation for developing tools to design precision\nmedicines. However, due to the higher costs and expertise required for such\nexperiments, its translation into a regular clinical practice might be\nchallenging. Despite the implementation of modern deep learning to enhance\ninformation obtained from histological images using AI, efforts have been\nconstrained by limitations in the diversity of information. In this paper, we\ndeveloped a model, HistoSPACE that explore the diversity of histological images\navailable with ST data to extract molecular insights from tissue image. Our\nproposed study built an image encoder derived from universal image autoencoder.\nThis image encoder was connected to convolution blocks to built the final\nmodel. It was further fine tuned with the help of ST-Data. This model is\nnotably lightweight in compared to traditional histological models. Our\ndeveloped model demonstrates significant efficiency compared to contemporary\nalgorithms, revealing a correlation of 0.56 in leave-one-out cross-validation.\nFinally, its robustness was validated through an independent dataset, showing a\nwell matched preditction with predefined disease pathology.\n","authors":["Shivam Kumar","Samrat Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2408.03592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03591v1","updated":"2024-08-07T07:09:14Z","published":"2024-08-07T07:09:14Z","title":"Focal Depth Estimation: A Calibration-Free, Subject- and Daytime\n  Invariant Approach","summary":"  In an era where personalized technology is increasingly intertwined with\ndaily life, traditional eye-tracking systems and autofocal glasses face a\nsignificant challenge: the need for frequent, user-specific calibration, which\nimpedes their practicality. This study introduces a groundbreaking\ncalibration-free method for estimating focal depth, leveraging machine learning\ntechniques to analyze eye movement features within short sequences. Our\napproach, distinguished by its innovative use of LSTM networks and\ndomain-specific feature engineering, achieves a mean absolute error (MAE) of\nless than 10 cm, setting a new focal depth estimation accuracy standard. This\nadvancement promises to enhance the usability of autofocal glasses and pave the\nway for their seamless integration into extended reality environments, marking\na significant leap forward in personalized visual technology.\n","authors":["Benedikt W. Hosp","Bjrn Severitt","Rajat Agarwala","Evgenia Rusak","Yannick Sauer","Siegfried Wahl"],"pdf_url":"https://arxiv.org/pdf/2408.03591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11936v2","updated":"2024-08-07T06:43:51Z","published":"2024-07-16T17:26:50Z","title":"Thermal Imaging and Radar for Remote Sleep Monitoring of Breathing and\n  Apnea","summary":"  Polysomnography (PSG), the current gold standard method for monitoring and\ndetecting sleep disorders, is cumbersome and costly. At-home testing solutions,\nknown as home sleep apnea testing (HSAT), exist. However, they are\ncontact-based, a feature which limits the ability of some patient populations\nto tolerate testing and discourages widespread deployment. Previous work on\nnon-contact sleep monitoring for sleep apnea detection either estimates\nrespiratory effort using radar or nasal airflow using a thermal camera, but has\nnot compared the two or used them together. We conducted a study on 10\nparticipants, ages 34 - 78, with suspected sleep disorders using a hardware\nsetup with a synchronized radar and thermal camera. We show the first\ncomparison of radar and thermal imaging for sleep monitoring, and find that our\nthermal imaging method outperforms radar significantly. Our thermal imaging\nmethod detects apneas with an accuracy of 0.99, a precision of 0.68, a recall\nof 0.74, an F1 score of 0.71, and an intra-class correlation of 0.70; our radar\nmethod detects apneas with an accuracy of 0.83, a precision of 0.13, a recall\nof 0.86, an F1 score of 0.22, and an intra-class correlation of 0.13. We also\npresent a novel proposal for classifying obstructive and central sleep apnea by\nleveraging a multimodal setup. This method could be used accurately detect and\nclassify apneas during sleep with non-contact sensors, thereby improving\ndiagnostic capacities in patient populations unable to tolerate current\ntechnology.\n","authors":["Kai Del Regno","Alexander Vilesov","Adnan Armouti","Anirudh Bindiganavale Harish","Selim Emir Can","Ashley Kita","Achuta Kadambi"],"pdf_url":"https://arxiv.org/pdf/2407.11936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03574v1","updated":"2024-08-07T06:26:04Z","published":"2024-08-07T06:26:04Z","title":"Teach CLIP to Develop a Number Sense for Ordinal Regression","summary":"  Ordinal regression is a fundamental problem within the field of computer\nvision, with customised well-trained models on specific tasks. While\npre-trained vision-language models (VLMs) have exhibited impressive performance\non various vision tasks, their potential for ordinal regression has received\nless exploration. In this study, we first investigate CLIP's potential for\nordinal regression, from which we expect the model could generalise to\ndifferent ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP\nfails on this task, since current VLMs have a well-documented limitation of\nencapsulating compositional concepts such as number sense. We propose a simple\nyet effective method called NumCLIP to improve the quantitative understanding\nof VLMs. We disassemble the exact image to number-specific text matching\nproblem into coarse classification and fine prediction stages. We discretize\nand phrase each numerical bin with common language concept to better leverage\nthe available pre-trained alignment in CLIP. To consider the inherent\ncontinuous property of ordinal regression, we propose a novel fine-grained\ncross-modal ranking-based regularisation loss specifically designed to keep\nboth semantic and ordinal alignment in CLIP's feature space. Experimental\nresults on three general ordinal regression tasks demonstrate the effectiveness\nof NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating\nand image aesthetics assessment task, respectively. Code is publicly available\nat https://github.com/xmed-lab/NumCLIP.\n","authors":["Yao Du","Qiang Zhai","Weihang Dai","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.03574v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03568v1","updated":"2024-08-07T06:11:25Z","published":"2024-08-07T06:11:25Z","title":"A comparative study of generative adversarial networks for image\n  recognition algorithms based on deep learning and traditional methods","summary":"  In this paper, an image recognition algorithm based on the combination of\ndeep learning and generative adversarial network (GAN) is studied, and compared\nwith traditional image recognition methods. The purpose of this study is to\nevaluate the advantages and application prospects of deep learning technology,\nespecially GAN, in the field of image recognition. Firstly, this paper reviews\nthe basic principles and techniques of traditional image recognition methods,\nincluding the classical algorithms based on feature extraction such as SIFT,\nHOG and their combination with support vector machine (SVM), random forest, and\nother classifiers. Then, the working principle, network structure, and unique\nadvantages of GAN in image generation and recognition are introduced. In order\nto verify the effectiveness of GAN in image recognition, a series of\nexperiments are designed and carried out using multiple public image data sets\nfor training and testing. The experimental results show that compared with\ntraditional methods, GAN has excellent performance in processing complex\nimages, recognition accuracy, and anti-noise ability. Specifically, Gans are\nbetter able to capture high-dimensional features and details of images,\nsignificantly improving recognition performance. In addition, Gans shows unique\nadvantages in dealing with image noise, partial missing information, and\ngenerating high-quality images.\n","authors":["Yihao Zhong","Yijing Wei","Yingbin Liang","Xiqing Liu","Rongwei Ji","Yiru Cang"],"pdf_url":"https://arxiv.org/pdf/2408.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03567v1","updated":"2024-08-07T06:10:45Z","published":"2024-08-07T06:10:45Z","title":"Unlocking Exocentric Video-Language Data for Egocentric Video\n  Representation Learning","summary":"  We present EMBED (Egocentric Models Built with Exocentric Data), a method\ndesigned to transform exocentric video-language data for egocentric video\nrepresentation learning. Large-scale exocentric data covers diverse activities\nwith significant potential for egocentric learning, but inherent disparities\nbetween egocentric and exocentric data pose challenges in utilizing one view\nfor the other seamlessly. Egocentric videos predominantly feature close-up\nhand-object interactions, whereas exocentric videos offer a broader perspective\non human activities. Additionally, narratives in egocentric datasets are\ntypically more action-centric and closely linked with the visual content, in\ncontrast to the narrative styles found in exocentric datasets. To address these\nchallenges, we employ a data transformation framework to adapt exocentric data\nfor egocentric training, focusing on identifying specific video clips that\nemphasize hand-object interactions and transforming narration styles to align\nwith egocentric perspectives. By applying both vision and language style\ntransfer, our framework creates a new egocentric dataset derived from\nexocentric video-language data. Through extensive evaluations, we demonstrate\nthe effectiveness of EMBED, achieving state-of-the-art results across various\negocentric downstream tasks, including an absolute improvement of 4.7% on the\nEpic-Kitchens-100 multi-instance retrieval and 6.2% on the EGTEA classification\nbenchmarks in zero-shot settings. Furthermore, EMBED enables egocentric\nvideo-language models to perform competitively in exocentric tasks. Finally, we\nshowcase EMBED's application across various exocentric datasets, exhibiting\nstrong generalization capabilities when applied to different exocentric\ndatasets.\n","authors":["Zi-Yi Dou","Xitong Yang","Tushar Nagarajan","Huiyu Wang","Jing Huang","Nanyun Peng","Kris Kitani","Fu-Jen Chu"],"pdf_url":"https://arxiv.org/pdf/2408.03567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11755v3","updated":"2024-08-07T06:05:42Z","published":"2024-03-18T13:03:24Z","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","summary":"  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n","authors":["M. Jehanzeb Mirza","Leonid Karlinsky","Wei Lin","Sivan Doveh","Jakub Micorek","Mateusz Kozinski","Hilde Kuehne","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2403.11755v3.pdf","comment":"ECCV Camera Ready. Code & Data:\n  https://jmiemirza.github.io/Meta-Prompting/"},{"id":"http://arxiv.org/abs/2402.14461v2","updated":"2024-08-07T06:01:18Z","published":"2024-02-22T11:40:49Z","title":"S^2Former-OR: Single-Stage Bi-Modal Transformer for Scene Graph\n  Generation in OR","summary":"  Scene graph generation (SGG) of surgical procedures is crucial in enhancing\nholistically cognitive intelligence in the operating room (OR). However,\nprevious works have primarily relied on multi-stage learning, where the\ngenerated semantic scene graphs depend on intermediate processes with pose\nestimation and object detection. This pipeline may potentially compromise the\nflexibility of learning multimodal representations, consequently constraining\nthe overall effectiveness. In this study, we introduce a novel single-stage\nbi-modal transformer framework for SGG in the OR, termed S^2Former-OR, aimed to\ncomplementally leverage multi-view 2D scenes and 3D point clouds for SGG in an\nend-to-end manner. Concretely, our model embraces a View-Sync Transfusion\nscheme to encourage multi-view visual information interaction. Concurrently, a\nGeometry-Visual Cohesion operation is designed to integrate the synergic 2D\nsemantic features into 3D point cloud features. Moreover, based on the\naugmented feature, we propose a novel relation-sensitive transformer decoder\nthat embeds dynamic entity-pair queries and relational trait priors, which\nenables the direct prediction of entity-pair relations for graph generation\nwithout intermediate steps. Extensive experiments have validated the superior\nSGG performance and lower computational cost of S^2Former-OR on 4D-OR\nbenchmark, compared with current OR-SGG methods, e.g., 3 percentage points\nPrecision increase and 24.2M reduction in model parameters. We further compared\nour method with generic single-stage SGG methods with broader metrics for a\ncomprehensive evaluation, with consistently better performance achieved.\n","authors":["Jialun Pei","Diandian Guo","Jingyang Zhang","Manxi Lin","Yueming Jin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2402.14461v2.pdf","comment":"This work has been accepted by TMI2024"},{"id":"http://arxiv.org/abs/2408.03564v1","updated":"2024-08-07T05:56:05Z","published":"2024-08-07T05:56:05Z","title":"Underwater litter monitoring using consumer-grade aerial-aquatic speedy\n  scanner (AASS) and deep learning based super-resolution reconstruction and\n  detection network","summary":"  Underwater litter is widely spread across aquatic environments such as lakes,\nrivers, and oceans, significantly impacting natural ecosystems. Current\nmonitoring technologies for detecting underwater litter face limitations in\nsurvey efficiency, cost, and environmental conditions, highlighting the need\nfor efficient, consumer-grade technologies for automatic detection. This\nresearch introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with\nSuper-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network.\nAASS enhances data acquisition efficiency over traditional methods, capturing\nhigh-quality images that accurately identify underwater waste. SRR improves\nimage-resolution by mitigating motion blur and insufficient resolution, thereby\nenhancing detection tasks. Specifically, the RCAN model achieved the highest\nmean average precision (mAP) of 78.6% for detection accuracy on reconstructed\nimages among the tested SRR models. With a magnification factor of 4, the SRR\ntest set shows an improved mAP compared to the conventional bicubic set. These\nresults demonstrate the effectiveness of the proposed method in detecting\nunderwater litter.\n","authors":["Fan Zhao","Yongying Liu","Jiaqi Wang","Yijia Chen","Dianhan Xi","Xinlei Shao","Shigeru Tabeta","Katsunori Mizuno"],"pdf_url":"https://arxiv.org/pdf/2408.03564v1.pdf","comment":"The earlier version of this conference paper was accepted at OCEANS\n  2024-Halifax, Canada and was selected for inclusion in the Student Poster\n  Competition (SPC) Program"},{"id":"http://arxiv.org/abs/2403.19026v3","updated":"2024-08-07T05:54:04Z","published":"2024-03-27T21:43:12Z","title":"EgoNav: Egocentric Scene-aware Human Trajectory Prediction","summary":"  Wearable collaborative robots stand to assist human wearers who need fall\nprevention assistance or wear exoskeletons. Such a robot needs to be able to\nconstantly adapt to the surrounding scene based on egocentric vision, and\npredict the ego motion of the wearer. In this work, we leveraged body-mounted\ncameras and sensors to anticipate the trajectory of human wearers through\ncomplex surroundings. To facilitate research in ego-motion prediction, we have\ncollected a comprehensive walking scene navigation dataset centered on the\nuser's perspective. We then present a method to predict human motion\nconditioning on the surrounding static scene. Our method leverages a diffusion\nmodel to produce a distribution of potential future trajectories, taking into\naccount the user's observation of the environment. To that end, we introduce a\ncompact representation to encode the user's visual memory of the surroundings,\nas well as an efficient sample-generating technique to speed up real-time\ninference of a diffusion model. We ablate our model and compare it to\nbaselines, and results show that our model outperforms existing methods on key\nmetrics of collision avoidance and trajectory mode coverage.\n","authors":["Weizhuo Wang","C. Karen Liu","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2403.19026v3.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.03559v1","updated":"2024-08-07T05:47:15Z","published":"2024-08-07T05:47:15Z","title":"Monitoring of Hermit Crabs Using drone-captured imagery and Deep\n  Learning based Super-Resolution Reconstruction and Improved YOLOv8","summary":"  Hermit crabs play a crucial role in coastal ecosystems by dispersing seeds,\ncleaning up debris, and disturbing soil. They serve as vital indicators of\nmarine environmental health, responding to climate change and pollution.\nTraditional survey methods, like quadrat sampling, are labor-intensive,\ntime-consuming, and environmentally dependent. This study presents an\ninnovative approach combining UAV-based remote sensing with Super-Resolution\nReconstruction (SRR) and the CRAB-YOLO detection network, a modification of\nYOLOv8s, to monitor hermit crabs. SRR enhances image quality by addressing\nissues such as motion blur and insufficient resolution, significantly improving\ndetection accuracy over conventional low-resolution fuzzy images. The CRAB-YOLO\nnetwork integrates three improvements for detection accuracy, hermit crab\ncharacteristics, and computational efficiency, achieving state-of-the-art\n(SOTA) performance compared to other mainstream detection models. The RDN\nnetworks demonstrated the best image reconstruction performance, and CRAB-YOLO\nachieved a mean average precision (mAP) of 69.5% on the SRR test set, a 40%\nimprovement over the conventional Bicubic method with a magnification factor of\n4. These results indicate that the proposed method is effective in detecting\nhermit crabs, offering a cost-effective and automated solution for extensive\nhermit crab monitoring, thereby aiding coastal benthos conservation.\n","authors":["Fan Zhao","Yijia Chen","Dianhan Xi","Yongying Liu","Jiaqi Wang","Shigeru Tabeta","Katsunori Mizuno"],"pdf_url":"https://arxiv.org/pdf/2408.03559v1.pdf","comment":"The earlier version of this conference paper was presented at OCEANS\n  2024-Singapore and was selected for inclusion in the Student Poster\n  Competition (SPC) Program"},{"id":"http://arxiv.org/abs/2408.03558v1","updated":"2024-08-07T05:47:06Z","published":"2024-08-07T05:47:06Z","title":"D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion\n  Methods","summary":"  In image processing, one of the most challenging tasks is to render an\nimage's semantic meaning using a variety of artistic approaches. Existing\ntechniques for arbitrary style transfer (AST) frequently experience\nmode-collapse, over-stylization, or under-stylization due to a disparity\nbetween the style and content images. We propose a novel framework called\nD$^2$Styler (Discrete Diffusion Styler) that leverages the discrete\nrepresentational capability of VQ-GANs and the advantages of discrete\ndiffusion, including stable training and avoidance of mode collapse. Our method\nuses Adaptive Instance Normalization (AdaIN) features as a context guide for\nthe reverse diffusion process. This makes it easy to move features from the\nstyle image to the content image without bias. The proposed method\nsubstantially enhances the visual quality of style-transferred images, allowing\nthe combination of content and style in a visually appealing manner. We take\nstyle images from the WikiArt dataset and content images from the COCO dataset.\nExperimental results demonstrate that D$^2$Styler produces high-quality\nstyle-transferred images and outperforms twelve existing methods on nearly all\nthe metrics. The qualitative results and ablation studies provide further\ninsights into the efficacy of our technique. The code is available at\nhttps://github.com/Onkarsus13/D2Styler.\n","authors":["Onkar Susladkar","Gayatri Deshmukh","Sparsh Mittal","Parth Shastri"],"pdf_url":"https://arxiv.org/pdf/2408.03558v1.pdf","comment":"Paper accepted at 27th International Conference on Pattern\n  Recognition (ICPR), 2024"},{"id":"http://arxiv.org/abs/2106.11760v5","updated":"2024-08-07T05:28:30Z","published":"2021-06-19T06:25:10Z","title":"Fingerprinting Image-to-Image Generative Adversarial Networks","summary":"  Generative Adversarial Networks (GANs) have been widely used in various\napplication scenarios. Since the production of a commercial GAN requires\nsubstantial computational and human resources, the copyright protection of GANs\nis urgently needed. This paper presents a novel fingerprinting scheme for the\nIntellectual Property (IP) protection of image-to-image GANs based on a trusted\nthird party. We break through the stealthiness and robustness bottlenecks\nsuffered by previous fingerprinting methods for classification models being\nnaively transferred to GANs. Specifically, we innovatively construct a\ncomposite deep learning model from the target GAN and a classifier. Then we\ngenerate fingerprint samples from this composite model, and embed them in the\nclassifier for effective ownership verification. This scheme inspires some\nconcrete methodologies to practically protect the modern image-to-image\ntranslation GANs. Theoretical analysis proves that these methods can satisfy\ndifferent security requirements necessary for IP protection. We also conduct\nextensive experiments to show that our solutions outperform existing\nstrategies.\n","authors":["Guanlin Li","Guowen Xu","Han Qiu","Shangwei Guo","Run Wang","Jiwei Li","Tianwei Zhang","Rongxing Lu"],"pdf_url":"https://arxiv.org/pdf/2106.11760v5.pdf","comment":"Accepted by EuroS&P 2024"},{"id":"http://arxiv.org/abs/2408.03551v1","updated":"2024-08-07T05:23:52Z","published":"2024-08-07T05:23:52Z","title":"VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy\n  Prediction","summary":"  Monocular 3D semantic occupancy prediction is becoming important in robot\nvision due to the compactness of using a single RGB camera. However, existing\nmethods often do not adequately account for camera perspective geometry,\nresulting in information imbalance along the depth range of the image. To\naddress this issue, we propose a vanishing point (VP) guided monocular 3D\nsemantic occupancy prediction framework named VPOcc. Our framework consists of\nthree novel modules utilizing VP. First, in the VPZoomer module, we initially\nutilize VP in feature extraction to achieve information balanced feature\nextraction across the scene by generating a zoom-in image based on VP. Second,\nwe perform perspective geometry-aware feature aggregation by sampling points\ntowards VP using a VP-guided cross-attention (VPCA) module. Finally, we create\nan information-balanced feature volume by effectively fusing original and\nzoom-in voxel feature volumes with a balanced feature volume fusion (BVFV)\nmodule. Experiments demonstrate that our method achieves state-of-the-art\nperformance for both IoU and mIoU on SemanticKITTI and SSCBench-KITTI360. These\nresults are obtained by effectively addressing the information imbalance in\nimages through the utilization of VP. Our code will be available at\nwww.github.com/anonymous.\n","authors":["Junsu Kim","Junhee Lee","Ukcheol Shin","Jean Oh","Kyungdon Joo"],"pdf_url":"https://arxiv.org/pdf/2408.03551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01089v4","updated":"2024-08-07T05:22:57Z","published":"2023-02-02T13:22:18Z","title":"Curriculum Learning for ab initio Deep Learned Refractive Optics","summary":"  Deep optical optimization has recently emerged as a new paradigm for\ndesigning computational imaging systems using only the output image as the\nobjective. However, it has been limited to either simple optical systems\nconsisting of a single element such as a diffractive optical element (DOE) or\nmetalens, or the fine-tuning of compound lenses from good initial designs. Here\nwe present a DeepLens design method based on curriculum learning, which is able\nto learn optical designs of compound lenses ab initio from randomly initialized\nsurfaces without human intervention, therefore overcoming the need for a good\ninitial design. We demonstrate the effectiveness of our approach by fully\nautomatically designing both classical imaging lenses and a large field-of-view\nextended depth-of-field computational lens in a cellphone-style form factor,\nwith highly aspheric surfaces and a short back focal length.\n","authors":["Xinge Yang","Qiang Fu","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2302.01089v4.pdf","comment":"Automatically design computational lenses from scratch with\n  differentiable ray tracing"},{"id":"http://arxiv.org/abs/2404.06605v3","updated":"2024-08-07T05:10:43Z","published":"2024-04-09T20:24:29Z","title":"RoadBEV: Road Surface Reconstruction in Bird's Eye View","summary":"  Road surface conditions, especially geometry profiles, enormously affect\ndriving performance of autonomous vehicles. Vision-based online road\nreconstruction promisingly captures road information in advance. Existing\nsolutions like monocular depth estimation and stereo matching suffer from\nmodest performance. The recent technique of Bird's-Eye-View (BEV) perception\nprovides immense potential to more reliable and accurate reconstruction. This\npaper uniformly proposes two simple yet effective models for road elevation\nreconstruction in BEV named RoadBEV-mono and RoadBEV-stereo, which estimate\nroad elevation with monocular and stereo images, respectively. The former\ndirectly fits elevation values based on voxel features queried from image view,\nwhile the latter efficiently recognizes road elevation patterns based on BEV\nvolume representing correlation between left and right voxel features.\nInsightful analyses reveal their consistence and difference with the\nperspective view. Experiments on real-world dataset verify the models'\neffectiveness and superiority. Elevation errors of RoadBEV-mono and\nRoadBEV-stereo achieve 1.83 cm and 0.50 cm, respectively. Our models are\npromising for practical road preview, providing essential information for\npromoting safety and comfort of autonomous vehicles. The code is released at\nhttps://github.com/ztsrxh/RoadBEV\n","authors":["Tong Zhao","Lei Yang","Yichen Xie","Mingyu Ding","Masayoshi Tomizuka","Yintao Wei"],"pdf_url":"https://arxiv.org/pdf/2404.06605v3.pdf","comment":"Accepted by IEEE TITS https://ieeexplore.ieee.org/document/10618926"},{"id":"http://arxiv.org/abs/2305.10856v3","updated":"2024-08-07T04:59:01Z","published":"2023-05-18T10:18:59Z","title":"Spatial-Frequency Discriminability for Revealing Adversarial\n  Perturbations","summary":"  The vulnerability of deep neural networks to adversarial perturbations has\nbeen widely perceived in the computer vision community. From a security\nperspective, it poses a critical risk for modern vision systems, e.g., the\npopular Deep Learning as a Service (DLaaS) frameworks. For protecting deep\nmodels while not modifying them, current algorithms typically detect\nadversarial patterns through discriminative decomposition for natural and\nadversarial data. However, these decompositions are either biased towards\nfrequency resolution or spatial resolution, thus failing to capture adversarial\npatterns comprehensively. Also, when the detector relies on few fixed features,\nit is practical for an adversary to fool the model while evading the detector\n(i.e., defense-aware attack). Motivated by such facts, we propose a\ndiscriminative detector relying on a spatial-frequency Krawtchouk\ndecomposition. It expands the above works from two aspects: 1) the introduced\nKrawtchouk basis provides better spatial-frequency discriminability, capturing\nthe differences between natural and adversarial data comprehensively in both\nspatial and frequency distributions, w.r.t. the common trigonometric or wavelet\nbasis; 2) the extensive features formed by the Krawtchouk decomposition allows\nfor adaptive feature selection and secrecy mechanism, significantly increasing\nthe difficulty of the defense-aware attack, w.r.t. the detector with few fixed\nfeatures. Theoretical and numerical analyses demonstrate the uniqueness and\nusefulness of our detector, exhibiting competitive scores on several deep\nmodels and image sets against a variety of adversarial attacks.\n","authors":["Chao Wang","Shuren Qi","Zhiqiu Huang","Yushu Zhang","Rushi Lan","Xiaochun Cao","Feng-Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2305.10856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03545v1","updated":"2024-08-07T04:50:05Z","published":"2024-08-07T04:50:05Z","title":"CLIP-based Point Cloud Classification via Point Cloud to Image\n  Translation","summary":"  Point cloud understanding is an inherently challenging problem because of the\nsparse and unordered structure of the point cloud in the 3D space. Recently,\nContrastive Vision-Language Pre-training (CLIP) based point cloud\nclassification model i.e. PointCLIP has added a new direction in the point\ncloud classification research domain. In this method, at first multi-view depth\nmaps are extracted from the point cloud and passed through the CLIP visual\nencoder. To transfer the 3D knowledge to the network, a small network called an\nadapter is fine-tuned on top of the CLIP visual encoder. PointCLIP has two\nlimitations. Firstly, the point cloud depth maps lack image information which\nis essential for tasks like classification and recognition. Secondly, the\nadapter only relies on the global representation of the multi-view features.\nMotivated by this observation, we propose a Pretrained Point Cloud to Image\nTranslation Network (PPCITNet) that produces generalized colored images along\nwith additional salient visual cues to the point cloud depth maps so that it\ncan achieve promising performance on point cloud classification and\nunderstanding. In addition, we propose a novel viewpoint adapter that combines\nthe view feature processed by each viewpoint as well as the global intertwined\nknowledge that exists across the multi-view features. The experimental results\ndemonstrate the superior performance of the proposed model over existing\nstate-of-the-art CLIP-based models on ModelNet10, ModelNet40, and ScanobjectNN\ndatasets.\n","authors":["Shuvozit Ghose","Manyi Li","Yiming Qian","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03545v1.pdf","comment":"Accepted by ICPR2024"},{"id":"http://arxiv.org/abs/2408.03542v1","updated":"2024-08-07T04:42:10Z","published":"2024-08-07T04:42:10Z","title":"Automatic identification of the area covered by acorn trees in the\n  dehesa (pastureland) Extremadura of Spain","summary":"  The acorn is the fruit of the oak and is an important crop in the Spanish\ndehesa extreme\\~na, especially for the value it provides in the Iberian pig\nfood to obtain the \"acorn\" certification. For this reason, we want to maximise\nthe production of Iberian pigs with the appropriate weight. Hence the need to\nknow the area covered by the crowns of the acorn trees, to determine the\ncovered wooded area (CWA, from the Spanish Superficie Arbolada Cubierta SAC)\nand thereby estimate the number of Iberian pigs that can be released per\nhectare, as indicated by the royal decree 4/2014. In this work, we propose the\nautomatic estimation of the CWA, through aerial digital images (orthophotos) of\nthe pastureland of Extremadura, and with this, to offer the possibility of\ndetermining the number of Iberian pigs to be released in a specific plot of\nland. Among the main issues for automatic detection are, first, the correct\nidentification of acorn trees, secondly, correctly discriminating the shades of\nthe acorn trees and, finally, detect the arbuscles (young acorn trees not yet\nproductive, or shrubs that are not oaks). These difficulties represent a real\nchallenge, both for the automatic segmentation process and for manual\nsegmentation. In this work, the proposed method for automatic segmentation is\nbased on the clustering algorithm proposed by Gustafson-Kessel (GK) but the\nmodified version of Babuska (GK-B) and on the use of real orthophotos. The\nobtained results are promising both in their comparison with the real images\nand when compared with the images segmented by hand. The whole set of\northophotos used in this work correspond to an approximate area of 142\nhectares, and the results are of great interest to producers of certified\n\"acorn\" pork.\n","authors":["Ojeda-Magaa Benjamin","Ruelas Ruben","Quintanilla-Dominguez Joel","Gomez-Barba Leopoldo","Lopez de Herrera Juan","Robledo-Hernandez Jose","Tarquis Ana"],"pdf_url":"https://arxiv.org/pdf/2408.03542v1.pdf","comment":"22 pages, 15 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2408.03540v1","updated":"2024-08-07T04:38:03Z","published":"2024-08-07T04:38:03Z","title":"PoseMamba: Monocular 3D Human Pose Estimation with Bidirectional\n  Global-Local Spatio-Temporal State Space Model","summary":"  Transformers have significantly advanced the field of 3D human pose\nestimation (HPE). However, existing transformer-based methods primarily use\nself-attention mechanisms for spatio-temporal modeling, leading to a quadratic\ncomplexity, unidirectional modeling of spatio-temporal relationships, and\ninsufficient learning of spatial-temporal correlations. Recently, the Mamba\narchitecture, utilizing the state space model (SSM), has exhibited superior\nlong-range modeling capabilities in a variety of vision tasks with linear\ncomplexity. In this paper, we propose PoseMamba, a novel purely SSM-based\napproach with linear complexity for 3D human pose estimation in monocular\nvideo. Specifically, we propose a bidirectional global-local spatio-temporal\nSSM block that comprehensively models human joint relations within individual\nframes as well as temporal correlations across frames. Within this\nbidirectional global-local spatio-temporal SSM block, we introduce a reordering\nstrategy to enhance the local modeling capability of the SSM. This strategy\nprovides a more logical geometric scanning order and integrates it with the\nglobal SSM, resulting in a combined global-local spatial scan. We have\nquantitatively and qualitatively evaluated our approach using two benchmark\ndatasets: Human3.6M and MPI-INF-3DHP. Extensive experiments demonstrate that\nPoseMamba achieves state-of-the-art performance on both datasets while\nmaintaining a smaller model size and reducing computational costs. The code and\nmodels will be released.\n","authors":["Yunlong Huang","Junshuo Liu","Ke Xian","Robert Caiming Qiu"],"pdf_url":"https://arxiv.org/pdf/2408.03540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03538v1","updated":"2024-08-07T04:35:06Z","published":"2024-08-07T04:35:06Z","title":"PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time\n  High-Quality Relighting","summary":"  We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a\nreal-time high-quality relighting method for Gaussian splats in low-frequency\nlighting environments that captures soft shadows and interreflections by\nprecomputing 3D Gaussian splats' radiance transfer. Existing studies have\ndemonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields'\nefficiency for dynamic lighting scenarios. However, the current relighting\nmethod based on 3DGS still struggles to compute high-quality shadow and\nindirect illumination in real time for dynamic light, leading to unrealistic\nrendering results. We solve this problem by precomputing the expensive\ntransport simulations required for complex transfer functions like shadowing,\nthe resulting transfer functions are represented as dense sets of vectors or\nmatrices for every Gaussian splat. We introduce distinct precomputing methods\ntailored for training and rendering stages, along with unique ray tracing and\nindirect lighting precomputation techniques for 3D Gaussian splats to\naccelerate training speed and compute accurate indirect lighting related to\nenvironment light. Experimental analyses demonstrate that our approach achieves\nstate-of-the-art visual quality while maintaining competitive training times\nand allows high-quality real-time (30+ fps) relighting for dynamic light and\nrelatively complex scenes at 1080p resolution.\n","authors":["Yijia Guo","Yuanxi Bai","Liwen Hu","Ziyi Guo","Mianzhi Liu","Yu Cai","Tiejun Huang","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04346v2","updated":"2024-08-07T04:00:57Z","published":"2024-07-05T08:37:10Z","title":"MobileFlow: A Multimodal LLM For Mobile GUI Agent","summary":"  Currently, the integration of mobile Graphical User Interfaces (GUIs) is\nubiquitous in most people's daily lives. And the ongoing evolution of\nmultimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly\nbolstered the capabilities of GUI comprehension and user action analysis,\nshowcasing the potentiality of intelligent GUI assistants. However, current GUI\nAgents often need to access page layout information through calling system\nAPIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a\ncertain low resolution might result in the loss of fine-grained image details.\nAt the same time, the multimodal large models built for GUI Agents currently\nhave poor understanding and decision-making abilities for Chinese GUI\ninterfaces, making them difficult to apply to a large number of Chinese apps.\nThis paper introduces MobileFlow, a multimodal large language model\nmeticulously crafted for mobile GUI agents. Transforming from the open-source\nmodel Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21\nbillion parameters and is equipped with novel hybrid visual encoders, making it\npossible for variable resolutions of image inputs and good support for\nmultilingual GUI. By incorporating Mixture of Experts (MoE) expansions and\npioneering alignment training strategies, MobileFlow has the capacity to fully\ninterpret image data and comprehend user instructions for GUI interaction\ntasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task\nexecution by GUI agents on both public and our proposed evaluation metrics, and\nhas been successfully deployed in real-world business contexts, proving its\neffectiveness for practical applications.\n","authors":["Songqin Nong","Jiali Zhu","Rui Wu","Jiongchao Jin","Shuo Shan","Xiutian Huang","Wenhao Xu"],"pdf_url":"https://arxiv.org/pdf/2407.04346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18403v5","updated":"2024-08-07T03:30:30Z","published":"2023-05-28T15:15:48Z","title":"LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient\n  Fine-Tuning","summary":"  Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional\nperformance across various tasks through fine-tuning. Although low-rank\nadaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream\ntasks, their deployment is still hindered by the vast model scale and\ncomputational costs. Post-training model pruning offers a way to compress LLMs.\nHowever, the current pruning methods designed for LLMs are not compatible with\nLoRA. This is due to their utilization of unstructured pruning on LLMs,\nimpeding the merging of LoRA weights, or their dependence on the gradients of\npre-trained weights to guide pruning, which can impose significant memory\noverhead. To this end, we propose LoRAPrune, a new framework that delivers an\naccurate structured pruned model in a highly memory-efficient manner.\nSpecifically, we first design a LoRA-guided pruning criterion, which uses the\nweights and gradients of LoRA, rather than the gradients of pre-trained weights\nfor importance estimation. We subsequently integrate this criterion into an\niterative pruning process, effectively removing redundant channels and heads.\nExtensive experimental results demonstrate the superior performance of our\nLoRAPrune over existing approaches on the LLaMA series models. At a 50\\%\ncompression rate, LoRAPrune demonstrates superior performance over LLM-Pruner,\nachieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while\nalso decreasing memory usage by 52.6%. Besides, LoRAPrune also matches\nsemi-structural pruning across multiple LLMs, proving its wide applicability.\nThe code is available at https://github.com/aim-uofa/LoRAPrune.\n","authors":["Mingyang Zhang","Hao Chen","Chunhua Shen","Zhen Yang","Linlin Ou","Xinyi Yu","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2305.18403v5.pdf","comment":"accepted by acl 2024 findings"},{"id":"http://arxiv.org/abs/2408.03521v1","updated":"2024-08-07T03:16:33Z","published":"2024-08-07T03:16:33Z","title":"SwinShadow: Shifted Window for Ambiguous Adjacent Shadow Detection","summary":"  Shadow detection is a fundamental and challenging task in many computer\nvision applications. Intuitively, most shadows come from the occlusion of light\nby the object itself, resulting in the object and its shadow being contiguous\n(referred to as the adjacent shadow in this paper). In this case, when the\ncolor of the object is similar to that of the shadow, existing methods struggle\nto achieve accurate detection. To address this problem, we present SwinShadow,\na transformer-based architecture that fully utilizes the powerful shifted\nwindow mechanism for detecting adjacent shadows. The mechanism operates in two\nsteps. Initially, it applies local self-attention within a single window,\nenabling the network to focus on local details. Subsequently, it shifts the\nattention windows to facilitate inter-window attention, enabling the capture of\na broader range of adjacent information. These combined steps significantly\nimprove the network's capacity to distinguish shadows from nearby objects. And\nthe whole process can be divided into three parts: encoder, decoder, and\nfeature integration. During encoding, we adopt Swin Transformer to acquire\nhierarchical features. Then during decoding, for shallow layers, we propose a\ndeep supervision (DS) module to suppress the false positives and boost the\nrepresentation capability of shadow features for subsequent processing, while\nfor deep layers, we leverage a double attention (DA) module to integrate local\nand shifted window in one stage to achieve a larger receptive field and enhance\nthe continuity of information. Ultimately, a new multi-level aggregation (MLA)\nmechanism is applied to fuse the decoded features for mask prediction.\nExtensive experiments on three shadow detection benchmark datasets, SBU, UCF,\nand ISTD, demonstrate that our network achieves good performance in terms of\nbalance error rate (BER).\n","authors":["Yonghui Wang","Shaokai Liu","Li Li","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2408.03521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03516v1","updated":"2024-08-07T02:54:43Z","published":"2024-08-07T02:54:43Z","title":"Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in\n  Autonomous Driving","summary":"  This paper introduces a novel method for open-vocabulary 3D scene\nunderstanding in autonomous driving by combining Language Embedded 3D Gaussians\nwith Large Language Models (LLMs) for enhanced inference. We propose utilizing\nLLMs to generate contextually relevant canonical phrases for segmentation and\nscene interpretation. Our method leverages the contextual and semantic\ncapabilities of LLMs to produce a set of canonical phrases, which are then\ncompared with the language features embedded in the 3D Gaussians. This\nLLM-guided approach significantly improves zero-shot scene understanding and\ndetection of objects of interest, even in the most challenging or unfamiliar\nenvironments. Experimental results on the WayveScenes101 dataset demonstrate\nthat our approach surpasses state-of-the-art methods in terms of accuracy and\nflexibility for open-vocabulary object detection and segmentation. This work\nrepresents a significant advancement towards more intelligent, context-aware\nautonomous driving systems, effectively bridging 3D scene representation with\nhigh-level semantic understanding.\n","authors":["Amirhosein Chahe","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12223v2","updated":"2024-08-07T02:51:50Z","published":"2024-04-06T03:02:47Z","title":"Cascaded Multi-path Shortcut Diffusion Model for Medical Image\n  Translation","summary":"  Image-to-image translation is a vital component in medical imaging\nprocessing, with many uses in a wide range of imaging modalities and clinical\nscenarios. Previous methods include Generative Adversarial Networks (GANs) and\nDiffusion Models (DMs), which offer realism but suffer from instability and\nlack uncertainty estimation. Even though both GAN and DM methods have\nindividually exhibited their capability in medical image translation tasks, the\npotential of combining a GAN and DM to further improve translation performance\nand to enable uncertainty estimation remains largely unexplored. In this work,\nwe address these challenges by proposing a Cascade Multi-path Shortcut\nDiffusion Model (CMDM) for high-quality medical image translation and\nuncertainty estimation. To reduce the required number of iterations and ensure\nrobust performance, our method first obtains a conditional GAN-generated prior\nimage that will be used for the efficient reverse translation with a DM in the\nsubsequent step. Additionally, a multi-path shortcut diffusion strategy is\nemployed to refine translation results and estimate uncertainty. A cascaded\npipeline further enhances translation quality, incorporating residual averaging\nbetween cascades. We collected three different medical image datasets with two\nsub-tasks for each dataset to test the generalizability of our approach. Our\nexperimental results found that CMDM can produce high-quality translations\ncomparable to state-of-the-art methods while providing reasonable uncertainty\nestimations that correlate well with the translation error.\n","authors":["Yinchi Zhou","Tianqi Chen","Jun Hou","Huidong Xie","Nicha C. Dvornek","S. Kevin Zhou","David L. Wilson","James S. Duncan","Chi Liu","Bo Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.12223v2.pdf","comment":"Accepted at Medical Image Analysis Journal"},{"id":"http://arxiv.org/abs/2304.06345v3","updated":"2024-08-07T02:46:46Z","published":"2023-04-13T08:52:34Z","title":"ASR: Attention-alike Structural Re-parameterization","summary":"  The structural re-parameterization (SRP) technique is a novel deep learning\ntechnique that achieves interconversion between different network architectures\nthrough equivalent parameter transformations. This technique enables the\nmitigation of the extra costs for performance improvement during training, such\nas parameter size and inference time, through these transformations during\ninference, and therefore SRP has great potential for industrial and practical\napplications. The existing SRP methods have successfully considered many\ncommonly used architectures, such as normalizations, pooling methods, and\nmulti-branch convolution. However, the widely used attention modules which\ndrastically slow inference speed cannot be directly implemented by SRP due to\nthese modules usually act on the backbone network in a multiplicative manner\nand the modules' output is input-dependent during inference, which limits the\napplication scenarios of SRP. In this paper, we conduct extensive experiments\nfrom a statistical perspective and discover an interesting phenomenon Stripe\nObservation, which reveals that channel attention values quickly approach some\nconstant vectors during training. This observation inspires us to propose a\nsimple-yet-effective attention-alike structural re-parameterization (ASR) that\nallows us to achieve SRP for a given network while enjoying the effectiveness\nof the attention mechanism. Extensive experiments conducted on several standard\nbenchmarks demonstrate the effectiveness of ASR in generally improving the\nperformance of existing backbone networks, attention modules, and SRP methods\nwithout any elaborated model crafting. We also analyze the limitations and\nprovide experimental and theoretical evidence for the strong robustness of the\nproposed ASR.\n","authors":["Shanshan Zhong","Zhongzhan Huang","Wushao Wen","Jinghui Qin","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2304.06345v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2406.16087v4","updated":"2024-08-07T02:36:19Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03867v2","updated":"2024-08-07T02:32:40Z","published":"2023-11-07T10:31:41Z","title":"Supervised domain adaptation for building extraction from off-nadir\n  aerial images","summary":"  Building extraction $-$ needed for inventory management and planning of urban\nenvironment $-$ is affected by the misalignment between labels and off-nadir\nsource imagery in training data. Teacher-Student learning of noise-tolerant\nconvolutional neural networks (CNNs) is the existing solution, but the Student\nnetworks typically have lower accuracy and cannot surpass the Teacher's\nperformance. This paper proposes a supervised domain adaptation (SDA) of\nencoder-decoder networks (EDNs) between noisy and clean datasets to tackle the\nproblem. EDNs are configured with high-performing lightweight encoders such as\nEfficientNet, ResNeSt, and MobileViT. The proposed method is compared against\nthe existing Teacher-Student learning methods like knowledge distillation (KD)\nand deep mutual learning (DML) with three newly developed datasets. The methods\nare evaluated for different urban buildings (low-rise, mid-rise, high-rise, and\nskyscrapers), where misalignment increases with the increase in building height\nand spatial resolution. For a robust experimental design, 43 lightweight CNNs,\nfive optimisers, nine loss functions, and seven EDNs are benchmarked to obtain\nthe best-performing EDN for SDA. The SDA of the best-performing EDN from our\nstudy significantly outperformed KD and DML with up to 0.943, 0.868, 0.912, and\n0.697 F1 scores in the low-rise, mid-rise, high-rise, and skyscrapers\nrespectively. The proposed method and the experimental findings will be\nbeneficial in training robust CNNs for building extraction.\n","authors":["Bipul Neupane","Jagannath Aryal","Abbas Rajabifard"],"pdf_url":"https://arxiv.org/pdf/2311.03867v2.pdf","comment":"This work has been submitted to Elsevier for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03511v1","updated":"2024-08-07T02:28:37Z","published":"2024-08-07T02:28:37Z","title":"MoExtend: Tuning New Experts for Modality and Task Extension","summary":"  Large language models (LLMs) excel in various tasks but are primarily trained\non text data, limiting their application scope. Expanding LLM capabilities to\ninclude vision-language understanding is vital, yet training them on multimodal\ndata from scratch is challenging and costly. Existing instruction tuning\nmethods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs\nvia fully fine-tuning LLMs to bridge the modality gap. However, full\nfine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous\nknowledge, and high training costs particularly in the era of increasing tasks\nand modalities. To solve this issue, we introduce MoExtend, an effective\nframework designed to streamline the modality adaptation and extension of\nMixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts\ninto pre-trained MoE models, endowing them with novel knowledge without the\nneed to tune pretrained models such as MoE and vision encoders. This approach\nenables rapid adaptation and extension to new modal data or tasks, effectively\naddressing the challenge of accommodating new modalities within LLMs.\nFurthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk\nof catastrophic forgetting. Experimental results demonstrate the efficacy and\nefficiency of MoExtend in enhancing the multimodal capabilities of LLMs,\ncontributing to advancements in multimodal AI research. Code:\nhttps://github.com/zhongshsh/MoExtend.\n","authors":["Shanshan Zhong","Shanghua Gao","Zhongzhan Huang","Wushao Wen","Marinka Zitnik","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03511v1.pdf","comment":"ACL 2024 - SRW"},{"id":"http://arxiv.org/abs/2408.03507v1","updated":"2024-08-07T02:18:39Z","published":"2024-08-07T02:18:39Z","title":"GUI Element Detection Using SOTA YOLO Deep Learning Models","summary":"  Detection of Graphical User Interface (GUI) elements is a crucial task for\nautomatic code generation from images and sketches, GUI testing, and GUI\nsearch. Recent studies have leveraged both old-fashioned and modern computer\nvision (CV) techniques. Oldfashioned methods utilize classic image processing\nalgorithms (e.g. edge detection and contour detection) and modern methods use\nmature deep learning solutions for general object detection tasks. GUI element\ndetection, however, is a domain-specific case of object detection, in which\nobjects overlap more often, and are located very close to each other, plus the\nnumber of object classes is considerably lower, yet there are more objects in\nthe images compared to natural images. Hence, the studies that have been\ncarried out on comparing various object detection models, might not apply to\nGUI element detection. In this study, we evaluate the performance of the four\nmost recent successful YOLO models for general object detection tasks on GUI\nelement detection and investigate their accuracy performance in detecting\nvarious GUI elements.\n","authors":["Seyed Shayan Daneshvar","Shaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03503v1","updated":"2024-08-07T02:03:32Z","published":"2024-08-07T02:03:32Z","title":"Opening the Black Box of 3D Reconstruction Error Analysis with VECTOR","summary":"  Reconstruction of 3D scenes from 2D images is a technical challenge that\nimpacts domains from Earth and planetary sciences and space exploration to\naugmented and virtual reality. Typically, reconstruction algorithms first\nidentify common features across images and then minimize reconstruction errors\nafter estimating the shape of the terrain. This bundle adjustment (BA) step\noptimizes around a single, simplifying scalar value that obfuscates many\npossible causes of reconstruction errors (e.g., initial estimate of the\nposition and orientation of the camera, lighting conditions, ease of feature\ndetection in the terrain). Reconstruction errors can lead to inaccurate\nscientific inferences or endanger a spacecraft exploring a remote environment.\nTo address this challenge, we present VECTOR, a visual analysis tool that\nimproves error inspection for stereo reconstruction BA. VECTOR provides\nanalysts with previously unavailable visibility into feature locations, camera\npose, and computed 3D points. VECTOR was developed in partnership with the\nPerseverance Mars Rover and Ingenuity Mars Helicopter terrain reconstruction\nteam at the NASA Jet Propulsion Laboratory. We report on how this tool was used\nto debug and improve terrain reconstruction for the Mars 2020 mission.\n","authors":["Racquel Fygenson","Kazi Jawad","Isabel Li","Francois Ayoub","Robert G. Deen","Scott Davidoff","Dominik Moritz","Mauricio Hess-Flores"],"pdf_url":"https://arxiv.org/pdf/2408.03503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03500v1","updated":"2024-08-07T01:59:25Z","published":"2024-08-07T01:59:25Z","title":"e-Health CSIRO at RRG24: Entropy-Augmented Self-Critical Sequence\n  Training for Radiology Report Generation","summary":"  The Shared Task on Large-Scale Radiology Report Generation (RRG24) aims to\nexpedite the development of assistive systems for interpreting and reporting on\nchest X-ray (CXR) images. This task challenges participants to develop models\nthat generate the findings and impression sections of radiology reports from\nCXRs from a patient's study, using five different datasets. This paper outlines\nthe e-Health CSIRO team's approach, which achieved multiple first-place\nfinishes in RRG24. The core novelty of our approach lies in the addition of\nentropy regularisation to self-critical sequence training, to maintain a higher\nentropy in the token distribution. This prevents overfitting to common phrases\nand ensures a broader exploration of the vocabulary during training, essential\nfor handling the diversity of the radiology reports in the RRG24 datasets. Our\nmodel is available on Hugging Face https://huggingface.co/aehrc/cxrmate-rrg24.\n","authors":["Aaron Nicolson","Jinghui Liu","Jason Dowling","Anthony Nguyen","Bevan Koopman"],"pdf_url":"https://arxiv.org/pdf/2408.03500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03499v1","updated":"2024-08-07T01:50:34Z","published":"2024-08-07T01:50:34Z","title":"FacialPulse: An Efficient RNN-based Depression Detection via Temporal\n  Facial Landmarks","summary":"  Depression is a prevalent mental health disorder that significantly impacts\nindividuals' lives and well-being. Early detection and intervention are crucial\nfor effective treatment and management of depression. Recently, there are many\nend-to-end deep learning methods leveraging the facial expression features for\nautomatic depression detection. However, most current methods overlook the\ntemporal dynamics of facial expressions. Although very recent 3DCNN methods\nremedy this gap, they introduce more computational cost due to the selection of\nCNN-based backbones and redundant facial features.\n  To address the above limitations, by considering the timing correlation of\nfacial expressions, we propose a novel framework called FacialPulse, which\nrecognizes depression with high accuracy and speed. By harnessing the\nbidirectional nature and proficiently addressing long-term dependencies, the\nFacial Motion Modeling Module (FMMM) is designed in FacialPulse to fully\ncapture temporal features. Since the proposed FMMM has parallel processing\ncapabilities and has the gate mechanism to mitigate gradient vanishing, this\nmodule can also significantly boost the training speed.\n  Besides, to effectively use facial landmarks to replace original images to\ndecrease information redundancy, a Facial Landmark Calibration Module (FLCM) is\ndesigned to eliminate facial landmark errors to further improve recognition\naccuracy. Extensive experiments on the AVEC2014 dataset and MMDA dataset (a\ndepression dataset) demonstrate the superiority of FacialPulse on recognition\naccuracy and speed, with the average MAE (Mean Absolute Error) decreased by 21%\ncompared to baselines, and the recognition speed increased by 100% compared to\nstate-of-the-art methods. Codes are released at\nhttps://github.com/volatileee/FacialPulse.\n","authors":["Ruiqi Wang","Jinyang Huang","Jie Zhang","Xin Liu","Xiang Zhang","Zhi Liu","Peng Zhao","Sigui Chen","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00875v3","updated":"2024-08-07T01:50:29Z","published":"2024-04-01T03:10:36Z","title":"DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable\n  Primitive Assembly","summary":"  We present a differentiable rendering framework to learn structured 3D\nabstractions in the form of primitive assemblies from sparse RGB images\ncapturing a 3D object. By leveraging differentiable volume rendering, our\nmethod does not require 3D supervision. Architecturally, our network follows\nthe general pipeline of an image-conditioned neural radiance field (NeRF)\nexemplified by pixelNeRF for color prediction. As our core contribution, we\nintroduce differential primitive assembly (DPA) into NeRF to output a 3D\noccupancy field in place of density prediction, where the predicted occupancies\nserve as opacity values for volume rendering. Our network, coined DPA-Net,\nproduces a union of convexes, each as an intersection of convex quadric\nprimitives, to approximate the target 3D object, subject to an abstraction loss\nand a masking loss, both defined in the image space upon volume rendering. With\ntest-time adaptation and additional sampling and loss designs aimed at\nimproving the accuracy and compactness of the obtained assemblies, our method\ndemonstrates superior performance over state-of-the-art alternatives for 3D\nprimitive abstraction from sparse views.\n","authors":["Fenggen Yu","Yiming Qian","Xu Zhang","Francisca Gil-Ureta","Brian Jackson","Eric Bennett","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.00875v3.pdf","comment":"14 pages, accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2308.15068v4","updated":"2024-08-07T01:04:29Z","published":"2023-08-29T07:00:35Z","title":"A Comprehensive Augmentation Framework for Anomaly Detection","summary":"  Data augmentation methods are commonly integrated into the training of\nanomaly detection models. Previous approaches have primarily focused on\nreplicating real-world anomalies or enhancing diversity, without considering\nthat the standard of anomaly varies across different classes, potentially\nleading to a biased training distribution.This paper analyzes crucial traits of\nsimulated anomalies that contribute to the training of reconstructive networks\nand condenses them into several methods, thus creating a comprehensive\nframework by selectively utilizing appropriate combinations.Furthermore, we\nintegrate this framework with a reconstruction-based approach and concurrently\npropose a split training strategy that alleviates the issue of overfitting\nwhile avoiding introducing interference to the reconstruction process. The\nevaluations conducted on the MVTec anomaly detection dataset demonstrate that\nour method outperforms the previous state-of-the-art approach, particularly in\nterms of object classes. To evaluate generalizability, we generate a simulated\ndataset comprising anomalies with diverse characteristics since the original\ntest samples only include specific types of anomalies and may lead to biased\nevaluations. Experimental results demonstrate that our approach exhibits\npromising potential for generalizing effectively to various unforeseen\nanomalies encountered in real-world scenarios.\n","authors":["Jiang Lin","Yaping Yan"],"pdf_url":"https://arxiv.org/pdf/2308.15068v4.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.03287v2","updated":"2024-08-07T09:07:01Z","published":"2024-08-06T16:35:25Z","title":"Malicious Internet Entity Detection Using Local Graph Inference","summary":"  Detection of malicious behavior in a large network is a challenging problem\nfor machine learning in computer security, since it requires a model with high\nexpressive power and scalable inference. Existing solutions struggle to achieve\nthis feat -- current cybersec-tailored approaches are still limited in\nexpressivity, and methods successful in other domains do not scale well for\nlarge volumes of data, rendering frequent retraining impossible. This work\nproposes a new perspective for learning from graph data that is modeling\nnetwork entity interactions as a large heterogeneous graph. High expressivity\nof the method is achieved with neural network architecture HMILnet that\nnaturally models this type of data and provides theoretical guarantees. The\nscalability is achieved by pursuing local graph inference, i.e., classifying\nindividual vertices and their neighborhood as independent samples. Our\nexperiments exhibit improvement over the state-of-the-art Probabilistic Threat\nPropagation (PTP) algorithm, show a further threefold accuracy improvement when\nadditional data is used, which is not possible with the PTP algorithm, and\ndemonstrate the generalization capabilities of the method to new, previously\nunseen entities.\n","authors":["Simon Mandlik","Tomas Pevny","Vaclav Smidl","Lukas Bajer"],"pdf_url":"https://arxiv.org/pdf/2408.03287v2.pdf","comment":"A preprint. Full publication:\n  https://ieeexplore.ieee.org/document/10418120"},{"id":"http://arxiv.org/abs/2408.03281v2","updated":"2024-08-07T01:00:55Z","published":"2024-08-06T16:28:30Z","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","summary":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","authors":["Boxi Cao","Mengjie Ren","Hongyu Lin","Xianpei Han","Feng Zhang","Junfeng Zhan","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03281v2.pdf","comment":"ACL 2024;Benchmark at https://github.com/c-box/StructEval\n  ;Leaderboard at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"},{"id":"http://arxiv.org/abs/2408.03029v2","updated":"2024-08-07T05:59:46Z","published":"2024-08-06T08:22:16Z","title":"Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning","summary":"  Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to relevant baselines.\n","authors":["Haozhe Ma","Zhengding Luo","Thanh Vinh Vo","Kuankuan Sima","Tze-Yun Leong"],"pdf_url":"https://arxiv.org/pdf/2408.03029v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11652v5","updated":"2024-08-07T05:14:24Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v5.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.02835v2","updated":"2024-08-07T08:09:02Z","published":"2024-08-05T21:12:12Z","title":"Training a multilayer dynamical spintronic network with standard machine\n  learning tools to perform time series classification","summary":"  The ability to process time-series at low energy cost is critical for many\napplications. Recurrent neural network, which can perform such tasks, are\ncomputationally expensive when implementing in software on conventional\ncomputers. Here we propose to implement a recurrent neural network in hardware\nusing spintronic oscillators as dynamical neurons. Using numerical simulations,\nwe build a multi-layer network and demonstrate that we can use backpropagation\nthrough time (BPTT) and standard machine learning tools to train this network.\nLeveraging the transient dynamics of the spintronic oscillators, we solve the\nsequential digits classification task with $89.83\\pm2.91~\\%$ accuracy, as good\nas the equivalent software network. We devise guidelines on how to choose the\ntime constant of the oscillators as well as hyper-parameters of the network to\nadapt to different input time scales.\n","authors":["Erwan Plouet","Ddalo Sanz-Hernndez","Aymeric Vecchiola","Julie Grollier","Frank Mizrahi"],"pdf_url":"https://arxiv.org/pdf/2408.02835v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.03936v1","updated":"2024-08-07T17:54:21Z","published":"2024-08-07T17:54:21Z","title":"SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic\n  Performance for Mercosur Common Nomenclature","summary":"  Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.\n","authors":["Vincius Di Oliveira","Yuri Faanha Bezerra","Li Weigang","Pedro Carvalho Brom","Victor Rafael R. Celestino"],"pdf_url":"https://arxiv.org/pdf/2408.03936v1.pdf","comment":"13 pages, 1 figure, to be publish in International Conference on Web\n  Information Systems and Technologies - WEBIST 2024 proceedings"},{"id":"http://arxiv.org/abs/2310.12294v3","updated":"2024-08-07T17:46:32Z","published":"2023-10-18T19:55:11Z","title":"Open-Set Multivariate Time-Series Anomaly Detection","summary":"  Numerous methods for time-series anomaly detection (TSAD) have emerged in\nrecent years, most of which are unsupervised and assume that only normal\nsamples are available during the training phase, due to the challenge of\nobtaining abnormal data in real-world scenarios. Still, limited samples of\nabnormal data are often available, albeit they are far from representative of\nall possible anomalies. Supervised methods can be utilized to classify normal\nand seen anomalies, but they tend to overfit to the seen anomalies present\nduring training, hence, they fail to generalize to unseen anomalies. We propose\nthe first algorithm to address the open-set TSAD problem, called Multivariate\nOpen-Set Time-Series Anomaly Detector (MOSAD), that leverages only a few shots\nof labeled anomalies during the training phase in order to achieve superior\nanomaly detection performance compared to both supervised and unsupervised TSAD\nalgorithms. MOSAD is a novel multi-head TSAD framework with a shared\nrepresentation space and specialized heads, including the Generative head, the\nDiscriminative head, and the Anomaly-Aware Contrastive head. The latter\nproduces a superior representation space for anomaly detection compared to\nconventional supervised contrastive learning. Extensive experiments on three\nreal-world datasets establish MOSAD as a new state-of-the-art in the TSAD\nfield.\n","authors":["Thomas Lai","Thi Kieu Khanh Ho","Narges Armanfard"],"pdf_url":"https://arxiv.org/pdf/2310.12294v3.pdf","comment":"Accepted to ECAI-2024"},{"id":"http://arxiv.org/abs/2408.03915v1","updated":"2024-08-07T17:20:52Z","published":"2024-08-07T17:20:52Z","title":"Hard to Explain: On the Computational Hardness of In-Distribution Model\n  Interpretation","summary":"  The ability to interpret Machine Learning (ML) models is becoming\nincreasingly essential. However, despite significant progress in the field,\nthere remains a lack of rigorous characterization regarding the innate\ninterpretability of different models. In an attempt to bridge this gap, recent\nwork has demonstrated that it is possible to formally assess interpretability\nby studying the computational complexity of explaining the decisions of various\nmodels. In this setting, if explanations for a particular model can be obtained\nefficiently, the model is considered interpretable (since it can be explained\n``easily''). However, if generating explanations over an ML model is\ncomputationally intractable, it is considered uninterpretable. Prior research\nidentified two key factors that influence the complexity of interpreting an ML\nmodel: (i) the type of the model (e.g., neural networks, decision trees, etc.);\nand (ii) the form of explanation (e.g., contrastive explanations, Shapley\nvalues, etc.). In this work, we claim that a third, important factor must also\nbe considered for this analysis -- the underlying distribution over which the\nexplanation is obtained. Considering the underlying distribution is key in\navoiding explanations that are socially misaligned, i.e., convey information\nthat is biased and unhelpful to users. We demonstrate the significant influence\nof the underlying distribution on the resulting overall interpretation\ncomplexity, in two settings: (i) prediction models paired with an external\nout-of-distribution (OOD) detector; and (ii) prediction models designed to\ninherently generate socially aligned explanations. Our findings prove that the\nexpressiveness of the distribution can significantly influence the overall\ncomplexity of interpretation, and identify essential prerequisites that a model\nmust possess to generate socially aligned explanations.\n","authors":["Guy Amir","Shahaf Bassan","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2408.03915v1.pdf","comment":"To appear in ECAI 2024"},{"id":"http://arxiv.org/abs/2408.03913v1","updated":"2024-08-07T17:19:15Z","published":"2024-08-07T17:19:15Z","title":"AdapMTL: Adaptive Pruning Framework for Multitask Learning Model","summary":"  In the domain of multimedia and multimodal processing, the efficient handling\nof diverse data streams such as images, video, and sensor data is paramount.\nModel compression and multitask learning (MTL) are crucial in this field,\noffering the potential to address the resource-intensive demands of processing\nand interpreting multiple forms of media simultaneously. However, effectively\ncompressing a multitask model presents significant challenges due to the\ncomplexities of balancing sparsity allocation and accuracy performance across\nmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive\npruning framework for MTL models. AdapMTL leverages multiple learnable soft\nthresholds independently assigned to the shared backbone and the task-specific\nheads to capture the nuances in different components' sensitivity to pruning.\nDuring training, it co-optimizes the soft thresholds and MTL model weights to\nautomatically determine the suitable sparsity level at each component to\nachieve both high task accuracy and high overall sparsity. It further\nincorporates an adaptive weighting mechanism that dynamically adjusts the\nimportance of task-specific losses based on each task's robustness to pruning.\nWe demonstrate the effectiveness of AdapMTL through comprehensive experiments\non popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different\narchitectures, showcasing superior performance compared to state-of-the-art\npruning methods.\n","authors":["Mingcan Xiang","Steven Jiaxun Tang","Qizheng Yang","Hui Guan","Tongping Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03913v1.pdf","comment":"13 pages, 9 figures, Published at ACM Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.03909v1","updated":"2024-08-07T17:13:46Z","published":"2024-08-07T17:13:46Z","title":"LaFA: Latent Feature Attacks on Non-negative Matrix Factorization","summary":"  As Machine Learning (ML) applications rapidly grow, concerns about\nadversarial attacks compromising their reliability have gained significant\nattention. One unsupervised ML method known for its resilience to such attacks\nis Non-negative Matrix Factorization (NMF), an algorithm that decomposes input\ndata into lower-dimensional latent features. However, the introduction of\npowerful computational tools such as Pytorch enables the computation of\ngradients of the latent features with respect to the original data, raising\nconcerns about NMF's reliability. Interestingly, naively deriving the\nadversarial loss for NMF as in the case of ML would result in the\nreconstruction loss, which can be shown theoretically to be an ineffective\nattacking objective. In this work, we introduce a novel class of attacks in NMF\ntermed Latent Feature Attacks (LaFA), which aim to manipulate the latent\nfeatures produced by the NMF process. Our method utilizes the Feature Error\n(FE) loss directly on the latent features. By employing FE loss, we generate\nperturbations in the original data that significantly affect the extracted\nlatent features, revealing vulnerabilities akin to those found in other ML\ntechniques. To handle large peak-memory overhead from gradient back-propagation\nin FE attacks, we develop a method based on implicit differentiation which\nenables their scaling to larger datasets. We validate NMF vulnerabilities and\nFE attacks effectiveness through extensive experiments on synthetic and\nreal-world data.\n","authors":["Minh Vu","Ben Nebgen","Erik Skau","Geigh Zollicoffer","Juan Castorena","Kim Rasmussen","Boian Alexandrov","Manish Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2408.03909v1.pdf","comment":"LA-UR-24-26951"},{"id":"http://arxiv.org/abs/2402.14049v2","updated":"2024-08-07T17:09:10Z","published":"2024-02-21T18:25:04Z","title":"Generative Adversarial Models for Extreme Geospatial Downscaling","summary":"  Addressing the challenges of climate change requires accurate and\nhigh-resolution mapping of geospatial data, especially climate and weather\nvariables. However, many existing geospatial datasets, such as the gridded\noutputs of the state-of-the-art numerical climate models (e.g., general\ncirculation models), are only available at very coarse spatial resolutions due\nto the model complexity and extremely high computational demand.\nDeep-learning-based methods, particularly generative adversarial networks\n(GANs) and their variants, have proved effective for refining natural images\nand have shown great promise in improving geospatial datasets. This paper\ndescribes a conditional GAN-based stochastic geospatial downscaling method that\ncan accommodates very high scaling factors. Compared to most existing methods,\nthe method can generate high-resolution accurate climate datasets from very\nlow-resolution inputs. More importantly, the method explicitly considers the\nuncertainty inherent to the downscaling process that tends to be ignored in\nexisting methods. Given an input, the method can produce a multitude of\nplausible high-resolution samples instead of one single deterministic result.\nThese samples allow for an empirical exploration and inferences of model\nuncertainty and robustness. With a case study of gridded climate datasets (wind\nvelocity and solar irradiance), we demonstrate the performances of the\nframework in downscaling tasks with large scaling factors (up to $64\\times$)\nand highlight the advantages of the framework with a comprehensive comparison\nwith commonly used and most recent downscaling methods, including area-to-point\n(ATP) kriging, deep image prior (DIP), enhanced super-resolution generative\nadversarial networks (ESRGAN), physics-informed resolution-enhancing GAN (PhIRE\nGAN), and an efficient diffusion model for remote sensing image\nsuper-resolution (EDiffSR).\n","authors":["Guiye Li","Guofeng Cao"],"pdf_url":"https://arxiv.org/pdf/2402.14049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13218v3","updated":"2024-08-07T16:57:06Z","published":"2024-07-18T07:04:33Z","title":"LiNR: Model Based Neural Retrieval on GPUs at LinkedIn","summary":"  This paper introduces LiNR, LinkedIn's large-scale, GPU-based retrieval\nsystem. LiNR supports a billion-sized index on GPU models. We discuss our\nexperiences and challenges in creating scalable, differentiable search indexes\nusing TensorFlow and PyTorch at production scale. In LiNR, both items and model\nweights are integrated into the model binary. Viewing index construction as a\nform of model training, we describe scaling our system for large indexes,\nincorporating full scans and efficient filtering. A key focus is on enabling\nattribute-based pre-filtering for exhaustive GPU searches, addressing the\ncommon challenge of post-filtering in KNN searches that often reduces system\nquality. We further provide multi-embedding retrieval algorithms and strategies\nfor tackling cold start issues in retrieval. Our advancements in supporting\nlarger indexes through quantization are also discussed. We believe LiNR\nrepresents one of the industry's first Live-updated model-based retrieval\nindexes. Applied to out-of-network post recommendations on LinkedIn Feed, LiNR\nhas contributed to a 3% relative increase in professional daily active users.\nWe envisage LiNR as a step towards integrating retrieval and ranking into a\nsingle GPU model, simplifying complex infrastructures and enabling end-to-end\noptimization of the entire differentiable infrastructure through gradient\ndescent.\n","authors":["Fedor Borisyuk","Qingquan Song","Mingzhou Zhou","Ganesh Parameswaran","Madhu Arun","Siva Popuri","Tugrul Bingol","Zhuotao Pei","Kuang-Hsuan Lee","Lu Zheng","Qizhan Shao","Ali Naqvi","Sen Zhou","Aman Gupta"],"pdf_url":"https://arxiv.org/pdf/2407.13218v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.04262v3","updated":"2024-08-07T16:54:40Z","published":"2023-02-08T18:55:49Z","title":"Algorithmic Collective Action in Machine Learning","summary":"  We initiate a principled study of algorithmic collective action on digital\nplatforms that deploy machine learning algorithms. We propose a simple\ntheoretical model of a collective interacting with a firm's learning algorithm.\nThe collective pools the data of participating individuals and executes an\nalgorithmic strategy by instructing participants how to modify their own data\nto achieve a collective goal. We investigate the consequences of this model in\nthree fundamental learning-theoretic settings: the case of a nonparametric\noptimal learning algorithm, a parametric risk minimizer, and gradient-based\noptimization. In each setting, we come up with coordinated algorithmic\nstrategies and characterize natural success criteria as a function of the\ncollective's size. Complementing our theory, we conduct systematic experiments\non a skill classification task involving tens of thousands of resumes from a\ngig platform for freelancers. Through more than two thousand model training\nruns of a BERT-like language model, we see a striking correspondence emerge\nbetween our empirical observations and the predictions made by our theory.\nTaken together, our theory and experiments broadly support the conclusion that\nalgorithmic collectives of exceedingly small fractional size can exert\nsignificant control over a platform's learning algorithm.\n","authors":["Moritz Hardt","Eric Mazumdar","Celestine Mendler-Dnner","Tijana Zrnic"],"pdf_url":"https://arxiv.org/pdf/2302.04262v3.pdf","comment":"Published at ICML 2023; Revision corrects epsilon-dependence in the\n  analysis"},{"id":"http://arxiv.org/abs/2402.06859v2","updated":"2024-08-07T16:54:23Z","published":"2024-02-10T01:47:10Z","title":"LiRank: Industrial Large Scale Ranking Models at LinkedIn","summary":"  We present LiRank, a large-scale ranking framework at LinkedIn that brings to\nproduction state-of-the-art modeling architectures and optimization methods. We\nunveil several modeling improvements, including Residual DCN, which adds\nattention and residual connections to the famous DCNv2 architecture. We share\ninsights into combining and tuning SOTA architectures to create a unified\nmodel, including Dense Gating, Transformers and Residual DCN. We also propose\nnovel techniques for calibration and describe how we productionalized deep\nlearning based explore/exploit methods. To enable effective, production-grade\nserving of large ranking models, we detail how to train and compress models\nusing quantization and vocabulary compression. We provide details about the\ndeployment setup for large-scale use cases of Feed ranking, Jobs\nRecommendations, and Ads click-through rate (CTR) prediction. We summarize our\nlearnings from various A/B tests by elucidating the most effective technical\napproaches. These ideas have contributed to relative metrics improvements\nacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%\nqualified job applications for Jobs search and recommendations, and +4.3% for\nAds CTR. We hope this work can provide practical insights and solutions for\npractitioners interested in leveraging large-scale deep ranking systems.\n","authors":["Fedor Borisyuk","Mingzhou Zhou","Qingquan Song","Siyu Zhu","Birjodh Tiwana","Ganesh Parameswaran","Siddharth Dangi","Lars Hertel","Qiang Xiao","Xiaochen Hou","Yunbo Ouyang","Aman Gupta","Sheallika Singh","Dan Liu","Hailing Cheng","Lei Le","Jonathan Hung","Sathiya Keerthi","Ruoyan Wang","Fengyu Zhang","Mohit Kothari","Chen Zhu","Daqi Sun","Yun Dai","Xun Luan","Sirou Zhu","Zhiwei Wang","Neil Daftary","Qianqi Shen","Chengming Jiang","Haichao Wei","Maneesh Varshney","Amol Ghoting","Souvik Ghosh"],"pdf_url":"https://arxiv.org/pdf/2402.06859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03877v1","updated":"2024-08-07T16:27:45Z","published":"2024-08-07T16:27:45Z","title":"Knowledge Probing for Graph Representation Learning","summary":"  Graph learning methods have been extensively applied in diverse application\nareas. However, what kind of inherent graph properties e.g. graph proximity,\ngraph structural information has been encoded into graph representation\nlearning for downstream tasks is still under-explored. In this paper, we\npropose a novel graph probing framework (GraphProbe) to investigate and\ninterpret whether the family of graph learning methods has encoded different\nlevels of knowledge in graph representation learning. Based on the intrinsic\nproperties of graphs, we design three probes to systematically investigate the\ngraph representation learning process from different perspectives, respectively\nthe node-wise level, the path-wise level, and the structural level. We\nconstruct a thorough evaluation benchmark with nine representative graph\nlearning methods from random walk based approaches, basic graph neural networks\nand self-supervised graph methods, and probe them on six benchmark datasets for\nnode classification, link prediction and graph classification. The experimental\nevaluation verify that GraphProbe can estimate the capability of graph\nrepresentation learning. Remaking results have been concluded: GCN and\nWeightedGCN methods are relatively versatile methods achieving better results\nwith respect to different tasks.\n","authors":["Mingyu Zhao","Xingyu Huang","Ziyu Lyu","Yanlin Wang","Lixin Cui","Lu Bai"],"pdf_url":"https://arxiv.org/pdf/2408.03877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03872v1","updated":"2024-08-07T16:22:21Z","published":"2024-08-07T16:22:21Z","title":"Inter-Series Transformer: Attending to Products in Time Series\n  Forecasting","summary":"  Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.\n","authors":["Rares Cristian","Pavithra Harsha","Clemente Ocejo","Georgia Perakis","Brian Quanz","Ioannis Spantidakis","Hamza Zerhouni"],"pdf_url":"https://arxiv.org/pdf/2408.03872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08973v2","updated":"2024-08-07T16:21:25Z","published":"2024-04-13T11:40:05Z","title":"PraFFL: A Preference-Aware Scheme in Fair Federated Learning","summary":"  Fairness in federated learning has emerged as a critical concern, aiming to\ndevelop an unbiased model for any special group (e.g., male or female) of\nsensitive features. However, there is a trade-off between model performance and\nfairness, i.e., improving model fairness will decrease model performance.\nExisting approaches have characterized such a trade-off by introducing\nhyperparameters to quantify client's preferences for model fairness and model\nperformance. Nevertheless, these approaches are limited to scenarios where each\nclient has only a single pre-defined preference, and fail to work in practical\nsystems where each client generally have multiple preferences. The key\nchallenge is to design a method that allows the model to adapt to diverse\npreferences of each client in real time. To this end, we propose a\nPreference-aware scheme in Fair Federated Learning paradigm (called PraFFL) to\ngenerate preference-wise model in real time. PraFFL can adaptively adjust the\nmodel based on each client's preferences to meet their needs. We theoretically\nprove that PraFFL can offer the optimal model tailored to an arbitrary\npreference of each client, and show its linear convergence. Experimental\nresults show that our proposed PraFFL outperforms five fair federated learning\nalgorithms in terms of the model's capability of adapting to clients' different\npreferences.\n","authors":["Rongguang Ye","Wei-Bin Kou","Ming Tang"],"pdf_url":"https://arxiv.org/pdf/2404.08973v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.02426v2","updated":"2024-08-07T16:20:56Z","published":"2024-03-04T19:18:53Z","title":"Digital Twins and Civil Engineering Phases: Reorienting Adoption\n  Strategies","summary":"  Digital twin (DT) technology has received immense attention over the years\ndue to the promises it presents to various stakeholders in science and\nengineering. As a result, different thematic areas of DT have been explored.\nThis is no different in specific fields such as manufacturing, automation, oil\nand gas, and civil engineering, leading to fragmented approaches for\nfield-specific applications. The civil engineering industry is further\ndisadvantaged in this regard as it relies on external techniques by other\nengineering fields for its DT adoption. A rising consequence of these\nextensions is a concentrated application of DT to the operations and\nmaintenance phase. On another spectrum, Building Information Modeling (BIM) is\npervasively utilized in the planning/design phase, and the transient nature of\nthe construction phase remains a challenge for its DT adoption. In this paper,\nwe present a phase-based development of DT in the Architecture, Engineering,\nand Construction industry. We commence by presenting succinct expositions on DT\nas a concept and as a service, and establish a five-level scale system.\nFurthermore, we present separately a systematic literature review of the\nconventional techniques employed at each civil engineering phase. In this\nregard, we identified enabling technologies such as computer vision for\nextended sensing and the Internet of Things for reliable integration.\nUltimately, we attempt to reveal DT as an important tool across the entire life\ncycle of civil engineering projects, and nudge researchers to think more\nholistically in their quest for the integration of DT for civil engineering\napplications.\n","authors":["Taiwo A. Adebiyi","Nafeezat A. Ajenifuja","Ruda Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.02426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03865v1","updated":"2024-08-07T16:13:43Z","published":"2024-08-07T16:13:43Z","title":"PackMamba: Efficient Processing of Variable-Length Sequences in Mamba\n  training","summary":"  With the evolution of large language models, traditional Transformer models\nbecome computationally demanding for lengthy sequences due to the quadratic\ngrowth in computation with respect to the sequence length. Mamba, emerging as a\ngroundbreaking architecture in the field of generative AI, demonstrates\nremarkable proficiency in handling elongated sequences with reduced\ncomputational and memory complexity. Nevertheless, the existing training\nframework of Mamba presents inefficiency with variable-length sequence inputs.\nEither single-sequence training results in low GPU utilization, or batched\nprocessing of variable-length sequences to a maximum length incurs considerable\nmemory and computational overhead. To address this problem, we analyze the\nperformance of bottleneck operators in Mamba under diverse tensor shapes and\nproposed PackMamba, a high-throughput Mamba that efficiently handles\nvariable-length sequences. Diving deep into state-space models (SSMs), we\nmodify the parallel operators to avoid passing information between individual\nsequences while maintaining high performance. Experimental results on an NVIDIA\nA100 GPU demonstrate throughput exceeding the baseline single-sequence\nprocessing scheme: 3.06x speedup on the 1.4B model and 2.62x on the 2.8B model.\n","authors":["Haoran Xu","Ziqian Liu","Rong Fu","Zhongling Su","Zerui Wang","Zheng Cai","Zhilin Pei","Xingcheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15636v3","updated":"2024-08-07T16:07:05Z","published":"2024-05-24T15:22:58Z","title":"Visualize and Paint GAN Activations","summary":"  We investigate how generated structures of GANs correlate with their\nactivations in hidden layers, with the purpose of better understanding the\ninner workings of those models and being able to paint structures with\nunconditionally trained GANs. This gives us more control over the generated\nimages, allowing to generate them from a semantic segmentation map while not\nrequiring such a segmentation in the training data. To this end we introduce\nthe concept of tileable features, allowing us to identify activations that work\nwell for painting.\n","authors":["Rudolf Herdt","Peter Maass"],"pdf_url":"https://arxiv.org/pdf/2405.15636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05941v3","updated":"2024-08-07T15:58:17Z","published":"2023-11-10T08:54:51Z","title":"Out-of-Distribution-Aware Electric Vehicle Charging","summary":"  We tackle the challenge of learning to charge Electric Vehicles (EVs) with\nOut-of-Distribution (OOD) data. Traditional scheduling algorithms typically\nfail to balance near-optimal average performance with worst-case guarantees,\nparticularly with OOD data. Model Predictive Control (MPC) is often too\nconservative and data-independent, whereas Reinforcement Learning (RL) tends to\nbe overly aggressive and fully trusts the data, hindering their ability to\nconsistently achieve the best-of-both-worlds. To bridge this gap, we introduce\na novel OOD-aware scheduling algorithm, denoted OOD-Charging. This algorithm\nemploys a dynamic \"awareness radius\", which updates in real-time based on the\nTemporal Difference (TD)-error that reflects the severity of OOD. The\nOOD-Charging algorithm allows for a more effective balance between consistency\nand robustness in EV charging schedules, thereby significantly enhancing\nadaptability and efficiency in real-world charging environments. Our results\ndemonstrate that this approach improves the scheduling reward reliably under\nreal OOD scenarios with remarkable shifts of EV charging behaviors caused by\nCOVID-19 in the Caltech ACN-Data.\n","authors":["Tongxin Li","Chenxi Sun"],"pdf_url":"https://arxiv.org/pdf/2311.05941v3.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.03849v1","updated":"2024-08-07T15:46:45Z","published":"2024-08-07T15:46:45Z","title":"Hate Speech Detection and Classification in Amharic Text with Deep\n  Learning","summary":"  Hate speech is a growing problem on social media. It can seriously impact\nsociety, especially in countries like Ethiopia, where it can trigger conflicts\namong diverse ethnic and religious groups. While hate speech detection in\nresource rich languages are progressing, for low resource languages such as\nAmharic are lacking. To address this gap, we develop Amharic hate speech data\nand SBi-LSTM deep learning model that can detect and classify text into four\ncategories of hate speech: racial, religious, gender, and non-hate speech. We\nhave annotated 5k Amharic social media post and comment data into four\ncategories. The data is annotated using a custom annotation tool by a total of\n100 native Amharic speakers. The model achieves a 94.8 F1-score performance.\nFuture improvements will include expanding the dataset and develop state-of-the\nart models.\n  Keywords: Amharic hate speech detection, classification, Amharic dataset,\nDeep Learning, SBi-LSTM\n","authors":["Samuel Minale Gashe","Seid Muhie Yimam","Yaregal Assabie"],"pdf_url":"https://arxiv.org/pdf/2408.03849v1.pdf","comment":"Dataset: https://data.mendeley.com/datasets/p74pfhz3yx/1"},{"id":"http://arxiv.org/abs/2306.07350v3","updated":"2024-08-07T15:36:15Z","published":"2023-06-12T18:16:33Z","title":"G-invariant diffusion maps","summary":"  The diffusion maps embedding of data lying on a manifold has shown success in\ntasks such as dimensionality reduction, clustering, and data visualization. In\nthis work, we consider embedding data sets that were sampled from a manifold\nwhich is closed under the action of a continuous matrix group. An example of\nsuch a data set is images whose planar rotations are arbitrary. The G-invariant\ngraph Laplacian, introduced in Part I of this work, admits eigenfunctions in\nthe form of tensor products between the elements of the irreducible unitary\nrepresentations of the group and eigenvectors of certain matrices. We employ\nthese eigenfunctions to derive diffusion maps that intrinsically account for\nthe group action on the data. In particular, we construct both equivariant and\ninvariant embeddings, which can be used to cluster and align the data points.\nWe demonstrate the utility of our construction in the problem of random\ncomputerized tomography.\n","authors":["Eitan Rosen","Xiuyuan Cheng","Yoel Shkolnisky"],"pdf_url":"https://arxiv.org/pdf/2306.07350v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03842v1","updated":"2024-08-07T15:35:25Z","published":"2024-08-07T15:35:25Z","title":"Bi-Level Spatial and Channel-aware Transformer for Learned Image\n  Compression","summary":"  Recent advancements in learned image compression (LIC) methods have\ndemonstrated superior performance over traditional hand-crafted codecs. These\nlearning-based methods often employ convolutional neural networks (CNNs) or\nTransformer-based architectures. However, these nonlinear approaches frequently\noverlook the frequency characteristics of images, which limits their\ncompression efficiency. To address this issue, we propose a novel\nTransformer-based image compression method that enhances the transformation\nstage by considering frequency components within the feature map. Our method\nintegrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),\nwhere a spatial-based branch independently handles high and low frequencies at\nthe attention layer, and a Channel-aware Self-Attention (CaSA) module captures\ninformation across channels, significantly improving compression performance.\nAdditionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)\nwithin the Transformer block to enhance the extraction of diverse and rich\ninformation, which is crucial for effective compression. These innovations\ncollectively improve the transformation's ability to project data into a more\ndecorrelated latent space, thereby boosting overall compression efficiency.\nExperimental results demonstrate that our framework surpasses state-of-the-art\nLIC methods in rate-distortion performance.\n","authors":["Hamidreza Soltani","Erfan Ghasemi"],"pdf_url":"https://arxiv.org/pdf/2408.03842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19588v2","updated":"2024-08-07T15:11:01Z","published":"2024-03-28T17:12:39Z","title":"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs","summary":"  This paper revives Densely Connected Convolutional Networks (DenseNets) and\nreveals the underrated effectiveness over predominant ResNet-style\narchitectures. We believe DenseNets' potential was overlooked due to untouched\ntraining methods and traditional design elements not fully revealing their\ncapabilities. Our pilot study shows dense connections through concatenation are\nstrong, demonstrating that DenseNets can be revitalized to compete with modern\narchitectures. We methodically refine suboptimal components - architectural\nadjustments, block redesign, and improved training recipes towards widening\nDenseNets and boosting memory efficiency while keeping concatenation shortcuts.\nOur models, employing simple architectural elements, ultimately surpass Swin\nTransformer, ConvNeXt, and DeiT-III - key architectures in the residual\nlearning lineage. Furthermore, our models exhibit near state-of-the-art\nperformance on ImageNet-1K, competing with the very recent models and\ndownstream tasks, ADE20k semantic segmentation, and COCO object\ndetection/instance segmentation. Finally, we provide empirical analyses that\nuncover the merits of the concatenation over additive shortcuts, steering a\nrenewed preference towards DenseNet-style designs. Our code is available at\nhttps://github.com/naver-ai/rdnet.\n","authors":["Donghyun Kim","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19588v2.pdf","comment":"ECCV 2024. Code at https://github.com/naver-ai/rdnet"},{"id":"http://arxiv.org/abs/2301.04660v2","updated":"2024-08-07T15:07:41Z","published":"2023-01-11T19:00:00Z","title":"Anomalies, Representations, and Self-Supervision","summary":"  We develop a self-supervised method for density-based anomaly detection using\ncontrastive learning, and test it using event-level anomaly data from CMS\nADC2021. The AnomalyCLR technique is data-driven and uses augmentations of the\nbackground data to mimic non-Standard-Model events in a model-agnostic way. It\nuses a permutation-invariant Transformer Encoder architecture to map the\nobjects measured in a collider event to the representation space, where the\ndata augmentations define a representation space which is sensitive to\npotential anomalous features. An AutoEncoder trained on background\nrepresentations then computes anomaly scores for a variety of signals in the\nrepresentation space. With AnomalyCLR we find significant improvements on\nperformance metrics for all signals when compared to the raw data baseline.\n","authors":["Barry M. Dillon","Luigi Favaro","Friedrich Feiden","Tanmoy Modak","Tilman Plehn"],"pdf_url":"https://arxiv.org/pdf/2301.04660v2.pdf","comment":"19 pages, 3 figures, journal version"},{"id":"http://arxiv.org/abs/2408.03819v1","updated":"2024-08-07T14:55:04Z","published":"2024-08-07T14:55:04Z","title":"Leveraging Variation Theory in Counterfactual Data Augmentation for\n  Optimized Active Learning","summary":"  Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL.\n","authors":["Simret Araya Gebreegziabher","Kuangshi Ai","Zheng Zhang","Elena L. Glassman","Toby Jia-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2408.03819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03816v1","updated":"2024-08-07T14:52:06Z","published":"2024-08-07T14:52:06Z","title":"Early Prediction of Causes (not Effects) in Healthcare by Long-Term\n  Clinical Time Series Forecasting","summary":"  Machine learning for early syndrome diagnosis aims to solve the intricate\ntask of predicting a ground truth label that most often is the outcome (effect)\nof a medical consensus definition applied to observed clinical measurements\n(causes), given clinical measurements observed several hours before. Instead of\nfocusing on the prediction of the future effect, we propose to directly predict\nthe causes via time series forecasting (TSF) of clinical variables and\ndetermine the effect by applying the gold standard consensus definition to the\nforecasted values. This method has the invaluable advantage of being\nstraightforwardly interpretable to clinical practitioners, and because model\ntraining does not rely on a particular label anymore, the forecasted data can\nbe used to predict any consensus-based label. We exemplify our method by means\nof long-term TSF with Transformer models, with a focus on accurate prediction\nof sparse clinical variables involved in the SOFA-based Sepsis-3 definition and\nthe new Simplified Acute Physiology Score (SAPS-II) definition. Our experiments\nare conducted on two datasets and show that contrary to recent proposals which\nadvocate set function encoders for time series and direct multi-step decoders,\nbest results are achieved by a combination of standard dense encoders with\niterative multi-step decoders. The key for success of iterative multi-step\ndecoding can be attributed to its ability to capture cross-variate dependencies\nand to a student forcing training strategy that teaches the model to rely on\nits own previous time step predictions for the next time step prediction.\n","authors":["Michael Staniek","Marius Fracarolli","Michael Hagmann","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2408.03816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03806v1","updated":"2024-08-07T14:32:36Z","published":"2024-08-07T14:32:36Z","title":"Trustworthy Image Semantic Communication with GenAI: Explainablity,\n  Controllability, and Efficiency","summary":"  Image semantic communication (ISC) has garnered significant attention for its\npotential to achieve high efficiency in visual content transmission. However,\nexisting ISC systems based on joint source-channel coding face challenges in\ninterpretability, operability, and compatibility. To address these limitations,\nwe propose a novel trustworthy ISC framework. This approach leverages text\nextraction and segmentation mapping techniques to convert images into\nexplainable semantics, while employing Generative Artificial Intelligence\n(GenAI) for multiple downstream inference tasks. We also introduce a multi-rate\nISC transmission protocol that dynamically adapts to both the received\nexplainable semantic content and specific task requirements at the receiver.\nSimulation results demonstrate that our framework achieves explainable\nlearning, decoupled training, and compatible transmission in various\napplication scenarios. Finally, some intriguing research directions and\napplication scenarios are identified.\n","authors":["Xijun Wang","Dongshan Ye","Chenyuan Feng","Howard H. Yang","Xiang Chen","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2408.03806v1.pdf","comment":"8 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.11075v2","updated":"2024-08-07T14:05:28Z","published":"2024-07-13T04:29:36Z","title":"A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)","summary":"  Through this comprehensive survey of Kolmogorov-Arnold Networks(KAN), we have\ngained a thorough understanding of its theoretical foundation, architectural\ndesign, application scenarios, and current research progress. KAN, with its\nunique architecture and flexible activation functions, excels in handling\ncomplex data patterns and nonlinear relationships, demonstrating wide-ranging\napplication potential. While challenges remain, KAN is poised to pave the way\nfor innovative solutions in various fields, potentially revolutionizing how we\napproach complex computational problems.\n","authors":["Yuntian Hou","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.11075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15793v2","updated":"2024-08-07T13:59:46Z","published":"2024-07-22T16:51:28Z","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning","summary":"  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Emanuele Frascaroli","Aniello Panariello","Pietro Buzzega","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.15793v2.pdf","comment":"15 pages, 1 figure. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2404.09657v3","updated":"2024-08-07T13:44:01Z","published":"2024-04-15T10:45:12Z","title":"Sampling for Model Predictive Trajectory Planning in Autonomous Driving\n  using Normalizing Flows","summary":"  Alongside optimization-based planners, sampling-based approaches are often\nused in trajectory planning for autonomous driving due to their simplicity.\nModel predictive path integral control is a framework that builds upon\noptimization principles while incorporating stochastic sampling of input\ntrajectories. This paper investigates several sampling approaches for\ntrajectory generation. In this context, normalizing flows originating from the\nfield of variational inference are considered for the generation of sampling\ndistributions, as they model transformations of simple to more complex\ndistributions. Accordingly, learning-based normalizing flow models are trained\nfor a more efficient exploration of the input domain for the task at hand. The\ndeveloped algorithm and the proposed sampling distributions are evaluated in\ntwo simulation scenarios.\n","authors":["Georg Rabenstein","Lars Ullrich","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.09657v3.pdf","comment":"Accepted to be published as part of the 2024 IEEE Intelligent\n  Vehicles Symposium (IV), Jeju Shinhwa World, Jeju Island, Korea, June 2-5,\n  2024"},{"id":"http://arxiv.org/abs/2408.03765v1","updated":"2024-08-07T13:36:03Z","published":"2024-08-07T13:36:03Z","title":"Reliable Node Similarity Matrix Guided Contrastive Graph Clustering","summary":"  Graph clustering, which involves the partitioning of nodes within a graph\ninto disjoint clusters, holds significant importance for numerous subsequent\napplications. Recently, contrastive learning, known for utilizing supervisory\ninformation, has demonstrated encouraging results in deep graph clustering.\nThis methodology facilitates the learning of favorable node representations for\nclustering by attracting positively correlated node pairs and distancing\nnegatively correlated pairs within the representation space. Nevertheless, a\nsignificant limitation of existing methods is their inadequacy in thoroughly\nexploring node-wise similarity. For instance, some hypothesize that the node\nsimilarity matrix within the representation space is identical, ignoring the\ninherent semantic relationships among nodes. Given the fundamental role of\ninstance similarity in clustering, our research investigates contrastive graph\nclustering from the perspective of the node similarity matrix. We argue that an\nideal node similarity matrix within the representation space should accurately\nreflect the inherent semantic relationships among nodes, ensuring the\npreservation of semantic similarities in the learned representations. In\nresponse to this, we introduce a new framework, Reliable Node Similarity Matrix\nGuided Contrastive Graph Clustering (NS4GC), which estimates an approximately\nideal node similarity matrix within the representation space to guide\nrepresentation learning. Our method introduces node-neighbor alignment and\nsemantic-aware sparsification, ensuring the node similarity matrix is both\naccurate and efficiently sparse. Comprehensive experiments conducted on $8$\nreal-world datasets affirm the efficacy of learning the node similarity matrix\nand the superior performance of NS4GC.\n","authors":["Yunhui Liu","Xinyi Gao","Tieke He","Tao Zheng","Jianhua Zhao","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2408.03765v1.pdf","comment":"Accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)"},{"id":"http://arxiv.org/abs/2408.01953v2","updated":"2024-08-07T13:24:38Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v2.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2207.03927v2","updated":"2024-08-07T13:15:55Z","published":"2022-07-08T14:27:52Z","title":"BAST: Binaural Audio Spectrogram Transformer for Binaural Sound\n  Localization","summary":"  Accurate sound localization in a reverberation environment is essential for\nhuman auditory perception. Recently, Convolutional Neural Networks (CNNs) have\nbeen utilized to model the binaural human auditory pathway. However, CNN shows\nbarriers in capturing the global acoustic features. To address this issue, we\npropose a novel end-to-end Binaural Audio Spectrogram Transformer (BAST) model\nto predict the sound azimuth in both anechoic and reverberation environments.\nTwo modes of implementation, i.e. BAST-SP and BAST-NSP corresponding to BAST\nmodel with shared and non-shared parameters respectively, are explored. Our\nmodel with subtraction interaural integration and hybrid loss achieves an\nangular distance of 1.29 degrees and a Mean Square Error of 1e-3 at all\nazimuths, significantly surpassing CNN based model. The exploratory analysis of\nthe BAST's performance on the left-right hemifields and anechoic and\nreverberation environments shows its generalization ability as well as the\nfeasibility of binaural Transformers in sound localization. Furthermore, the\nanalysis of the attention maps is provided to give additional insights on the\ninterpretation of the localization process in a natural reverberant\nenvironment.\n","authors":["Sheng Kuang","Jie Shi","Kiki van der Heijden","Siamak Mehrkanoon"],"pdf_url":"https://arxiv.org/pdf/2207.03927v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.07118v2","updated":"2024-08-07T13:14:00Z","published":"2024-02-11T07:27:01Z","title":"Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding\n  Remote Smartphone-based Consultation","summary":"  Blindness and other eye diseases are a global health concern, particularly in\nlow- and middle-income countries like India. In this regard, during the\nCOVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi\nattachment for smartphone-based eye imaging gained in use. However, quality of\nuser-captured image often remained inadequate, requiring clinician vetting and\ndelays. In this backdrop, we propose an AI-based quality assessment system with\ninstant feedback mimicking clinicians' judgments and tested on patient-captured\nimages. Dividing the complex problem hierarchically, here we tackle a\nnontrivial part, and demonstrate a proof of the concept.\n","authors":["Dhruv Srikanth","Jayang Gurung","N Satya Deepika","Vineet Joshi","Lopamudra Giri","Pravin Vaddavalli","Soumya Jana"],"pdf_url":"https://arxiv.org/pdf/2402.07118v2.pdf","comment":"4 pages, Presented at IEEE EMBC 2024"},{"id":"http://arxiv.org/abs/2404.15213v2","updated":"2024-08-07T13:11:14Z","published":"2024-03-28T10:15:10Z","title":"Automatic Classification of Subjective Time Perception Using Multi-modal\n  Physiological Data of Air Traffic Controllers","summary":"  In high-pressure environments where human individuals must simultaneously\nmonitor multiple entities, communicate effectively, and maintain intense focus,\nthe perception of time becomes a critical factor influencing performance and\nwell-being. One indicator of well-being can be the person's subjective time\nperception. In our project $ChronoPilot$, we aim to develop a device that\nmodulates human subjective time perception. In this study, we present a method\nto automatically assess the subjective time perception of air traffic\ncontrollers, a group often faced with demanding conditions, using their\nphysiological data and eleven state-of-the-art machine learning classifiers.\nThe physiological data consist of photoplethysmogram, electrodermal activity,\nand temperature data. We find that the support vector classifier works best\nwith an accuracy of 79 % and electrodermal activity provides the most\ndescriptive biomarker. These findings are an important step towards closing the\nfeedback loop of our $ChronoPilot$-device to automatically modulate the user's\nsubjective time perception. This technological advancement may promise\nimprovements in task management, stress reduction, and overall productivity in\nhigh-stakes professions.\n","authors":["Till Aust","Eirini Balta","Argiro Vatakis","Heiko Hamann"],"pdf_url":"https://arxiv.org/pdf/2404.15213v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03747v1","updated":"2024-08-07T13:01:10Z","published":"2024-08-07T13:01:10Z","title":"Online Model-based Anomaly Detection in Multivariate Time Series:\n  Taxonomy, Survey, Research Challenges and Future Directions","summary":"  Time-series anomaly detection plays an important role in engineering\nprocesses, like development, manufacturing and other operations involving\ndynamic systems. These processes can greatly benefit from advances in the\nfield, as state-of-the-art approaches may aid in cases involving, for example,\nhighly dimensional data. To provide the reader with understanding of the\nterminology, this survey introduces a novel taxonomy where a distinction\nbetween online and offline, and training and inference is made. Additionally,\nit presents the most popular data sets and evaluation metrics used in the\nliterature, as well as a detailed analysis. Furthermore, this survey provides\nan extensive overview of the state-of-the-art model-based online semi- and\nunsupervised anomaly detection approaches for multivariate time-series data,\ncategorising them into different model families and other properties. The\nbiggest research challenge revolves around benchmarking, as currently there is\nno reliable way to compare different approaches against one another. This\nproblem is two-fold: on the one hand, public data sets suffers from at least\none fundamental flaw, while on the other hand, there is a lack of intuitive and\nrepresentative evaluation metrics in the field. Moreover, the way most\npublications choose a detection threshold disregards real-world conditions,\nwhich hinders the application in the real world. To allow for tangible advances\nin the field, these issues must be addressed in future work.\n","authors":["Lucas Correia","Jan-Christoph Goos","Philipp Klein","Thomas Bck","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2408.03747v1.pdf","comment":"Submitted to Engineering Applications of Artificial Intelligence\n  journal"},{"id":"http://arxiv.org/abs/2408.03746v1","updated":"2024-08-07T12:59:58Z","published":"2024-08-07T12:59:58Z","title":"Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion\n  Posterior Sampling","summary":"  Bayesian Last Layer (BLL) models focus solely on uncertainty in the output\nlayer of neural networks, demonstrating comparable performance to more complex\nBayesian models. However, the use of Gaussian priors for last layer weights in\nBayesian Last Layer (BLL) models limits their expressive capacity when faced\nwith non-Gaussian, outlier-rich, or high-dimensional datasets. To address this\nshortfall, we introduce a novel approach that combines diffusion techniques and\nimplicit priors for variational learning of Bayesian last layer weights. This\nmethod leverages implicit distributions for modeling weight priors in BLL,\ncoupled with diffusion samplers for approximating true posterior predictions,\nthereby establishing a comprehensive Bayesian prior and posterior estimation\nstrategy. By delivering an explicit and computationally efficient variational\nlower bound, our method aims to augment the expressive abilities of BLL models,\nenhancing model accuracy, calibration, and out-of-distribution detection\nproficiency. Through detailed exploration and experimental validation, We\nshowcase the method's potential for improving predictive accuracy and\nuncertainty quantification while ensuring computational efficiency.\n","authors":["Jian Xu","Zhiqi Lin","Shigui Li","Min Chen","Junmei Yang","Delu Zeng","John Paisley"],"pdf_url":"https://arxiv.org/pdf/2408.03746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18381v4","updated":"2024-08-07T12:59:31Z","published":"2023-05-28T06:53:41Z","title":"Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation","summary":"  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n","authors":["Yue Xu","Yong-Lu Li","Kaitong Cui","Ziyu Wang","Cewu Lu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2305.18381v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2307.12754v4","updated":"2024-08-07T12:51:46Z","published":"2023-07-24T12:52:55Z","title":"Nonparametric Linear Feature Learning in Regression Through\n  Regularisation","summary":"  Representation learning plays a crucial role in automated feature selection,\nparticularly in the context of high-dimensional data, where non-parametric\nmethods often struggle. In this study, we focus on supervised learning\nscenarios where the pertinent information resides within a lower-dimensional\nlinear subspace of the data, namely the multi-index model. If this subspace\nwere known, it would greatly enhance prediction, computation, and\ninterpretation. To address this challenge, we propose a novel method for joint\nlinear feature learning and non-parametric function estimation, aimed at more\neffectively leveraging hidden features for learning. Our approach employs\nempirical risk minimisation, augmented with a penalty on function derivatives,\nensuring versatility. Leveraging the orthogonality and rotation invariance\nproperties of Hermite polynomials, we introduce our estimator, named RegFeaL.\nBy using alternative minimisation, we iteratively rotate the data to improve\nalignment with leading directions. We establish that the expected risk of our\nmethod converges in high-probability to the minimal risk under minimal\nassumptions and with explicit rates. Additionally, we provide empirical results\ndemonstrating the performance of RegFeaL in various experiments.\n","authors":["Bertille Follain","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2307.12754v4.pdf","comment":"45 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.03735v1","updated":"2024-08-07T12:42:09Z","published":"2024-08-07T12:42:09Z","title":"Advancing Multimodal Large Language Models with Quantization-Aware Scale\n  Learning for Efficient Adaptation","summary":"  This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.\n","authors":["Jingjing Xie","Yuxin Zhang","Mingbao Lin","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03735v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.03733v1","updated":"2024-08-07T12:41:56Z","published":"2024-08-07T12:41:56Z","title":"Bayes-optimal learning of an extensive-width neural network from\n  quadratically many samples","summary":"  We consider the problem of learning a target function corresponding to a\nsingle hidden layer neural network, with a quadratic activation function after\nthe first layer, and random weights. We consider the asymptotic limit where the\ninput dimension and the network width are proportionally large. Recent work\n[Cui & al '23] established that linear regression provides Bayes-optimal test\nerror to learn such a function when the number of available samples is only\nlinear in the dimension. That work stressed the open challenge of theoretically\nanalyzing the optimal test error in the more interesting regime where the\nnumber of samples is quadratic in the dimension. In this paper, we solve this\nchallenge for quadratic activations and derive a closed-form expression for the\nBayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE,\nwhich combines approximate message passing with rotationally invariant matrix\ndenoising, and that asymptotically achieves the optimal performance.\nTechnically, our result is enabled by establishing a link with recent works on\noptimal denoising of extensive-rank matrices and on the ellipsoid fitting\nproblem. We further show empirically that, in the absence of noise,\nrandomly-initialized gradient descent seems to sample the space of weights,\nleading to zero training loss, and averaging over initialization leads to a\ntest error equal to the Bayes-optimal one.\n","authors":["Antoine Maillard","Emanuele Troiani","Simon Martin","Florent Krzakala","Lenka Zdeborov"],"pdf_url":"https://arxiv.org/pdf/2408.03733v1.pdf","comment":"47 pages"},{"id":"http://arxiv.org/abs/2408.03732v1","updated":"2024-08-07T12:38:23Z","published":"2024-08-07T12:38:23Z","title":"Question Rephrasing for Quantifying Uncertainty in Large Language\n  Models: Applications in Molecular Chemistry Tasks","summary":"  Uncertainty quantification enables users to assess the reliability of\nresponses generated by large language models (LLMs). We present a novel\nQuestion Rephrasing technique to evaluate the input uncertainty of LLMs, which\nrefers to the uncertainty arising from equivalent variations of the inputs\nprovided to LLMs. This technique is integrated with sampling methods that\nmeasure the output uncertainty of LLMs, thereby offering a more comprehensive\nuncertainty assessment. We validated our approach on property prediction and\nreaction prediction for molecular chemistry tasks.\n","authors":["Zizhang Chen","Pengyu Hong","Sandeep Madireddy"],"pdf_url":"https://arxiv.org/pdf/2408.03732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03728v1","updated":"2024-08-07T12:33:46Z","published":"2024-08-07T12:33:46Z","title":"A Convex-optimization-based Layer-wise Post-training Pruner for Large\n  Language Models","summary":"  Pruning is a critical strategy for compressing trained large language models\n(LLMs), aiming at substantial memory conservation and computational\nacceleration without compromising performance. However, existing pruning\nmethods often necessitate inefficient retraining for billion-scale LLMs or rely\non heuristic methods such as the optimal brain surgeon framework, which degrade\nperformance. In this paper, we introduce FISTAPruner, the first post-training\npruner based on convex optimization models and algorithms. Specifically, we\npropose a convex optimization model incorporating $\\ell_1$ norm to induce\nsparsity and utilize the FISTA solver for optimization. FISTAPruner\nincorporates an intra-layer cumulative error correction mechanism and supports\nparallel pruning. We comprehensively evaluate FISTAPruner on models such as\nOPT, LLaMA, LLaMA-2, and LLaMA-3 with 125M to 70B parameters under unstructured\nand 2:4 semi-structured sparsity, demonstrating superior performance over\nexisting state-of-the-art methods across various language benchmarks.\n","authors":["Pengxiang Zhao","Hanyu Hu","Ping Li","Yi Zheng","Zhefeng Wang","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.03728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03706v1","updated":"2024-08-07T11:44:32Z","published":"2024-08-07T11:44:32Z","title":"Local Topology Measures of Contextual Language Model Latent Spaces With\n  Applications to Dialogue Term Extraction","summary":"  A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.\n","authors":["Benjamin Matthias Ruppik","Michael Heck","Carel van Niekerk","Renato Vukovic","Hsien-chin Lin","Shutong Feng","Marcus Zibrowius","Milica Gai"],"pdf_url":"https://arxiv.org/pdf/2408.03706v1.pdf","comment":"Accepted as a long paper to SIGDIAL 2024. 9 pages, 2 figures, 3\n  tables"},{"id":"http://arxiv.org/abs/2312.10271v2","updated":"2024-08-07T11:32:19Z","published":"2023-12-16T00:23:21Z","title":"Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse\n  Training Data","summary":"  Deep learning based methods for image reconstruction are state-of-the-art for\na variety of imaging tasks. However, neural networks often perform worse if the\ntraining data differs significantly from the data they are applied to. For\nexample, a model trained for accelerated magnetic resonance imaging (MRI) on\none scanner performs worse on another scanner. In this work, we investigate the\nimpact of the training data on a model's performance and robustness for\naccelerated MRI. We find that models trained on the combination of various data\ndistributions, such as those obtained from different MRI scanners and\nanatomies, exhibit robustness equal or superior to models trained on the best\nsingle distribution for a specific target distribution. Thus training on such\ndiverse data tends to improve robustness. Furthermore, training on such a\ndiverse dataset does not compromise in-distribution performance, i.e., a model\ntrained on diverse data yields in-distribution performance at least as good as\nmodels trained on the more narrow individual distributions. Our results suggest\nthat training a model for imaging on a variety of distributions tends to yield\na more effective and robust model than maintaining separate models for\nindividual distributions.\n","authors":["Kang Lin","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2312.10271v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.03694v1","updated":"2024-08-07T11:14:18Z","published":"2024-08-07T11:14:18Z","title":"A Blockchain-based Reliable Federated Meta-learning for Metaverse: A\n  Dual Game Framework","summary":"  The metaverse, envisioned as the next digital frontier for avatar-based\nvirtual interaction, involves high-performance models. In this dynamic\nenvironment, users' tasks frequently shift, requiring fast model\npersonalization despite limited data. This evolution consumes extensive\nresources and requires vast data volumes. To address this, meta-learning\nemerges as an invaluable tool for metaverse users, with federated meta-learning\n(FML), offering even more tailored solutions owing to its adaptive\ncapabilities. However, the metaverse is characterized by users heterogeneity\nwith diverse data structures, varied tasks, and uneven sample sizes,\npotentially undermining global training outcomes due to statistical difference.\nGiven this, an urgent need arises for smart coalition formation that accounts\nfor these disparities. This paper introduces a dual game-theoretic framework\nfor metaverse services involving meta-learners as workers to manage FML. A\nblockchain-based cooperative coalition formation game is crafted, grounded on a\nreputation metric, user similarity, and incentives. We also introduce a novel\nreputation system based on users' historical contributions and potential\ncontributions to present tasks, leveraging correlations between past and new\ntasks. Finally, a Stackelberg game-based incentive mechanism is presented to\nattract reliable workers to participate in meta-learning, minimizing users'\nenergy costs, increasing payoffs, boosting FML efficacy, and improving\nmetaverse utility. Results show that our dual game framework outperforms\nbest-effort, random, and non-uniform clustering schemes - improving training\nperformance by up to 10%, cutting completion times by as much as 30%, enhancing\nmetaverse utility by more than 25%, and offering up to 5% boost in training\nefficiency over non-blockchain systems, effectively countering misbehaving\nusers.\n","authors":["Emna Baccour","Aiman Erbad","Amr Mohamed","Mounir Hamdi","Mohsen Guizani"],"pdf_url":"https://arxiv.org/pdf/2408.03694v1.pdf","comment":"Accepted in IEEE Internet of Things Journal"},{"id":"http://arxiv.org/abs/2408.03691v1","updated":"2024-08-07T11:13:19Z","published":"2024-08-07T11:13:19Z","title":"Generative Design of Periodic Orbits in the Restricted Three-Body\n  Problem","summary":"  The Three-Body Problem has fascinated scientists for centuries and it has\nbeen crucial in the design of modern space missions. Recent developments in\nGenerative Artificial Intelligence hold transformative promise for addressing\nthis longstanding problem. This work investigates the use of Variational\nAutoencoder (VAE) and its internal representation to generate periodic orbits.\nWe utilize a comprehensive dataset of periodic orbits in the Circular\nRestricted Three-Body Problem (CR3BP) to train deep-learning architectures that\ncapture key orbital characteristics, and we set up physical evaluation metrics\nfor the generated trajectories. Through this investigation, we seek to enhance\nthe understanding of how Generative AI can improve space mission planning and\nastrodynamics research, leading to novel, data-driven approaches in the field.\n","authors":["Alvaro Francisco Gil","Walther Litteri","Victor Rodriguez-Fernandez","David Camacho","Massimiliano Vasile"],"pdf_url":"https://arxiv.org/pdf/2408.03691v1.pdf","comment":"SPAICE Conference 2024 (7 pages)"},{"id":"http://arxiv.org/abs/2408.03669v1","updated":"2024-08-07T10:24:59Z","published":"2024-08-07T10:24:59Z","title":"Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep\n  Graph Neural Networks","summary":"  The drastic performance degradation of Graph Neural Networks (GNNs) as the\ndepth of the graph propagation layers exceeds 8-10 is widely attributed to a\nphenomenon of Over-smoothing. Although recent research suggests that\nOver-smoothing may not be the dominant reason for such a performance\ndegradation, they have not provided rigorous analysis from a theoretical view,\nwhich warrants further investigation. In this paper, we systematically analyze\nthe real dominant problem in deep GNNs and identify the issues that these GNNs\ntowards addressing Over-smoothing essentially work on via empirical experiments\nand theoretical gradient analysis. We theoretically prove that the difficult\ntraining problem of deep MLPs is actually the main challenge, and various\nexisting methods that supposedly tackle Over-smoothing actually improve the\ntrainability of MLPs, which is the main reason for their performance gains. Our\nfurther investigation into trainability issues reveals that properly\nconstrained smaller upper bounds of gradient flow notably enhance the\ntrainability of GNNs. Experimental results on diverse datasets demonstrate\nconsistency between our theoretical findings and empirical evidence. Our\nanalysis provides new insights in constructing deep graph models.\n","authors":["Jie Peng","Runlin Lei","Zhewei Wei"],"pdf_url":"https://arxiv.org/pdf/2408.03669v1.pdf","comment":"CIKM2024"},{"id":"http://arxiv.org/abs/2403.04202v5","updated":"2024-08-07T10:10:34Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v5.pdf","comment":"Accepted at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA)"},{"id":"http://arxiv.org/abs/2408.03664v1","updated":"2024-08-07T10:06:04Z","published":"2024-08-07T10:06:04Z","title":"AI-Driven approach for sustainable extraction of earth's subsurface\n  renewable energy while minimizing seismic activity","summary":"  Deep Geothermal Energy, Carbon Capture and Storage, and Hydrogen Storage hold\nconsiderable promise for meeting the energy sector's large-scale requirements\nand reducing CO$_2$ emissions. However, the injection of fluids into the\nEarth's crust, essential for these activities, can induce or trigger\nearthquakes. In this paper, we highlight a new approach based on Reinforcement\nLearning for the control of human-induced seismicity in the highly complex\nenvironment of an underground reservoir. This complex system poses significant\nchallenges in the control design due to parameter uncertainties and unmodeled\ndynamics. We show that the reinforcement learning algorithm can interact\nefficiently with a robust controller, by choosing the controller parameters in\nreal-time, reducing human-induced seismicity and allowing the consideration of\nfurther production objectives, \\textit{e.g.}, minimal control power.\nSimulations are presented for a simplified underground reservoir under various\nenergy demand scenarios, demonstrating the reliability and effectiveness of the\nproposed control-reinforcement learning approach.\n","authors":["Diego Gutierrez-Oribio","Alexandros Stathas","Ioannis Stefanou"],"pdf_url":"https://arxiv.org/pdf/2408.03664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18613v2","updated":"2024-08-07T09:46:49Z","published":"2024-03-27T14:28:44Z","title":"Scalable Lipschitz Estimation for CNNs","summary":"  Estimating the Lipschitz constant of deep neural networks is of growing\ninterest as it is useful for informing on generalisability and adversarial\nrobustness. Convolutional neural networks (CNNs) in particular, underpin much\nof the recent success in computer vision related applications. However,\nalthough existing methods for estimating the Lipschitz constant can be tight,\nthey have limited scalability when applied to CNNs. To tackle this, we propose\na novel method to accelerate Lipschitz constant estimation for CNNs. The core\nidea is to divide a large convolutional block via a joint layer and width-wise\npartition, into a collection of smaller blocks. We prove an upper-bound on the\nLipschitz constant of the larger block in terms of the Lipschitz constants of\nthe smaller blocks. Through varying the partition factor, the resulting method\ncan be adjusted to prioritise either accuracy or scalability and permits\nparallelisation. We demonstrate an enhanced scalability and comparable accuracy\nto existing baselines through a range of experiments.\n","authors":["Yusuf Sulehman","Tingting Mu"],"pdf_url":"https://arxiv.org/pdf/2403.18613v2.pdf","comment":"An inconsistency between the input of the flattened convolutional\n  block and the flattened, partitioned input impacts the validity of the\n  proposed Lipschitz bound"},{"id":"http://arxiv.org/abs/2408.03655v1","updated":"2024-08-07T09:45:24Z","published":"2024-08-07T09:45:24Z","title":"Consumer Transactions Simulation through Generative Adversarial Networks","summary":"  In the rapidly evolving domain of large-scale retail data systems,\nenvisioning and simulating future consumer transactions has become a crucial\narea of interest. It offers significant potential to fortify demand forecasting\nand fine-tune inventory management. This paper presents an innovative\napplication of Generative Adversarial Networks (GANs) to generate synthetic\nretail transaction data, specifically focusing on a novel system architecture\nthat combines consumer behavior modeling with stock-keeping unit (SKU)\navailability constraints to address real-world assortment optimization\nchallenges. We diverge from conventional methodologies by integrating SKU data\ninto our GAN architecture and using more sophisticated embedding methods (e.g.,\nhyper-graphs). This design choice enables our system to generate not only\nsimulated consumer purchase behaviors but also reflects the dynamic interplay\nbetween consumer behavior and SKU availability -- an aspect often overlooked,\namong others, because of data scarcity in legacy retail simulation models. Our\nGAN model generates transactions under stock constraints, pioneering a\nresourceful experimental system with practical implications for real-world\nretail operation and strategy. Preliminary results demonstrate enhanced realism\nin simulated transactions measured by comparing generated items with real ones\nusing methods employed earlier in related studies. This underscores the\npotential for more accurate predictive modeling.\n","authors":["Sergiy Tkachuk","Szymon ukasik","Anna Wrblewska"],"pdf_url":"https://arxiv.org/pdf/2408.03655v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.03652v1","updated":"2024-08-07T09:34:55Z","published":"2024-08-07T09:34:55Z","title":"mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search","summary":"  Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.\n","authors":["Ahmed Abdou","Tasneem Mohsen"],"pdf_url":"https://arxiv.org/pdf/2408.03652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.03703v2","updated":"2024-08-07T09:18:40Z","published":"2022-08-07T12:02:48Z","title":"Granger Causality using Neural Networks","summary":"  Dependence between nodes in a network is an important concept that pervades\nmany areas including finance, politics, sociology, genomics and the brain\nsciences. One way to characterize dependence between components of a\nmultivariate time series data is via Granger Causality (GC). Standard\ntraditional approaches to GC estimation / inference commonly assume linear\ndynamics, however such simplification does not hold in many real-world\napplications where signals are inherently non-linear. In such cases, imposing\nlinear models such as vector autoregressive (VAR) models can lead to\nmis-characterization of true Granger Causal interactions. To overcome this\nlimitation, Tank et al (IEEE Transactions on Pattern Analysis and Machine\nLearning, 2022) proposed a solution that uses neural networks with sparse\nregularization penalties. The regularization encourages learnable weights to be\nsparse, which enables inference on GC. This paper overcomes the limitations of\ncurrent methods by leveraging advances in machine learning and deep learning\nwhich have been demonstrated to learn hidden patterns in the data. We propose\nnovel classes of models that can handle underlying non-linearity in a\ncomputationally efficient manner, simultaneously providing GC and lag order\nselection. Firstly, we present the Learned Kernel VAR (LeKVAR) model that\nlearns kernel parameterized by a shared neural net followed by penalization on\nlearnable weights to discover GC structure. Secondly, we show one can directly\ndecouple lags and individual time series importance via decoupled penalties.\nThis is important as we want to select the lag order during the process of GC\nestimation. This decoupling acts as a filtering and can be extended to any DL\nmodel including Multi-Layer Perceptrons (MLP), Recurrent Neural Networks (RNN),\nLong Short Term Memory Networks (LSTM), Transformers etc, for simultaneous GC\nestimation and lag selection.\n","authors":["Malik Shahid Sultan","Samuel Horvath","Hernando Ombao"],"pdf_url":"https://arxiv.org/pdf/2208.03703v2.pdf","comment":"To be Submitted to a Journal work Presented at JSM. arXiv admin note:\n  text overlap with arXiv:1802.05842 by other authors"},{"id":"http://arxiv.org/abs/2407.19265v2","updated":"2024-08-07T09:16:12Z","published":"2024-07-27T14:16:25Z","title":"Towards Robust Few-shot Class Incremental Learning in Audio\n  Classification using Contrastive Representation","summary":"  In machine learning applications, gradual data ingress is common, especially\nin audio processing where incremental learning is vital for real-time\nanalytics. Few-shot class-incremental learning addresses challenges arising\nfrom limited incoming data. Existing methods often integrate additional\ntrainable components or rely on a fixed embedding extractor post-training on\nbase sessions to mitigate concerns related to catastrophic forgetting and the\ndangers of model overfitting. However, using cross-entropy loss alone during\nbase session training is suboptimal for audio data. To address this, we propose\nincorporating supervised contrastive learning to refine the representation\nspace, enhancing discriminative power and leading to better generalization\nsince it facilitates seamless integration of incremental classes, upon arrival.\nExperimental results on NSynth and LibriSpeech datasets with 100 classes, as\nwell as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art\nperformance.\n","authors":["Riyansha Singh","Parinita Nema","Vinod K Kurmi"],"pdf_url":"https://arxiv.org/pdf/2407.19265v2.pdf","comment":"INTERSPEECH 2024 accepted"},{"id":"http://arxiv.org/abs/2403.14973v2","updated":"2024-08-07T08:53:25Z","published":"2024-03-22T06:04:11Z","title":"Pose-Aware Self-Supervised Learning with Viewpoint Trajectory\n  Regularization","summary":"  Learning visual features from unlabeled images has proven successful for\nsemantic categorization, often by mapping different $views$ of the same object\nto the same feature to achieve recognition invariance. However, visual\nrecognition involves not only identifying $what$ an object is but also\nunderstanding $how$ it is presented. For example, seeing a car from the side\nversus head-on is crucial for deciding whether to stay put or jump out of the\nway. While unsupervised feature learning for downstream viewpoint reasoning is\nimportant, it remains under-explored, partly due to the lack of a standardized\nevaluation method and benchmarks.\n  We introduce a new dataset of adjacent image triplets obtained from a\nviewpoint trajectory, without any semantic or pose labels. We benchmark both\nsemantic classification and pose estimation accuracies on the same visual\nfeature. Additionally, we propose a viewpoint trajectory regularization loss\nfor learning features from unlabeled image triplets. Our experiments\ndemonstrate that this approach helps develop a visual representation that\nencodes object identity and organizes objects by their poses, retaining\nsemantic classification accuracy while achieving emergent global pose awareness\nand better generalization to novel objects. Our dataset and code are available\nat http://pwang.pw/trajSSL/.\n","authors":["Jiayun Wang","Yubei Chen","Stella X. Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14973v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03636v1","updated":"2024-08-07T08:51:10Z","published":"2024-08-07T08:51:10Z","title":"Time is Not Enough: Time-Frequency based Explanation for Time-Series\n  Black-Box Models","summary":"  Despite the massive attention given to time-series explanations due to their\nextensive applications, a notable limitation in existing approaches is their\nprimary reliance on the time-domain. This overlooks the inherent characteristic\nof time-series data containing both time and frequency features. In this work,\nwe present Spectral eXplanation (SpectralX), an XAI framework that provides\ntime-frequency explanations for time-series black-box classifiers. This easily\nadaptable framework enables users to \"plug-in\" various perturbation-based XAI\nmethods for any pre-trained time-series classification models to assess their\nimpact on the explanation quality without having to modify the framework\narchitecture. Additionally, we introduce Feature Importance Approximations\n(FIA), a new perturbation-based XAI method. These methods consist of feature\ninsertion, deletion, and combination techniques to enhance computational\nefficiency and class-specific explanations in time-series classification tasks.\nWe conduct extensive experiments in the generated synthetic dataset and various\nUCR Time-Series datasets to first compare the explanation performance of FIA\nand other existing perturbation-based XAI methods in both time-domain and\ntime-frequency domain, and then show the superiority of our FIA in the\ntime-frequency domain with the SpectralX framework. Finally, we conduct a user\nstudy to confirm the practicality of our FIA in SpectralX framework for\nclass-specific time-frequency based time-series explanations. The source code\nis available in https://github.com/gustmd0121/Time_is_not_Enough\n","authors":["Hyunseung Chung","Sumin Jo","Yeonsu Kwon","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2408.03636v1.pdf","comment":"Accepted to CIKM 2024 (10 pages, 4 figures, 6 tables)"},{"id":"http://arxiv.org/abs/2408.03626v1","updated":"2024-08-07T08:37:23Z","published":"2024-08-07T08:37:23Z","title":"On the choice of the non-trainable internal weights in random feature\n  maps","summary":"  The computationally cheap machine learning architecture of random feature\nmaps can be viewed as a single-layer feedforward network in which the weights\nof the hidden layer are random but fixed and only the outer weights are learned\nvia linear regression. The internal weights are typically chosen from a\nprescribed distribution. The choice of the internal weights significantly\nimpacts the accuracy of random feature maps. We address here the task of how to\nbest select the internal weights. In particular, we consider the forecasting\nproblem whereby random feature maps are used to learn a one-step propagator map\nfor a dynamical system. We provide a computationally cheap hit-and-run\nalgorithm to select good internal weights which lead to good forecasting skill.\nWe show that the number of good features is the main factor controlling the\nforecasting skill of random feature maps and acts as an effective feature\ndimension. Lastly, we compare random feature maps with single-layer feedforward\nneural networks in which the internal weights are now learned using gradient\ndescent. We find that random feature maps have superior forecasting\ncapabilities whilst having several orders of magnitude lower computational\ncost.\n","authors":["Pinak Mandal","Georg A. Gottwald"],"pdf_url":"https://arxiv.org/pdf/2408.03626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01185v5","updated":"2024-08-07T08:31:05Z","published":"2023-12-02T17:24:17Z","title":"A ripple in time: a discontinuity in American history","summary":"  In this technical note we suggest a novel approach to discover temporal\n(related and unrelated to language dilation) and personality (authorship\nattribution) in historical datasets. We exemplify our approach on the State of\nthe Union speeches given by the past 42 US presidents: this dataset is known\nfor its relatively small amount of data, and high variability of the amount and\nstyle of texts. Nevertheless we manage to achieve about 95\\% accuracy on the\nauthorship attribution task, and pin down the date of writing to a single\npresidential term.\n","authors":["Alexander Kolpakov","Igor Rivin"],"pdf_url":"https://arxiv.org/pdf/2312.01185v5.pdf","comment":"6 pages, 8 figures; GitHub repository\n  (https://github.com/sashakolpakov/ripple_in_time); restructured manuscript"},{"id":"http://arxiv.org/abs/2408.03619v1","updated":"2024-08-07T08:23:42Z","published":"2024-08-07T08:23:42Z","title":"Making Robust Generalizers Less Rigid with Soft Ascent-Descent","summary":"  While the traditional formulation of machine learning tasks is in terms of\nperformance on average, in practice we are often interested in how well a\ntrained model performs on rare or difficult data points at test time. To\nachieve more robust and balanced generalization, methods applying\nsharpness-aware minimization to a subset of worst-case examples have proven\nsuccessful for image classification tasks, but only using deep neural networks\nin a scenario where the most difficult points are also the least common. In\nthis work, we show how such a strategy can dramatically break down under more\ndiverse models, and as a more robust alternative, instead of typical sharpness\nwe propose and evaluate a training criterion which penalizes poor loss\nconcentration, which can be easily combined with loss transformations such as\nCVaR or DRO that control tail emphasis.\n","authors":["Matthew J. Holland","Toma Hamada"],"pdf_url":"https://arxiv.org/pdf/2408.03619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14573v2","updated":"2024-08-07T08:22:35Z","published":"2024-07-21T06:27:45Z","title":"Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization","summary":"  Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.\n","authors":["Orson Mengara"],"pdf_url":"https://arxiv.org/pdf/2407.14573v2.pdf","comment":"END :jumps-Diffusion and stock market: Better quantify uncertainty in\n  financial simulations"},{"id":"http://arxiv.org/abs/2304.09914v4","updated":"2024-08-07T08:20:43Z","published":"2023-04-19T18:32:49Z","title":"The Face of Populism: Examining Differences in Facial Emotional\n  Expressions of Political Leaders Using Machine Learning","summary":"  Populist rhetoric employed on online media is characterized as deeply\nimpassioned and often imbued with strong emotions. The aim of this paper is to\nempirically investigate the differences in affective nonverbal communication of\npolitical leaders. We use a deep-learning approach to process a sample of 220\nYouTube videos of political leaders from 15 different countries, analyze their\nfacial expressions of emotion and then examine differences in average emotion\nscores representing the relative presence of 6 emotional states (anger,\ndisgust, fear, happiness, sadness, and surprise) and a neutral expression for\neach frame of the YouTube video. Based on a sample of manually coded images, we\nfind that this deep-learning approach has 53-60\\% agreement with human labels.\nWe observe statistically significant differences in the average score of\nnegative emotions between groups of leaders with varying degrees of populist\nrhetoric.\n","authors":["Sara Major","Aleksandar Tomaevi"],"pdf_url":"https://arxiv.org/pdf/2304.09914v4.pdf","comment":"Version 4.0: Annotation study added, supplementary information\n  extended"},{"id":"http://arxiv.org/abs/2408.03618v1","updated":"2024-08-07T08:19:44Z","published":"2024-08-07T08:19:44Z","title":"A Logical Fallacy-Informed Framework for Argument Generation","summary":"  Despite the remarkable performance of Large Language Models (LLMs), they\nstill struggle with generating logically sound arguments, resulting in\npotential risks such as spreading misinformation. An important factor\ncontributing to LLMs' suboptimal performance in generating coherent arguments\nis their oversight of logical fallacies. To address this issue, we introduce\nFIPO, a fallacy-informed framework that leverages preference optimization\nmethods to steer LLMs toward logically sound arguments. FIPO includes a\nclassification loss, to capture the fine-grained information on fallacy\ncategories. Our results on argumentation datasets show that our method reduces\nthe fallacy errors by up to 17.5%. Furthermore, our human evaluation results\nindicate that the quality of the generated arguments by our method\nsignificantly outperforms the fine-tuned baselines, as well as prior preference\noptimization methods, such as DPO. These findings highlight the importance of\nensuring models are aware of logical fallacies for effective argument\ngeneration.\n","authors":["Luca Mouchel","Debjit Paul","Shaobo Cui","Robert West","Antoine Bosselut","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2408.03618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03617v1","updated":"2024-08-07T08:18:51Z","published":"2024-08-07T08:18:51Z","title":"Is Child-Directed Speech Effective Training Data for Language Models?","summary":"  While high-performing language models are typically trained on hundreds of\nbillions of words, human children become fluent language users with a much\nsmaller amount of data. What are the features of the data they receive, and how\ndo these features support language modeling objectives? To investigate this\nquestion, we train GPT-2 models on 29M words of English-language child-directed\nspeech and a new matched, synthetic dataset (TinyDialogues), comparing to a\nheterogeneous blend of datasets from the BabyLM challenge. We evaluate both the\nsyntactic and semantic knowledge of these models using developmentally-inspired\nevaluations. Through pretraining experiments, we test whether the global\ndevelopmental ordering or the local discourse ordering of children's training\ndata support high performance relative to other datasets. The local properties\nof the data affect model results, but somewhat surprisingly, global properties\ndo not. Further, child language input is not uniquely valuable for training\nlanguage models. These findings support the hypothesis that, rather than\nproceeding from better data, children's learning is instead substantially more\nefficient than current language modeling techniques.\n","authors":["Steven Y. Feng","Noah D. Goodman","Michael C. Frank"],"pdf_url":"https://arxiv.org/pdf/2408.03617v1.pdf","comment":"Preprint. Code and data will be released soon"},{"id":"http://arxiv.org/abs/2404.15311v2","updated":"2024-08-07T08:14:56Z","published":"2024-04-02T17:01:51Z","title":"Fusing Pretrained ViTs with TCNet for Enhanced EEG Regression","summary":"  The task of Electroencephalogram (EEG) analysis is paramount to the\ndevelopment of Brain-Computer Interfaces (BCIs). However, to reach the goal of\ndeveloping robust, useful BCIs depends heavily on the speed and the accuracy at\nwhich BCIs can understand neural dynamics. In response to that goal, this paper\ndetails the integration of pre-trained Vision Transformers (ViTs) with Temporal\nConvolutional Networks (TCNet) to enhance the precision of EEG regression. The\ncore of this approach lies in harnessing the sequential data processing\nstrengths of ViTs along with the superior feature extraction capabilities of\nTCNet, to significantly improve EEG analysis accuracy. In addition, we analyze\nthe importance of how to construct optimal patches for the attention mechanism\nto analyze, balancing both speed and accuracy tradeoffs. Our results showcase a\nsubstantial improvement in regression accuracy, as evidenced by the reduction\nof Root Mean Square Error (RMSE) from 55.4 to 51.8 on EEGEyeNet's Absolute\nPosition Task, outperforming existing state-of-the-art models. Without\nsacrificing performance, we increase the speed of this model by an order of\nmagnitude (up to 4.32x faster). This breakthrough not only sets a new benchmark\nin EEG regression analysis but also opens new avenues for future research in\nthe integration of transformer architectures with specialized feature\nextraction methods for diverse EEG datasets.\n","authors":["Eric Modesitt","Haicheng Yin","Williams Huang Wang","Brian Lu"],"pdf_url":"https://arxiv.org/pdf/2404.15311v2.pdf","comment":"Accepted HCI International 2024"},{"id":"http://arxiv.org/abs/2408.03612v1","updated":"2024-08-07T08:08:08Z","published":"2024-08-07T08:08:08Z","title":"JARViS: Detecting Actions in Video Using Unified Actor-Scene Context\n  Relation Modeling","summary":"  Video action detection (VAD) is a formidable vision task that involves the\nlocalization and classification of actions within the spatial and temporal\ndimensions of a video clip. Among the myriad VAD architectures, two-stage VAD\nmethods utilize a pre-trained person detector to extract the region of interest\nfeatures, subsequently employing these features for action detection. However,\nthe performance of two-stage VAD methods has been limited as they depend solely\non localized actor features to infer action semantics. In this study, we\npropose a new two-stage VAD framework called Joint Actor-scene context Relation\nmodeling based on Visual Semantics (JARViS), which effectively consolidates\ncross-modal action semantics distributed globally across spatial and temporal\ndimensions using Transformer attention. JARViS employs a person detector to\nproduce densely sampled actor features from a keyframe. Concurrently, it uses a\nvideo backbone to create spatio-temporal scene features from a video clip.\nFinally, the fine-grained interactions between actors and scenes are modeled\nthrough a Unified Action-Scene Context Transformer to directly output the final\nset of actions in parallel. Our experimental results demonstrate that JARViS\noutperforms existing methods by significant margins and achieves\nstate-of-the-art performance on three popular VAD datasets, including AVA,\nUCF101-24, and JHMDB51-21.\n","authors":["Seok Hwan Lee","Taein Son","Soo Won Seo","Jisong Kim","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2408.03612v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2404.08271v3","updated":"2024-08-07T08:00:43Z","published":"2024-04-12T06:50:32Z","title":"Transfer Learning Study of Motion Transformer-based Trajectory\n  Predictions","summary":"  Trajectory planning in autonomous driving is highly dependent on predicting\nthe emergent behavior of other road users. Learning-based methods are currently\nshowing impressive results in simulation-based challenges, with\ntransformer-based architectures technologically leading the way. Ultimately,\nhowever, predictions are needed in the real world. In addition to the shifts\nfrom simulation to the real world, many vehicle- and country-specific shifts,\ni.e. differences in sensor systems, fusion and perception algorithms as well as\ntraffic rules and laws, are on the agenda. Since models that can cover all\nsystem setups and design domains at once are not yet foreseeable, model\nadaptation plays a central role. Therefore, a simulation-based study on\ntransfer learning techniques is conducted on basis of a transformer-based\nmodel. Furthermore, the study aims to provide insights into possible trade-offs\nbetween computational time and performance to support effective transfers into\nthe real world.\n","authors":["Lars Ullrich","Alex McMaster","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.08271v3.pdf","comment":"Published in 2024 IEEE Intelligent Vehicles Symposium (IV), Jeju\n  Shinhwa World, Jeju Island, Korea, June 2-5, 2024"},{"id":"http://arxiv.org/abs/2408.03608v1","updated":"2024-08-07T07:54:19Z","published":"2024-08-07T07:54:19Z","title":"InPer: Whole-Process Domain Generalization via Causal Intervention and\n  Perturbation","summary":"  Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer.\n","authors":["Luyao Tang","Yuxuan Yuan","Chaoqi Chen","Xinghao Ding","Yue Huang"],"pdf_url":"https://arxiv.org/pdf/2408.03608v1.pdf","comment":"Accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2407.18990v2","updated":"2024-08-07T07:46:39Z","published":"2024-07-25T12:07:55Z","title":"Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM\n  Tuning in Real-World Applications","summary":"  Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.\n","authors":["Alon Halfon","Shai Gretz","Ofir Arviv","Artem Spector","Orith Toledo-Ronen","Yoav Katz","Liat Ein-Dor","Michal Shmueli-Scheuer","Noam Slonim"],"pdf_url":"https://arxiv.org/pdf/2407.18990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03603v1","updated":"2024-08-07T07:46:08Z","published":"2024-08-07T07:46:08Z","title":"EnJa: Ensemble Jailbreak on Large Language Models","summary":"  As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.\n","authors":["Jiahao Zhang","Zilong Wang","Ruofan Wang","Xingjun Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.03603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03599v1","updated":"2024-08-07T07:36:49Z","published":"2024-08-07T07:36:49Z","title":"Activations Through Extensions: A Framework To Boost Performance Of\n  Neural Networks","summary":"  Activation functions are non-linearities in neural networks that allow them\nto learn complex mapping between inputs and outputs. Typical choices for\nactivation functions are ReLU, Tanh, Sigmoid etc., where the choice generally\ndepends on the application domain. In this work, we propose a\nframework/strategy that unifies several works on activation functions and\ntheoretically explains the performance benefits of these works. We also propose\nnovel techniques that originate from the framework and allow us to obtain\n``extensions'' (i.e. special generalizations of a given neural network) of\nneural networks through operations on activation functions. We theoretically\nand empirically show that ``extensions'' of neural networks have performance\nbenefits compared to vanilla neural networks with insignificant space and time\ncomplexity costs on standard test functions. We also show the benefits of\nneural network ``extensions'' in the time-series domain on real-world datasets.\n","authors":["Chandramouli Kamanchi","Sumatra Mukherjee","Kameshwaran Sampath","Pankaj Dayama","Arindam Jati","Vijay Ekambaram","Dzung Phan"],"pdf_url":"https://arxiv.org/pdf/2408.03599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17640v2","updated":"2024-08-07T07:29:39Z","published":"2024-05-27T20:24:03Z","title":"Probabilistically Plausible Counterfactual Explanations with Normalizing\n  Flows","summary":"  We present PPCEF, a novel method for generating probabilistically plausible\ncounterfactual explanations (CFs). PPCEF advances beyond existing methods by\ncombining a probabilistic formulation that leverages the data distribution with\nthe optimization of plausibility within a unified framework. Compared to\nreference approaches, our method enforces plausibility by directly optimizing\nthe explicit density function without assuming a particular family of\nparametrized distributions. This ensures CFs are not only valid (i.e., achieve\nclass change) but also align with the underlying data's probability density.\nFor that purpose, our approach leverages normalizing flows as powerful density\nestimators to capture the complex high-dimensional data distribution.\nFurthermore, we introduce a novel loss that balances the trade-off between\nachieving class change and maintaining closeness to the original instance while\nalso incorporating a probabilistic plausibility term. PPCEF's unconstrained\nformulation allows for efficient gradient-based optimization with batch\nprocessing, leading to orders of magnitude faster computation compared to prior\nmethods. Moreover, the unconstrained formulation of PPCEF allows for the\nseamless integration of future constraints tailored to specific counterfactual\nproperties. Finally, extensive evaluations demonstrate PPCEF's superiority in\ngenerating high-quality, probabilistically plausible counterfactual\nexplanations in high-dimensional tabular settings. This makes PPCEF a powerful\ntool for not only interpreting complex machine learning models but also for\nimproving fairness, accountability, and trust in AI systems.\n","authors":["Patryk Wielopolski","Oleksii Furman","Jerzy Stefanowski","Maciej Ziba"],"pdf_url":"https://arxiv.org/pdf/2405.17640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09032v2","updated":"2024-08-07T07:20:23Z","published":"2024-03-14T01:51:35Z","title":"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences","summary":"  Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires a deep assessment\nof LLMs' outputs. Existing methods and benchmarks rely primarily on automated\nmetrics and static analysis tools, which often fail to capture the nuances of\nuser instructions and LLM outputs. To address this gap, we propose using the\nLLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding\npreferences. Based on this approach, we present CodeUltraFeedback, a\ncomprehensive dataset designed to facilitate the evaluation and improvement of\nLLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each\nannotated with four responses generated from a diverse pool of 14 LLMs. These\nresponses are ranked based on five distinct coding preferences using GPT-3.5 as\na judge, providing both numerical scores and detailed textual feedback. Our\nanalysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are\ngenerally preferred over those from open-weight LLMs, highlighting significant\ndifferences in alignment between closed and open-weight models. In turn, we\nexplore the usage of CodeUltraFeedback as feedback data to fine-tune and align\nCodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement\nlearning from AI feedback (RLAIF) with direct preference optimization (DPO).\nThe resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in\nterms of alignment with coding preferences and shows improved functional\ncorrectness on the HumanEval+ benchmark compared to the original instruct\nmodel. Therefore, our contributions bridge the gap in preference tuning of LLMs\nfor code and set the stage for further advancements in model alignment and\nRLAIF in automated software engineering.\n","authors":["Martin Weyssow","Aton Kamanda","Houari Sahraoui"],"pdf_url":"https://arxiv.org/pdf/2403.09032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03591v1","updated":"2024-08-07T07:09:14Z","published":"2024-08-07T07:09:14Z","title":"Focal Depth Estimation: A Calibration-Free, Subject- and Daytime\n  Invariant Approach","summary":"  In an era where personalized technology is increasingly intertwined with\ndaily life, traditional eye-tracking systems and autofocal glasses face a\nsignificant challenge: the need for frequent, user-specific calibration, which\nimpedes their practicality. This study introduces a groundbreaking\ncalibration-free method for estimating focal depth, leveraging machine learning\ntechniques to analyze eye movement features within short sequences. Our\napproach, distinguished by its innovative use of LSTM networks and\ndomain-specific feature engineering, achieves a mean absolute error (MAE) of\nless than 10 cm, setting a new focal depth estimation accuracy standard. This\nadvancement promises to enhance the usability of autofocal glasses and pave the\nway for their seamless integration into extended reality environments, marking\na significant leap forward in personalized visual technology.\n","authors":["Benedikt W. Hosp","Bjrn Severitt","Rajat Agarwala","Evgenia Rusak","Yannick Sauer","Siegfried Wahl"],"pdf_url":"https://arxiv.org/pdf/2408.03591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03590v1","updated":"2024-08-07T07:09:06Z","published":"2024-08-07T07:09:06Z","title":"Sensitivity analysis using the Metamodel of Optimal Prognosis","summary":"  In real case applications within the virtual prototyping process, it is not\nalways possible to reduce the complexity of the physical models and to obtain\nnumerical models which can be solved quickly. Usually, every single numerical\nsimulation takes hours or even days. Although the progresses in numerical\nmethods and high performance computing, in such cases, it is not possible to\nexplore various model configurations, hence efficient surrogate models are\nrequired. Generally the available meta-model techniques show several advantages\nand disadvantages depending on the investigated problem. In this paper we\npresent an automatic approach for the selection of the optimal suitable\nmeta-model for the actual problem. Together with an automatic reduction of the\nvariable space using advanced filter techniques an efficient approximation is\nenabled also for high dimensional problems. This filter techniques enable a\nreduction of the high dimensional variable space to a much smaller subspace\nwhere meta-model-based sensitivity analyses are carried out to assess the\ninfluence of important variables and to identify the optimal subspace with\ncorresponding surrogate model which enables the most accurate probabilistic\nanalysis. For this purpose we investigate variance-based and moment-free\nsensitivity measures in combination with advanced meta-models as moving least\nsquares and kriging.\n","authors":["Thomas Most","Johannes Will"],"pdf_url":"https://arxiv.org/pdf/2408.03590v1.pdf","comment":"presented at 8th Optimization and Stochastic Days, Weimar, Germany,\n  24-25 November, 2011"},{"id":"http://arxiv.org/abs/2408.03588v1","updated":"2024-08-07T07:04:29Z","published":"2024-08-07T07:04:29Z","title":"Facing the Music: Tackling Singing Voice Separation in Cinematic Audio\n  Source Separation","summary":"  Cinematic audio source separation (CASS) is a fairly new subtask of audio\nsource separation. A typical setup of CASS is a three-stem problem, with the\naim of separating the mixture into the dialogue stem (DX), music stem (MX), and\neffects stem (FX). In practice, however, several edge cases exist as some sound\nsources do not fit neatly in either of these three stems, necessitating the use\nof additional auxiliary stems in production. One very common edge case is the\nsinging voice in film audio, which may belong in either the DX or MX, depending\nheavily on the cinematic context. In this work, we demonstrate a very\nstraightforward extension of the dedicated-decoder Bandit and query-based\nsingle-decoder Banquet models to a four-stem problem, treating non-musical\ndialogue, instrumental music, singing voice, and effects as separate stems.\nInterestingly, the query-based Banquet model outperformed the dedicated-decoder\nBandit model. We hypothesized that this is due to a better feature alignment at\nthe bottleneck as enforced by the band-agnostic FiLM layer. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.\n","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_url":"https://arxiv.org/pdf/2408.03588v1.pdf","comment":"Submitted to the Late-Breaking Demo Session of the 25th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2024"},{"id":"http://arxiv.org/abs/2405.15081v3","updated":"2024-08-07T07:03:11Z","published":"2024-05-23T22:07:54Z","title":"Distributed Harmonization: Federated Clustered Batch Effect Adjustment\n  and Generalization","summary":"  Independent and identically distributed (i.i.d.) data is essential to many\ndata analysis and modeling techniques. In the medical domain, collecting data\nfrom multiple sites or institutions is a common strategy that guarantees\nsufficient clinical diversity, determined by the decentralized nature of\nmedical data. However, data from various sites are easily biased by the local\nenvironment or facilities, thereby violating the i.i.d. rule. A common strategy\nis to harmonize the site bias while retaining important biological information.\nThe ComBat is among the most popular harmonization approaches and has recently\nbeen extended to handle distributed sites. However, when faced with situations\ninvolving newly joined sites in training or evaluating data from unknown/unseen\nsites, ComBat lacks compatibility and requires retraining with data from all\nthe sites. The retraining leads to significant computational and logistic\noverhead that is usually prohibitive. In this work, we develop a novel Cluster\nComBat harmonization algorithm, which leverages cluster patterns of the data in\ndifferent sites and greatly advances the usability of ComBat harmonization. We\nuse extensive simulation and real medical imaging data from ADNI to demonstrate\nthe superiority of the proposed approach. Our codes are provided in\nhttps://github.com/illidanlab/distributed-cluster-harmonization.\n","authors":["Bao Hoang","Yijiang Pang","Siqi Liang","Liang Zhan","Paul Thompson","Jiayu Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.15081v3.pdf","comment":"11 pages, 7 figures, accepted to KDD2024-ADS"},{"id":"http://arxiv.org/abs/2407.21282v2","updated":"2024-08-07T07:00:13Z","published":"2024-07-31T02:12:05Z","title":"FedBChain: A Blockchain-enabled Federated Learning Framework for\n  Improving DeepConvLSTM with Comparative Strategy Insights","summary":"  Recent research in the field of Human Activity Recognition has shown that an\nimprovement in prediction performance can be achieved by reducing the number of\nLSTM layers. However, this kind of enhancement is only significant on\nmonolithic architectures, and when it runs on large-scale distributed training,\ndata security and privacy issues will be reconsidered, and its prediction\nperformance is unknown. In this paper, we introduce a novel framework:\nFedBChain, which integrates the federated learning paradigm based on a modified\nDeepConvLSTM architecture with a single LSTM layer. This framework performs\ncomparative tests of prediction performance on three different real-world\ndatasets based on three different hidden layer units (128, 256, and 512)\ncombined with five different federated learning strategies, respectively. The\nresults show that our architecture has significant improvements in Precision,\nRecall and F1-score compared to the centralized training approach on all\ndatasets with all hidden layer units for all strategies: FedAvg strategy\nimproves on average by 4.54%, FedProx improves on average by 4.57%,\nFedTrimmedAvg improves on average by 4.35%, Krum improves by 4.18% on average,\nand FedAvgM improves by 4.46% on average. Based on our results, it can be seen\nthat FedBChain not only improves in performance, but also guarantees the\nsecurity and privacy of user data compared to centralized training methods\nduring the training process. The code for our experiments is publicly available\n(https://github.com/Glen909/FedBChain).\n","authors":["Gaoxuan Li","Chern Hong Lim","Qiyao Ma","Xinyu Tang","Hwa Hui Tew","Fan Ding","Xuewen Luo"],"pdf_url":"https://arxiv.org/pdf/2407.21282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03585v1","updated":"2024-08-07T06:44:47Z","published":"2024-08-07T06:44:47Z","title":"Hierarchical Neural Constructive Solver for Real-world TSP Scenarios","summary":"  Existing neural constructive solvers for routing problems have predominantly\nemployed transformer architectures, conceptualizing the route construction as a\nset-to-sequence learning task. However, their efficacy has primarily been\ndemonstrated on entirely random problem instances that inadequately capture\nreal-world scenarios. In this paper, we introduce realistic Traveling Salesman\nProblem (TSP) scenarios relevant to industrial settings and derive the\nfollowing insights: (1) The optimal next node (or city) to visit often lies\nwithin proximity to the current node, suggesting the potential benefits of\nbiasing choices based on current locations. (2) Effectively solving the TSP\nrequires robust tracking of unvisited nodes and warrants succinct grouping\nstrategies. Building upon these insights, we propose integrating a learnable\nchoice layer inspired by Hypernetworks to prioritize choices based on the\ncurrent location, and a learnable approximate clustering algorithm inspired by\nthe Expectation-Maximization algorithm to facilitate grouping the unvisited\ncities. Together, these two contributions form a hierarchical approach towards\nsolving the realistic TSP by considering both immediate local neighbourhoods\nand learning an intermediate set of node representations. Our hierarchical\napproach yields superior performance compared to both classical and recent\ntransformer models, showcasing the efficacy of the key designs.\n","authors":["Yong Liang Goh","Zhiguang Cao","Yining Ma","Yanfei Dong","Mohammed Haroon Dupty","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2408.03585v1.pdf","comment":"Accepted to KDD 2024"},{"id":"http://arxiv.org/abs/2408.03574v1","updated":"2024-08-07T06:26:04Z","published":"2024-08-07T06:26:04Z","title":"Teach CLIP to Develop a Number Sense for Ordinal Regression","summary":"  Ordinal regression is a fundamental problem within the field of computer\nvision, with customised well-trained models on specific tasks. While\npre-trained vision-language models (VLMs) have exhibited impressive performance\non various vision tasks, their potential for ordinal regression has received\nless exploration. In this study, we first investigate CLIP's potential for\nordinal regression, from which we expect the model could generalise to\ndifferent ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP\nfails on this task, since current VLMs have a well-documented limitation of\nencapsulating compositional concepts such as number sense. We propose a simple\nyet effective method called NumCLIP to improve the quantitative understanding\nof VLMs. We disassemble the exact image to number-specific text matching\nproblem into coarse classification and fine prediction stages. We discretize\nand phrase each numerical bin with common language concept to better leverage\nthe available pre-trained alignment in CLIP. To consider the inherent\ncontinuous property of ordinal regression, we propose a novel fine-grained\ncross-modal ranking-based regularisation loss specifically designed to keep\nboth semantic and ordinal alignment in CLIP's feature space. Experimental\nresults on three general ordinal regression tasks demonstrate the effectiveness\nof NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating\nand image aesthetics assessment task, respectively. Code is publicly available\nat https://github.com/xmed-lab/NumCLIP.\n","authors":["Yao Du","Qiang Zhai","Weihang Dai","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.03574v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03572v1","updated":"2024-08-07T06:16:17Z","published":"2024-08-07T06:16:17Z","title":"2D-OOB: Attributing Data Contribution through Joint Valuation Framework","summary":"  Data valuation has emerged as a powerful framework to quantify the\ncontribution of each datum to the training of a particular machine learning\nmodel. However, it is crucial to recognize that the quality of various cells\nwithin a single data point can vary greatly in practice. For example, even in\nthe case of an abnormal data point, not all cells are necessarily noisy. The\nsingle scalar valuation assigned by existing methods blurs the distinction\nbetween noisy and clean cells of a data point, thereby compromising the\ninterpretability of the valuation. In this paper, we propose 2D-OOB, an\nout-of-bag estimation framework for jointly determining helpful (or\ndetrimental) samples, as well as the particular cells that drive them. Our\ncomprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art\nperformance across multiple use cases, while being exponentially faster. 2D-OOB\nexcels in detecting and rectifying fine-grained outliers at the cell level, as\nwell as localizing backdoor triggers in data poisoning attacks.\n","authors":["Yifan Sun","Jingyan Shen","Yongchan Kwon"],"pdf_url":"https://arxiv.org/pdf/2408.03572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03569v1","updated":"2024-08-07T06:11:37Z","published":"2024-08-07T06:11:37Z","title":"Maximum a Posteriori Estimation for Linear Structural Dynamics Models\n  Using Bayesian Optimization with Rational Polynomial Chaos Expansions","summary":"  Bayesian analysis enables combining prior knowledge with measurement data to\nlearn model parameters. Commonly, one resorts to computing the maximum a\nposteriori (MAP) estimate, when only a point estimate of the parameters is of\ninterest. We apply MAP estimation in the context of structural dynamic models,\nwhere the system response can be described by the frequency response function.\nTo alleviate high computational demands from repeated expensive model calls, we\nutilize a rational polynomial chaos expansion (RPCE) surrogate model that\nexpresses the system frequency response as a rational of two polynomials with\ncomplex coefficients. We propose an extension to an existing sparse Bayesian\nlearning approach for RPCE based on Laplace's approximation for the posterior\ndistribution of the denominator coefficients. Furthermore, we introduce a\nBayesian optimization approach, which allows to adaptively enrich the\nexperimental design throughout the optimization process of MAP estimation.\nThereby, we utilize the expected improvement acquisition function as a means to\nidentify sample points in the input space that are possibly associated with\nlarge objective function values. The acquisition function is estimated through\nMonte Carlo sampling based on the posterior distribution of the expansion\ncoefficients identified in the sparse Bayesian learning process. By combining\nthe sparsity-inducing learning procedure with the sequential experimental\ndesign, we effectively reduce the number of model evaluations in the MAP\nestimation problem. We demonstrate the applicability of the presented methods\non the parameter updating problem of an algebraic two-degree-of-freedom system\nand the finite element model of a cross-laminated timber plate.\n","authors":["Felix Schneider","Iason Papaioannou","Bruno Sudret","Gerhard Mller"],"pdf_url":"https://arxiv.org/pdf/2408.03569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03568v1","updated":"2024-08-07T06:11:25Z","published":"2024-08-07T06:11:25Z","title":"A comparative study of generative adversarial networks for image\n  recognition algorithms based on deep learning and traditional methods","summary":"  In this paper, an image recognition algorithm based on the combination of\ndeep learning and generative adversarial network (GAN) is studied, and compared\nwith traditional image recognition methods. The purpose of this study is to\nevaluate the advantages and application prospects of deep learning technology,\nespecially GAN, in the field of image recognition. Firstly, this paper reviews\nthe basic principles and techniques of traditional image recognition methods,\nincluding the classical algorithms based on feature extraction such as SIFT,\nHOG and their combination with support vector machine (SVM), random forest, and\nother classifiers. Then, the working principle, network structure, and unique\nadvantages of GAN in image generation and recognition are introduced. In order\nto verify the effectiveness of GAN in image recognition, a series of\nexperiments are designed and carried out using multiple public image data sets\nfor training and testing. The experimental results show that compared with\ntraditional methods, GAN has excellent performance in processing complex\nimages, recognition accuracy, and anti-noise ability. Specifically, Gans are\nbetter able to capture high-dimensional features and details of images,\nsignificantly improving recognition performance. In addition, Gans shows unique\nadvantages in dealing with image noise, partial missing information, and\ngenerating high-quality images.\n","authors":["Yihao Zhong","Yijing Wei","Yingbin Liang","Xiqing Liu","Rongwei Ji","Yiru Cang"],"pdf_url":"https://arxiv.org/pdf/2408.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11755v3","updated":"2024-08-07T06:05:42Z","published":"2024-03-18T13:03:24Z","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","summary":"  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n","authors":["M. Jehanzeb Mirza","Leonid Karlinsky","Wei Lin","Sivan Doveh","Jakub Micorek","Mateusz Kozinski","Hilde Kuehne","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2403.11755v3.pdf","comment":"ECCV Camera Ready. Code & Data:\n  https://jmiemirza.github.io/Meta-Prompting/"},{"id":"http://arxiv.org/abs/2408.03561v1","updated":"2024-08-07T05:50:17Z","published":"2024-08-07T05:50:17Z","title":"MPC-Minimized Secure LLM Inference","summary":"  Many inference services based on large language models (LLMs) pose a privacy\nconcern, either revealing user prompts to the service or the proprietary\nweights to the user. Secure inference offers a solution to this problem through\nsecure multi-party computation (MPC), however, it is still impractical for\nmodern LLM workload due to the large overhead imposed by MPC. To address this\noverhead, we propose Marill, a framework that adapts LLM fine-tuning to\nminimize MPC usage during secure inference. Marill introduces high-level\narchitectural changes during fine-tuning that significantly reduce the number\nof expensive operations needed within MPC during inference, by removing some\nand relocating others outside MPC without compromising security. As a result,\nMarill-generated models are more efficient across all secure inference\nprotocols and our approach complements MPC-friendly approximations for such\noperations. Compared to standard fine-tuning, Marill results in 3.6-11.3x\nbetter runtime and 2.4-6.9x better communication during secure inference across\nvarious MPC settings, while typically preserving over 90% performance across\ndownstream tasks.\n","authors":["Deevashwer Rathee","Dacheng Li","Ion Stoica","Hao Zhang","Raluca Popa"],"pdf_url":"https://arxiv.org/pdf/2408.03561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03560v1","updated":"2024-08-07T05:48:05Z","published":"2024-08-07T05:48:05Z","title":"In2Core: Leveraging Influence Functions for Coreset Selection in\n  Instruction Finetuning of Large Language Models","summary":"  Despite advancements, fine-tuning Large Language Models (LLMs) remains costly\ndue to the extensive parameter count and substantial data requirements for\nmodel generalization. Accessibility to computing resources remains a barrier\nfor the open-source community. To address this challenge, we propose the\nIn2Core algorithm, which selects a coreset by analyzing the correlation between\ntraining and evaluation samples with a trained model. Notably, we assess the\nmodel's internal gradients to estimate this relationship, aiming to rank the\ncontribution of each training point. To enhance efficiency, we propose an\noptimization to compute influence functions with a reduced number of layers\nwhile achieving similar accuracy. By applying our algorithm to instruction\nfine-tuning data of LLMs, we can achieve similar performance with just 50% of\nthe training data. Meantime, using influence functions to analyze model\ncoverage to certain testing samples could provide a reliable and interpretable\nsignal on the training set's coverage of those test points.\n","authors":["Ayrton San Joaquin","Bin Wang","Zhengyuan Liu","Nicholas Asher","Brian Lim","Philippe Muller","Nancy Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03554v1","updated":"2024-08-07T05:30:10Z","published":"2024-08-07T05:30:10Z","title":"Empirical Analysis of Large Vision-Language Models against Goal\n  Hijacking via Visual Prompt Injection","summary":"  We explore visual prompt injection (VPI) that maliciously exploits the\nability of large vision-language models (LVLMs) to follow instructions drawn\nonto the input image. We propose a new VPI method, \"goal hijacking via visual\nprompt injection\" (GHVPI), that swaps the execution task of LVLMs from an\noriginal task to an alternative task designated by an attacker. The\nquantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and\ndemonstrates a notable attack success rate of 15.8%, which is an unignorable\nsecurity risk. Our analysis also shows that successful GHVPI requires high\ncharacter recognition capability and instruction-following ability in LVLMs.\n","authors":["Subaru Kimura","Ryota Tanaka","Shumpei Miyawaki","Jun Suzuki","Keisuke Sakaguchi"],"pdf_url":"https://arxiv.org/pdf/2408.03554v1.pdf","comment":"8 pages, 6 figures, Accepted to NAACL 2024 SRW"},{"id":"http://arxiv.org/abs/2106.11760v5","updated":"2024-08-07T05:28:30Z","published":"2021-06-19T06:25:10Z","title":"Fingerprinting Image-to-Image Generative Adversarial Networks","summary":"  Generative Adversarial Networks (GANs) have been widely used in various\napplication scenarios. Since the production of a commercial GAN requires\nsubstantial computational and human resources, the copyright protection of GANs\nis urgently needed. This paper presents a novel fingerprinting scheme for the\nIntellectual Property (IP) protection of image-to-image GANs based on a trusted\nthird party. We break through the stealthiness and robustness bottlenecks\nsuffered by previous fingerprinting methods for classification models being\nnaively transferred to GANs. Specifically, we innovatively construct a\ncomposite deep learning model from the target GAN and a classifier. Then we\ngenerate fingerprint samples from this composite model, and embed them in the\nclassifier for effective ownership verification. This scheme inspires some\nconcrete methodologies to practically protect the modern image-to-image\ntranslation GANs. Theoretical analysis proves that these methods can satisfy\ndifferent security requirements necessary for IP protection. We also conduct\nextensive experiments to show that our solutions outperform existing\nstrategies.\n","authors":["Guanlin Li","Guowen Xu","Han Qiu","Shangwei Guo","Run Wang","Jiwei Li","Tianwei Zhang","Rongxing Lu"],"pdf_url":"https://arxiv.org/pdf/2106.11760v5.pdf","comment":"Accepted by EuroS&P 2024"},{"id":"http://arxiv.org/abs/2302.01089v4","updated":"2024-08-07T05:22:57Z","published":"2023-02-02T13:22:18Z","title":"Curriculum Learning for ab initio Deep Learned Refractive Optics","summary":"  Deep optical optimization has recently emerged as a new paradigm for\ndesigning computational imaging systems using only the output image as the\nobjective. However, it has been limited to either simple optical systems\nconsisting of a single element such as a diffractive optical element (DOE) or\nmetalens, or the fine-tuning of compound lenses from good initial designs. Here\nwe present a DeepLens design method based on curriculum learning, which is able\nto learn optical designs of compound lenses ab initio from randomly initialized\nsurfaces without human intervention, therefore overcoming the need for a good\ninitial design. We demonstrate the effectiveness of our approach by fully\nautomatically designing both classical imaging lenses and a large field-of-view\nextended depth-of-field computational lens in a cellphone-style form factor,\nwith highly aspheric surfaces and a short back focal length.\n","authors":["Xinge Yang","Qiang Fu","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2302.01089v4.pdf","comment":"Automatically design computational lenses from scratch with\n  differentiable ray tracing"},{"id":"http://arxiv.org/abs/2408.02050v2","updated":"2024-08-07T05:15:53Z","published":"2024-08-04T14:57:44Z","title":"Recovering the state and dynamics of autonomous system with partial\n  states solution using neural networks","summary":"  In this paper we explore the performance of deep hidden physics model (M.\nRaissi 2018) for autonomous systems. These systems are described by set of\nordinary differential equations which do not explicitly depend on time. Such\nsystems can be found in nature and have applications in modeling chemical\nconcentrations, population dynamics, n-body problems in physics etc. In this\nwork we consider dynamics of states, which explain how the states will evolve\nare unknown to us. We approximate state and dynamics both using neural\nnetworks. We have considered examples of 2D linear/nonlinear and Lorenz\nsystems. We observe that even without knowing all the states information, we\ncan estimate dynamics of certain states whose state information are known.\n","authors":["Vijay Kag"],"pdf_url":"https://arxiv.org/pdf/2408.02050v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06237v3","updated":"2024-08-07T04:59:21Z","published":"2023-04-13T03:20:45Z","title":"Deep learning based ECG segmentation for delineation of diverse\n  arrhythmias","summary":"  Accurate delineation of key waveforms in an ECG is a critical step in\nextracting relevant features to support the diagnosis and treatment of heart\nconditions. Although deep learning based methods using segmentation models to\nlocate P, QRS, and T waves have shown promising results, their ability to\nhandle arrhythmias has not been studied in any detail. In this paper we\ninvestigate the effect of arrhythmias on delineation quality and develop\nstrategies to improve performance in such cases. We introduce a U-Net-like\nsegmentation model for ECG delineation with a particular focus on diverse\narrhythmias. This is followed by a post-processing algorithm which removes\nnoise and automatically determines the boundaries of P, QRS, and T waves. Our\nmodel has been trained on a diverse dataset and evaluated against the LUDB and\nQTDB datasets to show strong performance, with F1-scores exceeding 99% for QRS\nand T waves, and over 97% for P waves in the LUDB dataset. Furthermore, we\nassess various models across a wide array of arrhythmias and observe that\nmodels with a strong performance on standard benchmarks may still perform\npoorly on arrhythmias that are underrepresented in these benchmarks, such as\ntachycardias. We propose solutions to address this discrepancy.\n","authors":["Chankyu Joung","Mijin Kim","Taejin Paik","Seong-Ho Kong","Seung-Young Oh","Won Kyeong Jeon","Jae-hu Jeon","Joong-Sik Hong","Wan-Joong Kim","Woong Kook","Myung-Jin Cha","Otto van Koert"],"pdf_url":"https://arxiv.org/pdf/2304.06237v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03539v1","updated":"2024-08-07T04:35:38Z","published":"2024-08-07T04:35:38Z","title":"Deep Reinforcement Learning for Robotics: A Survey of Real-World\n  Successes","summary":"  Reinforcement learning (RL), particularly its combination with deep neural\nnetworks referred to as deep RL (DRL), has shown tremendous promise across a\nwide range of applications, suggesting its potential for enabling the\ndevelopment of sophisticated robotic behaviors. Robotics problems, however,\npose fundamental difficulties for the application of RL, stemming from the\ncomplexity and cost of interacting with the physical world. This article\nprovides a modern survey of DRL for robotics, with a particular focus on\nevaluating the real-world successes achieved with DRL in realizing several key\nrobotic competencies. Our analysis aims to identify the key factors underlying\nthose exciting successes, reveal underexplored areas, and provide an overall\ncharacterization of the status of DRL in robotics. We highlight several\nimportant avenues for future work, emphasizing the need for stable and\nsample-efficient real-world RL paradigms, holistic approaches for discovering\nand integrating various competencies to tackle complex long-horizon, open-world\ntasks, and principled development and evaluation procedures. This survey is\ndesigned to offer insights for both RL practitioners and roboticists toward\nharnessing RL's power to create generally capable real-world robotic systems.\n","authors":["Chen Tang","Ben Abbatematteo","Jiaheng Hu","Rohan Chandra","Roberto Martn-Martn","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2408.03539v1.pdf","comment":"The first three authors contributed equally. Accepted to Annual\n  Review of Control, Robotics, and Autonomous Systems"},{"id":"http://arxiv.org/abs/2408.03526v1","updated":"2024-08-07T03:37:25Z","published":"2024-08-07T03:37:25Z","title":"Minimum Enclosing Ball Synthetic Minority Oversampling Technique from a\n  Geometric Perspective","summary":"  Class imbalance refers to the significant difference in the number of samples\nfrom different classes within a dataset, making it challenging to identify\nminority class samples correctly. This issue is prevalent in real-world\nclassification tasks, such as software defect prediction, medical diagnosis,\nand fraud detection. The synthetic minority oversampling technique (SMOTE) is\nwidely used to address class imbalance issue, which is based on interpolation\nbetween randomly selected minority class samples and their neighbors. However,\ntraditional SMOTE and most of its variants only interpolate between existing\nsamples, which may be affected by noise samples in some cases and synthesize\nsamples that lack diversity. To overcome these shortcomings, this paper\nproposes the Minimum Enclosing Ball SMOTE (MEB-SMOTE) method from a geometry\nperspective. Specifically, MEB is innovatively introduced into the oversampling\nmethod to construct a representative point. Then, high-quality samples are\nsynthesized by interpolation between this representative point and the existing\nsamples. The rationale behind constructing a representative point is discussed,\ndemonstrating that the center of MEB is more suitable as the representative\npoint. To exhibit the superiority of MEB-SMOTE, experiments are conducted on 15\nreal-world imbalanced datasets. The results indicate that MEB-SMOTE can\neffectively improve the classification performance on imbalanced datasets.\n","authors":["Yi-Yang Shangguan","Shi-Shun Chen","Xiao-Yang Li"],"pdf_url":"https://arxiv.org/pdf/2408.03526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04360v2","updated":"2024-08-07T03:36:51Z","published":"2024-04-05T19:14:14Z","title":"Prompt Public Large Language Models to Synthesize Data for Private\n  On-device Applications","summary":"  Pre-training on public data is an effective method to improve the performance\nfor federated learning (FL) with differential privacy (DP). This paper\ninvestigates how large language models (LLMs) trained on public data can\nimprove the quality of pre-training data for the on-device language models\ntrained with DP and FL. We carefully design LLM prompts to filter and transform\nexisting public data, and generate new data to resemble the real user data\ndistribution. The model pre-trained on our synthetic dataset achieves relative\nimprovement of 19.0% and 22.8% in next word prediction accuracy compared to the\nbaseline model pre-trained on a standard public dataset, when evaluated over\nthe real user data in Gboard (Google Keyboard, a production mobile keyboard\napplication). Furthermore, our method achieves evaluation accuracy better than\nor comparable to the baseline during the DP FL fine-tuning over millions of\nmobile devices, and our final model outperforms the baseline in production A/B\ntesting. Our experiments demonstrate the strengths of LLMs in synthesizing data\nclose to the private distribution even without accessing the private data, and\nalso suggest future research directions to further reduce the distribution gap.\n","authors":["Shanshan Wu","Zheng Xu","Yanxiang Zhang","Yuanbo Zhang","Daniel Ramage"],"pdf_url":"https://arxiv.org/pdf/2404.04360v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2305.18403v5","updated":"2024-08-07T03:30:30Z","published":"2023-05-28T15:15:48Z","title":"LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient\n  Fine-Tuning","summary":"  Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional\nperformance across various tasks through fine-tuning. Although low-rank\nadaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream\ntasks, their deployment is still hindered by the vast model scale and\ncomputational costs. Post-training model pruning offers a way to compress LLMs.\nHowever, the current pruning methods designed for LLMs are not compatible with\nLoRA. This is due to their utilization of unstructured pruning on LLMs,\nimpeding the merging of LoRA weights, or their dependence on the gradients of\npre-trained weights to guide pruning, which can impose significant memory\noverhead. To this end, we propose LoRAPrune, a new framework that delivers an\naccurate structured pruned model in a highly memory-efficient manner.\nSpecifically, we first design a LoRA-guided pruning criterion, which uses the\nweights and gradients of LoRA, rather than the gradients of pre-trained weights\nfor importance estimation. We subsequently integrate this criterion into an\niterative pruning process, effectively removing redundant channels and heads.\nExtensive experimental results demonstrate the superior performance of our\nLoRAPrune over existing approaches on the LLaMA series models. At a 50\\%\ncompression rate, LoRAPrune demonstrates superior performance over LLM-Pruner,\nachieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while\nalso decreasing memory usage by 52.6%. Besides, LoRAPrune also matches\nsemi-structural pruning across multiple LLMs, proving its wide applicability.\nThe code is available at https://github.com/aim-uofa/LoRAPrune.\n","authors":["Mingyang Zhang","Hao Chen","Chunhua Shen","Zhen Yang","Linlin Ou","Xinyi Yu","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2305.18403v5.pdf","comment":"accepted by acl 2024 findings"},{"id":"http://arxiv.org/abs/2408.03516v1","updated":"2024-08-07T02:54:43Z","published":"2024-08-07T02:54:43Z","title":"Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in\n  Autonomous Driving","summary":"  This paper introduces a novel method for open-vocabulary 3D scene\nunderstanding in autonomous driving by combining Language Embedded 3D Gaussians\nwith Large Language Models (LLMs) for enhanced inference. We propose utilizing\nLLMs to generate contextually relevant canonical phrases for segmentation and\nscene interpretation. Our method leverages the contextual and semantic\ncapabilities of LLMs to produce a set of canonical phrases, which are then\ncompared with the language features embedded in the 3D Gaussians. This\nLLM-guided approach significantly improves zero-shot scene understanding and\ndetection of objects of interest, even in the most challenging or unfamiliar\nenvironments. Experimental results on the WayveScenes101 dataset demonstrate\nthat our approach surpasses state-of-the-art methods in terms of accuracy and\nflexibility for open-vocabulary object detection and segmentation. This work\nrepresents a significant advancement towards more intelligent, context-aware\nautonomous driving systems, effectively bridging 3D scene representation with\nhigh-level semantic understanding.\n","authors":["Amirhosein Chahe","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16087v4","updated":"2024-08-07T02:36:19Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03089v3","updated":"2024-08-07T02:06:13Z","published":"2024-07-03T13:26:31Z","title":"Spatio-Temporal Adaptive Diffusion Models for EEG Super-Resolution in\n  Epilepsy Diagnosis","summary":"  Electroencephalogram (EEG) technology, particularly high-density EEG (HD EEG)\ndevices, is widely used in fields such as neuroscience. HD EEG devices improve\nthe spatial resolution of EEG by placing more electrodes on the scalp, meeting\nthe requirements of clinical diagnostic applications such as epilepsy focus\nlocalization. However, this technique faces challenges such as high acquisition\ncosts and limited usage scenarios. In this paper, spatio-temporal adaptive\ndiffusion models (STADMs) are proposed to pioneer the use of diffusion models\nfor achieving spatial SR reconstruction from low-resolution (LR, 64 channels or\nfewer) EEG to high-resolution (HR, 256 channels) EEG. Specifically, a\nspatio-temporal condition module is designed to extract the spatio-temporal\nfeatures of LR EEG, which then serve as conditional inputs to guide the reverse\ndenoising process of diffusion models. Additionally, a multi-scale Transformer\ndenoising module is constructed to leverage multi-scale convolution blocks and\ncross-attention-based diffusion Transformer blocks for conditional guidance to\ngenerate subject-adaptive SR EEG. Experimental results demonstrate that the\nproposed method effectively enhances the spatial resolution of LR EEG and\nquantitatively outperforms existing methods. Furthermore, STADMs demonstrate\ntheir value by applying synthetic SR EEG to classification and source\nlocalization tasks of epilepsy patients, indicating their potential to\nsignificantly improve the spatial resolution of LR EEG.\n","authors":["Tong Zhou","Shuqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.03089v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09274v4","updated":"2024-08-07T01:46:11Z","published":"2024-01-17T15:25:50Z","title":"Avoiding strict saddle points of nonconvex regularized problems","summary":"  In this paper, we consider a class of non-convex and non-smooth sparse\noptimization problems, which encompass most existing nonconvex\nsparsity-inducing terms. We show the second-order optimality conditions only\ndepend on the nonzeros of the stationary points. We propose two damped\niterative reweighted algorithms including the iteratively reweighted $\\ell_1$\nalgorithm (DIRL$_1$) and the iteratively reweighted $\\ell_2$ (DIRL$_2$)\nalgorithm, to solve these problems. For DIRL$_1$, we show the reweighted\n$\\ell_1$ subproblem has support identification property so that DIRL$_1$\nlocally reverts to a gradient descent algorithm around a stationary point. For\nDIRL$_2$, we show the solution map of the reweighted $\\ell_2$ subproblem is\ndifferentiable and Lipschitz continuous everywhere. Therefore, the map of\nDIRL$_1$ and DIRL$_2$ and their inverse are Lipschitz continuous, and the\nstrict saddle points are their unstable fixed points. By applying the stable\nmanifold theorem, these algorithms are shown to converge only to local\nminimizers with randomly initialization when the strictly saddle point property\nis assumed.\n","authors":["Luwei Bai","Yaohua Hu","Hao Wang","Xiaoqi Yang"],"pdf_url":"https://arxiv.org/pdf/2401.09274v4.pdf","comment":"34 pages,4 figures"},{"id":"http://arxiv.org/abs/2408.03497v1","updated":"2024-08-07T01:37:10Z","published":"2024-08-07T01:37:10Z","title":"Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and\n  Tabnet with SMOTEENN","summary":"  Bank credit risk is a significant challenge in modern financial transactions,\nand the ability to identify qualified credit card holders among a large number\nof applicants is crucial for the profitability of a bank'sbank's credit card\nbusiness. In the past, screening applicants'applicants' conditions often\nrequired a significant amount of manual labor, which was time-consuming and\nlabor-intensive. Although the accuracy and reliability of previously used ML\nmodels have been continuously improving, the pursuit of more reliable and\npowerful AI intelligent models is undoubtedly the unremitting pursuit by major\nbanks in the financial industry. In this study, we used a dataset of over\n40,000 records provided by a commercial bank as the research object. We\ncompared various dimensionality reduction techniques such as PCA and T-SNE for\npreprocessing high-dimensional datasets and performed in-depth adaptation and\ntuning of distributed models such as LightGBM and XGBoost, as well as deep\nmodels like Tabnet. After a series of research and processing, we obtained\nexcellent research results by combining SMOTEENN with these techniques. The\nexperiments demonstrated that LightGBM combined with PCA and SMOTEENN\ntechniques can assist banks in accurately predicting potential high-quality\ncustomers, showing relatively outstanding performance compared to other models.\n","authors":["Chang Yu","Yixin Jin","Qianwen Xing","Ye Zhang","Shaobo Guo","Shuchen Meng"],"pdf_url":"https://arxiv.org/pdf/2408.03497v1.pdf","comment":"8 pagess on IEEE ICPICS"},{"id":"http://arxiv.org/abs/2407.02702v3","updated":"2024-08-07T01:35:58Z","published":"2024-07-02T22:51:01Z","title":"Practical Guide for Causal Pathways and Sub-group Disparity Analysis","summary":"  In this study, we introduce the application of causal disparity analysis to\nunveil intricate relationships and causal pathways between sensitive attributes\nand the targeted outcomes within real-world observational data. Our methodology\ninvolves employing causal decomposition analysis to quantify and examine the\ncausal interplay between sensitive attributes and outcomes. We also emphasize\nthe significance of integrating heterogeneity assessment in causal disparity\nanalysis to gain deeper insights into the impact of sensitive attributes within\nspecific sub-groups on outcomes. Our two-step investigation focuses on datasets\nwhere race serves as the sensitive attribute. The results on two datasets\nindicate the benefit of leveraging causal analysis and heterogeneity assessment\nnot only for quantifying biases in the data but also for disentangling their\ninfluences on outcomes. We demonstrate that the sub-groups identified by our\napproach to be affected the most by disparities are the ones with the largest\nML classification errors. We also show that grouping the data only based on a\nsensitive attribute is not enough, and through these analyses, we can find\nsub-groups that are directly affected by disparities. We hope that our findings\nwill encourage the adoption of such methodologies in future ethical AI\npractices and bias audits, fostering a more equitable and fair technological\nlandscape.\n","authors":["Farnaz Kohankhaki","Shaina Raza","Oluwanifemi Bamgbose","Deval Pandya","Elham Dolatabadi"],"pdf_url":"https://arxiv.org/pdf/2407.02702v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03490v1","updated":"2024-08-07T01:01:35Z","published":"2024-08-07T01:01:35Z","title":"Simultaneous and Meshfree Topology Optimization with Physics-informed\n  Gaussian Processes","summary":"  Topology optimization (TO) provides a principled mathematical approach for\noptimizing the performance of a structure by designing its material spatial\ndistribution in a pre-defined domain and subject to a set of constraints. The\nmajority of existing TO approaches leverage numerical solvers for design\nevaluations during the optimization and hence have a nested nature and rely on\ndiscretizing the design variables. Contrary to these approaches, herein we\ndevelop a new class of TO methods based on the framework of Gaussian processes\n(GPs) whose mean functions are parameterized via deep neural networks.\nSpecifically, we place GP priors on all design and state variables to represent\nthem via parameterized continuous functions. These GPs share a deep neural\nnetwork as their mean function but have as many independent kernels as there\nare state and design variables. We estimate all the parameters of our model in\na single for loop that optimizes a penalized version of the performance metric\nwhere the penalty terms correspond to the state equations and design\nconstraints. Attractive features of our approach include $(1)$ having a\nbuilt-in continuation nature since the performance metric is optimized at the\nsame time that the state equations are solved, and $(2)$ being\ndiscretization-invariant and accommodating complex domains and topologies. To\ntest our method against conventional TO approaches implemented in commercial\nsoftware, we evaluate it on four problems involving the minimization of\ndissipated power in Stokes flow. The results indicate that our approach does\nnot need filtering techniques, has consistent computational costs, and is\nhighly robust against random initializations and problem setup.\n","authors":["Amin Yousefpour","Shirin Hosseinmardi","Carlos Mora","Ramin Bostanabad"],"pdf_url":"https://arxiv.org/pdf/2408.03490v1.pdf","comment":null}]},"2024-08-08T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.19674v4","updated":"2024-08-08T02:39:15Z","published":"2024-07-29T03:30:09Z","title":"Advancing Prompt Learning through an External Layer","summary":"  Prompt learning represents a promising method for adapting pre-trained\nvision-language models (VLMs) to various downstream tasks by learning a set of\ntext embeddings. One challenge inherent to these methods is the poor\ngeneralization performance due to the invalidity of the learned text embeddings\nfor unseen tasks. A straightforward approach to bridge this gap is to freeze\nthe text embeddings in prompts, which results in a lack of capacity to adapt\nVLMs for downstream tasks. To address this dilemma, we propose a paradigm\ncalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose a\ntextual external layer and learnable visual embeddings for adapting VLMs to\ndownstream tasks. The learnable external layer is built upon valid embeddings\nof pre-trained CLIP. This design considers the balance of learning capabilities\nbetween the two branches. To align the textual and visual features, we propose\na novel two-pronged approach: i) we introduce the optimal transport as the\ndiscrepancy metric to align the vision and text modalities, and ii) we\nintroduce a novel strengthening feature to enhance the interaction between\nthese two modalities. Four representative experiments (i.e., base-to-novel\ngeneralization, few-shot learning, cross-dataset generalization, domain shifts\ngeneralization) across 15 datasets demonstrate that our method outperforms the\nexisting prompt learning method.\n","authors":["Fangming Cui","Xun Yang","Chao Wu","Liang Xiao","Xinmei Tian"],"pdf_url":"https://arxiv.org/pdf/2407.19674v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14153v3","updated":"2024-08-08T16:20:02Z","published":"2024-07-19T09:32:30Z","title":"ESP-MedSAM: Efficient Self-Prompting SAM for Universal\n  Domain-Generalized Image Segmentation","summary":"  The universality of deep neural networks across different modalities and\ntheir generalization capabilities to unseen domains play an essential role in\nmedical image segmentation. The recent Segment Anything Model (SAM) has\ndemonstrated its potential in both settings. However, the huge computational\ncosts, demand for manual annotations as prompts and conflict-prone decoding\nprocess of SAM degrade its generalizability and applicability in clinical\nscenarios. To address these issues, we propose an efficient self-prompting SAM\nfor universal domain-generalized medical image segmentation, named ESP-MedSAM.\nSpecifically, we first devise the Multi-Modal Decoupled Knowledge Distillation\n(MMDKD) strategy to construct a lightweight semi-parameter sharing image\nencoder that produces discriminative visual features for diverse modalities.\nFurther, we introduce the Self-Patch Prompt Generator (SPPG) to automatically\ngenerate high-quality dense prompt embeddings for guiding segmentation\ndecoding. Finally, we design the Query-Decoupled Modality Decoder (QDMD) that\nleverages a one-to-one strategy to provide an independent decoding channel for\nevery modality. Extensive experiments indicate that ESP-MedSAM outperforms\nstate-of-the-arts in diverse medical imaging segmentation tasks, displaying\nsuperior modality universality and generalization capabilities. Especially,\nESP-MedSAM uses only 4.5\\% parameters compared to SAM-H. The source code is\navailable at https://github.com/xq141839/ESP-MedSAM.\n","authors":["Qing Xu","Jiaxuan Li","Xiangjian He","Ziyu Liu","Zhen Chen","Wenting Duan","Chenxin Li","Maggie M. He","Fiseha B. Tesema","Wooi P. Cheah","Yi Wang","Rong Qu","Jonathan M. Garibaldi"],"pdf_url":"https://arxiv.org/pdf/2407.14153v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2405.06342v3","updated":"2024-08-08T03:25:02Z","published":"2024-05-10T09:18:17Z","title":"Compression-Realized Deep Structural Network for Video Quality\n  Enhancement","summary":"  This paper focuses on the task of quality enhancement for compressed videos.\nAlthough deep network-based video restorers achieve impressive progress, most\nof the existing methods lack a structured design to optimally leverage the\npriors within compression codecs. Since the quality degradation of the video is\nprimarily induced by the compression algorithm, a new paradigm is urgently\nneeded for a more ``conscious'' process of quality enhancement. As a result, we\npropose the Compression-Realized Deep Structural Network (CRDS), introducing\nthree inductive biases aligned with the three primary processes in the classic\ncompression codec, merging the strengths of classical encoder architecture with\ndeep network capabilities. Inspired by the residual extraction and domain\ntransformation process in the codec, a pre-trained Latent Degradation Residual\nAuto-Encoder is proposed to transform video frames into a latent feature space,\nand the mutual neighborhood attention mechanism is integrated for precise\nmotion estimation and residual extraction. Furthermore, drawing inspiration\nfrom the quantization noise distribution of the codec, CRDS proposes a novel\nProgressive Denoising framework with intermediate supervision that decomposes\nthe quality enhancement into a series of simpler denoising sub-tasks.\nExperimental results on datasets like LDV 2.0 and MFQE 2.0 indicate our\napproach surpasses state-of-the-art models.\n","authors":["Hanchi Sun","Xiaohong Liu","Xinyang Jiang","Yifei Shen","Dongsheng Li","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2405.06342v3.pdf","comment":"Accepted by ACM MM'24"},{"id":"http://arxiv.org/abs/2408.01669v3","updated":"2024-08-08T11:19:37Z","published":"2024-08-03T05:35:13Z","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","summary":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","authors":["Chaolei Tan","Zihang Lin","Junfu Pu","Zhongang Qi","Wei-Yi Pei","Zhi Qu","Yexin Wang","Ying Shan","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2408.01669v3.pdf","comment":"Accepted to ACM MM 2024. Project page: https://synopground.github.io/"},{"id":"http://arxiv.org/abs/2407.11652v6","updated":"2024-08-08T08:44:29Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v6.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.03361v2","updated":"2024-08-08T02:43:06Z","published":"2024-08-06T17:59:21Z","title":"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards\n  General Medical AI","summary":"  Large Vision-Language Models (LVLMs) are capable of handling diverse data\ntypes such as imaging, text, and physiological signals, and can be applied in\nvarious fields. In the medical field, LVLMs have a high potential to offer\nsubstantial assistance for diagnosis and treatment. Before that, it is crucial\nto develop benchmarks to evaluate LVLMs' effectiveness in various medical\napplications. Current benchmarks are often built upon specific academic\nliterature, mainly focusing on a single domain, and lacking varying perceptual\ngranularities. Thus, they face specific challenges, including limited clinical\nrelevance, incomplete evaluations, and insufficient guidance for interactive\nLVLMs. To address these limitations, we developed the GMAI-MMBench, the most\ncomprehensive general medical AI benchmark with well-categorized data structure\nand multi-perceptual granularity to date. It is constructed from 285 datasets\nacross 39 medical image modalities, 18 clinical-related tasks, 18 departments,\nand 4 perceptual granularities in a Visual Question Answering (VQA) format.\nAdditionally, we implemented a lexical tree structure that allows users to\ncustomize evaluation tasks, accommodating various assessment needs and\nsubstantially supporting medical AI research and applications. We evaluated 50\nLVLMs, and the results show that even the advanced GPT-4o only achieves an\naccuracy of 52%, indicating significant room for improvement. Moreover, we\nidentified five key insufficiencies in current cutting-edge LVLMs that need to\nbe addressed to advance the development of better medical applications. We\nbelieve that GMAI-MMBench will stimulate the community to build the next\ngeneration of LVLMs toward GMAI.\n  Project Page: https://uni-medical.github.io/GMAI-MMBench.github.io/\n","authors":["Pengcheng Chen","Jin Ye","Guoan Wang","Yanjun Li","Zhongying Deng","Wei Li","Tianbin Li","Haodong Duan","Ziyan Huang","Yanzhou Su","Benyou Wang","Shaoting Zhang","Bin Fu","Jianfei Cai","Bohan Zhuang","Eric J Seibel","Junjun He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.03361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17485v3","updated":"2024-08-08T03:48:38Z","published":"2024-02-27T13:10:11Z","title":"EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions","summary":"  In this work, we tackle the challenge of enhancing the realism and\nexpressiveness in talking head video generation by focusing on the dynamic and\nnuanced relationship between audio cues and facial movements. We identify the\nlimitations of traditional techniques that often fail to capture the full\nspectrum of human expressions and the uniqueness of individual facial styles.\nTo address these issues, we propose EMO, a novel framework that utilizes a\ndirect audio-to-video synthesis approach, bypassing the need for intermediate\n3D models or facial landmarks. Our method ensures seamless frame transitions\nand consistent identity preservation throughout the video, resulting in highly\nexpressive and lifelike animations. Experimental results demonsrate that EMO is\nable to produce not only convincing speaking videos but also singing videos in\nvarious styles, significantly outperforming existing state-of-the-art\nmethodologies in terms of expressiveness and realism.\n","authors":["Linrui Tian","Qi Wang","Bang Zhang","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2402.17485v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03030v2","updated":"2024-08-08T06:32:30Z","published":"2024-08-06T08:24:47Z","title":"Nighttime Pedestrian Detection Based on Fore-Background Contrast\n  Learning","summary":"  The significance of background information is frequently overlooked in\ncontemporary research concerning channel attention mechanisms. This study\naddresses the issue of suboptimal single-spectral nighttime pedestrian\ndetection performance under low-light conditions by incorporating background\ninformation into the channel attention mechanism. Despite numerous studies\nfocusing on the development of efficient channel attention mechanisms, the\nrelevance of background information has been largely disregarded. By adopting a\ncontrast learning approach, we reexamine channel attention with regard to\npedestrian objects and background information for nighttime pedestrian\ndetection, resulting in the proposed Fore-Background Contrast Attention (FBCA).\nFBCA possesses two primary attributes: (1) channel descriptors form remote\ndependencies with global spatial feature information; (2) the integration of\nbackground information enhances the distinction between channels concentrating\non low-light pedestrian features and those focusing on background information.\nConsequently, the acquired channel descriptors exhibit a higher semantic level\nand spatial accuracy. Experimental outcomes demonstrate that FBCA significantly\noutperforms existing methods in single-spectral nighttime pedestrian detection,\nachieving state-of-the-art results on the NightOwls and TJU-DHD-pedestrian\ndatasets. Furthermore, this methodology also yields performance improvements\nfor the multispectral LLVIP dataset. These findings indicate that integrating\nbackground information into the channel attention mechanism effectively\nmitigates detector performance degradation caused by illumination factors in\nnighttime scenarios.\n","authors":["He Yao","Yongjun Zhang","Huachun Jian","Li Zhang","Ruzhong Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.03030v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.03685v2","updated":"2024-08-08T13:52:44Z","published":"2024-08-07T10:53:07Z","title":"RL-ADN: A High-Performance Deep Reinforcement Learning Environment for\n  Optimal Energy Storage Systems Dispatch in Active Distribution Networks","summary":"  Deep Reinforcement Learning (DRL) presents a promising avenue for optimizing\nEnergy Storage Systems (ESSs) dispatch in distribution networks. This paper\nintroduces RL-ADN, an innovative open-source library specifically designed for\nsolving the optimal ESSs dispatch in active distribution networks. RL-ADN\noffers unparalleled flexibility in modeling distribution networks, and ESSs,\naccommodating a wide range of research goals. A standout feature of RL-ADN is\nits data augmentation module, based on Gaussian Mixture Model and Copula (GMC)\nfunctions, which elevates the performance ceiling of DRL agents. Additionally,\nRL-ADN incorporates the Laurent power flow solver, significantly reducing the\ncomputational burden of power flow calculations during training without\nsacrificing accuracy. The effectiveness of RL-ADN is demonstrated using in\ndifferent sizes of distribution networks, showing marked performance\nimprovements in the adaptability of DRL algorithms for ESS dispatch tasks. This\nenhancement is particularly beneficial from the increased diversity of training\nscenarios. Furthermore, RL-ADN achieves a tenfold increase in computational\nefficiency during training, making it highly suitable for large-scale network\napplications. The library sets a new benchmark in DRL-based ESSs dispatch in\ndistribution networks and it is poised to advance DRL applications in\ndistribution network operations significantly. RL-ADN is available at:\nhttps://github.com/ShengrenHou/RL-ADN and\nhttps://github.com/distributionnetworksTUDelft/RL-ADN.\n","authors":["Shengren Hou","Shuyi Gao","Weijie Xia","Edgar Mauricio Salazar Duque","Peter Palensky","Pedro P. Vergara"],"pdf_url":"https://arxiv.org/pdf/2408.03685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11652v6","updated":"2024-08-08T08:44:29Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v6.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.03508v2","updated":"2024-08-08T15:37:19Z","published":"2024-08-07T02:19:17Z","title":"Autonomous, Self-driving Multi-Step Growth of Semiconductor\n  Heterostructures Guided by Machine Learning","summary":"  The semiconductor industry has prioritized automating repetitive tasks by\nclosed-loop, autonomous experimentation which enables accelerated optimization\nof complex multi-step processes. The emergence of machine learning (ML) has\nushered in automated process with minimal human intervention. In this work, we\ndevelop SemiEpi, a self-driving automation platform capable of executing\nmolecular beam epitaxy (MBE) growth with multi-steps, continuous in-situ\nmonitoring, and on-the-fly feedback control. By integrating standard hardware,\nhomemade software, curve fitting, and multiple ML models, SemiEpi operates\nautonomously, eliminating the need for extensive expertise in MBE processes to\nachieve optimal outcomes. The platform actively learns from previous\nexperimental results, identifying favorable conditions and proposing new\nexperiments to achieve the desired results. We standardize and optimize growth\nfor InAs/GaAs quantum dots (QDs) heterostructures to showcase the power of\nML-guided multi-step growth. A temperature calibration was implemented to get\nthe initial growth condition, and fine control of the process was executed\nusing ML. Leveraging RHEED movies acquired during the growth, SemiEpi\nsuccessfully identified and optimized a novel route for multi-step\nheterostructure growth. This work demonstrates the capabilities of closed-loop,\nML-guided systems in addressing challenges in multi-step growth for any device.\nOur method is critical to achieve repeatable materials growth using\ncommercially scalable tools. Our strategy facilitates the development of a\nhardware-independent process and enhancing process repeatability and stability,\neven without exhaustive knowledge of growth parameters.\n","authors":["Chao Shen","Wenkang Zhan","Hongyu Sun","Kaiyao Xin","Bo Xu","Zhanguo Wang","Chao Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.03508v2.pdf","comment":"5 figures"}]}}